doc	unit1_toks	unit2_toks	unit1_txt	unit2_txt	u1_raw	u2_raw	s1_toks	s2_toks	unit1_sent	unit2_sent	dir	rel_type	orig_label	label
D14-1101	1-6	7-21	We propose a neural network approach	to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	We propose a neural network approach	to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	1-21	1-21	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	1<2	none	enablement	enablement
D14-1101	1-6	22-41	We propose a neural network approach	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	We propose a neural network approach	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	1-21	22-41	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	1<2	none	elab-aspect	elab-aspect
D14-1101	42-51	52-59	Since these statistics are encoded as dense continuous features ,	it is not trivial to combine these features	Since these statistics are encoded as dense continuous features ,	it is not trivial to combine these features	42-65	42-65	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	1>2	none	cause	cause
D14-1101	22-41	52-59	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	it is not trivial to combine these features	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	it is not trivial to combine these features	22-41	42-65	We investigated several types of corpus-wide information for the words , such as word embeddings and POS tag distributions .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	1<2	none	elab-addition	elab-addition
D14-1101	52-59	60-65	it is not trivial to combine these features	comparing with sparse discrete features .	it is not trivial to combine these features	comparing with sparse discrete features .	42-65	42-65	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	Since these statistics are encoded as dense continuous features , it is not trivial to combine these features comparing with sparse discrete features .	1<2	none	comparison	comparison
D14-1101	1-6	66-84	We propose a neural network approach	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network	We propose a neural network approach	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network	1-21	66-94	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	1<2	none	elab-aspect	elab-aspect
D14-1101	66-84	85-94	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network	that captures the non-linear interactions among the continuous features .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network	that captures the non-linear interactions among the continuous features .	66-94	66-94	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	Our tagger is designed as a combination of a linear model for discrete features and a feed-forward neural network that captures the non-linear interactions among the continuous features .	1<2	none	elab-addition	elab-addition
D14-1101	95-107	108-120	By using several recent advances in the activation functions for neural networks ,	the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	By using several recent advances in the activation functions for neural networks ,	the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	95-120	95-120	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	1>2	none	manner-means	manner-means
D14-1101	1-6	108-120	We propose a neural network approach	the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	We propose a neural network approach	the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	1-21	95-120	We propose a neural network approach to benefit from the non-linearity of corpus-wide statistics for part-of-speech ( POS ) tagging .	By using several recent advances in the activation functions for neural networks , the proposed method marks new state-of-the-art accuracies for English POS tagging tasks .	1<2	none	evaluation	evaluation
D14-1102	1-12	39-56	Different approaches to high-quality grammatical error correction have been proposed recently ,	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	Different approaches to high-quality grammatical error correction have been proposed recently ,	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	1-22	39-62	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	1>2	none	bg-compare	bg-compare
D14-1102	1-12	13-22	Different approaches to high-quality grammatical error correction have been proposed recently ,	many of which have their own strengths and weaknesses .	Different approaches to high-quality grammatical error correction have been proposed recently ,	many of which have their own strengths and weaknesses .	1-22	1-22	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	1<2	none	elab-example	elab-example
D14-1102	1-12	23-38	Different approaches to high-quality grammatical error correction have been proposed recently ,	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	Different approaches to high-quality grammatical error correction have been proposed recently ,	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	1-22	23-38	Different approaches to high-quality grammatical error correction have been proposed recently , many of which have their own strengths and weaknesses .	Most of these approaches are based on classification or statistical machine translation ( SMT ) .	1<2	none	elab-addition	elab-addition
D14-1102	39-56	57-62	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	to improve the correction quality .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	to improve the correction quality .	39-62	39-62	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	1<2	none	evaluation	evaluation
D14-1102	39-56	63-76	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	39-62	63-76	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	We adopt the system combination technique of Heafield and Lavie ( 2010 ) .	1<2	none	elab-aspect	elab-aspect
D14-1102	39-56	77-94	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task ,	In this paper , we propose to combine the output from a classification-based system and an SMT-based system	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task ,	39-62	77-103	In this paper , we propose to combine the output from a classification-based system and an SMT-based system to improve the correction quality .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	1<2	none	evaluation	evaluation
D14-1102	77-94	95-103	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task ,	outperforming the best system in the shared task .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task ,	outperforming the best system in the shared task .	77-103	77-103	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	We achieve an F0.5 score of 39.39 % on the test set of the CoNLL-2014 shared task , outperforming the best system in the shared task .	1<2	none	comparison	comparison
D14-1103	1-7	8-12	In this paper we propose a method	to increase dependency parser performance	In this paper we propose a method	to increase dependency parser performance	1-31	1-31	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	1<2	none	enablement	enablement
D14-1103	8-12	13-19	to increase dependency parser performance	without using additional labeled or unlabeled data	to increase dependency parser performance	without using additional labeled or unlabeled data	1-31	1-31	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	1<2	none	elab-addition	elab-addition
D14-1103	1-7	20-31	In this paper we propose a method	by refining the layer of predicted part-of-speech ( POS ) tags .	In this paper we propose a method	by refining the layer of predicted part-of-speech ( POS ) tags .	1-31	1-31	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	1<2	none	manner-means	manner-means
D14-1103	1-7	32-38	In this paper we propose a method	We perform experiments on English and German	In this paper we propose a method	We perform experiments on English and German	1-31	32-46	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	We perform experiments on English and German and show significant improvements for both languages .	1<2	none	evaluation	evaluation
D14-1103	32-38	39-46	We perform experiments on English and German	and show significant improvements for both languages .	We perform experiments on English and German	and show significant improvements for both languages .	32-46	32-46	We perform experiments on English and German and show significant improvements for both languages .	We perform experiments on English and German and show significant improvements for both languages .	1<2	none	joint	joint
D14-1103	20-31	47-62	by refining the layer of predicted part-of-speech ( POS ) tags .	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	by refining the layer of predicted part-of-speech ( POS ) tags .	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	1-31	47-62	In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech ( POS ) tags .	The refinement is based on generative split-merge training for Hidden Markov models ( HMMs ) .	1<2	none	elab-addition	elab-addition
D14-1104	1-12	63-76	Importance weighting is a generalization of various statistical bias correction techniques .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	Importance weighting is a generalization of various statistical bias correction techniques .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	1-12	63-76	Importance weighting is a generalization of various statistical bias correction techniques .	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	1>2	none	bg-goal	bg-goal
D14-1104	13-22	23-32	While our labeled data in NLP is heavily biased ,	importance weighting has seen only few applications in NLP ,	While our labeled data in NLP is heavily biased ,	importance weighting has seen only few applications in NLP ,	13-45	13-45	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	1>2	none	contrast	contrast
D14-1104	1-12	23-32	Importance weighting is a generalization of various statistical bias correction techniques .	importance weighting has seen only few applications in NLP ,	Importance weighting is a generalization of various statistical bias correction techniques .	importance weighting has seen only few applications in NLP ,	1-12	13-45	Importance weighting is a generalization of various statistical bias correction techniques .	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	1<2	none	elab-addition	elab-addition
D14-1104	23-32	33-45	importance weighting has seen only few applications in NLP ,	most of them relying on a small amount of labeled target data .	importance weighting has seen only few applications in NLP ,	most of them relying on a small amount of labeled target data .	13-45	13-45	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	1<2	none	elab-addition	elab-addition
D14-1104	13-22	46-48,53-62	While our labeled data in NLP is heavily biased ,	The publication bias <*> makes it hard to say whether researchers have tried .	While our labeled data in NLP is heavily biased ,	The publication bias <*> makes it hard to say whether researchers have tried .	13-45	46-62	While our labeled data in NLP is heavily biased , importance weighting has seen only few applications in NLP , most of them relying on a small amount of labeled target data .	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	1<2	none	elab-addition	elab-addition
D14-1104	46-48,53-62	49-52	The publication bias <*> makes it hard to say whether researchers have tried .	toward reporting positive results	The publication bias <*> makes it hard to say whether researchers have tried .	toward reporting positive results	46-62	46-62	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	The publication bias toward reporting positive results makes it hard to say whether researchers have tried .	1<2	none	elab-addition	elab-addition
D14-1104	63-76	77-85	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	In this setup , we only have unlabeled data	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	In this setup , we only have unlabeled data	63-76	77-99	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	1<2	none	elab-aspect	elab-aspect
D14-1104	77-85	86-99	In this setup , we only have unlabeled data	and thus only indirect access to the bias in emission and transition probabilities .	In this setup , we only have unlabeled data	and thus only indirect access to the bias in emission and transition probabilities .	77-99	77-99	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	1<2	none	joint	joint
D14-1104	77-85	100-112	In this setup , we only have unlabeled data	Moreover , most errors in POS tagging are due to unseen words ,	In this setup , we only have unlabeled data	Moreover , most errors in POS tagging are due to unseen words ,	77-99	100-120	In this setup , we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	1<2	none	progression	progression
D14-1104	100-112	113-120	Moreover , most errors in POS tagging are due to unseen words ,	and there , importance weighting cannot help .	Moreover , most errors in POS tagging are due to unseen words ,	and there , importance weighting cannot help .	100-120	100-120	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	Moreover , most errors in POS tagging are due to unseen words , and there , importance weighting cannot help .	1<2	none	joint	joint
D14-1104	63-76	121-141	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights ,	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights ,	63-76	121-146	This paper presents a negative result on unsupervised domain adaptation for POS tagging .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	1<2	none	evaluation	evaluation
D14-1104	121-141	142-146	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights ,	to support these claims .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights ,	to support these claims .	121-146	121-146	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	We present experiments with a wide variety of weight functions , quantilizations , as well as with randomly generated weights , to support these claims .	1<2	none	enablement	enablement
D14-1105	1-17	39-43	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	We describe our initial efforts	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	We describe our initial efforts	1-17	39-73	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	1>2	none	bg-goal	bg-goal
D14-1105	1-17	18-38	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	1-17	18-38	Code-mixing is frequently observed in user generated content on social media , especially from multilingual users .	The linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar .	1<2	none	elab-addition	elab-addition
D14-1105	39-43	44-53	We describe our initial efforts	to create a multi-level annotated corpus of Hindi-English code-mixed text	We describe our initial efforts	to create a multi-level annotated corpus of Hindi-English code-mixed text	39-73	39-73	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	1<2	none	enablement	enablement
D14-1105	44-53	54-58	to create a multi-level annotated corpus of Hindi-English code-mixed text	collated from Facebook forums ,	to create a multi-level annotated corpus of Hindi-English code-mixed text	collated from Facebook forums ,	39-73	39-73	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	1<2	none	elab-addition	elab-addition
D14-1105	44-53	59-73	to create a multi-level annotated corpus of Hindi-English code-mixed text	and explore language identification , back-transliteration , normalization and POS tagging of this data .	to create a multi-level annotated corpus of Hindi-English code-mixed text	and explore language identification , back-transliteration , normalization and POS tagging of this data .	39-73	39-73	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	1<2	none	joint	joint
D14-1105	39-43	74-76	We describe our initial efforts	Our results show	We describe our initial efforts	Our results show	39-73	74-93	We describe our initial efforts to create a multi-level annotated corpus of Hindi-English code-mixed text collated from Facebook forums , and explore language identification , back-transliteration , normalization and POS tagging of this data .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	1<2	none	evaluation	evaluation
D14-1105	74-76	77-87	Our results show	that language identification and transliteration for Hindi are two major challenges	Our results show	that language identification and transliteration for Hindi are two major challenges	74-93	74-93	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	1<2	none	attribution	attribution
D14-1105	77-87	88-93	that language identification and transliteration for Hindi are two major challenges	that impact POS tagging accuracy .	that language identification and transliteration for Hindi are two major challenges	that impact POS tagging accuracy .	74-93	74-93	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy .	1<2	none	elab-addition	elab-addition
D14-1106	1-9	10-14	We investigate grammatical error detection in spoken language ,	and present a data-driven method	We investigate grammatical error detection in spoken language ,	and present a data-driven method	1-27	1-27	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	1<2	none	joint	joint
D14-1106	10-14	15-19	and present a data-driven method	to train a dependency parser	and present a data-driven method	to train a dependency parser	1-27	1-27	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	1<2	none	enablement	enablement
D14-1106	15-19	20-27	to train a dependency parser	to automatically identify and label grammatical errors .	to train a dependency parser	to automatically identify and label grammatical errors .	1-27	1-27	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	1<2	none	enablement	enablement
D14-1106	10-14	28-37	and present a data-driven method	This method is agnostic to the label set used ,	and present a data-driven method	This method is agnostic to the label set used ,	1-27	28-50	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	1<2	none	elab-addition	elab-addition
D14-1106	28-37	38-42,46-50	This method is agnostic to the label set used ,	and the only manual annotations <*> are grammatical error labels .	This method is agnostic to the label set used ,	and the only manual annotations <*> are grammatical error labels .	28-50	28-50	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	1<2	none	joint	joint
D14-1106	38-42,46-50	43-45	and the only manual annotations <*> are grammatical error labels .	needed for training	and the only manual annotations <*> are grammatical error labels .	needed for training	28-50	28-50	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	This method is agnostic to the label set used , and the only manual annotations needed for training are grammatical error labels .	1<2	none	elab-addition	elab-addition
D14-1106	1-9	51-52	We investigate grammatical error detection in spoken language ,	We find	We investigate grammatical error detection in spoken language ,	We find	1-27	51-73	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	1<2	none	elab-aspect	elab-aspect
D14-1106	51-52	53-61	We find	that the proposed system is robust to disfluencies ,	We find	that the proposed system is robust to disfluencies ,	51-73	51-73	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	1<2	none	attribution	attribution
D14-1106	53-61	62-73	that the proposed system is robust to disfluencies ,	so that a separate stage to elide disfluencies is not required .	that the proposed system is robust to disfluencies ,	so that a separate stage to elide disfluencies is not required .	51-73	51-73	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	We find that the proposed system is robust to disfluencies , so that a separate stage to elide disfluencies is not required .	1<2	none	result	result
D14-1106	1-9	74-84	We investigate grammatical error detection in spoken language ,	The proposed system outperforms two baseline systems on two different corpora	We investigate grammatical error detection in spoken language ,	The proposed system outperforms two baseline systems on two different corpora	1-27	74-92	We investigate grammatical error detection in spoken language , and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors .	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	1<2	none	evaluation	evaluation
D14-1106	74-84	85-92	The proposed system outperforms two baseline systems on two different corpora	that use different sets of error tags .	The proposed system outperforms two baseline systems on two different corpora	that use different sets of error tags .	74-92	74-92	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	1<2	none	elab-addition	elab-addition
D14-1106	74-84	93-109	The proposed system outperforms two baseline systems on two different corpora	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 ,	The proposed system outperforms two baseline systems on two different corpora	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 ,	74-92	93-122	The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	1<2	none	elab-addition	elab-addition
D14-1106	93-109	110-122	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 ,	as compared to a baseline F1 of 0.350 on the same data .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 ,	as compared to a baseline F1 of 0.350 on the same data .	93-122	93-122	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	It is able to identify utterances with grammatical errors with an F1-score as high as 0.623 , as compared to a baseline F1 of 0.350 on the same data .	1<2	none	comparison	comparison
D14-1107	1-7	8-15	We introduce a new CCG parsing model	which is factored on lexical category assignments .	We introduce a new CCG parsing model	which is factored on lexical category assignments .	1-15	1-15	We introduce a new CCG parsing model which is factored on lexical category assignments .	We introduce a new CCG parsing model which is factored on lexical category assignments .	1<2	none	elab-addition	elab-addition
D14-1107	1-7	16-28	We introduce a new CCG parsing model	Parsing is then simply a deterministic search for the most probable category sequence	We introduce a new CCG parsing model	Parsing is then simply a deterministic search for the most probable category sequence	1-15	16-34	We introduce a new CCG parsing model which is factored on lexical category assignments .	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	1<2	none	elab-addition	elab-addition
D14-1107	16-28	29-34	Parsing is then simply a deterministic search for the most probable category sequence	that supports a CCG derivation .	Parsing is then simply a deterministic search for the most probable category sequence	that supports a CCG derivation .	16-34	16-34	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	Parsing is then simply a deterministic search for the most probable category sequence that supports a CCG derivation .	1<2	none	elab-addition	elab-addition
D14-1107	1-7	35-60	We introduce a new CCG parsing model	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	We introduce a new CCG parsing model	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	1-15	35-60	We introduce a new CCG parsing model which is factored on lexical category assignments .	The parser is extremely simple , with a tiny feature set , no POS tagger , and no statistical model of the derivation or dependencies .	1<2	none	elab-addition	elab-addition
D14-1107	1-7	61-74	We introduce a new CCG parsing model	Formulating the model in this way allows a highly effective heuristic for A∗parsing ,	We introduce a new CCG parsing model	Formulating the model in this way allows a highly effective heuristic for A∗parsing ,	1-15	61-80	We introduce a new CCG parsing model which is factored on lexical category assignments .	Formulating the model in this way allows a highly effective heuristic for A∗parsing , which makes parsing extremely fast .	1<2	none	elab-addition	elab-addition
D14-1107	61-74	75-80	Formulating the model in this way allows a highly effective heuristic for A∗parsing ,	which makes parsing extremely fast .	Formulating the model in this way allows a highly effective heuristic for A∗parsing ,	which makes parsing extremely fast .	61-80	61-80	Formulating the model in this way allows a highly effective heuristic for A∗parsing , which makes parsing extremely fast .	Formulating the model in this way allows a highly effective heuristic for A∗parsing , which makes parsing extremely fast .	1<2	none	elab-addition	elab-addition
D14-1107	81-88	89-95	Compared to the standard C&C CCG parser ,	our model is more accurate out-of-domain ,	Compared to the standard C&C CCG parser ,	our model is more accurate out-of-domain ,	81-109	81-109	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	1>2	none	comparison	comparison
D14-1107	1-7	89-95	We introduce a new CCG parsing model	our model is more accurate out-of-domain ,	We introduce a new CCG parsing model	our model is more accurate out-of-domain ,	1-15	81-109	We introduce a new CCG parsing model which is factored on lexical category assignments .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	1<2	none	evaluation	evaluation
D14-1107	89-95	96-100	our model is more accurate out-of-domain ,	is four times faster ,	our model is more accurate out-of-domain ,	is four times faster ,	81-109	81-109	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	1<2	none	joint	joint
D14-1107	96-100	101-104	is four times faster ,	has higher coverage ,	is four times faster ,	has higher coverage ,	81-109	81-109	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	1<2	none	joint	joint
D14-1107	101-104	105-109	has higher coverage ,	and is greatly simplified .	has higher coverage ,	and is greatly simplified .	81-109	81-109	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	Compared to the standard C&C CCG parser , our model is more accurate out-of-domain , is four times faster , has higher coverage , and is greatly simplified .	1<2	none	joint	joint
D14-1107	1-7	110-112	We introduce a new CCG parsing model	We also show	We introduce a new CCG parsing model	We also show	1-15	110-126	We introduce a new CCG parsing model which is factored on lexical category assignments .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	1<2	none	evaluation	evaluation
D14-1107	110-112	113-126	We also show	that using our parser improves the performance of a state-of-the-art question answering system .	We also show	that using our parser improves the performance of a state-of-the-art question answering system .	110-126	110-126	We also show that using our parser improves the performance of a state-of-the-art question answering system .	We also show that using our parser improves the performance of a state-of-the-art question answering system .	1<2	none	attribution	attribution
D14-1108	1-12	13-19	We describe a new dependency parser for English tweets , TWEEBOPARSER .	The parser builds on several contributions :	We describe a new dependency parser for English tweets , TWEEBOPARSER .	The parser builds on several contributions :	1-12	13-56	We describe a new dependency parser for English tweets , TWEEBOPARSER .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	1<2	none	elab-addition	elab-addition
D14-1108	13-19	20-33	The parser builds on several contributions :	new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions	The parser builds on several contributions :	new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions	13-56	13-56	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	1<2	none	elab-enumember	elab-enumember
D14-1108	20-33	34-38	new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions	informed by the domain ;	new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions	informed by the domain ;	13-56	13-56	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	1<2	none	elab-addition	elab-addition
D14-1108	13-19	39-45	The parser builds on several contributions :	adaptations to a statistical parsing algorithm ;	The parser builds on several contributions :	adaptations to a statistical parsing algorithm ;	13-56	13-56	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	1<2	none	elab-enumember	elab-enumember
D14-1108	39-45	46-56	adaptations to a statistical parsing algorithm ;	and a new approach to exploiting out-of-domain Penn Treebank data .	adaptations to a statistical parsing algorithm ;	and a new approach to exploiting out-of-domain Penn Treebank data .	13-56	13-56	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	The parser builds on several contributions : new syntactic annotations for a corpus of tweets ( TWEEBANK ) , with conventions informed by the domain ; adaptations to a statistical parsing algorithm ; and a new approach to exploiting out-of-domain Penn Treebank data .	1<2	none	joint	joint
D14-1108	1-12	57-59	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our experiments show	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our experiments show	1-12	57-84	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	1<2	none	evaluation	evaluation
D14-1108	57-59	60-76	Our experiments show	that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set	Our experiments show	that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set	57-84	57-84	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	1<2	none	attribution	attribution
D14-1108	57-59	77-84	Our experiments show	and measure the benefit of our contributions .	Our experiments show	and measure the benefit of our contributions .	57-84	57-84	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	Our experiments show that the parser achieves over 80 % unlabeled attachment accuracy on our new , high-quality test set and measure the benefit of our contributions .	1<2	none	joint	joint
D14-1108	1-12	85-96	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	1-12	85-96	We describe a new dependency parser for English tweets , TWEEBOPARSER .	Our dataset and parser can be found at http : //www.ark.cs.cmu.edu/TweetNLP .	1<2	none	elab-addition	elab-addition
D14-1109	1-13	31-40	Dependency parsing with high-order features results in a provably hard decoding problem .	In contrast , we explore , analyze , and demonstrate	Dependency parsing with high-order features results in a provably hard decoding problem .	In contrast , we explore , analyze , and demonstrate	1-13	31-131	Dependency parsing with high-order features results in a provably hard decoding problem .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1>2	none	bg-compare	bg-compare
D14-1109	1-13	14-24	Dependency parsing with high-order features results in a provably hard decoding problem .	A lot of work has gone into developing powerful optimization methods	Dependency parsing with high-order features results in a provably hard decoding problem .	A lot of work has gone into developing powerful optimization methods	1-13	14-30	Dependency parsing with high-order features results in a provably hard decoding problem .	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	1<2	none	elab-addition	elab-addition
D14-1109	14-24	25-30	A lot of work has gone into developing powerful optimization methods	for solving these combinatorial problems .	A lot of work has gone into developing powerful optimization methods	for solving these combinatorial problems .	14-30	14-30	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems .	1<2	none	elab-addition	elab-addition
D14-1109	31-40	41-55	In contrast , we explore , analyze , and demonstrate	that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing :	In contrast , we explore , analyze , and demonstrate	that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing :	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	attribution	attribution
D14-1109	31-40	56-64	In contrast , we explore , analyze , and demonstrate	a) we analytically quantify the number of local optima	In contrast , we explore , analyze , and demonstrate	a) we analytically quantify the number of local optima	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	elab-aspect	elab-aspect
D14-1109	56-64	65-78	a) we analytically quantify the number of local optima	that the greedy method has to overcome in the context of first-order parsing ;	a) we analytically quantify the number of local optima	that the greedy method has to overcome in the context of first-order parsing ;	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	elab-addition	elab-addition
D14-1109	31-40	79-81	In contrast , we explore , analyze , and demonstrate	b) we show	In contrast , we explore , analyze , and demonstrate	b) we show	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	elab-aspect	elab-aspect
D14-1109	79-81	82-98	b) we show	that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ;	b) we show	that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ;	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	attribution	attribution
D14-1109	31-40	99-102	In contrast , we explore , analyze , and demonstrate	c) we empirically demonstrate	In contrast , we explore , analyze , and demonstrate	c) we empirically demonstrate	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	evaluation	evaluation
D14-1109	99-102	103-121	c) we empirically demonstrate	that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods	c) we empirically demonstrate	that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	attribution	attribution
D14-1109	103-121	122-131	that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods	when evaluated on 14 languages of non-projective CoNLL datasets .	that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods	when evaluated on 14 languages of non-projective CoNLL datasets .	31-131	31-131	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	In contrast , we explore , analyze , and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing : a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing ; b) we show that , as a decoding algorithm , the greedy method surpasses dual decomposition in second-order parsing ; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	temporal	temporal
D14-1110	1-5	38-54	Most word representation methods assume	In this paper , we present a unified model for joint word sense representation and disambiguation ,	Most word representation methods assume	In this paper , we present a unified model for joint word sense representation and disambiguation ,	1-14	38-64	Most word representation methods assume that each word owns a single semantic vector .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	1>2	none	bg-compare	bg-compare
D14-1110	1-5	6-14	Most word representation methods assume	that each word owns a single semantic vector .	Most word representation methods assume	that each word owns a single semantic vector .	1-14	1-14	Most word representation methods assume that each word owns a single semantic vector .	Most word representation methods assume that each word owns a single semantic vector .	1<2	none	attribution	attribution
D14-1110	1-5	15-18	Most word representation methods assume	This is usually problematic	Most word representation methods assume	This is usually problematic	1-14	15-37	Most word representation methods assume that each word owns a single semantic vector .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	1<2	none	elab-addition	elab-addition
D14-1110	15-18	19-24	This is usually problematic	because lexical ambiguity is ubiquitous ,	This is usually problematic	because lexical ambiguity is ubiquitous ,	15-37	15-37	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	1<2	none	cause	cause
D14-1110	19-24	25-29	because lexical ambiguity is ubiquitous ,	which is also the problem	because lexical ambiguity is ubiquitous ,	which is also the problem	15-37	15-37	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	1<2	none	elab-addition	elab-addition
D14-1110	25-29	30-37	which is also the problem	to be resolved by word sense disambiguation .	which is also the problem	to be resolved by word sense disambiguation .	15-37	15-37	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	This is usually problematic because lexical ambiguity is ubiquitous , which is also the problem to be resolved by word sense disambiguation .	1<2	none	elab-addition	elab-addition
D14-1110	38-54	55-64	In this paper , we present a unified model for joint word sense representation and disambiguation ,	which will assign distinct representations for each word sense .	In this paper , we present a unified model for joint word sense representation and disambiguation ,	which will assign distinct representations for each word sense .	38-64	38-64	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	1<2	none	elab-addition	elab-addition
D14-1110	38-54	65-89	In this paper , we present a unified model for joint word sense representation and disambiguation ,	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other :	In this paper , we present a unified model for joint word sense representation and disambiguation ,	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other :	38-64	65-127	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	1<2	none	elab-addition	elab-addition
D14-1110	65-89	90-103	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other :	( 1 ) high-quality WSR will capture rich information about words and senses ,	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other :	( 1 ) high-quality WSR will capture rich information about words and senses ,	65-127	65-127	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	1<2	none	elab-enumember	elab-enumember
D14-1110	90-103	104-110	( 1 ) high-quality WSR will capture rich information about words and senses ,	which should be helpful for WSD ,	( 1 ) high-quality WSR will capture rich information about words and senses ,	which should be helpful for WSD ,	65-127	65-127	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	1<2	none	elab-addition	elab-addition
D14-1110	90-103	111-121	( 1 ) high-quality WSR will capture rich information about words and senses ,	and ( 2 ) high-quality WSD will provide reliable disambiguated corpora	( 1 ) high-quality WSR will capture rich information about words and senses ,	and ( 2 ) high-quality WSD will provide reliable disambiguated corpora	65-127	65-127	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	1<2	none	joint	joint
D14-1110	111-121	122-127	and ( 2 ) high-quality WSD will provide reliable disambiguated corpora	for learning better sense representations .	and ( 2 ) high-quality WSD will provide reliable disambiguated corpora	for learning better sense representations .	65-127	65-127	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	The basic idea is that both word sense representation ( WSR ) and word sense disambiguation ( WSD ) will benefit from each other : ( 1 ) high-quality WSR will capture rich information about words and senses , which should be helpful for WSD , and ( 2 ) high-quality WSD will provide reliable disambiguated corpora for learning better sense representations .	1<2	none	elab-addition	elab-addition
D14-1110	38-54	128-130	In this paper , we present a unified model for joint word sense representation and disambiguation ,	Experimental results show	In this paper , we present a unified model for joint word sense representation and disambiguation ,	Experimental results show	38-64	128-164	In this paper , we present a unified model for joint word sense representation and disambiguation , which will assign distinct representations for each word sense .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	1<2	none	evaluation	evaluation
D14-1110	128-130	131-141	Experimental results show	that , our model improves the performance of contextual word similarity	Experimental results show	that , our model improves the performance of contextual word similarity	128-164	128-164	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	1<2	none	attribution	attribution
D14-1110	131-141	142-147	that , our model improves the performance of contextual word similarity	compared to existing WSR methods ,	that , our model improves the performance of contextual word similarity	compared to existing WSR methods ,	128-164	128-164	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	1<2	none	comparison	comparison
D14-1110	131-141	148-155	that , our model improves the performance of contextual word similarity	outperforms state-of-the-art supervised methods on domain-specific WSD ,	that , our model improves the performance of contextual word similarity	outperforms state-of-the-art supervised methods on domain-specific WSD ,	128-164	128-164	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	1<2	none	elab-addition	elab-addition
D14-1110	148-155	156-164	outperforms state-of-the-art supervised methods on domain-specific WSD ,	and achieves competitive performance on coarse-grained all-words WSD .	outperforms state-of-the-art supervised methods on domain-specific WSD ,	and achieves competitive performance on coarse-grained all-words WSD .	128-164	128-164	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	Experimental results show that , our model improves the performance of contextual word similarity compared to existing WSR methods , outperforms state-of-the-art supervised methods on domain-specific WSD , and achieves competitive performance on coarse-grained all-words WSD .	1<2	none	joint	joint
D14-1111	1-9	22-31	Compositional distributional semantics is a subfield of Computational Linguistics	In this paper , we explore implementations of a framework	Compositional distributional semantics is a subfield of Computational Linguistics	In this paper , we explore implementations of a framework	1-21	22-62	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	1>2	none	bg-general	bg-general
D14-1111	1-9	10-12	Compositional distributional semantics is a subfield of Computational Linguistics	which investigates methods	Compositional distributional semantics is a subfield of Computational Linguistics	which investigates methods	1-21	1-21	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	1<2	none	elab-addition	elab-addition
D14-1111	10-12	13-21	which investigates methods	for representing the meanings of phrases and sentences .	which investigates methods	for representing the meanings of phrases and sentences .	1-21	1-21	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences .	1<2	none	elab-addition	elab-addition
D14-1111	22-31	32-40	In this paper , we explore implementations of a framework	based on Combinatory Categorial Grammar ( CCG ) ,	In this paper , we explore implementations of a framework	based on Combinatory Categorial Grammar ( CCG ) ,	22-62	22-62	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	1<2	none	bg-general	bg-general
D14-1111	32-40	41-49	based on Combinatory Categorial Grammar ( CCG ) ,	in which words with certain grammatical types have meanings	based on Combinatory Categorial Grammar ( CCG ) ,	in which words with certain grammatical types have meanings	22-62	22-62	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	1<2	none	elab-addition	elab-addition
D14-1111	41-49	50-53	in which words with certain grammatical types have meanings	represented by multi-linear maps	in which words with certain grammatical types have meanings	represented by multi-linear maps	22-62	22-62	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	1<2	none	elab-addition	elab-addition
D14-1111	50-53	54-62	represented by multi-linear maps	( i.e. multi-dimensional arrays , or tensors ) .	represented by multi-linear maps	( i.e. multi-dimensional arrays , or tensors ) .	22-62	22-62	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	1<2	none	elab-addition	elab-addition
D14-1111	22-31	63-77	In this paper , we explore implementations of a framework	An obstacle to full implemen-tation of the framework is the size of these tensors .	In this paper , we explore implementations of a framework	An obstacle to full implemen-tation of the framework is the size of these tensors .	22-62	63-77	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	An obstacle to full implemen-tation of the framework is the size of these tensors .	1<2	none	elab-addition	elab-addition
D14-1111	22-31	78-96	In this paper , we explore implementations of a framework	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	In this paper , we explore implementations of a framework	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	22-62	78-96	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausibility/selectional preference task .	1<2	none	elab-aspect	elab-aspect
D14-1111	22-31	97-98	In this paper , we explore implementations of a framework	We find	In this paper , we explore implementations of a framework	We find	22-62	97-129	In this paper , we explore implementations of a framework based on Combinatory Categorial Grammar ( CCG ) , in which words with certain grammatical types have meanings represented by multi-linear maps ( i.e. multi-dimensional arrays , or tensors ) .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	1<2	none	evaluation	evaluation
D14-1111	97-98	99-115	We find	that the matrices perform as well as , and sometimes even better than , full tensors ,	We find	that the matrices perform as well as , and sometimes even better than , full tensors ,	97-129	97-129	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	1<2	none	attribution	attribution
D14-1111	99-115	116-123	that the matrices perform as well as , and sometimes even better than , full tensors ,	allowing a reduction in the number of parameters	that the matrices perform as well as , and sometimes even better than , full tensors ,	allowing a reduction in the number of parameters	97-129	97-129	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	1<2	none	elab-addition	elab-addition
D14-1111	116-123	124-129	allowing a reduction in the number of parameters	needed to model the framework .	allowing a reduction in the number of parameters	needed to model the framework .	97-129	97-129	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	We find that the matrices perform as well as , and sometimes even better than , full tensors , allowing a reduction in the number of parameters needed to model the framework .	1<2	none	elab-addition	elab-addition
D14-1112	1-8	9-19	In this paper we propose a computational method	for determining the orthographic similarity between Romanian and related languages .	In this paper we propose a computational method	for determining the orthographic similarity between Romanian and related languages .	1-19	1-19	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	1<2	none	elab-addition	elab-addition
D14-1112	1-8	20-25	In this paper we propose a computational method	We account for etymons and cognates	In this paper we propose a computational method	We account for etymons and cognates	1-19	20-45	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	1<2	none	elab-aspect	elab-aspect
D14-1112	20-25	26-41	We account for etymons and cognates	and we investigate not only the number of related words , but also their forms ,	We account for etymons and cognates	and we investigate not only the number of related words , but also their forms ,	20-45	20-45	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	1<2	none	joint	joint
D14-1112	26-41	42-45	and we investigate not only the number of related words , but also their forms ,	quantifying orthographic similarities .	and we investigate not only the number of related words , but also their forms ,	quantifying orthographic similarities .	20-45	20-45	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	We account for etymons and cognates and we investigate not only the number of related words , but also their forms , quantifying orthographic similarities .	1<2	none	elab-addition	elab-addition
D14-1112	1-8	46-55	In this paper we propose a computational method	The method we propose is adaptable to any language ,	In this paper we propose a computational method	The method we propose is adaptable to any language ,	1-19	46-62	In this paper we propose a computational method for determining the orthographic similarity between Romanian and related languages .	The method we propose is adaptable to any language , as far as resources are available .	1<2	none	evaluation	evaluation
D14-1112	46-55	56-62	The method we propose is adaptable to any language ,	as far as resources are available .	The method we propose is adaptable to any language ,	as far as resources are available .	46-62	46-62	The method we propose is adaptable to any language , as far as resources are available .	The method we propose is adaptable to any language , as far as resources are available .	1<2	none	condition	condition
D14-1113	1-14	52-59	There is rising interest in vector-space word embeddings and their use in NLP ,	We present an extension to the Skip-gram model	There is rising interest in vector-space word embeddings and their use in NLP ,	We present an extension to the Skip-gram model	1-27	52-68	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	1>2	none	bg-compare	bg-compare
D14-1113	1-14	15-27	There is rising interest in vector-space word embeddings and their use in NLP ,	especially given recent methods for their fast estimation at very large scale .	There is rising interest in vector-space word embeddings and their use in NLP ,	especially given recent methods for their fast estimation at very large scale .	1-27	1-27	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	1<2	none	condition	condition
D14-1113	1-14	28-42	There is rising interest in vector-space word embeddings and their use in NLP ,	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy	There is rising interest in vector-space word embeddings and their use in NLP ,	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy	1-27	28-51	There is rising interest in vector-space word embeddings and their use in NLP , especially given recent methods for their fast estimation at very large scale .	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	1<2	none	contrast	contrast
D14-1113	28-42	43-51	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy	and thus jeopardizing their useful-ness for downstream tasks .	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy	and thus jeopardizing their useful-ness for downstream tasks .	28-51	28-51	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	Nearly all this work , however , assumes a single vector per word type—ignoring polysemy and thus jeopardizing their useful-ness for downstream tasks .	1<2	none	joint	joint
D14-1113	52-59	60-68	We present an extension to the Skip-gram model	that efficiently learns multiple embeddings per word type .	We present an extension to the Skip-gram model	that efficiently learns multiple embeddings per word type .	52-68	52-68	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	1<2	none	elab-addition	elab-addition
D14-1113	52-59	69-74	We present an extension to the Skip-gram model	It differs from recent related work	We present an extension to the Skip-gram model	It differs from recent related work	52-68	69-102	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	1<2	none	elab-addition	elab-addition
D14-1113	69-74	75-84	It differs from recent related work	by jointly performing word sense discrimination and embedding learning ,	It differs from recent related work	by jointly performing word sense discrimination and embedding learning ,	69-102	69-102	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	1<2	none	manner-means	manner-means
D14-1113	75-84	85-102	by jointly performing word sense discrimination and embedding learning ,	by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	by jointly performing word sense discrimination and embedding learning ,	by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	69-102	69-102	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	It differs from recent related work by jointly performing word sense discrimination and embedding learning , by non-parametrically estimating the number of senses per word type , and by its efficiency and scalability .	1<2	none	joint	joint
D14-1113	52-59	103-114	We present an extension to the Skip-gram model	We present new state-of-the-art results in the word similarity in context task	We present an extension to the Skip-gram model	We present new state-of-the-art results in the word similarity in context task	52-68	103-137	We present an extension to the Skip-gram model that efficiently learns multiple embeddings per word type .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	1<2	none	evaluation	evaluation
D14-1113	103-114	115-118	We present new state-of-the-art results in the word similarity in context task	and demonstrate its scalability	We present new state-of-the-art results in the word similarity in context task	and demonstrate its scalability	103-137	103-137	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	1<2	none	joint	joint
D14-1113	115-118	119-137	and demonstrate its scalability	by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	and demonstrate its scalability	by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	103-137	103-137	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	We present new state-of-the-art results in the word similarity in context task and demonstrate its scalability by training with one machine on a corpus of nearly 1 billion tokens in less than 6 hours .	1<2	none	manner-means	manner-means
D14-1114	1-5	48-59	Knowledge graphs are recently used	In this paper , we tackle the problem of intent understanding with	Knowledge graphs are recently used	In this paper , we tackle the problem of intent understanding with	1-23	48-93	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	1>2	none	bg-goal	bg-goal
D14-1114	1-5	6-17	Knowledge graphs are recently used	for enriching query representations in an entity-aware way for the rich facts	Knowledge graphs are recently used	for enriching query representations in an entity-aware way for the rich facts	1-23	1-23	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	1<2	none	elab-addition	elab-addition
D14-1114	6-17	18-23	for enriching query representations in an entity-aware way for the rich facts	organized around entities in it .	for enriching query representations in an entity-aware way for the rich facts	organized around entities in it .	1-23	1-23	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	1<2	none	elab-addition	elab-addition
D14-1114	1-5	24-34	Knowledge graphs are recently used	However , few of the methods pay attention to non-entity words	Knowledge graphs are recently used	However , few of the methods pay attention to non-entity words	1-23	24-47	Knowledge graphs are recently used for enriching query representations in an entity-aware way for the rich facts organized around entities in it .	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	1<2	none	contrast	contrast
D14-1114	24-34	35-40	However , few of the methods pay attention to non-entity words	and clicked websites in queries ,	However , few of the methods pay attention to non-entity words	and clicked websites in queries ,	24-47	24-47	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	1<2	none	joint	joint
D14-1114	35-40	41-47	and clicked websites in queries ,	which also help conveying user intent .	and clicked websites in queries ,	which also help conveying user intent .	24-47	24-47	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	However , few of the methods pay attention to non-entity words and clicked websites in queries , which also help conveying user intent .	1<2	none	elab-addition	elab-addition
D14-1114	48-59	60-82	In this paper , we tackle the problem of intent understanding with	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	In this paper , we tackle the problem of intent understanding with	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	48-93	48-93	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	1<2	none	manner-means	manner-means
D14-1114	60-82	83-88	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	to exploit and expand knowledge graph	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	to exploit and expand knowledge graph	48-93	48-93	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	1<2	none	enablement	enablement
D14-1114	60-82	89-93	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	which we call "tailor" .	innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way	which we call "tailor" .	48-93	48-93	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	1<2	none	elab-addition	elab-addition
D14-1114	48-59	94-107	In this paper , we tackle the problem of intent understanding with	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log	In this paper , we tackle the problem of intent understanding with	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log	48-93	94-129	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	1<2	none	elab-process_step	elab-process_step
D14-1114	94-107	108-112	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log	to initialize intent representation ,	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log	to initialize intent representation ,	94-129	94-129	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	1<2	none	enablement	enablement
D14-1114	48-59	113-120	In this paper , we tackle the problem of intent understanding with	then propagate the enriched features in a graph	In this paper , we tackle the problem of intent understanding with	then propagate the enriched features in a graph	48-93	94-129	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	1<2	none	elab-process_step	elab-process_step
D14-1114	113-120	121-124	then propagate the enriched features in a graph	consisting of intent topics	then propagate the enriched features in a graph	consisting of intent topics	94-129	94-129	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	1<2	none	elab-addition	elab-addition
D14-1114	113-120	125-129	then propagate the enriched features in a graph	using an unsupervised algorithm .	then propagate the enriched features in a graph	using an unsupervised algorithm .	94-129	94-129	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	We collaboratively exploit global knowledge in knowledge graphs and local contexts in query log to initialize intent representation , then propagate the enriched features in a graph consisting of intent topics using an unsupervised algorithm .	1<2	none	manner-means	manner-means
D14-1114	48-59	130-144	In this paper , we tackle the problem of intent understanding with	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	In this paper , we tackle the problem of intent understanding with	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	48-93	130-144	In this paper , we tackle the problem of intent understanding with innovatively representing entity words , refiners and clicked urls as intent topics in a unified knowledge graph based framework , in a way to exploit and expand knowledge graph which we call "tailor" .	The experiments prove intent topics with knowledge graph enriched features significantly enhance intent understanding .	1<2	none	evaluation	evaluation
D14-1115	1-30	31-51	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	1-30	31-51	The role of Web search queries has been demonstrated in the extraction of attributes of instances and classes , or of sets of related instances and their class labels .	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	1>2	none	bg-general	bg-general
D14-1115	52-60	61-78	Similarly to previous work in open-domain information extraction ,	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	Similarly to previous work in open-domain information extraction ,	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	52-83	52-83	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	1>2	none	comparison	comparison
D14-1115	31-51	61-78	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	31-51	52-83	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	1<2	none	elab-addition	elab-addition
D14-1115	61-78	79-83	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	associated with open-domain classes .	knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions	associated with open-domain classes .	52-83	52-83	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	Similarly to previous work in open-domain information extraction , knowledge extracted from text - in this case , from queries - takes the form of lexicalized assertions associated with open-domain classes .	1<2	none	elab-addition	elab-addition
D14-1115	31-51	84-86	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Experimental results indicate	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Experimental results indicate	31-51	84-110	This paper explores the acquisition of open-domain commonsense knowledge , usually available as factual knowledge , from Web search queries .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	1<2	none	evaluation	evaluation
D14-1115	84-86	87-88	Experimental results indicate	that facts	Experimental results indicate	that facts	84-110	84-110	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	1<2	none	attribution	attribution
D14-1115	87-88	89-93	that facts	extracted from queries complement ,	that facts	extracted from queries complement ,	84-110	84-110	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	1<2	none	elab-addition	elab-addition
D14-1115	89-93	94-102	extracted from queries complement ,	and have competitive accuracy levels relative to , facts	extracted from queries complement ,	and have competitive accuracy levels relative to , facts	84-110	84-110	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	1<2	none	joint	joint
D14-1115	94-102	103-110	and have competitive accuracy levels relative to , facts	extracted from Web documents by previous methods .	and have competitive accuracy levels relative to , facts	extracted from Web documents by previous methods .	84-110	84-110	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	Experimental results indicate that facts extracted from queries complement , and have competitive accuracy levels relative to , facts extracted from Web documents by previous methods .	1<2	none	elab-addition	elab-addition
D14-1116	1-19	60-68	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	To this end , we propose a novel method	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	To this end , we propose a novel method	1-36	60-72	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	To this end , we propose a novel method using first-order logic .	1>2	none	bg-compare	bg-compare
D14-1116	1-19	20-28,33-36	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	the key objective of which is to translate questions <*> into structured queries .	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	the key objective of which is to translate questions <*> into structured queries .	1-36	1-36	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	1<2	none	elab-addition	elab-addition
D14-1116	20-28,33-36	29-32	the key objective of which is to translate questions <*> into structured queries .	posed using natural language	the key objective of which is to translate questions <*> into structured queries .	posed using natural language	1-36	1-36	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	1<2	none	elab-addition	elab-addition
D14-1116	1-19	37-50	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	This technique can help common users to directly access open-structured knowledge on the Web	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data ,	This technique can help common users to directly access open-structured knowledge on the Web	1-36	37-59	Question Answering over Linked Data ( QALD ) aims to evaluate a question answering system over structured data , the key objective of which is to translate questions posed using natural language into structured queries .	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	1<2	none	elab-addition	elab-addition
D14-1116	37-50	51-59	This technique can help common users to directly access open-structured knowledge on the Web	and , accordingly , has attracted much attention .	This technique can help common users to directly access open-structured knowledge on the Web	and , accordingly , has attracted much attention .	37-59	37-59	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	This technique can help common users to directly access open-structured knowledge on the Web and , accordingly , has attracted much attention .	1<2	none	joint	joint
D14-1116	60-68	69-72	To this end , we propose a novel method	using first-order logic .	To this end , we propose a novel method	using first-order logic .	60-72	60-72	To this end , we propose a novel method using first-order logic .	To this end , we propose a novel method using first-order logic .	1<2	none	manner-means	manner-means
D14-1116	60-68	73-76,99-108	To this end , we propose a novel method	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	To this end , we propose a novel method	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	60-72	73-108	To this end , we propose a novel method using first-order logic .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	1<2	none	elab-aspect	elab-aspect
D14-1116	73-76,99-108	77-87	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	for resolving the ambiguities in the main three steps of QALD	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	for resolving the ambiguities in the main three steps of QALD	73-108	73-108	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	1<2	none	elab-addition	elab-addition
D14-1116	77-87	88-98	for resolving the ambiguities in the main three steps of QALD	( phrase detection , phrase-to-semantic-item mapping and semantic item grouping )	for resolving the ambiguities in the main three steps of QALD	( phrase detection , phrase-to-semantic-item mapping and semantic item grouping )	73-108	73-108	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	1<2	none	elab-enumember	elab-enumember
D14-1116	73-76,99-108	109-119	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	All clauses can then produce interacted effects in a unified framework	We formulate the knowledge <*> as first-order logic clauses in a Markov Logic Network .	All clauses can then produce interacted effects in a unified framework	73-108	109-126	We formulate the knowledge for resolving the ambiguities in the main three steps of QALD ( phrase detection , phrase-to-semantic-item mapping and semantic item grouping ) as first-order logic clauses in a Markov Logic Network .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	1<2	none	elab-addition	elab-addition
D14-1116	109-119	120-126	All clauses can then produce interacted effects in a unified framework	and can jointly resolve all ambiguities .	All clauses can then produce interacted effects in a unified framework	and can jointly resolve all ambiguities .	109-126	109-126	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	1<2	none	joint	joint
D14-1116	109-119	127-139	All clauses can then produce interacted effects in a unified framework	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	All clauses can then produce interacted effects in a unified framework	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	109-126	127-139	All clauses can then produce interacted effects in a unified framework and can jointly resolve all ambiguities .	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	1<2	none	progression	progression
D14-1116	127-139	140-150	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	In this way , our method can cover more text expressions	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	In this way , our method can cover more text expressions	127-139	140-162	Moreover , our method adopts a pattern-learning strategy for semantic item grouping .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	1<2	none	elab-addition	elab-addition
D14-1116	140-150	151-157	In this way , our method can cover more text expressions	and answer more questions than previous methods	In this way , our method can cover more text expressions	and answer more questions than previous methods	140-162	140-162	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	1<2	none	joint	joint
D14-1116	151-157	158-162	and answer more questions than previous methods	using manually designed patterns .	and answer more questions than previous methods	using manually designed patterns .	140-162	140-162	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	In this way , our method can cover more text expressions and answer more questions than previous methods using manually designed patterns .	1<2	none	manner-means	manner-means
D14-1116	60-68	163-165,169-176	To this end , we propose a novel method	The experimental results <*> demonstrate the effectiveness of the proposed method .	To this end , we propose a novel method	The experimental results <*> demonstrate the effectiveness of the proposed method .	60-72	163-176	To this end , we propose a novel method using first-order logic .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	1<2	none	evaluation	evaluation
D14-1116	163-165,169-176	166-168	The experimental results <*> demonstrate the effectiveness of the proposed method .	using open benchmarks	The experimental results <*> demonstrate the effectiveness of the proposed method .	using open benchmarks	163-176	163-176	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	The experimental results using open benchmarks demonstrate the effectiveness of the proposed method .	1<2	none	manner-means	manner-means
D14-1117	1-12	32-39	Much recent work focuses on formal interpretation of natural question utterances ,	Here we address two limitations of this approach	Much recent work focuses on formal interpretation of natural question utterances ,	Here we address two limitations of this approach	1-31	32-48	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	1>2	none	bg-compare	bg-compare
D14-1117	1-12	13-31	Much recent work focuses on formal interpretation of natural question utterances ,	with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Much recent work focuses on formal interpretation of natural question utterances ,	with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	1-31	1-31	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	Much recent work focuses on formal interpretation of natural question utterances , with the goal of executing the resulting structured queries on knowledge graphs ( KGs ) such as Freebase .	1<2	none	elab-addition	elab-addition
D14-1117	32-39	40-48	Here we address two limitations of this approach	when applied to open-domain , entity-oriented Web queries .	Here we address two limitations of this approach	when applied to open-domain , entity-oriented Web queries .	32-48	32-48	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	1<2	none	temporal	temporal
D14-1117	32-39	49-57	Here we address two limitations of this approach	First , Web queries are rarely well-formed questions .	Here we address two limitations of this approach	First , Web queries are rarely well-formed questions .	32-48	49-57	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	First , Web queries are rarely well-formed questions .	1<2	none	elab-enumember	elab-enumember
D14-1117	49-57	58-74	First , Web queries are rarely well-formed questions .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	First , Web queries are rarely well-formed questions .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	49-57	58-74	First , Web queries are rarely well-formed questions .	They are "telegraphic" , with missing verbs , prepositions , clauses , case and phrase clues .	1<2	none	elab-addition	elab-addition
D14-1117	32-39	75-82	Here we address two limitations of this approach	Second , the KG is always incomplete ,	Here we address two limitations of this approach	Second , the KG is always incomplete ,	32-48	75-89	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Second , the KG is always incomplete , unable to directly answer many queries .	1<2	none	elab-enumember	elab-enumember
D14-1117	75-82	83-89	Second , the KG is always incomplete ,	unable to directly answer many queries .	Second , the KG is always incomplete ,	unable to directly answer many queries .	75-89	75-89	Second , the KG is always incomplete , unable to directly answer many queries .	Second , the KG is always incomplete , unable to directly answer many queries .	1<2	none	elab-addition	elab-addition
D14-1117	32-39	90-94	Here we address two limitations of this approach	We propose a novel technique	Here we address two limitations of this approach	We propose a novel technique	32-48	90-129	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	1<2	none	elab-aspect	elab-aspect
D14-1117	90-94	95-99	We propose a novel technique	to segment a telegraphic query	We propose a novel technique	to segment a telegraphic query	90-129	90-129	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	1<2	none	enablement	enablement
D14-1117	95-99	100-108	to segment a telegraphic query	and assign a coarse-grained purpose to each segment :	to segment a telegraphic query	and assign a coarse-grained purpose to each segment :	90-129	90-129	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	1<2	none	joint	joint
D14-1117	100-108	109-129	and assign a coarse-grained purpose to each segment :	a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	and assign a coarse-grained purpose to each segment :	a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	90-129	90-129	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	1<2	none	elab-enumember	elab-enumember
D14-1117	109-129	130-136	a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	The query seeks entity e2 ∈ t2	a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	The query seeks entity e2 ∈ t2	90-129	130-152	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	The query seeks entity e2 ∈ t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	1<2	none	elab-addition	elab-addition
D14-1117	130-136	137-145	The query seeks entity e2 ∈ t2	where r ( e1 , e2 ) holds ,	The query seeks entity e2 ∈ t2	where r ( e1 , e2 ) holds ,	130-152	130-152	The query seeks entity e2 ∈ t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	The query seeks entity e2 ∈ t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	1<2	none	elab-addition	elab-addition
D14-1117	130-136	146-152	The query seeks entity e2 ∈ t2	further evidenced by schema-agnostic words s .	The query seeks entity e2 ∈ t2	further evidenced by schema-agnostic words s .	130-152	130-152	The query seeks entity e2 ∈ t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	The query seeks entity e2 ∈ t2 where r ( e1 , e2 ) holds , further evidenced by schema-agnostic words s .	1<2	none	elab-addition	elab-addition
D14-1117	90-94	153-163	We propose a novel technique	Query segmentation is integrated with the KG and an unstructured corpus	We propose a novel technique	Query segmentation is integrated with the KG and an unstructured corpus	90-129	153-174	We propose a novel technique to segment a telegraphic query and assign a coarse-grained purpose to each segment : a base entity e1 , a relation type r , a target entity type t2 , and contextual words s .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	1<2	none	elab-addition	elab-addition
D14-1117	153-163	164-174	Query segmentation is integrated with the KG and an unstructured corpus	where mentions of entities have been linked to the KG .	Query segmentation is integrated with the KG and an unstructured corpus	where mentions of entities have been linked to the KG .	153-174	153-174	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	Query segmentation is integrated with the KG and an unstructured corpus where mentions of entities have been linked to the KG .	1<2	none	elab-addition	elab-addition
D14-1117	32-39	175-186	Here we address two limitations of this approach	We do not trust the best or any specific query segmentation .	Here we address two limitations of this approach	We do not trust the best or any specific query segmentation .	32-48	175-186	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	We do not trust the best or any specific query segmentation .	1<2	none	elab-aspect	elab-aspect
D14-1117	175-186	187-200	We do not trust the best or any specific query segmentation .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	We do not trust the best or any specific query segmentation .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	175-186	187-200	We do not trust the best or any specific query segmentation .	Instead , evidence in favor of candidate e2s are aggregated across several segmentations .	1<2	none	contrast	contrast
D14-1117	32-39	201-214,230-236	Here we address two limitations of this approach	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	Here we address two limitations of this approach	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	32-48	201-236	Here we address two limitations of this approach when applied to open-domain , entity-oriented Web queries .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	1<2	none	evaluation	evaluation
D14-1117	201-214,230-236	215-220	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	using over a thousand telegraphic queries	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	using over a thousand telegraphic queries	201-236	201-236	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	1<2	none	manner-means	manner-means
D14-1117	215-220	221-229	using over a thousand telegraphic queries	adapted from TREC , INEX , and WebQuestions ,	using over a thousand telegraphic queries	adapted from TREC , INEX , and WebQuestions ,	201-236	201-236	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	1<2	none	elab-addition	elab-addition
D14-1117	201-214,230-236	237-255	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , <*> show the efficacy of our approach .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	201-236	237-255	Extensive experiments on the ClueWeb corpus and parts of Freebase as our KG , using over a thousand telegraphic queries adapted from TREC , INEX , and WebQuestions , show the efficacy of our approach .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	1<2	none	exp-evidence	exp-evidence
D14-1117	237-255	256-264	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	237-255	256-264	For one benchmark , MAP improves from 0.2-0.29 ( competitive baselines ) to 0.42 ( our system ) .	NDCG @ 10 improves from 0.29-0.36 to 0.54 .	1<2	none	contrast	contrast
D14-1118	1-17	69-80	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	In this paper , we propose a novel question difficulty estimation approach	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	In this paper , we propose a novel question difficulty estimation approach	1-17	69-102	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	1>2	none	bg-compare	bg-compare
D14-1118	1-17	18-29	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Previous studies propose to solve this problem based on the question-user comparisons	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Previous studies propose to solve this problem based on the question-user comparisons	1-17	18-36	Estimating questions' difficulty levels is an important task in community question answering ( CQA ) services .	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	1<2	none	elab-addition	elab-addition
D14-1118	18-29	30-36	Previous studies propose to solve this problem based on the question-user comparisons	extracted from the question answering threads .	Previous studies propose to solve this problem based on the question-user comparisons	extracted from the question answering threads .	18-36	18-36	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	1<2	none	elab-addition	elab-addition
D14-1118	18-29	37-44	Previous studies propose to solve this problem based on the question-user comparisons	However , they suffer from data sparseness problem	Previous studies propose to solve this problem based on the question-user comparisons	However , they suffer from data sparseness problem	18-36	37-55	Previous studies propose to solve this problem based on the question-user comparisons extracted from the question answering threads .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	1<2	none	contrast	contrast
D14-1118	37-44	45-55	However , they suffer from data sparseness problem	as each question only gets a limited number of comparisons .	However , they suffer from data sparseness problem	as each question only gets a limited number of comparisons .	37-55	37-55	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	1<2	none	cause	cause
D14-1118	37-44	56-63	However , they suffer from data sparseness problem	Moreover , they cannot handle newly posted questions	However , they suffer from data sparseness problem	Moreover , they cannot handle newly posted questions	37-55	56-68	However , they suffer from data sparseness problem as each question only gets a limited number of comparisons .	Moreover , they cannot handle newly posted questions which get no comparisons .	1<2	none	progression	progression
D14-1118	56-63	64-68	Moreover , they cannot handle newly posted questions	which get no comparisons .	Moreover , they cannot handle newly posted questions	which get no comparisons .	56-68	56-68	Moreover , they cannot handle newly posted questions which get no comparisons .	Moreover , they cannot handle newly posted questions which get no comparisons .	1<2	none	elab-addition	elab-addition
D14-1118	69-80	81-88	In this paper , we propose a novel question difficulty estimation approach	called Regularized Competition Model ( RCM ) ,	In this paper , we propose a novel question difficulty estimation approach	called Regularized Competition Model ( RCM ) ,	69-102	69-102	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	1<2	none	elab-addition	elab-addition
D14-1118	81-88	89-102	called Regularized Competition Model ( RCM ) ,	which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	called Regularized Competition Model ( RCM ) ,	which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	69-102	69-102	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	1<2	none	elab-addition	elab-addition
D14-1118	103-107	108-116	By incorporating textual information ,	RCM can effectively deal with data sparseness problem .	By incorporating textual information ,	RCM can effectively deal with data sparseness problem .	103-116	103-116	By incorporating textual information , RCM can effectively deal with data sparseness problem .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	1>2	none	manner-means	manner-means
D14-1118	81-88	108-116	called Regularized Competition Model ( RCM ) ,	RCM can effectively deal with data sparseness problem .	called Regularized Competition Model ( RCM ) ,	RCM can effectively deal with data sparseness problem .	69-102	103-116	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	By incorporating textual information , RCM can effectively deal with data sparseness problem .	1<2	none	elab-addition	elab-addition
D14-1118	69-80	117-123	In this paper , we propose a novel question difficulty estimation approach	We further employ a K-Nearest Neighbor approach	In this paper , we propose a novel question difficulty estimation approach	We further employ a K-Nearest Neighbor approach	69-102	117-138	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	1<2	none	elab-aspect	elab-aspect
D14-1118	117-123	124-132	We further employ a K-Nearest Neighbor approach	to estimate difficulty levels of newly posted questions ,	We further employ a K-Nearest Neighbor approach	to estimate difficulty levels of newly posted questions ,	117-138	117-138	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	1<2	none	enablement	enablement
D14-1118	124-132	133-138	to estimate difficulty levels of newly posted questions ,	again by leveraging textual similarities .	to estimate difficulty levels of newly posted questions ,	again by leveraging textual similarities .	117-138	117-138	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	We further employ a K-Nearest Neighbor approach to estimate difficulty levels of newly posted questions , again by leveraging textual similarities .	1<2	none	manner-means	manner-means
D14-1118	69-80	139-146	In this paper , we propose a novel question difficulty estimation approach	Experiments on two publicly available data sets show	In this paper , we propose a novel question difficulty estimation approach	Experiments on two publicly available data sets show	69-102	139-173	In this paper , we propose a novel question difficulty estimation approach called Regularized Competition Model ( RCM ) , which naturally combines question-user comparisons and questions' textual descriptions into a unified framework .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	1<2	none	evaluation	evaluation
D14-1118	139-146	147-165	Experiments on two publicly available data sets show	that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods ,	Experiments on two publicly available data sets show	that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods ,	139-173	139-173	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	1<2	none	attribution	attribution
D14-1118	147-165	166-173	that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods ,	demonstrating the advantage of incorporating textual information .	that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods ,	demonstrating the advantage of incorporating textual information .	139-173	139-173	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	1<2	none	elab-addition	elab-addition
D14-1118	139-146	174-178	Experiments on two publicly available data sets show	More interestingly , we observe	Experiments on two publicly available data sets show	More interestingly , we observe	139-173	174-194	Experiments on two publicly available data sets show that for both well-resolved and newly-posted questions , RCM performs the estimation task significantly better than existing methods , demonstrating the advantage of incorporating textual information .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	1<2	none	progression	progression
D14-1118	174-178	179-185	More interestingly , we observe	that RCM might provide an automatic way	More interestingly , we observe	that RCM might provide an automatic way	174-194	174-194	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	1<2	none	attribution	attribution
D14-1118	179-185	186-194	that RCM might provide an automatic way	to quantitatively measure the knowledge levels of words .	that RCM might provide an automatic way	to quantitatively measure the knowledge levels of words .	174-194	174-194	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	More interestingly , we observe that RCM might provide an automatic way to quantitatively measure the knowledge levels of words .	1<2	none	enablement	enablement
D14-1119	1-12	19-29	A poll consists of a question and a set of predefined answers	We present the new problem of vote prediction on comments ,	A poll consists of a question and a set of predefined answers	We present the new problem of vote prediction on comments ,	1-18	19-47	A poll consists of a question and a set of predefined answers from which voters can select .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	1>2	none	bg-general	bg-general
D14-1119	1-12	13-18	A poll consists of a question and a set of predefined answers	from which voters can select .	A poll consists of a question and a set of predefined answers	from which voters can select .	1-18	1-18	A poll consists of a question and a set of predefined answers from which voters can select .	A poll consists of a question and a set of predefined answers from which voters can select .	1<2	none	elab-addition	elab-addition
D14-1119	19-29	30-39	We present the new problem of vote prediction on comments ,	which involves determining which of these answers a voter selected	We present the new problem of vote prediction on comments ,	which involves determining which of these answers a voter selected	19-47	19-47	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	1<2	none	elab-addition	elab-addition
D14-1119	30-39	40-42	which involves determining which of these answers a voter selected	given a comment	which involves determining which of these answers a voter selected	given a comment	19-47	19-47	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	1<2	none	condition	condition
D14-1119	40-42	43-44	given a comment	she wrote	given a comment	she wrote	19-47	19-47	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	1<2	none	elab-addition	elab-addition
D14-1119	43-44	45-47	she wrote	after voting .	she wrote	after voting .	19-47	19-47	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	1<2	none	temporal	temporal
D14-1119	48-52	53-58	To address this task ,	we exploit not only the information	To address this task ,	we exploit not only the information	48-75	48-75	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	1>2	none	enablement	enablement
D14-1119	19-29	53-58	We present the new problem of vote prediction on comments ,	we exploit not only the information	We present the new problem of vote prediction on comments ,	we exploit not only the information	19-47	48-75	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	1<2	none	elab-aspect	elab-aspect
D14-1119	53-58	59-62	we exploit not only the information	extracted from the comments	we exploit not only the information	extracted from the comments	48-75	48-75	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	1<2	none	elab-addition	elab-addition
D14-1119	53-58	63-75	we exploit not only the information	but also extra-textual information such as user demographic information and inter-comment constraints .	we exploit not only the information	but also extra-textual information such as user demographic information and inter-comment constraints .	48-75	48-75	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	To address this task , we exploit not only the information extracted from the comments but also extra-textual information such as user demographic information and inter-comment constraints .	1<2	none	progression	progression
D14-1119	76-78	93-94	In an evaluation	we show	In an evaluation	we show	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1>2	none	condition	condition
D14-1119	76-78	79-83	In an evaluation	involving nearly one million comments	In an evaluation	involving nearly one million comments	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	elab-addition	elab-addition
D14-1119	79-83	84-92	involving nearly one million comments	collected from the popular SodaHead social polling website ,	involving nearly one million comments	collected from the popular SodaHead social polling website ,	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	elab-addition	elab-addition
D14-1119	19-29	93-94	We present the new problem of vote prediction on comments ,	we show	We present the new problem of vote prediction on comments ,	we show	19-47	76-114	We present the new problem of vote prediction on comments , which involves determining which of these answers a voter selected given a comment she wrote after voting .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	evaluation	evaluation
D14-1119	93-94	95-99,105-108	we show	that a vote prediction system <*> can be improved significantly	we show	that a vote prediction system <*> can be improved significantly	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	attribution	attribution
D14-1119	95-99,105-108	100-104	that a vote prediction system <*> can be improved significantly	that exploits only textual information	that a vote prediction system <*> can be improved significantly	that exploits only textual information	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	elab-addition	elab-addition
D14-1119	105-108	109-114	can be improved significantly	when extended with extra-textual information .	can be improved significantly	when extended with extra-textual information .	76-114	76-114	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	In an evaluation involving nearly one million comments collected from the popular SodaHead social polling website , we show that a vote prediction system that exploits only textual information can be improved significantly when extended with extra-textual information .	1<2	none	temporal	temporal
D14-1120	1-11	12-17	In this paper we first exploit cash-tags ( " $ "	followed by stocks' ticker symbols )	In this paper we first exploit cash-tags ( " $ "	followed by stocks' ticker symbols )	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	elab-addition	elab-addition
D14-1120	1-11	18-19	In this paper we first exploit cash-tags ( " $ "	in Twitter	In this paper we first exploit cash-tags ( " $ "	in Twitter	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	elab-addition	elab-addition
D14-1120	1-11	20-25	In this paper we first exploit cash-tags ( " $ "	to build a stock network ,	In this paper we first exploit cash-tags ( " $ "	to build a stock network ,	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	enablement	enablement
D14-1120	20-25	26-29	to build a stock network ,	where nodes are stocks	to build a stock network ,	where nodes are stocks	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	elab-addition	elab-addition
D14-1120	26-29	30-32	where nodes are stocks	connected by edges	where nodes are stocks	connected by edges	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	elab-addition	elab-addition
D14-1120	30-32	33-40	connected by edges	when two stocks co-occur frequently in tweets .	connected by edges	when two stocks co-occur frequently in tweets .	1-40	1-40	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	1<2	none	temporal	temporal
D14-1120	1-11	41-47	In this paper we first exploit cash-tags ( " $ "	We then employ a labeled topic model	In this paper we first exploit cash-tags ( " $ "	We then employ a labeled topic model	1-40	41-68	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	1<2	none	elab-process_step	elab-process_step
D14-1120	41-47	48-57	We then employ a labeled topic model	to jointly model both the tweets and the network structure	We then employ a labeled topic model	to jointly model both the tweets and the network structure	41-68	41-68	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	1<2	none	enablement	enablement
D14-1120	48-57	58-68	to jointly model both the tweets and the network structure	to assign each node and each edge a topic respectively .	to jointly model both the tweets and the network structure	to assign each node and each edge a topic respectively .	41-68	41-68	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	1<2	none	enablement	enablement
D14-1120	41-47	69-84	We then employ a labeled topic model	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	We then employ a labeled topic model	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	41-68	69-84	We then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively .	This Semantic Stock Network ( SSN ) summarizes discussion topics about stocks and stock relations .	1<2	none	elab-addition	elab-addition
D14-1120	1-11	85-87	In this paper we first exploit cash-tags ( " $ "	We further show	In this paper we first exploit cash-tags ( " $ "	We further show	1-40	85-110	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	1<2	none	evaluation	evaluation
D14-1120	85-87	88-110	We further show	that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	We further show	that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	85-110	85-110	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	1<2	none	attribution	attribution
D14-1120	85-87	111-127	We further show	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	We further show	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	85-110	111-127	We further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stock's market .	For prediction , we propose to regress the topic-sentiment time-series and the stock's price time series .	1<2	none	elab-addition	elab-addition
D14-1120	1-11	128-130	In this paper we first exploit cash-tags ( " $ "	Experimental results demonstrate	In this paper we first exploit cash-tags ( " $ "	Experimental results demonstrate	1-40	128-148	In this paper we first exploit cash-tags ( " $ " followed by stocks' ticker symbols ) in Twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	1<2	none	evaluation	evaluation
D14-1120	128-130	131-148	Experimental results demonstrate	that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	Experimental results demonstrate	that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	128-148	128-148	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	Experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .	1<2	none	attribution	attribution
D14-1121	1-17	18-21	Demographic lexica have potential for widespread use in social science , economic , and business applications .	We derive predictive lexica	Demographic lexica have potential for widespread use in social science , economic , and business applications .	We derive predictive lexica	1-17	18-51	Demographic lexica have potential for widespread use in social science , economic , and business applications .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	1>2	none	bg-goal	bg-goal
D14-1121	18-21	22-26	We derive predictive lexica	( words and weights )	We derive predictive lexica	( words and weights )	18-51	18-51	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	1<2	none	elab-addition	elab-addition
D14-1121	18-21	27-30	We derive predictive lexica	for age and gender	We derive predictive lexica	for age and gender	18-51	18-51	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	1<2	none	elab-addition	elab-addition
D14-1121	18-21	31-51	We derive predictive lexica	using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	We derive predictive lexica	using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	18-51	18-51	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	1<2	none	manner-means	manner-means
D14-1121	18-21	52-54,58-72	We derive predictive lexica	The lexica , <*> achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	We derive predictive lexica	The lexica , <*> achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	18-51	52-89	We derive predictive lexica ( words and weights ) for age and gender using regression and classification models from word usage in Facebook , blog , and Twitter data with associated demographic labels .	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	1<2	none	elab-addition	elab-addition
D14-1121	52-54,58-72	55-57	The lexica , <*> achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	made publicly available,	The lexica , <*> achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	made publicly available,	52-89	52-89	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	1<2	none	elab-addition	elab-addition
D14-1121	58-72	73-89	achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	and were evaluated for generalization across social media genres as well as in limited message situations .	achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter ,	and were evaluated for generalization across social media genres as well as in limited message situations .	52-89	52-89	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	The lexica , made publicly available, achieved state-of-the-art accuracy in language based age and gender prediction over Facebook and Twitter , and were evaluated for generalization across social media genres as well as in limited message situations .	1<2	none	joint	joint
D14-1122	1-9	65-79	Dependency parsing is a core task in NLP ,	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	Dependency parsing is a core task in NLP ,	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	1-29	65-87	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	1>2	none	bg-compare	bg-compare
D14-1122	1-9	10-29	Dependency parsing is a core task in NLP ,	and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	Dependency parsing is a core task in NLP ,	and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	1-29	1-29	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	1<2	none	joint	joint
D14-1122	1-9	30-42,48-55	Dependency parsing is a core task in NLP ,	In the era of social media , a big challenge is that parsers <*> typically suffer from the domain mismatch issue ,	Dependency parsing is a core task in NLP ,	In the era of social media , a big challenge is that parsers <*> typically suffer from the domain mismatch issue ,	1-29	30-64	Dependency parsing is a core task in NLP , and it is widely used by many applications such as information extraction , ques-tion answering , and machine translation .	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	1<2	none	elab-addition	elab-addition
D14-1122	30-42,48-55	43-47	In the era of social media , a big challenge is that parsers <*> typically suffer from the domain mismatch issue ,	trained on traditional newswire corpora	In the era of social media , a big challenge is that parsers <*> typically suffer from the domain mismatch issue ,	trained on traditional newswire corpora	30-64	30-64	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	1<2	none	elab-addition	elab-addition
D14-1122	48-55	56-64	typically suffer from the domain mismatch issue ,	and thus perform poorly on social media data .	typically suffer from the domain mismatch issue ,	and thus perform poorly on social media data .	30-64	30-64	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	In the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data .	1<2	none	result	result
D14-1122	65-79	80-87	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	( the Chinese equivalent of Twitter ) .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	( the Chinese equivalent of Twitter ) .	65-87	65-87	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	1<2	none	elab-addition	elab-addition
D14-1122	65-79	88-102	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks :	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks :	65-87	88-125	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	1<2	none	elab-aspect	elab-aspect
D14-1122	88-102	103-113	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks :	for each task , we use a programmable probabilistic first-order logic	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks :	for each task , we use a programmable probabilistic first-order logic	88-125	88-125	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	1<2	none	elab-addition	elab-addition
D14-1122	103-113	114-125	for each task , we use a programmable probabilistic first-order logic	to infer the dependency arc of a token in the sentence .	for each task , we use a programmable probabilistic first-order logic	to infer the dependency arc of a token in the sentence .	88-125	88-125	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	We formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic first-order logic to infer the dependency arc of a token in the sentence .	1<2	none	enablement	enablement
D14-1122	65-79	126-130	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	In experiments , we show	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo	In experiments , we show	65-87	126-157	We present a new GFL/FUDG-annotated Chinese treebank with more than 18K tokens from Sina Weibo ( the Chinese equivalent of Twitter ) .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	1<2	none	evaluation	evaluation
D14-1122	126-130	131-148	In experiments , we show	that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline	In experiments , we show	that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline	126-157	126-157	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	1<2	none	attribution	attribution
D14-1122	131-148	149-157	that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline	that is trained on the same in-domain data .	that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline	that is trained on the same in-domain data .	126-157	126-157	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	In experiments , we show that the proposed model outperforms an off-the-shelf Stanford Chinese parser , as well as a strong MaltParser baseline that is trained on the same in-domain data .	1<2	none	elab-addition	elab-addition
D14-1123	1-12	64-80	Microblog has become a major platform for information about real-world events .	In this study , we focus on the problem of community-related event detection by community emotions .	Microblog has become a major platform for information about real-world events .	In this study , we focus on the problem of community-related event detection by community emotions .	1-12	64-80	Microblog has become a major platform for information about real-world events .	In this study , we focus on the problem of community-related event detection by community emotions .	1>2	none	bg-compare	bg-compare
D14-1123	1-12	13-26	Microblog has become a major platform for information about real-world events .	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	Microblog has become a major platform for information about real-world events .	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	1-12	13-26	Microblog has become a major platform for information about real-world events .	Automatically discovering real-world events from microblog has attracted the attention of many researchers .	1<2	none	elab-addition	elab-addition
D14-1123	1-12	27-42	Microblog has become a major platform for information about real-world events .	However , most of existing work ignore the importance of emotion information for event detection .	Microblog has become a major platform for information about real-world events .	However , most of existing work ignore the importance of emotion information for event detection .	1-12	27-42	Microblog has become a major platform for information about real-world events .	However , most of existing work ignore the importance of emotion information for event detection .	1<2	none	contrast	contrast
D14-1123	1-12	43-44	Microblog has become a major platform for information about real-world events .	We argue	Microblog has become a major platform for information about real-world events .	We argue	1-12	43-63	Microblog has become a major platform for information about real-world events .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	1<2	none	elab-addition	elab-addition
D14-1123	43-44	45-55	We argue	that people's emotional reactions immediately reflect the occurring of real-world events	We argue	that people's emotional reactions immediately reflect the occurring of real-world events	43-63	43-63	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	1<2	none	attribution	attribution
D14-1123	45-55	56-63	that people's emotional reactions immediately reflect the occurring of real-world events	and should be important for event detection .	that people's emotional reactions immediately reflect the occurring of real-world events	and should be important for event detection .	43-63	43-63	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	We argue that people's emotional reactions immediately reflect the occurring of real-world events and should be important for event detection .	1<2	none	joint	joint
D14-1123	81-85	86-90	To address the problem ,	we propose a novel framework	To address the problem ,	we propose a novel framework	81-111	81-111	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	1>2	none	enablement	enablement
D14-1123	64-80	86-90	In this study , we focus on the problem of community-related event detection by community emotions .	we propose a novel framework	In this study , we focus on the problem of community-related event detection by community emotions .	we propose a novel framework	64-80	81-111	In this study , we focus on the problem of community-related event detection by community emotions .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	1<2	none	elab-aspect	elab-aspect
D14-1123	86-90	91-98	we propose a novel framework	which include the following three key components :	we propose a novel framework	which include the following three key components :	81-111	81-111	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	1<2	none	elab-addition	elab-addition
D14-1123	91-98	99-111	which include the following three key components :	microblog emotion classification , community emotion aggregation and community emotion burst detection .	which include the following three key components :	microblog emotion classification , community emotion aggregation and community emotion burst detection .	81-111	81-111	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	To address the problem , we propose a novel framework which include the following three key components : microblog emotion classification , community emotion aggregation and community emotion burst detection .	1<2	none	elab-enumember	elab-enumember
D14-1123	64-80	112-121	In this study , we focus on the problem of community-related event detection by community emotions .	We evaluate our approach on real microblog data sets .	In this study , we focus on the problem of community-related event detection by community emotions .	We evaluate our approach on real microblog data sets .	64-80	112-121	In this study , we focus on the problem of community-related event detection by community emotions .	We evaluate our approach on real microblog data sets .	1<2	none	evaluation	evaluation
D14-1123	112-121	122-131	We evaluate our approach on real microblog data sets .	Experimental results demonstrate the effectiveness of the proposed framework .	We evaluate our approach on real microblog data sets .	Experimental results demonstrate the effectiveness of the proposed framework .	112-121	122-131	We evaluate our approach on real microblog data sets .	Experimental results demonstrate the effectiveness of the proposed framework .	1<2	none	exp-evidence	exp-evidence
D14-1124	1-23	67-83	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	1-23	67-96	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	1>2	none	bg-goal	bg-goal
D14-1124	1-23	24-33	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Detecting disagreement in this domain is a considerable challenge .	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Detecting disagreement in this domain is a considerable challenge .	1-23	24-33	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Detecting disagreement in this domain is a considerable challenge .	1<2	none	elab-addition	elab-addition
D14-1124	1-23	34-44	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Many topics are unique to the conversation on the forum ,	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Many topics are unique to the conversation on the forum ,	1-23	34-66	Casual online forums such as Reddit , Slashdot and Digg , are continuing to increase in popularity as a means of communication .	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	1<2	none	elab-addition	elab-addition
D14-1124	34-44	45-66	Many topics are unique to the conversation on the forum ,	and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	Many topics are unique to the conversation on the forum ,	and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	34-66	34-66	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	Many topics are unique to the conversation on the forum , and the appearance of disagreement may be much more subtle than on political blogs or social media sites such as twitter .	1<2	none	joint	joint
D14-1124	67-83	84	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	showing	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	showing	67-96	67-96	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	1<2	none	elab-addition	elab-addition
D14-1124	84	85-96	showing	that disagreement detection in this domain is difficult even for humans .	showing	that disagreement detection in this domain is difficult even for humans .	67-96	67-96	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	1<2	none	attribution	attribution
D14-1124	67-83	97-101	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	We then proceed to show	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot ,	We then proceed to show	67-96	97-139	In this analysis we present a crowd-sourced annotated corpus for topic level disagreement detection in Slashdot , showing that disagreement detection in this domain is difficult even for humans .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	1<2	none	elab-process_step	elab-process_step
D14-1124	97-101	102-107,116-125	We then proceed to show	that a new set of features <*> significantly improves the performance on disagreement detection over a baseline	We then proceed to show	that a new set of features <*> significantly improves the performance on disagreement detection over a baseline	97-139	97-139	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	1<2	none	attribution	attribution
D14-1124	102-107,116-125	108-115	that a new set of features <*> significantly improves the performance on disagreement detection over a baseline	determined from the rhetorical structure of the conversation	that a new set of features <*> significantly improves the performance on disagreement detection over a baseline	determined from the rhetorical structure of the conversation	97-139	97-139	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	1<2	none	elab-addition	elab-addition
D14-1124	116-125	126-139	significantly improves the performance on disagreement detection over a baseline	consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	significantly improves the performance on disagreement detection over a baseline	consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	97-139	97-139	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	We then proceed to show that a new set of features determined from the rhetorical structure of the conversation significantly improves the performance on disagreement detection over a baseline consisting of unigram/bigram features , discourse markers , structural features and meta-post features .	1<2	none	elab-addition	elab-addition
D14-1125	1-13	35-38	Recently , work in NLP was initiated on a type of opinion inference	This paper addresses methods	Recently , work in NLP was initiated on a type of opinion inference	This paper addresses methods	1-34	35-54	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	1>2	none	bg-goal	bg-goal
D14-1125	1-13	14-15	Recently , work in NLP was initiated on a type of opinion inference	that arises	Recently , work in NLP was initiated on a type of opinion inference	that arises	1-34	1-34	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	1<2	none	elab-addition	elab-addition
D14-1125	14-15	16-21	that arises	when opinions are expressed toward events	that arises	when opinions are expressed toward events	1-34	1-34	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	1<2	none	temporal	temporal
D14-1125	16-21	22-29	when opinions are expressed toward events	which have positive or negative effects on entities	when opinions are expressed toward events	which have positive or negative effects on entities	1-34	1-34	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	1<2	none	elab-addition	elab-addition
D14-1125	22-29	30-34	which have positive or negative effects on entities	( +/-effect events ) .	which have positive or negative effects on entities	( +/-effect events ) .	1-34	1-34	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	Recently , work in NLP was initiated on a type of opinion inference that arises when opinions are expressed toward events which have positive or negative effects on entities ( +/-effect events ) .	1<2	none	elab-addition	elab-addition
D14-1125	35-38	39-46	This paper addresses methods	for creating a lexicon of such events ,	This paper addresses methods	for creating a lexicon of such events ,	35-54	35-54	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	1<2	none	elab-addition	elab-addition
D14-1125	35-38	47-54	This paper addresses methods	to support such work on opinion inference .	This paper addresses methods	to support such work on opinion inference .	35-54	35-54	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	1<2	none	enablement	enablement
D14-1125	35-38	55-60	This paper addresses methods	Due to significant sense ambiguity ,	This paper addresses methods	Due to significant sense ambiguity ,	35-54	55-72	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	1<2	none	elab-aspect	elab-aspect
D14-1125	55-60	61-72	Due to significant sense ambiguity ,	our goal is to develop a sense-level rather than word-level lexicon .	Due to significant sense ambiguity ,	our goal is to develop a sense-level rather than word-level lexicon .	55-72	55-72	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	Due to significant sense ambiguity , our goal is to develop a sense-level rather than word-level lexicon .	1<2	none	result	result
D14-1125	73-82	83-87	To maximize the effectiveness of different types of information ,	we combine a graph-based method	To maximize the effectiveness of different types of information ,	we combine a graph-based method	73-98	73-98	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	1>2	none	enablement	enablement
D14-1125	35-38	83-87	This paper addresses methods	we combine a graph-based method	This paper addresses methods	we combine a graph-based method	35-54	73-98	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	1<2	none	elab-aspect	elab-aspect
D14-1125	83-87	88-90	we combine a graph-based method	using WordNet1 relations	we combine a graph-based method	using WordNet1 relations	73-98	73-98	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	1<2	none	manner-means	manner-means
D14-1125	88-90	91-94	using WordNet1 relations	and a standard classifier	using WordNet1 relations	and a standard classifier	73-98	73-98	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	1<2	none	joint	joint
D14-1125	91-94	95-98	and a standard classifier	using gloss information .	and a standard classifier	using gloss information .	73-98	73-98	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	To maximize the effectiveness of different types of information , we combine a graph-based method using WordNet1 relations and a standard classifier using gloss information .	1<2	none	manner-means	manner-means
D14-1125	35-38	99-108	This paper addresses methods	A hybrid between the two gives the best results .	This paper addresses methods	A hybrid between the two gives the best results .	35-54	99-108	This paper addresses methods for creating a lexicon of such events , to support such work on opinion inference .	A hybrid between the two gives the best results .	1<2	none	evaluation	evaluation
D14-1125	99-108	109-113	A hybrid between the two gives the best results .	Further , we provide evidence	A hybrid between the two gives the best results .	Further , we provide evidence	99-108	109-136	A hybrid between the two gives the best results .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	1<2	none	progression	progression
D14-1125	109-113	114-120	Further , we provide evidence	that the model is an effective way	Further , we provide evidence	that the model is an effective way	109-136	109-136	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	1<2	none	elab-addition	elab-addition
D14-1125	114-120	121-124	that the model is an effective way	to guide manual annotation	that the model is an effective way	to guide manual annotation	109-136	109-136	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	1<2	none	enablement	enablement
D14-1125	121-124	125-128	to guide manual annotation	to find +/-effect senses	to guide manual annotation	to find +/-effect senses	109-136	109-136	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	1<2	none	enablement	enablement
D14-1125	125-128	129-136	to find +/-effect senses	that are not in the seed set .	to find +/-effect senses	that are not in the seed set .	109-136	109-136	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	Further , we provide evidence that the model is an effective way to guide manual annotation to find +/-effect senses that are not in the seed set .	1<2	none	elab-addition	elab-addition
D14-1126	1-10	11-24	Aspect-based opinion mining has attracted lots of attention today .	In this paper , we address the problem of product aspect rating prediction ,	Aspect-based opinion mining has attracted lots of attention today .	In this paper , we address the problem of product aspect rating prediction ,	1-10	11-40	Aspect-based opinion mining has attracted lots of attention today .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	1>2	none	elab-addition	elab-addition
D14-1126	11-24	107-116	In this paper , we address the problem of product aspect rating prediction ,	we propose a sentiment-aligned topic model ( SATM ) ,	In this paper , we address the problem of product aspect rating prediction ,	we propose a sentiment-aligned topic model ( SATM ) ,	11-40	102-134	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	1>2	none	bg-goal	bg-goal
D14-1126	11-24	25-34	In this paper , we address the problem of product aspect rating prediction ,	where we would like to extract the product aspects ,	In this paper , we address the problem of product aspect rating prediction ,	where we would like to extract the product aspects ,	11-40	11-40	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	1<2	none	elab-addition	elab-addition
D14-1126	25-34	35-40	where we would like to extract the product aspects ,	and predict aspect ratings simultaneously .	where we would like to extract the product aspects ,	and predict aspect ratings simultaneously .	11-40	11-40	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	1<2	none	joint	joint
D14-1126	11-24	41-46	In this paper , we address the problem of product aspect rating prediction ,	Topic models have been widely adapted	In this paper , we address the problem of product aspect rating prediction ,	Topic models have been widely adapted	11-40	41-71	In this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	1<2	none	elab-addition	elab-addition
D14-1126	41-46	47-53	Topic models have been widely adapted	to jointly model aspects and sentiments ,	Topic models have been widely adapted	to jointly model aspects and sentiments ,	41-71	41-71	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	1<2	none	enablement	enablement
D14-1126	41-46	54-63	Topic models have been widely adapted	but existing models may not do the prediction task well	Topic models have been widely adapted	but existing models may not do the prediction task well	41-71	41-71	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	1<2	none	comparison	comparison
D14-1126	54-63	64-71	but existing models may not do the prediction task well	due to their weakness in sentiment extraction .	but existing models may not do the prediction task well	due to their weakness in sentiment extraction .	41-71	41-71	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	1<2	none	cause	cause
D14-1126	64-71	72-85	due to their weakness in sentiment extraction .	The sentiment topics usually do not have clear correspondence to commonly used ratings ,	due to their weakness in sentiment extraction .	The sentiment topics usually do not have clear correspondence to commonly used ratings ,	41-71	72-101	Topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	1<2	none	elab-addition	elab-addition
D14-1126	72-85	86-96	The sentiment topics usually do not have clear correspondence to commonly used ratings ,	and the model may fail to extract certain kinds of sentiments	The sentiment topics usually do not have clear correspondence to commonly used ratings ,	and the model may fail to extract certain kinds of sentiments	72-101	72-101	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	1<2	none	joint	joint
D14-1126	86-96	97-101	and the model may fail to extract certain kinds of sentiments	due to skewed data .	and the model may fail to extract certain kinds of sentiments	due to skewed data .	72-101	72-101	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	The sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data .	1<2	none	cause	cause
D14-1126	102-106	107-116	To tackle this problem ,	we propose a sentiment-aligned topic model ( SATM ) ,	To tackle this problem ,	we propose a sentiment-aligned topic model ( SATM ) ,	102-134	102-134	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	1>2	none	enablement	enablement
D14-1126	107-116	117-125	we propose a sentiment-aligned topic model ( SATM ) ,	where we incorporate two types of external knowledge :	we propose a sentiment-aligned topic model ( SATM ) ,	where we incorporate two types of external knowledge :	102-134	102-134	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	1<2	none	elab-addition	elab-addition
D14-1126	117-125	126-134	where we incorporate two types of external knowledge :	product-level overall rating distribution and word-level sentiment lexicon .	where we incorporate two types of external knowledge :	product-level overall rating distribution and word-level sentiment lexicon .	102-134	102-134	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	1<2	none	elab-enumember	elab-enumember
D14-1126	107-116	135-139	we propose a sentiment-aligned topic model ( SATM ) ,	Experiments on real dataset demonstrate	we propose a sentiment-aligned topic model ( SATM ) ,	Experiments on real dataset demonstrate	102-134	135-160	To tackle this problem , we propose a sentiment-aligned topic model ( SATM ) , where we incorporate two types of external knowledge : product-level overall rating distribution and word-level sentiment lexicon .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	1<2	none	evaluation	evaluation
D14-1126	135-139	140-149	Experiments on real dataset demonstrate	that SATM is effective on product aspect rating prediction ,	Experiments on real dataset demonstrate	that SATM is effective on product aspect rating prediction ,	135-160	135-160	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	1<2	none	attribution	attribution
D14-1126	140-149	150-154	that SATM is effective on product aspect rating prediction ,	and it achieves better performance	that SATM is effective on product aspect rating prediction ,	and it achieves better performance	135-160	135-160	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	1<2	none	joint	joint
D14-1126	150-154	155-160	and it achieves better performance	compared to the existing approaches .	and it achieves better performance	compared to the existing approaches .	135-160	135-160	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	Experiments on real dataset demonstrate that SATM is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .	1<2	none	comparison	comparison
D14-1127	1-6	7-15	We present a weakly supervised approach	for learning hashtags , hashtag patterns , and phrases	We present a weakly supervised approach	for learning hashtags , hashtag patterns , and phrases	1-31	1-31	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	1<2	none	elab-addition	elab-addition
D14-1127	7-15	16-20	for learning hashtags , hashtag patterns , and phrases	associated with five emotions :	for learning hashtags , hashtag patterns , and phrases	associated with five emotions :	1-31	1-31	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	1<2	none	elab-addition	elab-addition
D14-1127	16-20	21-31	associated with five emotions :	AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	associated with five emotions :	AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	1-31	1-31	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	1<2	none	elab-enumember	elab-enumember
D14-1127	32-35	44-47	Starting with seed hashtags	we train emotion classifiers	Starting with seed hashtags	we train emotion classifiers	32-59	32-59	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	1>2	none	elab-addition	elab-addition
D14-1127	32-35	36-43	Starting with seed hashtags	to label an initial set of tweets ,	Starting with seed hashtags	to label an initial set of tweets ,	32-59	32-59	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	1<2	none	enablement	enablement
D14-1127	1-6	44-47	We present a weakly supervised approach	we train emotion classifiers	We present a weakly supervised approach	we train emotion classifiers	1-31	32-59	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	1<2	none	elab-process_step	elab-process_step
D14-1127	44-47	48-50	we train emotion classifiers	and use them	we train emotion classifiers	and use them	32-59	32-59	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	1<2	none	joint	joint
D14-1127	48-50	51-59	and use them	to learn new emotion hashtags and hashtag patterns .	and use them	to learn new emotion hashtags and hashtag patterns .	32-59	32-59	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	1<2	none	enablement	enablement
D14-1127	1-6	60-68	We present a weakly supervised approach	This process then repeats in a bootstrapping framework .	We present a weakly supervised approach	This process then repeats in a bootstrapping framework .	1-31	60-68	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	This process then repeats in a bootstrapping framework .	1<2	none	elab-process_step	elab-process_step
D14-1127	44-47	69-77	we train emotion classifiers	Emotion phrases are also extracted from the learned hashtags	we train emotion classifiers	Emotion phrases are also extracted from the learned hashtags	32-59	69-85	Starting with seed hashtags to label an initial set of tweets , we train emotion classifiers and use them to learn new emotion hashtags and hashtag patterns .	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	1<2	none	elab-addition	elab-addition
D14-1127	69-77	78-85	Emotion phrases are also extracted from the learned hashtags	and used to create phrase-based emotion classifiers .	Emotion phrases are also extracted from the learned hashtags	and used to create phrase-based emotion classifiers .	69-85	69-85	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	Emotion phrases are also extracted from the learned hashtags and used to create phrase-based emotion classifiers .	1<2	none	joint	joint
D14-1127	1-6	86-87	We present a weakly supervised approach	We show	We present a weakly supervised approach	We show	1-31	86-114	We present a weakly supervised approach for learning hashtags , hashtag patterns , and phrases associated with five emotions : AFFECTION , ANGER/RAGE , FEAR/ANXIETY , JOY , and SADNESS/DISAPPOINTMENT .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	1<2	none	evaluation	evaluation
D14-1127	86-87	88-101	We show	that the learned set of emotion indicators yields a substantial improve-ment in F-scores ,	We show	that the learned set of emotion indicators yields a substantial improve-ment in F-scores ,	86-114	86-114	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	1<2	none	attribution	attribution
D14-1127	88-101	102-114	that the learned set of emotion indicators yields a substantial improve-ment in F-scores ,	ranging from + % 5 to + % 18 over baseline classifiers .	that the learned set of emotion indicators yields a substantial improve-ment in F-scores ,	ranging from + % 5 to + % 18 over baseline classifiers .	86-114	86-114	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	We show that the learned set of emotion indicators yields a substantial improve-ment in F-scores , ranging from + % 5 to + % 18 over baseline classifiers .	1<2	none	elab-example	elab-example
D14-1128	1-5	6-12	We put forward the hypothesis	that high-accuracy sentiment analysis is only possible	We put forward the hypothesis	that high-accuracy sentiment analysis is only possible	1-22	1-22	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	1<2	none	elab-addition	elab-addition
D14-1128	6-12	13-22	that high-accuracy sentiment analysis is only possible	if word senses with different polarity are accurately recognized .	that high-accuracy sentiment analysis is only possible	if word senses with different polarity are accurately recognized .	1-22	1-22	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	1<2	none	condition	condition
D14-1128	1-5	23-36	We put forward the hypothesis	We provide evidence for this hypothesis in a case study for the adjective "hard"	We put forward the hypothesis	We provide evidence for this hypothesis in a case study for the adjective "hard"	1-22	23-52	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	1<2	none	elab-aspect	elab-aspect
D14-1128	23-36	37-42	We provide evidence for this hypothesis in a case study for the adjective "hard"	and propose contextually enhanced sentiment lexicons	We provide evidence for this hypothesis in a case study for the adjective "hard"	and propose contextually enhanced sentiment lexicons	23-52	23-52	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	1<2	none	joint	joint
D14-1128	37-42	43-52	and propose contextually enhanced sentiment lexicons	that contain the information necessary for sentiment-relevant sense disambiguation .	and propose contextually enhanced sentiment lexicons	that contain the information necessary for sentiment-relevant sense disambiguation .	23-52	23-52	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	We provide evidence for this hypothesis in a case study for the adjective "hard" and propose contextually enhanced sentiment lexicons that contain the information necessary for sentiment-relevant sense disambiguation .	1<2	none	elab-addition	elab-addition
D14-1128	1-5	53-56	We put forward the hypothesis	An experimental evaluation demonstrates	We put forward the hypothesis	An experimental evaluation demonstrates	1-22	53-74	We put forward the hypothesis that high-accuracy sentiment analysis is only possible if word senses with different polarity are accurately recognized .	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	1<2	none	evaluation	evaluation
D14-1128	53-56	57-65	An experimental evaluation demonstrates	that senses with different polarity can be distinguished well	An experimental evaluation demonstrates	that senses with different polarity can be distinguished well	53-74	53-74	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	1<2	none	attribution	attribution
D14-1128	57-65	66-74	that senses with different polarity can be distinguished well	using a combination of standard and novel features .	that senses with different polarity can be distinguished well	using a combination of standard and novel features .	53-74	53-74	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	An experimental evaluation demonstrates that senses with different polarity can be distinguished well using a combination of standard and novel features .	1<2	none	manner-means	manner-means
D14-1129	1-21	22-30	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	In this paper , we propose a link-based approach	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	In this paper , we propose a link-based approach	1-21	22-40	Identifying parallel web pages from bilingual web sites is a crucial step of bilingual resource construction for cross-lingual information processing .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	1>2	none	bg-general	bg-general
D14-1129	22-30	31-40	In this paper , we propose a link-based approach	to distinguish parallel web pages from bilingual web sites .	In this paper , we propose a link-based approach	to distinguish parallel web pages from bilingual web sites .	22-40	22-40	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	1<2	none	enablement	enablement
D14-1129	41-46	65-66	Compared with the existing methods ,	we hypothesize	Compared with the existing methods ,	we hypothesize	41-81	41-81	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1>2	none	comparison	comparison
D14-1129	41-46	47-53	Compared with the existing methods ,	which only employ the internal translation similarity	Compared with the existing methods ,	which only employ the internal translation similarity	41-81	41-81	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1<2	none	elab-addition	elab-addition
D14-1129	47-53	54-64	which only employ the internal translation similarity	( such as content-based similarity and page structural similarity ) ,	which only employ the internal translation similarity	( such as content-based similarity and page structural similarity ) ,	41-81	41-81	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1<2	none	elab-example	elab-example
D14-1129	22-30	65-66	In this paper , we propose a link-based approach	we hypothesize	In this paper , we propose a link-based approach	we hypothesize	22-40	41-81	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1<2	none	elab-aspect	elab-aspect
D14-1129	65-66	67-75	we hypothesize	that the external translation similarity is an effective feature	we hypothesize	that the external translation similarity is an effective feature	41-81	41-81	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1<2	none	attribution	attribution
D14-1129	67-75	76-81	that the external translation similarity is an effective feature	to identify parallel web pages .	that the external translation similarity is an effective feature	to identify parallel web pages .	41-81	41-81	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	1<2	none	enablement	enablement
D14-1129	82-94	95-116	Within a bilingual web site , web pages are interconnected by hyperlinks .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	Within a bilingual web site , web pages are interconnected by hyperlinks .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	82-94	95-128	Within a bilingual web site , web pages are interconnected by hyperlinks .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	1>2	none	elab-addition	elab-addition
D14-1129	65-66	95-116	we hypothesize	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	we hypothesize	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	41-81	95-128	Compared with the existing methods , which only employ the internal translation similarity ( such as content-based similarity and page structural similarity ) , we hypothesize that the external translation similarity is an effective feature to identify parallel web pages .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	1<2	none	elab-addition	elab-addition
D14-1129	95-116	117-128	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	which can be adopted as an important source of external similarity .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	which can be adopted as an important source of external similarity .	95-128	95-128	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	1<2	none	elab-addition	elab-addition
D14-1129	95-116	129-141	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	Thus , the translation similarity of page pairs will influence each other .	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages ,	Thus , the translation similarity of page pairs will influence each other .	95-128	129-141	The basic idea of our method is that the translation similarity of two pages can be inferred from their neighbor pages , which can be adopted as an important source of external similarity .	Thus , the translation similarity of page pairs will influence each other .	1<2	none	result	result
D14-1129	22-30	142-146	In this paper , we propose a link-based approach	An iterative algorithm is developed	In this paper , we propose a link-based approach	An iterative algorithm is developed	22-40	142-158	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	1<2	none	elab-aspect	elab-aspect
D14-1129	142-146	147-158	An iterative algorithm is developed	to estimate the external translation similarity and the final translation similarity .	An iterative algorithm is developed	to estimate the external translation similarity and the final translation similarity .	142-158	142-158	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	1<2	none	enablement	enablement
D14-1129	142-146	159-171	An iterative algorithm is developed	Both internal and external similarity measures are combined in the iterative algorithm .	An iterative algorithm is developed	Both internal and external similarity measures are combined in the iterative algorithm .	142-158	159-171	An iterative algorithm is developed to estimate the external translation similarity and the final translation similarity .	Both internal and external similarity measures are combined in the iterative algorithm .	1<2	none	elab-addition	elab-addition
D14-1129	22-30	172-177	In this paper , we propose a link-based approach	Experiments on six bilingual websites demonstrate	In this paper , we propose a link-based approach	Experiments on six bilingual websites demonstrate	22-40	172-201	In this paper , we propose a link-based approach to distinguish parallel web pages from bilingual web sites .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	1<2	none	evaluation	evaluation
D14-1129	172-177	178-182	Experiments on six bilingual websites demonstrate	that our method is effective	Experiments on six bilingual websites demonstrate	that our method is effective	172-201	172-201	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	1<2	none	attribution	attribution
D14-1129	178-182	183-194	that our method is effective	and obtains significant improvement ( 6.2 % F-Score ) over the baseline	that our method is effective	and obtains significant improvement ( 6.2 % F-Score ) over the baseline	172-201	172-201	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	1<2	none	joint	joint
D14-1129	183-194	195-201	and obtains significant improvement ( 6.2 % F-Score ) over the baseline	which only utilizes internal translation similarity .	and obtains significant improvement ( 6.2 % F-Score ) over the baseline	which only utilizes internal translation similarity .	172-201	172-201	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	Experiments on six bilingual websites demonstrate that our method is effective and obtains significant improvement ( 6.2 % F-Score ) over the baseline which only utilizes internal translation similarity .	1<2	none	elab-addition	elab-addition
D14-1130	1-24	43-53	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	We present the first holistic , quantitative evaluation of these issues	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	We present the first holistic , quantitative evaluation of these issues	1-24	43-68	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	1>2	none	bg-goal	bg-goal
D14-1130	1-24	25-32	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	However , this distinction is artificial in practice	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	However , this distinction is artificial in practice	1-24	25-42	Analyses of computer aided translation typically focus on either frontend interfaces and human effort , or backend translation and machine learnability of corrections .	However , this distinction is artificial in practice since the frontend and backend must work in concert .	1<2	none	contrast	contrast
D14-1130	25-32	33-42	However , this distinction is artificial in practice	since the frontend and backend must work in concert .	However , this distinction is artificial in practice	since the frontend and backend must work in concert .	25-42	25-42	However , this distinction is artificial in practice since the frontend and backend must work in concert .	However , this distinction is artificial in practice since the frontend and backend must work in concert .	1<2	none	cause	cause
D14-1130	43-53	54-59	We present the first holistic , quantitative evaluation of these issues	by contrasting two assistive modes :	We present the first holistic , quantitative evaluation of these issues	by contrasting two assistive modes :	43-68	43-68	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	1<2	none	manner-means	manner-means
D14-1130	54-59	60-68	by contrasting two assistive modes :	post-editing and interactive machine translation ( MT ) .	by contrasting two assistive modes :	post-editing and interactive machine translation ( MT ) .	43-68	43-68	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	1<2	none	elab-enumember	elab-enumember
D14-1130	43-53	69-88	We present the first holistic , quantitative evaluation of these issues	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function	We present the first holistic , quantitative evaluation of these issues	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function	43-68	69-94	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	1<2	none	elab-aspect	elab-aspect
D14-1130	69-88	89-94	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function	for re-tuning to human corrections .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function	for re-tuning to human corrections .	69-94	69-94	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	We describe a new translator interface , extensive modifications to a phrase-based MT system , and a novel objective function for re-tuning to human corrections .	1<2	none	elab-addition	elab-addition
D14-1130	43-53	95-100	We present the first holistic , quantitative evaluation of these issues	Evaluation with professional bilingual translators shows	We present the first holistic , quantitative evaluation of these issues	Evaluation with professional bilingual translators shows	43-68	95-117	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	1<2	none	evaluation	evaluation
D14-1130	95-100	101-117	Evaluation with professional bilingual translators shows	that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	Evaluation with professional bilingual translators shows	that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	95-117	95-117	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	1<2	none	attribution	attribution
D14-1130	95-100	118-135	Evaluation with professional bilingual translators shows	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER	Evaluation with professional bilingual translators shows	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER	95-117	118-140	Evaluation with professional bilingual translators shows that post-edit is faster than interactive at the cost of translation quality for French-English and English-German .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	1<2	none	contrast	contrast
D14-1130	118-135	136-140	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER	versus re-tuning to post-edit .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER	versus re-tuning to post-edit .	118-140	118-140	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	However , re-tuning the MT system to interactive output leads to larger , statistically significant reductions in HTER versus re-tuning to post-edit .	1<2	none	contrast	contrast
D14-1130	43-53	141-142	We present the first holistic , quantitative evaluation of these issues	Analysis shows	We present the first holistic , quantitative evaluation of these issues	Analysis shows	43-68	141-156	We present the first holistic , quantitative evaluation of these issues by contrasting two assistive modes : post-editing and interactive machine translation ( MT ) .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	1<2	none	evaluation	evaluation
D14-1130	141-142	143-156	Analysis shows	that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	Analysis shows	that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	141-156	141-156	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	Analysis shows that tuning directly to HTER results in fine-grained corrections to subsequent machine output .	1<2	none	attribution	attribution
D14-1131	1-26	27-35	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	We replace this in-tractable intersection by a tractable relaxation	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	We replace this in-tractable intersection by a tractable relaxation	1-26	27-45	The combinatorial space of translation derivations in phrase-based statistical machine translation is given by the intersection between a translation lattice and a target language model .	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	1>2	none	bg-general	bg-general
D14-1131	27-35	36-45	We replace this in-tractable intersection by a tractable relaxation	which incorporates a low-order upperbound on the language model .	We replace this in-tractable intersection by a tractable relaxation	which incorporates a low-order upperbound on the language model .	27-45	27-45	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	1<2	none	elab-addition	elab-addition
D14-1131	46-60	61-73	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	We perform exact optimisation with unpruned language models of order 3 to 5	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	We perform exact optimisation with unpruned language models of order 3 to 5	46-60	61-88	Exact optimisation is achieved through a coarse-to-fine strategy with connections to adaptive rejection sampling .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	1>2	none	elab-addition	elab-addition
D14-1131	27-35	61-73	We replace this in-tractable intersection by a tractable relaxation	We perform exact optimisation with unpruned language models of order 3 to 5	We replace this in-tractable intersection by a tractable relaxation	We perform exact optimisation with unpruned language models of order 3 to 5	27-45	61-88	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	1<2	none	elab-aspect	elab-aspect
D14-1131	61-73	74-88	We perform exact optimisation with unpruned language models of order 3 to 5	and show search-error curves for beam search and cube pruning on standard test sets .	We perform exact optimisation with unpruned language models of order 3 to 5	and show search-error curves for beam search and cube pruning on standard test sets .	61-88	61-88	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	We perform exact optimisation with unpruned language models of order 3 to 5 and show search-error curves for beam search and cube pruning on standard test sets .	1<2	none	joint	joint
D14-1131	27-35	89-93	We replace this in-tractable intersection by a tractable relaxation	This is the first work	We replace this in-tractable intersection by a tractable relaxation	This is the first work	27-45	89-107	We replace this in-tractable intersection by a tractable relaxation which incorporates a low-order upperbound on the language model .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	1<2	none	evaluation	evaluation
D14-1131	89-93	94-107	This is the first work	to tractably tackle exact optimisation with language models of orders higher than 3 .	This is the first work	to tractably tackle exact optimisation with language models of orders higher than 3 .	89-107	89-107	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	This is the first work to tractably tackle exact optimisation with language models of orders higher than 3 .	1<2	none	elab-addition	elab-addition
D14-1132	1-9	45-46	Recent work by Cherry ( 2013 ) has shown	We show	Recent work by Cherry ( 2013 ) has shown	We show	1-23	45-77	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	1>2	none	bg-compare	bg-compare
D14-1132	1-9	10-23	Recent work by Cherry ( 2013 ) has shown	that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	Recent work by Cherry ( 2013 ) has shown	that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	1-23	1-23	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	1<2	none	attribution	attribution
D14-1132	1-9	24-44	Recent work by Cherry ( 2013 ) has shown	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	Recent work by Cherry ( 2013 ) has shown	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	1-23	24-44	Recent work by Cherry ( 2013 ) has shown that directly optimizing phrase-based reordering models towards BLEU can lead to significant gains .	Their approach is limited to small training sets of a few thousand sentences and a similar number of sparse features .	1<2	none	elab-addition	elab-addition
D14-1132	45-46	47-72	We show	how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences	We show	how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences	45-77	45-77	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	1<2	none	attribution	attribution
D14-1132	47-72	73-77	how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences	resulting in significant improvements .	how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences	resulting in significant improvements .	45-77	45-77	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	1<2	none	result	result
D14-1132	45-46	78-83	We show	A comparison to likelihood training demonstrates	We show	A comparison to likelihood training demonstrates	45-77	78-91	We show how the expected BLEU objective allows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements .	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	1<2	none	evaluation	evaluation
D14-1132	78-83	84-91	A comparison to likelihood training demonstrates	that expected BLEU is vastly more effective .	A comparison to likelihood training demonstrates	that expected BLEU is vastly more effective .	78-91	78-91	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	1<2	none	attribution	attribution
D14-1132	78-83	92-116	A comparison to likelihood training demonstrates	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	A comparison to likelihood training demonstrates	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	78-91	92-116	A comparison to likelihood training demonstrates that expected BLEU is vastly more effective .	Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup .	1<2	none	exp-evidence	exp-evidence
D14-1133	1-16	32-39	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses	In this work , we introduce an approach	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses	In this work , we introduce an approach	1-31	32-74	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1>2	none	bg-compare	bg-compare
D14-1133	1-16	17-21	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses	obtained by an initial decoding	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses	obtained by an initial decoding	1-31	1-31	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	1<2	none	elab-addition	elab-addition
D14-1133	17-21	22-31	obtained by an initial decoding	using an improved , but more costly scoring function .	obtained by an initial decoding	using an improved , but more costly scoring function .	1-31	1-31	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	Numerous works in Statistical Machine Translation ( SMT ) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved , but more costly scoring function .	1<2	none	manner-means	manner-means
D14-1133	32-39	40-43	In this work , we introduce an approach	that takes the hypotheses	In this work , we introduce an approach	that takes the hypotheses	32-74	32-74	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1<2	none	elab-addition	elab-addition
D14-1133	40-43	44-53	that takes the hypotheses	produced by a state-of-the-art , reranked phrase-based SMT system ,	that takes the hypotheses	produced by a state-of-the-art , reranked phrase-based SMT system ,	32-74	32-74	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1<2	none	elab-addition	elab-addition
D14-1133	32-39	54-61	In this work , we introduce an approach	and explores new parts of the search space	In this work , we introduce an approach	and explores new parts of the search space	32-74	32-74	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1<2	none	joint	joint
D14-1133	54-61	62-65	and explores new parts of the search space	by applying rewriting rules	and explores new parts of the search space	by applying rewriting rules	32-74	32-74	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1<2	none	manner-means	manner-means
D14-1133	62-65	66-74	by applying rewriting rules	selected on the basis of posterior phrase-level confidence .	by applying rewriting rules	selected on the basis of posterior phrase-level confidence .	32-74	32-74	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	1<2	none	elab-addition	elab-addition
D14-1133	32-39	75-89	In this work , we introduce an approach	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline	In this work , we introduce an approach	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline	32-74	75-107	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	1<2	none	elab-addition	elab-addition
D14-1133	75-89	90-95	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline	exploiting the same scoring function ,	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline	exploiting the same scoring function ,	75-107	75-107	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	1<2	none	elab-addition	elab-addition
D14-1133	90-95	96-107	exploiting the same scoring function ,	corresponding to a 5.4 BLEU improvement over the original Moses baseline .	exploiting the same scoring function ,	corresponding to a 5.4 BLEU improvement over the original Moses baseline .	75-107	75-107	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	In the medical domain , we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function , corresponding to a 5.4 BLEU improvement over the original Moses baseline .	1<2	none	elab-addition	elab-addition
D14-1133	32-39	108-109	In this work , we introduce an approach	We show	In this work , we introduce an approach	We show	32-74	108-133	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	1<2	none	evaluation	evaluation
D14-1133	108-109	110-121	We show	that if an indication of which phrases require rewriting is provided ,	We show	that if an indication of which phrases require rewriting is provided ,	108-133	108-133	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	1<2	none	attribution	attribution
D14-1133	110-121	122-133	that if an indication of which phrases require rewriting is provided ,	our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	that if an indication of which phrases require rewriting is provided ,	our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	108-133	108-133	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	We show that if an indication of which phrases require rewriting is provided , our automatic rewriting procedure yields an additional improvement of 1.5 BLEU .	1<2	none	result	result
D14-1133	32-39	134-136,143-160	In this work , we introduce an approach	Various analyses , <*> further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	In this work , we introduce an approach	Various analyses , <*> further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	32-74	134-160	In this work , we introduce an approach that takes the hypotheses produced by a state-of-the-art , reranked phrase-based SMT system , and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phrase-level confidence .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	1<2	none	evaluation	evaluation
D14-1133	134-136,143-160	137-142	Various analyses , <*> further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	including a manual error analysis ,	Various analyses , <*> further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	including a manual error analysis ,	134-160	134-160	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	Various analyses , including a manual error analysis , further illustrate the good performance and potential for improvement of our approach in spite of its simplicity .	1<2	none	elab-aspect	elab-aspect
D14-1134	1-3	38-47	We present methods	We propose using corpus-level statistics for lexicon learning decisions .	We present methods	We propose using corpus-level statistics for lexicon learning decisions .	1-17	38-47	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	We propose using corpus-level statistics for lexicon learning decisions .	1>2	none	bg-compare	bg-compare
D14-1134	1-3	4-8	We present methods	to control the lexicon size	We present methods	to control the lexicon size	1-17	1-17	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	1<2	none	enablement	enablement
D14-1134	4-8	9-17	to control the lexicon size	when learning a Combinatory Categorial Grammar semantic parser .	to control the lexicon size	when learning a Combinatory Categorial Grammar semantic parser .	1-17	1-17	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	1<2	none	temporal	temporal
D14-1134	1-3	18-23	We present methods	Existing methods incrementally expand the lexicon	We present methods	Existing methods incrementally expand the lexicon	1-17	18-37	We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	1<2	none	contrast	contrast
D14-1134	18-23	24-28	Existing methods incrementally expand the lexicon	by greedily adding entries ,	Existing methods incrementally expand the lexicon	by greedily adding entries ,	18-37	18-37	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	1<2	none	manner-means	manner-means
D14-1134	24-28	29-37	by greedily adding entries ,	considering a single training datapoint at a time .	by greedily adding entries ,	considering a single training datapoint at a time .	18-37	18-37	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .	1<2	none	elab-addition	elab-addition
D14-1134	38-47	48-50	We propose using corpus-level statistics for lexicon learning decisions .	We introduce voting	We propose using corpus-level statistics for lexicon learning decisions .	We introduce voting	38-47	48-73	We propose using corpus-level statistics for lexicon learning decisions .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	1<2	none	elab-aspect	elab-aspect
D14-1134	48-50	51-59	We introduce voting	to globally consider adding entries to the lexicon ,	We introduce voting	to globally consider adding entries to the lexicon ,	48-73	48-73	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	1<2	none	enablement	enablement
D14-1134	48-50	60-61	We introduce voting	and pruning	We introduce voting	and pruning	48-73	48-73	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	1<2	none	joint	joint
D14-1134	60-61	62-64	and pruning	to remove entries	and pruning	to remove entries	48-73	48-73	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	1<2	none	enablement	enablement
D14-1134	62-64	65-73	to remove entries	no longer required to explain the training data .	to remove entries	no longer required to explain the training data .	48-73	48-73	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .	1<2	none	elab-addition	elab-addition
D14-1134	38-47	74-90	We propose using corpus-level statistics for lexicon learning decisions .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions ,	We propose using corpus-level statistics for lexicon learning decisions .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions ,	38-47	74-113	We propose using corpus-level statistics for lexicon learning decisions .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	1<2	none	evaluation	evaluation
D14-1134	74-90	91-98	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions ,	achieving up to 25 % error reduction ,	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions ,	achieving up to 25 % error reduction ,	74-113	74-113	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	1<2	none	exp-evidence	exp-evidence
D14-1134	91-98	99-100	achieving up to 25 % error reduction ,	with lexicons	achieving up to 25 % error reduction ,	with lexicons	74-113	74-113	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	1<2	none	elab-addition	elab-addition
D14-1134	99-100	101-107	with lexicons	that are up to 70 % smaller	with lexicons	that are up to 70 % smaller	74-113	74-113	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	1<2	none	elab-addition	elab-addition
D14-1134	101-107	108-113	that are up to 70 % smaller	and are qualitatively less noisy .	that are up to 70 % smaller	and are qualitatively less noisy .	74-113	74-113	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	Our methods result in state-of-the-art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .	1<2	none	joint	joint
D14-1135	1-6	7-17	In this paper , we demonstrate	that significant performance gains can be achieved in CCG semantic parsing	In this paper , we demonstrate	that significant performance gains can be achieved in CCG semantic parsing	1-26	1-26	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	1<2	none	attribution	attribution
D14-1135	7-17	18-26	that significant performance gains can be achieved in CCG semantic parsing	by introducing a linguistically motivated grammar induction scheme .	that significant performance gains can be achieved in CCG semantic parsing	by introducing a linguistically motivated grammar induction scheme .	1-26	1-26	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	1<2	none	manner-means	manner-means
D14-1135	1-6	27-33	In this paper , we demonstrate	We present a new morpho-syntactic factored lexicon	In this paper , we demonstrate	We present a new morpho-syntactic factored lexicon	1-26	27-48	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	1<2	none	elab-aspect	elab-aspect
D14-1135	27-33	34-48	We present a new morpho-syntactic factored lexicon	that models systematic variations in morphology , syntax , and semantics across word classes .	We present a new morpho-syntactic factored lexicon	that models systematic variations in morphology , syntax , and semantics across word classes .	27-48	27-48	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	1<2	none	elab-addition	elab-addition
D14-1135	27-33	49-57	We present a new morpho-syntactic factored lexicon	The grammar uses domain-independent facts about the English language	We present a new morpho-syntactic factored lexicon	The grammar uses domain-independent facts about the English language	27-48	49-77	We present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	1<2	none	elab-addition	elab-addition
D14-1135	49-57	58-64	The grammar uses domain-independent facts about the English language	to restrict the number of incorrect parses	The grammar uses domain-independent facts about the English language	to restrict the number of incorrect parses	49-77	49-77	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	1<2	none	enablement	enablement
D14-1135	58-64	65-69	to restrict the number of incorrect parses	that must be considered ,	to restrict the number of incorrect parses	that must be considered ,	49-77	49-77	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	1<2	none	elab-addition	elab-addition
D14-1135	49-57	70-77	The grammar uses domain-independent facts about the English language	thereby enabling effective learning from less data .	The grammar uses domain-independent facts about the English language	thereby enabling effective learning from less data .	49-77	49-77	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	The grammar uses domain-independent facts about the English language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data .	1<2	none	result	result
D14-1135	1-6	78-90	In this paper , we demonstrate	Experiments in benchmark domains match previous models with one quarter of the data	In this paper , we demonstrate	Experiments in benchmark domains match previous models with one quarter of the data	1-26	78-109	In this paper , we demonstrate that significant performance gains can be achieved in CCG semantic parsing by introducing a linguistically motivated grammar induction scheme .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	1<2	none	evaluation	evaluation
D14-1135	78-90	91-100	Experiments in benchmark domains match previous models with one quarter of the data	and provide new state-of-the-art results with all available data ,	Experiments in benchmark domains match previous models with one quarter of the data	and provide new state-of-the-art results with all available data ,	78-109	78-109	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	1<2	none	joint	joint
D14-1135	91-100	101-109	and provide new state-of-the-art results with all available data ,	including up to 45 % relative test-error reduction .	and provide new state-of-the-art results with all available data ,	including up to 45 % relative test-error reduction .	78-109	78-109	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	Experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .	1<2	none	exp-evidence	exp-evidence
D14-1136	1-14	15-31	We present a model for the automatic semantic analysis of requirements elicitation documents .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming ,	We present a model for the automatic semantic analysis of requirements elicitation documents .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming ,	1-14	15-40	We present a model for the automatic semantic analysis of requirements elicitation documents .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	1<2	none	elab-addition	elab-addition
D14-1136	15-31	32-40	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming ,	which can be directly translated into executable code .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming ,	which can be directly translated into executable code .	15-40	15-40	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	Our target semantic representation employs live sequence charts , a multi-modal visual language for scenario-based programming , which can be directly translated into executable code .	1<2	none	elab-addition	elab-addition
D14-1136	1-14	41-65	We present a model for the automatic semantic analysis of requirements elicitation documents .	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	We present a model for the automatic semantic analysis of requirements elicitation documents .	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	1-14	41-65	We present a model for the automatic semantic analysis of requirements elicitation documents .	The architecture we propose integrates sentence-level and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context .	1<2	none	elab-aspect	elab-aspect
D14-1136	1-14	66-68	We present a model for the automatic semantic analysis of requirements elicitation documents .	We show empirically	We present a model for the automatic semantic analysis of requirements elicitation documents .	We show empirically	1-14	66-102	We present a model for the automatic semantic analysis of requirements elicitation documents .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	1<2	none	evaluation	evaluation
D14-1136	66-68	69-77	We show empirically	that the discourse-based model consistently outperforms the sentence-based model	We show empirically	that the discourse-based model consistently outperforms the sentence-based model	66-102	66-102	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	1<2	none	attribution	attribution
D14-1136	69-77	78-81	that the discourse-based model consistently outperforms the sentence-based model	when constructing a system	that the discourse-based model consistently outperforms the sentence-based model	when constructing a system	66-102	66-102	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	1<2	none	temporal	temporal
D14-1136	78-81	82-102	when constructing a system	that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	when constructing a system	that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	66-102	66-102	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	We show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .	1<2	none	elab-addition	elab-addition
D14-1137	1-5	6-16	We propose a novel model	for parsing natural language sentences into their formal semantic representations .	We propose a novel model	for parsing natural language sentences into their formal semantic representations .	1-16	1-16	We propose a novel model for parsing natural language sentences into their formal semantic representations .	We propose a novel model for parsing natural language sentences into their formal semantic representations .	1<2	none	elab-addition	elab-addition
D14-1137	1-5	17-29	We propose a novel model	The model is able to perform integrated lexicon acquisition and semantic parsing ,	We propose a novel model	The model is able to perform integrated lexicon acquisition and semantic parsing ,	1-16	17-62	We propose a novel model for parsing natural language sentences into their formal semantic representations .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	1<2	none	elab-addition	elab-addition
D14-1137	17-29	30-52	The model is able to perform integrated lexicon acquisition and semantic parsing ,	mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner ,	The model is able to perform integrated lexicon acquisition and semantic parsing ,	mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner ,	17-62	17-62	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	1<2	none	elab-addition	elab-addition
D14-1137	30-52	53-62	mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner ,	where certain overlappings amongst such word sequences are allowed .	mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner ,	where certain overlappings amongst such word sequences are allowed .	17-62	17-62	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	The model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed .	1<2	none	elab-addition	elab-addition
D14-1137	1-5	63-72	We propose a novel model	It defines distributions over the novel relaxed hybrid tree structures	We propose a novel model	It defines distributions over the novel relaxed hybrid tree structures	1-16	63-80	We propose a novel model for parsing natural language sentences into their formal semantic representations .	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	1<2	none	elab-addition	elab-addition
D14-1137	63-72	73-80	It defines distributions over the novel relaxed hybrid tree structures	which jointly represent both sentences and semantics .	It defines distributions over the novel relaxed hybrid tree structures	which jointly represent both sentences and semantics .	63-80	63-80	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	1<2	none	elab-addition	elab-addition
D14-1137	63-72	81-96	It defines distributions over the novel relaxed hybrid tree structures	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	It defines distributions over the novel relaxed hybrid tree structures	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	63-80	81-96	It defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics .	Such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding .	1<2	none	elab-addition	elab-addition
D14-1137	97-102	103-113	Trained under a discriminative setting ,	our model is able to incorporate a rich set of features	Trained under a discriminative setting ,	our model is able to incorporate a rich set of features	97-126	97-126	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	1>2	none	elab-addition	elab-addition
D14-1137	1-5	103-113	We propose a novel model	our model is able to incorporate a rich set of features	We propose a novel model	our model is able to incorporate a rich set of features	1-16	97-126	We propose a novel model for parsing natural language sentences into their formal semantic representations .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	1<2	none	evaluation	evaluation
D14-1137	103-113	114-126	our model is able to incorporate a rich set of features	where certain unbounded long-distance dependencies can be captured in a principled manner .	our model is able to incorporate a rich set of features	where certain unbounded long-distance dependencies can be captured in a principled manner .	97-126	97-126	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	1<2	none	elab-addition	elab-addition
D14-1137	103-113	127-130	our model is able to incorporate a rich set of features	We demonstrate through experiments	our model is able to incorporate a rich set of features	We demonstrate through experiments	97-126	127-163	Trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	1<2	none	exp-evidence	exp-evidence
D14-1137	131-140	141-150	that by exploiting a large collection of simple features ,	our model is shown to be competitive to previous works	that by exploiting a large collection of simple features ,	our model is shown to be competitive to previous works	127-163	127-163	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	1>2	none	manner-means	manner-means
D14-1137	127-130	141-150	We demonstrate through experiments	our model is shown to be competitive to previous works	We demonstrate through experiments	our model is shown to be competitive to previous works	127-163	127-163	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	1<2	none	attribution	attribution
D14-1137	141-150	151-163	our model is shown to be competitive to previous works	and achieves state-of-the-art performance on standard benchmark data across four different languages .	our model is shown to be competitive to previous works	and achieves state-of-the-art performance on standard benchmark data across four different languages .	127-163	127-163	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	1<2	none	joint	joint
D14-1137	127-130	164-174	We demonstrate through experiments	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	We demonstrate through experiments	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	127-163	164-174	We demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-the-art performance on standard benchmark data across four different languages .	The system and code can be downloaded from http ://statnlp.org/research/sp/ .	1<2	none	elab-addition	elab-addition
D14-1138	1-10	54-69	The anchor words algorithm performs provably efficient topic model inference	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	The anchor words algorithm performs provably efficient topic model inference	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	1-23	42-69	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	1>2	none	bg-compare	bg-compare
D14-1138	1-10	11-23	The anchor words algorithm performs provably efficient topic model inference	by finding an approximate convex hull in a high-dimensional word co-occurrence space .	The anchor words algorithm performs provably efficient topic model inference	by finding an approximate convex hull in a high-dimensional word co-occurrence space .	1-23	1-23	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	1<2	none	manner-means	manner-means
D14-1138	1-10	24-35	The anchor words algorithm performs provably efficient topic model inference	However , the existing greedy algorithm often selects poor anchor words ,	The anchor words algorithm performs provably efficient topic model inference	However , the existing greedy algorithm often selects poor anchor words ,	1-23	24-41	The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space .	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	1<2	none	contrast	contrast
D14-1138	24-35	36-41	However , the existing greedy algorithm often selects poor anchor words ,	reducing topic quality and interpretability .	However , the existing greedy algorithm often selects poor anchor words ,	reducing topic quality and interpretability .	24-41	24-41	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	However , the existing greedy algorithm often selects poor anchor words , reducing topic quality and interpretability .	1<2	none	result	result
D14-1138	42-53	54-69	Rather than finding an approximate convex hull in a high-dimensional space ,	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Rather than finding an approximate convex hull in a high-dimensional space ,	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	42-69	42-69	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	1>2	none	contrast	contrast
D14-1138	54-69	70-75	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Such low-dimensional embeddings both improve topics	we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Such low-dimensional embeddings both improve topics	42-69	70-86	Rather than finding an approximate convex hull in a high-dimensional space , we propose to find an exact convex hull in a visualizable 2- or 3-dimensional space .	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	1<2	none	evaluation	evaluation
D14-1138	70-75	76-86	Such low-dimensional embeddings both improve topics	and clearly show users why the algorithm selects certain words .	Such low-dimensional embeddings both improve topics	and clearly show users why the algorithm selects certain words .	70-86	70-86	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words .	1<2	none	joint	joint
D14-1139	1-7	8-16	We generalize contrastive estimation in two ways	that permit adding more knowledge to unsupervised learning .	We generalize contrastive estimation in two ways	that permit adding more knowledge to unsupervised learning .	1-16	1-16	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	1<2	none	elab-addition	elab-addition
D14-1139	1-7	17-42	We generalize contrastive estimation in two ways	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	We generalize contrastive estimation in two ways	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	1-16	17-42	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is .	1<2	none	elab-enumember	elab-enumember
D14-1139	1-7	43-52	We generalize contrastive estimation in two ways	The second allows specifying structural preferences on the latent variable	We generalize contrastive estimation in two ways	The second allows specifying structural preferences on the latent variable	1-16	43-58	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	The second allows specifying structural preferences on the latent variable used to explain the observations .	1<2	none	elab-enumember	elab-enumember
D14-1139	43-52	53-58	The second allows specifying structural preferences on the latent variable	used to explain the observations .	The second allows specifying structural preferences on the latent variable	used to explain the observations .	43-58	43-58	The second allows specifying structural preferences on the latent variable used to explain the observations .	The second allows specifying structural preferences on the latent variable used to explain the observations .	1<2	none	elab-addition	elab-addition
D14-1139	1-7	59-64	We generalize contrastive estimation in two ways	They require setting additional hyperparameters ,	We generalize contrastive estimation in two ways	They require setting additional hyperparameters ,	1-16	59-85	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	1<2	none	elab-addition	elab-addition
D14-1139	59-64	65-72	They require setting additional hyperparameters ,	which can be problematic in unsupervised learning ,	They require setting additional hyperparameters ,	which can be problematic in unsupervised learning ,	59-85	59-85	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	1<2	none	elab-addition	elab-addition
D14-1139	59-64	73-85	They require setting additional hyperparameters ,	so we investigate new methods for unsupervised model selection and system combination .	They require setting additional hyperparameters ,	so we investigate new methods for unsupervised model selection and system combination .	59-85	59-85	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	They require setting additional hyperparameters , which can be problematic in unsupervised learning , so we investigate new methods for unsupervised model selection and system combination .	1<2	none	result	result
D14-1139	1-7	86-92	We generalize contrastive estimation in two ways	We instantiate these ideas for part-of-speech induction	We generalize contrastive estimation in two ways	We instantiate these ideas for part-of-speech induction	1-16	86-112	We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	1<2	none	evaluation	evaluation
D14-1139	86-92	93-96	We instantiate these ideas for part-of-speech induction	without tag dictionaries ,	We instantiate these ideas for part-of-speech induction	without tag dictionaries ,	86-112	86-112	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	1<2	none	elab-addition	elab-addition
D14-1139	86-92	97-112	We instantiate these ideas for part-of-speech induction	improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	We instantiate these ideas for part-of-speech induction	improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	86-112	86-112	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task .	1<2	none	elab-addition	elab-addition
D14-1140	1-12	13-24	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	while receiving input words— between languages with drastically different word orders :	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	while receiving input words— between languages with drastically different word orders :	1-39	1-39	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	1<2	none	temporal	temporal
D14-1140	13-24	25-39	while receiving input words— between languages with drastically different word orders :	from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	while receiving input words— between languages with drastically different word orders :	from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	1-39	1-39	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	1<2	none	elab-enumember	elab-enumember
D14-1140	1-12	40-53	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	In traditional machine translation , a translator must "wait" for source material to appear	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	In traditional machine translation , a translator must "wait" for source material to appear	1-39	40-57	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	1<2	none	bg-compare	bg-compare
D14-1140	40-53	54-57	In traditional machine translation , a translator must "wait" for source material to appear	before translation begins .	In traditional machine translation , a translator must "wait" for source material to appear	before translation begins .	40-57	40-57	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	1<2	none	temporal	temporal
D14-1140	40-53	58-61	In traditional machine translation , a translator must "wait" for source material to appear	We remove this bottleneck	In traditional machine translation , a translator must "wait" for source material to appear	We remove this bottleneck	40-57	58-69	In traditional machine translation , a translator must "wait" for source material to appear before translation begins .	We remove this bottleneck by predicting the final verb in advance .	1<2	none	elab-addition	elab-addition
D14-1140	58-61	62-69	We remove this bottleneck	by predicting the final verb in advance .	We remove this bottleneck	by predicting the final verb in advance .	58-69	58-69	We remove this bottleneck by predicting the final verb in advance .	We remove this bottleneck by predicting the final verb in advance .	1<2	none	manner-means	manner-means
D14-1140	1-12	70-73	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We use reinforcement learning	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We use reinforcement learning	1-39	70-88	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	1<2	none	elab-aspect	elab-aspect
D14-1140	70-73	74-88	We use reinforcement learning	to learn when to trust predictions about unseen , future portions of the sentence .	We use reinforcement learning	to learn when to trust predictions about unseen , future portions of the sentence .	70-88	70-88	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	We use reinforcement learning to learn when to trust predictions about unseen , future portions of the sentence .	1<2	none	enablement	enablement
D14-1140	1-12	89-100	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We also introduce an evaluation metric to measure expeditiousness and quality .	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We also introduce an evaluation metric to measure expeditiousness and quality .	1-39	89-100	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We also introduce an evaluation metric to measure expeditiousness and quality .	1<2	none	elab-aspect	elab-aspect
D14-1140	1-12	101-102	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We show	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation	We show	1-39	101-114	We introduce a reinforcement learning-based approach to simultaneous machine translation—producing a translation while receiving input words— between languages with drastically different word orders : from verb-final languages ( e.g. , German ) to verb-medial languages ( English ) .	We show that our new translation model outperforms batch and monotone translation strategies .	1<2	none	evaluation	evaluation
D14-1140	101-102	103-114	We show	that our new translation model outperforms batch and monotone translation strategies .	We show	that our new translation model outperforms batch and monotone translation strategies .	101-114	101-114	We show that our new translation model outperforms batch and monotone translation strategies .	We show that our new translation model outperforms batch and monotone translation strategies .	1<2	none	attribution	attribution
D14-1141	1-25	56-67	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	In this work , we describe a new algorithm for PCFG induction	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	In this work , we describe a new algorithm for PCFG induction	1-25	56-88	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	1>2	none	bg-goal	bg-goal
D14-1141	26-32	33-42	Although it is a difficult task ,	work in this area is still very much in demand	Although it is a difficult task ,	work in this area is still very much in demand	26-55	26-55	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	1>2	none	contrast	contrast
D14-1141	1-25	33-42	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	work in this area is still very much in demand	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	work in this area is still very much in demand	1-25	26-55	The task of unsupervised induction of probabilistic context-free grammars ( PCFGs ) has attracted a lot of attention in the field of computational linguistics .	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	1<2	none	elab-addition	elab-addition
D14-1141	33-42	43-55	work in this area is still very much in demand	since it can contribute to the advancement of language parsing and modelling .	work in this area is still very much in demand	since it can contribute to the advancement of language parsing and modelling .	26-55	26-55	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	Although it is a difficult task , work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling .	1<2	none	cause	cause
D14-1141	56-67	68-72	In this work , we describe a new algorithm for PCFG induction	based on a principled approach	In this work , we describe a new algorithm for PCFG induction	based on a principled approach	56-88	56-88	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	1<2	none	bg-general	bg-general
D14-1141	68-72	73-88	based on a principled approach	and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	based on a principled approach	and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	56-88	56-88	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	1<2	none	joint	joint
D14-1141	56-67	89-99	In this work , we describe a new algorithm for PCFG induction	Moreover , this algorithm can work on large grammars and datasets	In this work , we describe a new algorithm for PCFG induction	Moreover , this algorithm can work on large grammars and datasets	56-88	89-107	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	1<2	none	progression	progression
D14-1141	89-99	100-107	Moreover , this algorithm can work on large grammars and datasets	and infers correctly even from small samples .	Moreover , this algorithm can work on large grammars and datasets	and infers correctly even from small samples .	89-107	89-107	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	Moreover , this algorithm can work on large grammars and datasets and infers correctly even from small samples .	1<2	none	joint	joint
D14-1141	56-67	108-110	In this work , we describe a new algorithm for PCFG induction	Our analysis shows	In this work , we describe a new algorithm for PCFG induction	Our analysis shows	56-88	108-130	In this work , we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	1<2	none	evaluation	evaluation
D14-1141	108-110	111-115,120-130	Our analysis shows	that the type of grammars <*> are , in theory , capable of modelling natural language .	Our analysis shows	that the type of grammars <*> are , in theory , capable of modelling natural language .	108-130	108-130	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	1<2	none	attribution	attribution
D14-1141	111-115,120-130	116-119	that the type of grammars <*> are , in theory , capable of modelling natural language .	induced by our algorithm	that the type of grammars <*> are , in theory , capable of modelling natural language .	induced by our algorithm	108-130	108-130	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	1<2	none	elab-addition	elab-addition
D14-1141	108-110	131-135	Our analysis shows	One of our experiments shows	Our analysis shows	One of our experiments shows	108-130	131-151	Our analysis shows that the type of grammars induced by our algorithm are , in theory , capable of modelling natural language .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	1<2	none	exp-evidence	exp-evidence
D14-1141	131-135	136-151	One of our experiments shows	that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	One of our experiments shows	that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	131-151	131-151	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus .	1<2	none	attribution	attribution
D14-1142	1-37	38-43,50-59	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	In this work , an approach <*> is proposed for the task of native language identification .	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	In this work , an approach <*> is proposed for the task of native language identification .	1-37	38-59	A common approach in text mining tasks such as text categorization , authorship identification or plagiarism detection is to rely on features like words , part-of-speech tags , stems , or some other high-level linguistic features .	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	1>2	none	bg-compare	bg-compare
D14-1142	38-43,50-59	44-49	In this work , an approach <*> is proposed for the task of native language identification .	that uses character n-grams as features	In this work , an approach <*> is proposed for the task of native language identification .	that uses character n-grams as features	38-59	38-59	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	1<2	none	elab-addition	elab-addition
D14-1142	60-66	67-73	Instead of doing standard feature selection ,	the proposed approach combines several string kernels	Instead of doing standard feature selection ,	the proposed approach combines several string kernels	60-78	60-78	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	1>2	none	contrast	contrast
D14-1142	38-43,50-59	67-73	In this work , an approach <*> is proposed for the task of native language identification .	the proposed approach combines several string kernels	In this work , an approach <*> is proposed for the task of native language identification .	the proposed approach combines several string kernels	38-59	60-78	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	1<2	none	elab-addition	elab-addition
D14-1142	67-73	74-78	the proposed approach combines several string kernels	using multiple kernel learning .	the proposed approach combines several string kernels	using multiple kernel learning .	60-78	60-78	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	1<2	none	manner-means	manner-means
D14-1142	74-78	79-93	using multiple kernel learning .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	using multiple kernel learning .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	60-78	79-93	Instead of doing standard feature selection , the proposed approach combines several string kernels using multiple kernel learning .	Kernel Ridge Regression and Kernel Discriminant Analysis are independently used in the learning stage .	1<2	none	elab-addition	elab-addition
D14-1142	38-43,50-59	94-96,106	In this work , an approach <*> is proposed for the task of native language identification .	The empirical results <*> indicate	In this work , an approach <*> is proposed for the task of native language identification .	The empirical results <*> indicate	38-59	94-140	In this work , an approach that uses character n-grams as features is proposed for the task of native language identification .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	evaluation	evaluation
D14-1142	94-96,106	97-101	The empirical results <*> indicate	obtained in all the experiments	The empirical results <*> indicate	obtained in all the experiments	94-140	94-140	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	elab-addition	elab-addition
D14-1142	97-101	102-105	obtained in all the experiments	conducted in this work	obtained in all the experiments	conducted in this work	94-140	94-140	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	elab-addition	elab-addition
D14-1142	106	107-121	indicate	that the proposed approach achieves state of the art performance in native language identification ,	indicate	that the proposed approach achieves state of the art performance in native language identification ,	94-140	94-140	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	attribution	attribution
D14-1142	107-121	122-124	that the proposed approach achieves state of the art performance in native language identification ,	reaching an accuracy	that the proposed approach achieves state of the art performance in native language identification ,	reaching an accuracy	94-140	94-140	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	elab-addition	elab-addition
D14-1142	122-124	125-140	reaching an accuracy	that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	reaching an accuracy	that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	94-140	94-140	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	1<2	none	elab-addition	elab-addition
D14-1142	94-96,106	141-149	The empirical results <*> indicate	Furthermore , the proposed approach has an important advantage	The empirical results <*> indicate	Furthermore , the proposed approach has an important advantage	94-140	141-159	The empirical results obtained in all the experiments conducted in this work indicate that the proposed approach achieves state of the art performance in native language identification , reaching an accuracy that is 1.7 % above the top scoring system of the 2013 NLI Shared Task .	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	1<2	none	progression	progression
D14-1142	141-149	150-159	Furthermore , the proposed approach has an important advantage	in that it is language independent and linguistic theory neutral.	Furthermore , the proposed approach has an important advantage	in that it is language independent and linguistic theory neutral.	141-159	141-159	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	1<2	none	elab-addition	elab-addition
D14-1142	141-149	160-168	Furthermore , the proposed approach has an important advantage	In the cross-corpus experiment , the proposed approach shows	Furthermore , the proposed approach has an important advantage	In the cross-corpus experiment , the proposed approach shows	141-159	160-187	Furthermore , the proposed approach has an important advantage in that it is language independent and linguistic theory neutral.	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	1<2	none	elab-addition	elab-addition
D14-1142	160-168	169-176	In the cross-corpus experiment , the proposed approach shows	that it can also be topic independent ,	In the cross-corpus experiment , the proposed approach shows	that it can also be topic independent ,	160-187	160-187	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	1<2	none	attribution	attribution
D14-1142	169-176	177-187	that it can also be topic independent ,	improving the state of the art system by 32.3 % .	that it can also be topic independent ,	improving the state of the art system by 32.3 % .	160-187	160-187	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	In the cross-corpus experiment , the proposed approach shows that it can also be topic independent , improving the state of the art system by 32.3 % .	1<2	none	elab-addition	elab-addition
D14-1143	1-14	60-73	Predicting vocabulary of second language learners is essential to support their language learning ;	In this study , we propose a novel framework for this sampling method .	Predicting vocabulary of second language learners is essential to support their language learning ;	In this study , we propose a novel framework for this sampling method .	1-34	60-73	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	In this study , we propose a novel framework for this sampling method .	1>2	none	bg-goal	bg-goal
D14-1143	1-14	15-25	Predicting vocabulary of second language learners is essential to support their language learning ;	however , because of the large size of language vocabularies ,	Predicting vocabulary of second language learners is essential to support their language learning ;	however , because of the large size of language vocabularies ,	1-34	1-34	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	1<2	none	contrast	contrast
D14-1143	15-25	26-34	however , because of the large size of language vocabularies ,	we cannot collect information on the entire vocabulary .	however , because of the large size of language vocabularies ,	we cannot collect information on the entire vocabulary .	1-34	1-34	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	1<2	none	result	result
D14-1143	1-14	35-51	Predicting vocabulary of second language learners is essential to support their language learning ;	For practical measurements , we need to sample a small portion of words from the entire vocabulary	Predicting vocabulary of second language learners is essential to support their language learning ;	For practical measurements , we need to sample a small portion of words from the entire vocabulary	1-34	35-59	Predicting vocabulary of second language learners is essential to support their language learning ; however , because of the large size of language vocabularies , we cannot collect information on the entire vocabulary .	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	1<2	none	elab-addition	elab-addition
D14-1143	35-51	52-59	For practical measurements , we need to sample a small portion of words from the entire vocabulary	and predict the rest of the words .	For practical measurements , we need to sample a small portion of words from the entire vocabulary	and predict the rest of the words .	35-59	35-59	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	For practical measurements , we need to sample a small portion of words from the entire vocabulary and predict the rest of the words .	1<2	none	joint	joint
D14-1143	60-73	74-80	In this study , we propose a novel framework for this sampling method .	Current methods rely on simple heuristic techniques	In this study , we propose a novel framework for this sampling method .	Current methods rely on simple heuristic techniques	60-73	74-88	In this study , we propose a novel framework for this sampling method .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	1<2	none	contrast	contrast
D14-1143	74-80	81-88	Current methods rely on simple heuristic techniques	involving inflexible manual tuning by educational experts .	Current methods rely on simple heuristic techniques	involving inflexible manual tuning by educational experts .	74-88	74-88	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	1<2	none	elab-addition	elab-addition
D14-1143	74-80	89-100	Current methods rely on simple heuristic techniques	We formalize these heuristic techniques as a graph-based non-interactive active learning method	Current methods rely on simple heuristic techniques	We formalize these heuristic techniques as a graph-based non-interactive active learning method	74-88	89-107	Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	1<2	none	elab-addition	elab-addition
D14-1143	89-100	101-107	We formalize these heuristic techniques as a graph-based non-interactive active learning method	as applied to a special graph .	We formalize these heuristic techniques as a graph-based non-interactive active learning method	as applied to a special graph .	89-107	89-107	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph .	1<2	none	elab-addition	elab-addition
D14-1143	60-73	108-109	In this study , we propose a novel framework for this sampling method .	We show	In this study , we propose a novel framework for this sampling method .	We show	60-73	108-131	In this study , we propose a novel framework for this sampling method .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	1<2	none	evaluation	evaluation
D14-1143	110-115	116-120	that by extending the graph ,	we can support additional functionality	that by extending the graph ,	we can support additional functionality	108-131	108-131	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	1>2	none	manner-means	manner-means
D14-1143	108-109	116-120	We show	we can support additional functionality	We show	we can support additional functionality	108-131	108-131	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	1<2	none	attribution	attribution
D14-1143	116-120	121-125	we can support additional functionality	such as incorporating domain specificity	we can support additional functionality	such as incorporating domain specificity	108-131	108-131	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	1<2	none	elab-example	elab-example
D14-1143	121-125	126-131	such as incorporating domain specificity	and sampling from multiple corpora .	such as incorporating domain specificity	and sampling from multiple corpora .	108-131	108-131	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	We show that by extending the graph , we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora .	1<2	none	joint	joint
D14-1143	60-73	132-137	In this study , we propose a novel framework for this sampling method .	In our experiments , we show	In this study , we propose a novel framework for this sampling method .	In our experiments , we show	60-73	132-158	In this study , we propose a novel framework for this sampling method .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	1<2	none	evaluation	evaluation
D14-1143	132-137	138-150	In our experiments , we show	that our extended methods outperform other methods in terms of vocabulary prediction accuracy	In our experiments , we show	that our extended methods outperform other methods in terms of vocabulary prediction accuracy	132-158	132-158	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	1<2	none	attribution	attribution
D14-1143	138-150	151-158	that our extended methods outperform other methods in terms of vocabulary prediction accuracy	when the number of samples is small .	that our extended methods outperform other methods in terms of vocabulary prediction accuracy	when the number of samples is small .	132-158	132-158	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	In our experiments , we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small .	1<2	none	temporal	temporal
D14-1144	1-9,16-25	35-43	Language transfer , the characteristic second language usage patterns <*> is investigated by Second Language Acquisition ( SLA ) researchers	In this paper we develop and present a methodology	Language transfer , the characteristic second language usage patterns <*> is investigated by Second Language Acquisition ( SLA ) researchers	In this paper we develop and present a methodology	1-34	35-51	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	In this paper we develop and present a methodology for deriving ranked lists of such features .	1>2	none	bg-goal	bg-goal
D14-1144	1-9,16-25	10-15	Language transfer , the characteristic second language usage patterns <*> is investigated by Second Language Acquisition ( SLA ) researchers	caused by native language interference ,	Language transfer , the characteristic second language usage patterns <*> is investigated by Second Language Acquisition ( SLA ) researchers	caused by native language interference ,	1-34	1-34	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	1<2	none	cause	cause
D14-1144	16-25	26-34	is investigated by Second Language Acquisition ( SLA ) researchers	seeking to find overused and underused linguistic features .	is investigated by Second Language Acquisition ( SLA ) researchers	seeking to find overused and underused linguistic features .	1-34	1-34	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	Language transfer , the characteristic second language usage patterns caused by native language interference , is investigated by Second Language Acquisition ( SLA ) researchers seeking to find overused and underused linguistic features .	1<2	none	elab-addition	elab-addition
D14-1144	35-43	44-51	In this paper we develop and present a methodology	for deriving ranked lists of such features .	In this paper we develop and present a methodology	for deriving ranked lists of such features .	35-51	35-51	In this paper we develop and present a methodology for deriving ranked lists of such features .	In this paper we develop and present a methodology for deriving ranked lists of such features .	1<2	none	elab-addition	elab-addition
D14-1144	52-57	58-66	Using very large learner data ,	we show our method's ability to find relevant candidates	Using very large learner data ,	we show our method's ability to find relevant candidates	52-71	52-71	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	1>2	none	manner-means	manner-means
D14-1144	35-43	58-66	In this paper we develop and present a methodology	we show our method's ability to find relevant candidates	In this paper we develop and present a methodology	we show our method's ability to find relevant candidates	35-51	52-71	In this paper we develop and present a methodology for deriving ranked lists of such features .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	1<2	none	elab-aspect	elab-aspect
D14-1144	58-66	67-71	we show our method's ability to find relevant candidates	using sophisticated linguistic features .	we show our method's ability to find relevant candidates	using sophisticated linguistic features .	52-71	52-71	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	Using very large learner data , we show our method's ability to find relevant candidates using sophisticated linguistic features .	1<2	none	manner-means	manner-means
D14-1144	72-79	80-85	To illustrate its applicability to SLA research ,	we formulate plausible language transfer hypotheses	To illustrate its applicability to SLA research ,	we formulate plausible language transfer hypotheses	72-90	72-90	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	1>2	none	enablement	enablement
D14-1144	35-43	80-85	In this paper we develop and present a methodology	we formulate plausible language transfer hypotheses	In this paper we develop and present a methodology	we formulate plausible language transfer hypotheses	35-51	72-90	In this paper we develop and present a methodology for deriving ranked lists of such features .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	1<2	none	elab-aspect	elab-aspect
D14-1144	80-85	86-90	we formulate plausible language transfer hypotheses	supported by current evidence .	we formulate plausible language transfer hypotheses	supported by current evidence .	72-90	72-90	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	To illustrate its applicability to SLA research , we formulate plausible language transfer hypotheses supported by current evidence .	1<2	none	elab-addition	elab-addition
D14-1144	35-43	91-95	In this paper we develop and present a methodology	This is the first work	In this paper we develop and present a methodology	This is the first work	35-51	91-122	In this paper we develop and present a methodology for deriving ranked lists of such features .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	1<2	none	evaluation	evaluation
D14-1144	91-95	96-108	This is the first work	to extend Native Language Identification to a broader linguistic interpretation of learner data	This is the first work	to extend Native Language Identification to a broader linguistic interpretation of learner data	91-122	91-122	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	1<2	none	enablement	enablement
D14-1144	96-108	109-122	to extend Native Language Identification to a broader linguistic interpretation of learner data	and address the automatic extraction of underused features on a pernative language basis .	to extend Native Language Identification to a broader linguistic interpretation of learner data	and address the automatic extraction of underused features on a pernative language basis .	91-122	91-122	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	This is the first work to extend Native Language Identification to a broader linguistic interpretation of learner data and address the automatic extraction of underused features on a pernative language basis .	1<2	none	joint	joint
D14-1145	1-5	57-71	Languages spoken by immigrants change	In this study , we distinguish between the immigrant and the standard dialect of Turkish	Languages spoken by immigrants change	In this study , we distinguish between the immigrant and the standard dialect of Turkish	1-13	57-78	Languages spoken by immigrants change due to contact with the local languages .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	1>2	none	bg-goal	bg-goal
D14-1145	1-5	6-13	Languages spoken by immigrants change	due to contact with the local languages .	Languages spoken by immigrants change	due to contact with the local languages .	1-13	1-13	Languages spoken by immigrants change due to contact with the local languages .	Languages spoken by immigrants change due to contact with the local languages .	1<2	none	cause	cause
D14-1145	1-5	14-23	Languages spoken by immigrants change	Capturing these changes is problematic for current language technologies ,	Languages spoken by immigrants change	Capturing these changes is problematic for current language technologies ,	1-13	14-35	Languages spoken by immigrants change due to contact with the local languages .	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	1<2	none	elab-addition	elab-addition
D14-1145	14-23	24-35	Capturing these changes is problematic for current language technologies ,	which are typically developed for speakers of the standard dialect only .	Capturing these changes is problematic for current language technologies ,	which are typically developed for speakers of the standard dialect only .	14-35	14-35	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	Capturing these changes is problematic for current language technologies , which are typically developed for speakers of the standard dialect only .	1<2	none	elab-addition	elab-addition
D14-1145	36-45	46-50	Even when dialectal variants are available for such technologies ,	we still need to predict	Even when dialectal variants are available for such technologies ,	we still need to predict	36-56	36-56	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	1>2	none	condition	condition
D14-1145	1-5	46-50	Languages spoken by immigrants change	we still need to predict	Languages spoken by immigrants change	we still need to predict	1-13	36-56	Languages spoken by immigrants change due to contact with the local languages .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	1<2	none	elab-addition	elab-addition
D14-1145	46-50	51-56	we still need to predict	which dialect is being used .	we still need to predict	which dialect is being used .	36-56	36-56	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	Even when dialectal variants are available for such technologies , we still need to predict which dialect is being used .	1<2	none	attribution	attribution
D14-1145	57-71	72-78	In this study , we distinguish between the immigrant and the standard dialect of Turkish	by focusing on Light Verb Constructions .	In this study , we distinguish between the immigrant and the standard dialect of Turkish	by focusing on Light Verb Constructions .	57-78	57-78	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	1<2	none	manner-means	manner-means
D14-1145	57-71	79-89	In this study , we distinguish between the immigrant and the standard dialect of Turkish	We experiment with a number of grammatical and contextual features ,	In this study , we distinguish between the immigrant and the standard dialect of Turkish	We experiment with a number of grammatical and contextual features ,	57-78	79-100	In this study , we distinguish between the immigrant and the standard dialect of Turkish by focusing on Light Verb Constructions .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	1<2	none	evaluation	evaluation
D14-1145	79-89	90-100	We experiment with a number of grammatical and contextual features ,	achieving over 84 % accuracy ( 56 % baseline ) .	We experiment with a number of grammatical and contextual features ,	achieving over 84 % accuracy ( 56 % baseline ) .	79-100	79-100	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	We experiment with a number of grammatical and contextual features , achieving over 84 % accuracy ( 56 % baseline ) .	1<2	none	elab-example	elab-example
D14-1146	1-16	36-59	Readability is used to provide users with high-quality service in text recommendation or text visualization .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	Readability is used to provide users with high-quality service in text recommendation or text visualization .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	1-16	36-59	Readability is used to provide users with high-quality service in text recommendation or text visualization .	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	1>2	none	bg-goal	bg-goal
D14-1146	1-16	17-35	Readability is used to provide users with high-quality service in text recommendation or text visualization .	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	Readability is used to provide users with high-quality service in text recommendation or text visualization .	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	1-16	17-35	Readability is used to provide users with high-quality service in text recommendation or text visualization .	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	1<2	none	elab-addition	elab-addition
D14-1146	17-35	60-63	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	We suggest readability factors	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	We suggest readability factors	17-35	60-88	With the increasing use of hand-held devices , reading device is regarded as an important factor for readability .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	1<2	none	elab-addition	elab-addition
D14-1146	60-63	64-74	We suggest readability factors	that are strongly related with the readability of a specific device	We suggest readability factors	that are strongly related with the readability of a specific device	60-88	60-88	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	1<2	none	elab-addition	elab-addition
D14-1146	60-63	75-88	We suggest readability factors	by showing the correlations between various factors in each device and human-rated readability .	We suggest readability factors	by showing the correlations between various factors in each device and human-rated readability .	60-88	60-88	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	We suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability .	1<2	none	manner-means	manner-means
D14-1146	36-59	89-92	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	Our experimental results show	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	Our experimental results show	36-59	89-117	Therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	1<2	none	evaluation	evaluation
D14-1146	89-92	93-101	Our experimental results show	that each device has its own readability characteristics ,	Our experimental results show	that each device has its own readability characteristics ,	89-117	89-117	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	1<2	none	attribution	attribution
D14-1146	93-101	102-111	that each device has its own readability characteristics ,	and thus different weights should be imposed on readability factors	that each device has its own readability characteristics ,	and thus different weights should be imposed on readability factors	89-117	89-117	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	1<2	none	joint	joint
D14-1146	102-111	112-117	and thus different weights should be imposed on readability factors	according to the device type .	and thus different weights should be imposed on readability factors	according to the device type .	89-117	89-117	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	1<2	none	cause	cause
D14-1146	118-127	128-137	In order to prove the usefulness of the results ,	we apply the device-dependent readability to news article recommendation .	In order to prove the usefulness of the results ,	we apply the device-dependent readability to news article recommendation .	118-137	118-137	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	1>2	none	enablement	enablement
D14-1146	89-92	128-137	Our experimental results show	we apply the device-dependent readability to news article recommendation .	Our experimental results show	we apply the device-dependent readability to news article recommendation .	89-117	118-137	Our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type .	In order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .	1<2	none	elab-addition	elab-addition
D14-1147	1-8	9-14	We propose a new Chinese abbreviation prediction method	which can incorporate rich local information	We propose a new Chinese abbreviation prediction method	which can incorporate rich local information	1-20	1-20	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	1<2	none	elab-addition	elab-addition
D14-1147	9-14	15-20	which can incorporate rich local information	while generating the abbreviation globally .	which can incorporate rich local information	while generating the abbreviation globally .	1-20	1-20	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	1<2	none	temporal	temporal
D14-1147	21-27	28-34	Different to previous character tagging methods ,	we introduce the minimum semantic unit ,	Different to previous character tagging methods ,	we introduce the minimum semantic unit ,	21-57	21-57	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	1>2	none	contrast	contrast
D14-1147	1-8	28-34	We propose a new Chinese abbreviation prediction method	we introduce the minimum semantic unit ,	We propose a new Chinese abbreviation prediction method	we introduce the minimum semantic unit ,	1-20	21-57	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	1<2	none	elab-aspect	elab-aspect
D14-1147	28-34	35-46	we introduce the minimum semantic unit ,	which is more fine-grained than character but more coarse-grained than word ,	we introduce the minimum semantic unit ,	which is more fine-grained than character but more coarse-grained than word ,	21-57	21-57	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	1<2	none	elab-addition	elab-addition
D14-1147	28-34	47-57	we introduce the minimum semantic unit ,	to capture word level information in the sequence labeling framework .	we introduce the minimum semantic unit ,	to capture word level information in the sequence labeling framework .	21-57	21-57	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	1<2	none	enablement	enablement
D14-1147	58-68	69-82	To solve the "character duplication" problem in Chinese abbreviation prediction ,	we also use a substring tagging strategy to generate local substring tagging candidates .	To solve the "character duplication" problem in Chinese abbreviation prediction ,	we also use a substring tagging strategy to generate local substring tagging candidates .	58-82	58-82	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	1>2	none	enablement	enablement
D14-1147	28-34	69-82	we introduce the minimum semantic unit ,	we also use a substring tagging strategy to generate local substring tagging candidates .	we introduce the minimum semantic unit ,	we also use a substring tagging strategy to generate local substring tagging candidates .	21-57	58-82	Different to previous character tagging methods , we introduce the minimum semantic unit , which is more fine-grained than character but more coarse-grained than word , to capture word level information in the sequence labeling framework .	To solve the "character duplication" problem in Chinese abbreviation prediction , we also use a substring tagging strategy to generate local substring tagging candidates .	1<2	none	joint	joint
D14-1147	1-8	83-95	We propose a new Chinese abbreviation prediction method	We use an integer linear programming ( ILP ) formulation with various constraints	We propose a new Chinese abbreviation prediction method	We use an integer linear programming ( ILP ) formulation with various constraints	1-20	83-106	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	1<2	none	elab-aspect	elab-aspect
D14-1147	83-95	96-106	We use an integer linear programming ( ILP ) formulation with various constraints	to globally decode the final abbreviation from the generated candidates .	We use an integer linear programming ( ILP ) formulation with various constraints	to globally decode the final abbreviation from the generated candidates .	83-106	83-106	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	We use an integer linear programming ( ILP ) formulation with various constraints to globally decode the final abbreviation from the generated candidates .	1<2	none	enablement	enablement
D14-1147	1-8	107-108	We propose a new Chinese abbreviation prediction method	Experiments show	We propose a new Chinese abbreviation prediction method	Experiments show	1-20	107-122	We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	1<2	none	evaluation	evaluation
D14-1147	107-108	109-116	Experiments show	that our method outperforms the state-of-the-art systems ,	Experiments show	that our method outperforms the state-of-the-art systems ,	107-122	107-122	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	1<2	none	attribution	attribution
D14-1147	109-116	117-122	that our method outperforms the state-of-the-art systems ,	without using any extra resource .	that our method outperforms the state-of-the-art systems ,	without using any extra resource .	107-122	107-122	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	Experiments show that our method outperforms the state-of-the-art systems , without using any extra resource .	1<2	none	elab-addition	elab-addition
D14-1148	1-4	79-92	It has been shown	We propose to adapt Open IE technology for event-based stock price movement prediction ,	It has been shown	We propose to adapt Open IE technology for event-based stock price movement prediction ,	1-15	79-103	It has been shown that news events influence the trends of stock price movements .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	1>2	none	bg-compare	bg-compare
D14-1148	1-4	5-15	It has been shown	that news events influence the trends of stock price movements .	It has been shown	that news events influence the trends of stock price movements .	1-15	1-15	It has been shown that news events influence the trends of stock price movements .	It has been shown that news events influence the trends of stock price movements .	1<2	none	attribution	attribution
D14-1148	1-4	16-28	It has been shown	However , previous work on news-driven stock market prediction rely on shallow features	It has been shown	However , previous work on news-driven stock market prediction rely on shallow features	1-15	16-57	It has been shown that news events influence the trends of stock price movements .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	1<2	none	contrast	contrast
D14-1148	16-28	29-40	However , previous work on news-driven stock market prediction rely on shallow features	( such as bags-of-words , named entities and noun phrases ) ,	However , previous work on news-driven stock market prediction rely on shallow features	( such as bags-of-words , named entities and noun phrases ) ,	16-57	16-57	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	1<2	none	elab-example	elab-example
D14-1148	16-28	41-48	However , previous work on news-driven stock market prediction rely on shallow features	which do not capture structured entity-relation information ,	However , previous work on news-driven stock market prediction rely on shallow features	which do not capture structured entity-relation information ,	16-57	16-57	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	1<2	none	elab-addition	elab-addition
D14-1148	16-28	49-57	However , previous work on news-driven stock market prediction rely on shallow features	and hence cannot represent complete and exact events .	However , previous work on news-driven stock market prediction rely on shallow features	and hence cannot represent complete and exact events .	16-57	16-57	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	However , previous work on news-driven stock market prediction rely on shallow features ( such as bags-of-words , named entities and noun phrases ) , which do not capture structured entity-relation information , and hence cannot represent complete and exact events .	1<2	none	joint	joint
D14-1148	1-4	58-78	It has been shown	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	It has been shown	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	1-15	58-78	It has been shown that news events influence the trends of stock price movements .	Recent advances in Open Information Extraction ( Open IE ) techniques enable the extraction of structured events from web-scale data .	1<2	none	elab-addition	elab-addition
D14-1148	79-92	93-99	We propose to adapt Open IE technology for event-based stock price movement prediction ,	extracting structured events from large-scale public news	We propose to adapt Open IE technology for event-based stock price movement prediction ,	extracting structured events from large-scale public news	79-103	79-103	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	1<2	none	elab-addition	elab-addition
D14-1148	93-99	100-103	extracting structured events from large-scale public news	without manual efforts .	extracting structured events from large-scale public news	without manual efforts .	79-103	79-103	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	1<2	none	elab-addition	elab-addition
D14-1148	79-92	104-125	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	79-103	104-125	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Both linear and nonlinear models are employed to empirically investigate the hidden and complex relationships between events and the stock market .	1<2	none	elab-addition	elab-addition
D14-1148	79-92	126-128	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Largescale experiments show	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Largescale experiments show	79-103	126-152	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	1<2	none	evaluation	evaluation
D14-1148	126-128	129-140	Largescale experiments show	that the accuracy of S&P 500 index prediction is 60 % ,	Largescale experiments show	that the accuracy of S&P 500 index prediction is 60 % ,	126-152	126-152	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	1<2	none	attribution	attribution
D14-1148	129-140	141-152	that the accuracy of S&P 500 index prediction is 60 % ,	and that of individual stock prediction can be over 70 % .	that the accuracy of S&P 500 index prediction is 60 % ,	and that of individual stock prediction can be over 70 % .	126-152	126-152	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	Largescale experiments show that the accuracy of S&P 500 index prediction is 60 % , and that of individual stock prediction can be over 70 % .	1<2	none	joint	joint
D14-1148	79-92	153-163	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems	We propose to adapt Open IE technology for event-based stock price movement prediction ,	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems	79-103	153-171	We propose to adapt Open IE technology for event-based stock price movement prediction , extracting structured events from large-scale public news without manual efforts .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	1<2	none	evaluation	evaluation
D14-1148	153-163	164-171	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems	trained on S&P 500 stock historical data .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems	trained on S&P 500 stock historical data .	153-171	153-171	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	Our event-based system outperforms bags-of-words-based baselines , and previously reported systems trained on S&P 500 stock historical data .	1<2	none	elab-addition	elab-addition
D14-1149	1-11	21-41	Automatically identifying related specialist terms is a difficult and important task	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	Automatically identifying related specialist terms is a difficult and important task	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	1-20	21-48	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon — using co-occurrence networks of unstructured text .	1>2	none	bg-general	bg-general
D14-1149	1-11	12-20	Automatically identifying related specialist terms is a difficult and important task	required to understand the lexical structure of language .	Automatically identifying related specialist terms is a difficult and important task	required to understand the lexical structure of language .	1-20	1-20	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	Automatically identifying related specialist terms is a difficult and important task required to understand the lexical structure of language .	1<2	none	elab-addition	elab-addition
D14-1149	21-41	42-48	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	using co-occurrence networks of unstructured text .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	using co-occurrence networks of unstructured text .	21-48	21-48	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon — using co-occurrence networks of unstructured text .	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon — using co-occurrence networks of unstructured text .	1<2	none	manner-means	manner-means
D14-1149	21-41	49-52	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	Term clusters are identified	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	Term clusters are identified	21-48	49-78	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon — using co-occurrence networks of unstructured text .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	1<2	none	elab-addition	elab-addition
D14-1149	49-52	53-60	Term clusters are identified	by extracting communities in the co-occurrence graph ,	Term clusters are identified	by extracting communities in the co-occurrence graph ,	49-78	49-78	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	1<2	none	manner-means	manner-means
D14-1149	53-60	61-66	by extracting communities in the co-occurrence graph ,	after which the largest is discarded	by extracting communities in the co-occurrence graph ,	after which the largest is discarded	49-78	49-78	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	1<2	none	temporal	temporal
D14-1149	61-66	67-78	after which the largest is discarded	and the remaining words are ranked by centrality within a community .	after which the largest is discarded	and the remaining words are ranked by centrality within a community .	49-78	49-78	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	1<2	none	joint	joint
D14-1149	49-52	79-86	Term clusters are identified	The method is tractable on large corpora ,	Term clusters are identified	The method is tractable on large corpora ,	49-78	79-94	Term clusters are identified by extracting communities in the co-occurrence graph , after which the largest is discarded and the remaining words are ranked by centrality within a community .	The method is tractable on large corpora , requires no document structure and minimal normalization .	1<2	none	elab-addition	elab-addition
D14-1149	79-86	87-94	The method is tractable on large corpora ,	requires no document structure and minimal normalization .	The method is tractable on large corpora ,	requires no document structure and minimal normalization .	79-94	79-94	The method is tractable on large corpora , requires no document structure and minimal normalization .	The method is tractable on large corpora , requires no document structure and minimal normalization .	1<2	none	elab-addition	elab-addition
D14-1149	21-41	95-97	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	The results suggest	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon —	The results suggest	21-48	95-119	This paper develops a corpus-based method of extracting coherent clusters of satellite terminology— terms on the edge of the lexicon — using co-occurrence networks of unstructured text .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	1<2	none	evaluation	evaluation
D14-1149	95-97	98-119	The results suggest	that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	The results suggest	that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	95-119	95-119	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	1<2	none	attribution	attribution
D14-1149	95-97	120-123	The results suggest	The findings also confirm	The results suggest	The findings also confirm	95-119	120-151	The results suggest that the model is able to extract coherent groups of satellite terms in corpora with varying size , content and structure .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	1<2	none	joint	joint
D14-1149	120-123	124-131	The findings also confirm	that language consists of a densely connected core	The findings also confirm	that language consists of a densely connected core	120-151	120-151	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	1<2	none	attribution	attribution
D14-1149	124-131	132-151	that language consists of a densely connected core	( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	that language consists of a densely connected core	( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	120-151	120-151	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	The findings also confirm that language consists of a densely connected core ( observed in dictionaries ) and systematic , semantically coherent groups of terms at the edges of the lexicon .	1<2	none	joint	joint
D14-1150	1-23	24-29,40-43	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers ,	effective methods for extracting keyphrases , <*> are greatly needed .	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers ,	effective methods for extracting keyphrases , <*> are greatly needed .	1-43	1-43	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	1>2	none	condition	condition
D14-1150	24-29,40-43	44-59	effective methods for extracting keyphrases , <*> are greatly needed .	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	effective methods for extracting keyphrases , <*> are greatly needed .	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	1-43	44-66	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	1>2	none	bg-goal	bg-goal
D14-1150	24-29,40-43	30-39	effective methods for extracting keyphrases , <*> are greatly needed .	which provide a high-level topic description of a document ,	effective methods for extracting keyphrases , <*> are greatly needed .	which provide a high-level topic description of a document ,	1-43	1-43	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	Given the large amounts of online textual documents available these days , e.g. , news articles , weblogs , and scientific papers , effective methods for extracting keyphrases , which provide a high-level topic description of a document , are greatly needed .	1<2	none	elab-addition	elab-addition
D14-1150	44-59	60-66	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	which are embedded in citation networks .	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	which are embedded in citation networks .	44-66	44-66	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	1<2	none	elab-addition	elab-addition
D14-1150	44-59	67-74	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	To this end , we design novel features	In this paper , we propose a supervised model for keyphrase extraction from research papers ,	To this end , we design novel features	44-66	67-100	In this paper , we propose a supervised model for keyphrase extraction from research papers , which are embedded in citation networks .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	1<2	none	elab-addition	elab-addition
D14-1150	67-74	75-79	To this end , we design novel features	based on citation network information	To this end , we design novel features	based on citation network information	67-100	67-100	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	1<2	none	bg-general	bg-general
D14-1150	67-74	80-90	To this end , we design novel features	and use them in conjunction with traditional features for keyphrase extraction	To this end , we design novel features	and use them in conjunction with traditional features for keyphrase extraction	67-100	67-100	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	1<2	none	joint	joint
D14-1150	80-90	91-100	and use them in conjunction with traditional features for keyphrase extraction	to obtain remarkable improvements in performance over strong baselines .	and use them in conjunction with traditional features for keyphrase extraction	to obtain remarkable improvements in performance over strong baselines .	67-100	67-100	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	To this end , we design novel features based on citation network information and use them in conjunction with traditional features for keyphrase extraction to obtain remarkable improvements in performance over strong baselines .	1<2	none	enablement	enablement
D14-1151	1-6	7-18	We propose to use coreference chains	extracted from a large corpus as a resource for semantic tasks .	We propose to use coreference chains	extracted from a large corpus as a resource for semantic tasks .	1-18	1-18	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	1<2	none	elab-addition	elab-addition
D14-1151	1-6	19-31	We propose to use coreference chains	We extract three million coreference chains and train word embeddings on them .	We propose to use coreference chains	We extract three million coreference chains and train word embeddings on them .	1-18	19-31	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	We extract three million coreference chains and train word embeddings on them .	1<2	none	elab-addition	elab-addition
D14-1151	1-6	32-40	We propose to use coreference chains	Then , we compare these embeddings to word vectors	We propose to use coreference chains	Then , we compare these embeddings to word vectors	1-18	32-64	We propose to use coreference chains extracted from a large corpus as a resource for semantic tasks .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	1<2	none	evaluation	evaluation
D14-1151	32-40	41-45	Then , we compare these embeddings to word vectors	derived from raw text data	Then , we compare these embeddings to word vectors	derived from raw text data	32-64	32-64	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	1<2	none	elab-addition	elab-addition
D14-1151	46-47	48-64	and show	that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	and show	that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	32-64	32-64	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	1>2	none	attribution	attribution
D14-1151	32-40	48-64	Then , we compare these embeddings to word vectors	that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	Then , we compare these embeddings to word vectors	that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	32-64	32-64	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	Then , we compare these embeddings to word vectors derived from raw text data and show that coreference-based word embeddings improve F1 on the task of antonym classification by up to .09 .	1<2	none	progression	progression
D14-1152	1-11	12-20	This paper proposes to apply the continuous vector representations of words	for discovering keywords from a financial sentiment lexicon .	This paper proposes to apply the continuous vector representations of words	for discovering keywords from a financial sentiment lexicon .	1-20	1-20	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	1<2	none	enablement	enablement
D14-1152	21-27	28-41	In order to capture more keywords ,	we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	In order to capture more keywords ,	we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	21-41	21-41	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	1>2	none	enablement	enablement
D14-1152	1-11	28-41	This paper proposes to apply the continuous vector representations of words	we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	This paper proposes to apply the continuous vector representations of words	we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	1-20	21-41	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	In order to capture more keywords , we also incorporate syntactic information into the Continuous Bag-of-Words ( CBOW ) model .	1<2	none	elab-addition	elab-addition
D14-1152	1-11	42-50,56-66	This paper proposes to apply the continuous vector representations of words	Experimental results on a task of financial risk prediction <*> that the proposed approach is good at predicting financial risk .	This paper proposes to apply the continuous vector representations of words	Experimental results on a task of financial risk prediction <*> that the proposed approach is good at predicting financial risk .	1-20	42-66	This paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon .	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	1<2	none	evaluation	evaluation
D14-1152	51-55	56-66	using the discovered keywords demonstrate	that the proposed approach is good at predicting financial risk .	using the discovered keywords demonstrate	that the proposed approach is good at predicting financial risk .	42-66	42-66	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	Experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .	1>2	none	attribution	attribution
D14-1153	1-17	24-38	When it is not possible to compare the suspicious document to the source document ( s )	the evidence of plagiarism has to be looked for intrinsically in the document itself .	When it is not possible to compare the suspicious document to the source document ( s )	the evidence of plagiarism has to be looked for intrinsically in the document itself .	1-38	1-38	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	1>2	none	condition	condition
D14-1153	1-17	18-23	When it is not possible to compare the suspicious document to the source document ( s )	plagiarism has been committed from ,	When it is not possible to compare the suspicious document to the source document ( s )	plagiarism has been committed from ,	1-38	1-38	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	1<2	none	elab-addition	elab-addition
D14-1153	24-38	39-51	the evidence of plagiarism has to be looked for intrinsically in the document itself .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	the evidence of plagiarism has to be looked for intrinsically in the document itself .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	1-38	39-65	When it is not possible to compare the suspicious document to the source document ( s ) plagiarism has been committed from , the evidence of plagiarism has to be looked for intrinsically in the document itself .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	1>2	none	bg-goal	bg-goal
D14-1153	39-51	52-59	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	which is based on a new text representation	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	which is based on a new text representation	39-65	39-65	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	1<2	none	elab-addition	elab-addition
D14-1153	52-59	60-65	which is based on a new text representation	that we called n-gram classes .	which is based on a new text representation	that we called n-gram classes .	39-65	39-65	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	1<2	none	elab-addition	elab-addition
D14-1153	39-51	66-77	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	The proposed method was evaluated on three publicly available standard corpora .	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method	The proposed method was evaluated on three publicly available standard corpora .	39-65	66-77	In this paper , we introduce a novel languageindependent intrinsic plagiarism detection method which is based on a new text representation that we called n-gram classes .	The proposed method was evaluated on three publicly available standard corpora .	1<2	none	evaluation	evaluation
D14-1153	66-77	78-85	The proposed method was evaluated on three publicly available standard corpora .	The obtained results are comparable to the ones	The proposed method was evaluated on three publicly available standard corpora .	The obtained results are comparable to the ones	66-77	78-92	The proposed method was evaluated on three publicly available standard corpora .	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	1<2	none	elab-addition	elab-addition
D14-1153	78-85	86-92	The obtained results are comparable to the ones	obtained by the best state-of-the-art methods .	The obtained results are comparable to the ones	obtained by the best state-of-the-art methods .	78-92	78-92	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	The obtained results are comparable to the ones obtained by the best state-of-the-art methods .	1<2	none	elab-addition	elab-addition
D14-1154	1-9	10-23	Several recent papers on Arabic dialect identification have hinted	that using a word unigram model is sufficient and effective for the task .	Several recent papers on Arabic dialect identification have hinted	that using a word unigram model is sufficient and effective for the task .	1-23	1-23	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	1>2	none	attribution	attribution
D14-1154	10-23	24-41	that using a word unigram model is sufficient and effective for the task .	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	that using a word unigram model is sufficient and effective for the task .	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	1-23	24-41	Several recent papers on Arabic dialect identification have hinted that using a word unigram model is sufficient and effective for the task .	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	1>2	none	contrast	contrast
D14-1154	24-41	48-57	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	that training on the standard dataset does not generalize ,	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	that training on the standard dataset does not generalize ,	24-41	42-79	However , most previous work was done on a standard fairly homogeneous dataset of dialectal user comments .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	1>2	none	bg-compare	bg-compare
D14-1154	42-47	48-57	In this paper , we show	that training on the standard dataset does not generalize ,	In this paper , we show	that training on the standard dataset does not generalize ,	42-79	42-79	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	1>2	none	attribution	attribution
D14-1154	48-57	58-69	that training on the standard dataset does not generalize ,	because a unigram model may be tuned to topics in the comments	that training on the standard dataset does not generalize ,	because a unigram model may be tuned to topics in the comments	42-79	42-79	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	1<2	none	cause	cause
D14-1154	58-69	70-79	because a unigram model may be tuned to topics in the comments	and does not capture the distinguishing features of dialects .	because a unigram model may be tuned to topics in the comments	and does not capture the distinguishing features of dialects .	42-79	42-79	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	1<2	none	joint	joint
D14-1154	80-81	82-86	We show	that effective dialect identification requires	We show	that effective dialect identification requires	80-102	80-102	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	1>2	none	attribution	attribution
D14-1154	82-86	87-102	that effective dialect identification requires	that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	that effective dialect identification requires	that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	80-102	80-102	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	1>2	none	attribution	attribution
D14-1154	48-57	87-102	that training on the standard dataset does not generalize ,	that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	that training on the standard dataset does not generalize ,	that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	42-79	80-102	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	We show that effective dialect identification requires that we account for the distinguishing lexical , morphological , and phonological phenomena of dialects .	1<2	none	elab-addition	elab-addition
D14-1154	103-104	105-119	We show	that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	We show	that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	103-119	103-119	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	1>2	none	attribution	attribution
D14-1154	48-57	105-119	that training on the standard dataset does not generalize ,	that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	that training on the standard dataset does not generalize ,	that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	42-79	103-119	In this paper , we show that training on the standard dataset does not generalize , because a unigram model may be tuned to topics in the comments and does not capture the distinguishing features of dialects .	We show that accounting for such can improve dialect detection accuracy by nearly 10 % absolute .	1<2	none	evaluation	evaluation
D14-1155	1-14	15-38	In this paper , we explore the use of keyboard strokes as a means	to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	In this paper , we explore the use of keyboard strokes as a means	to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	1-38	1-38	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	1<2	none	elab-addition	elab-addition
D14-1155	39-40	41-61	We show	that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	We show	that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	39-61	39-61	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	1>2	none	attribution	attribution
D14-1155	1-14	41-61	In this paper , we explore the use of keyboard strokes as a means	that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	In this paper , we explore the use of keyboard strokes as a means	that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	1-38	39-61	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing .	1<2	none	elab-addition	elab-addition
D14-1155	62-64	65-80	Empirical results show	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	Empirical results show	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	62-85	62-85	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	1>2	none	attribution	attribution
D14-1155	1-14	65-80	In this paper , we explore the use of keyboard strokes as a means	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	In this paper , we explore the use of keyboard strokes as a means	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	1-38	62-85	In this paper , we explore the use of keyboard strokes as a means to access the real-time writing process of online authors , analogously to prosody in speech analysis , in the context of deception detection .	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	1<2	none	evaluation	evaluation
D14-1155	65-80	81-85	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	online reviews and essays .	that incorporating keystroke-based features lead to improved performance in deception detection in two different domains :	online reviews and essays .	62-85	62-85	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains : online reviews and essays .	1<2	none	elab-enumember	elab-enumember
D14-1156	1-6,19-28	77-88	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	This paper presents a continuation of such a general line of research	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	This paper presents a continuation of such a general line of research	1-28	77-95	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	1>2	none	bg-goal	bg-goal
D14-1156	1-6,19-28	7-18	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	that purports to quantify the acceptability of a given piece of text	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	that purports to quantify the acceptability of a given piece of text	1-28	1-28	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	1<2	none	elab-addition	elab-addition
D14-1156	1-6,19-28	29-45	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ;	Statistical language modeling ( LM ) <*> has long been an interesting yet challenging research area .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ;	1-28	29-76	Statistical language modeling ( LM ) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	1<2	none	elab-addition	elab-addition
D14-1156	29-45	46-61	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ;	one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ;	one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process	29-76	29-76	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	1<2	none	elab-addition	elab-addition
D14-1156	46-61	62-69	one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process	to enhance the representation of an input query	one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process	to enhance the representation of an input query	29-76	29-76	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	1<2	none	enablement	enablement
D14-1156	62-69	70-76	to enhance the representation of an input query	so as to improve retrieval effectiveness .	to enhance the representation of an input query	so as to improve retrieval effectiveness .	29-76	29-76	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	In particular , language modeling for information retrieval ( IR ) has enjoyed remarkable empirical success ; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness .	1<2	none	enablement	enablement
D14-1156	77-88	89-95	This paper presents a continuation of such a general line of research	and the main contribution is threefold .	This paper presents a continuation of such a general line of research	and the main contribution is threefold .	77-95	77-95	This paper presents a continuation of such a general line of research and the main contribution is threefold .	This paper presents a continuation of such a general line of research and the main contribution is threefold .	1<2	none	joint	joint
D14-1156	89-95	96-102	and the main contribution is threefold .	First , we propose a principled framework	and the main contribution is threefold .	First , we propose a principled framework	77-95	96-114	This paper presents a continuation of such a general line of research and the main contribution is threefold .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	1<2	none	elab-process_step	elab-process_step
D14-1156	96-102	103-114	First , we propose a principled framework	which can unify the relationships among several widely-used query modeling formulations .	First , we propose a principled framework	which can unify the relationships among several widely-used query modeling formulations .	96-114	96-114	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	First , we propose a principled framework which can unify the relationships among several widely-used query modeling formulations .	1<2	none	elab-addition	elab-addition
D14-1156	89-95	115-131	and the main contribution is threefold .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	and the main contribution is threefold .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	77-95	115-143	This paper presents a continuation of such a general line of research and the main contribution is threefold .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	1<2	none	elab-process_step	elab-process_step
D14-1156	115-131	132-137	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	by incorporating critical query-specific information cues	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	by incorporating critical query-specific information cues	115-143	115-143	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	1<2	none	manner-means	manner-means
D14-1156	115-131	138-143	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	to guide the model estimation .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation	to guide the model estimation .	115-143	115-143	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	Second , on top of the successfully developed framework , we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation .	1<2	none	enablement	enablement
D14-1156	89-95	144-161	and the main contribution is threefold .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	and the main contribution is threefold .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	77-95	144-161	This paper presents a continuation of such a general line of research and the main contribution is threefold .	Third , we further adopt and formalize such a framework to the speech recognition and summarization tasks .	1<2	none	elab-process_step	elab-process_step
D14-1156	77-88	162-187	This paper presents a continuation of such a general line of research	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	This paper presents a continuation of such a general line of research	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	77-95	162-187	This paper presents a continuation of such a general line of research and the main contribution is threefold .	A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks .	1<2	none	evaluation	evaluation
D14-1157	1-10	11-20	We study the topic dynamics of interactions in political debates	using the 2012 Republican presidential primary debates as data .	We study the topic dynamics of interactions in political debates	using the 2012 Republican presidential primary debates as data .	1-20	1-20	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	1<2	none	manner-means	manner-means
D14-1157	21-22	23-39	We show	that the tendency of candidates to shift topics changes over the course of the election campaign ,	We show	that the tendency of candidates to shift topics changes over the course of the election campaign ,	21-49	21-49	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	1>2	none	attribution	attribution
D14-1157	1-10	23-39	We study the topic dynamics of interactions in political debates	that the tendency of candidates to shift topics changes over the course of the election campaign ,	We study the topic dynamics of interactions in political debates	that the tendency of candidates to shift topics changes over the course of the election campaign ,	1-20	21-49	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	1<2	none	elab-addition	elab-addition
D14-1157	23-39	40-49	that the tendency of candidates to shift topics changes over the course of the election campaign ,	and that it is correlated with their relative power .	that the tendency of candidates to shift topics changes over the course of the election campaign ,	and that it is correlated with their relative power .	21-49	21-49	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	We show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power .	1<2	none	joint	joint
D14-1157	50-52	53-63	We also show	that our topic shift features help predict candidates' relative rankings .	We also show	that our topic shift features help predict candidates' relative rankings .	50-63	50-63	We also show that our topic shift features help predict candidates' relative rankings .	We also show that our topic shift features help predict candidates' relative rankings .	1>2	none	attribution	attribution
D14-1157	1-10	53-63	We study the topic dynamics of interactions in political debates	that our topic shift features help predict candidates' relative rankings .	We study the topic dynamics of interactions in political debates	that our topic shift features help predict candidates' relative rankings .	1-20	50-63	We study the topic dynamics of interactions in political debates using the 2012 Republican presidential primary debates as data .	We also show that our topic shift features help predict candidates' relative rankings .	1<2	none	evaluation	evaluation
D14-1158	1-17	18-27	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	where ensembles of low rank matrices and tensors are used	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	where ensembles of low rank matrices and tensors are used	1-37	1-37	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	1<2	none	elab-addition	elab-addition
D14-1158	18-27	28-37	where ensembles of low rank matrices and tensors are used	to obtain smoothed probability estimates of words in context .	where ensembles of low rank matrices and tensors are used	to obtain smoothed probability estimates of words in context .	1-37	1-37	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	1<2	none	enablement	enablement
D14-1158	1-17	38-52	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	Our method can be understood as a generalization of n-gram modeling to non-integer n ,	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	Our method can be understood as a generalization of n-gram modeling to non-integer n ,	1-37	38-67	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	1<2	none	elab-addition	elab-addition
D14-1158	38-52	53-67	Our method can be understood as a generalization of n-gram modeling to non-integer n ,	and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	Our method can be understood as a generalization of n-gram modeling to non-integer n ,	and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	38-67	38-67	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	Our method can be understood as a generalization of n-gram modeling to non-integer n , and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases .	1<2	none	joint	joint
D14-1158	1-17	68-71	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	PLRE training is efficient	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling	PLRE training is efficient	1-37	68-100	We present power low rank ensembles ( PLRE ) , a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context .	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	1<2	none	evaluation	evaluation
D14-1158	68-71	72-100	PLRE training is efficient	and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	PLRE training is efficient	and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	68-100	68-100	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task .	1<2	none	joint	joint
D14-1159	1-5	12-25	Machine reading calls for programs	but most current work only attempts to extract facts from redundant web-scale corpora .	Machine reading calls for programs	but most current work only attempts to extract facts from redundant web-scale corpora .	1-25	1-25	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	1>2	none	contrast	contrast
D14-1159	1-5	6-11	Machine reading calls for programs	that read and understand text ,	Machine reading calls for programs	that read and understand text ,	1-25	1-25	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	1<2	none	elab-addition	elab-addition
D14-1159	12-25	26-37	but most current work only attempts to extract facts from redundant web-scale corpora .	In this paper , we focus on a new reading comprehension task	but most current work only attempts to extract facts from redundant web-scale corpora .	In this paper , we focus on a new reading comprehension task	1-25	26-46	Machine reading calls for programs that read and understand text , but most current work only attempts to extract facts from redundant web-scale corpora .	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	1>2	none	bg-compare	bg-compare
D14-1159	26-37	38-46	In this paper , we focus on a new reading comprehension task	that requires complex reasoning over a single document .	In this paper , we focus on a new reading comprehension task	that requires complex reasoning over a single document .	26-46	26-46	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	1<2	none	elab-addition	elab-addition
D14-1159	26-37	47-51	In this paper , we focus on a new reading comprehension task	The input is a paragraph	In this paper , we focus on a new reading comprehension task	The input is a paragraph	26-46	47-78	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	1<2	none	elab-addition	elab-addition
D14-1159	47-51	52-56	The input is a paragraph	describing a biological process ,	The input is a paragraph	describing a biological process ,	47-78	47-78	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	1<2	none	elab-addition	elab-addition
D14-1159	47-51	57-63	The input is a paragraph	and the goal is to answer questions	The input is a paragraph	and the goal is to answer questions	47-78	47-78	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	1<2	none	progression	progression
D14-1159	57-63	64-78	and the goal is to answer questions	that require an understanding of the relations between entities and events in the process .	and the goal is to answer questions	that require an understanding of the relations between entities and events in the process .	47-78	47-78	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	1<2	none	elab-addition	elab-addition
D14-1159	79-83	84-89	To answer the questions ,	we first predict a rich structure	To answer the questions ,	we first predict a rich structure	79-96	79-96	To answer the questions , we first predict a rich structure representing the process in the paragraph .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	1>2	none	enablement	enablement
D14-1159	57-63	84-89	and the goal is to answer questions	we first predict a rich structure	and the goal is to answer questions	we first predict a rich structure	47-78	79-96	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	1<2	none	elab-process_step	elab-process_step
D14-1159	84-89	90-96	we first predict a rich structure	representing the process in the paragraph .	we first predict a rich structure	representing the process in the paragraph .	79-96	79-96	To answer the questions , we first predict a rich structure representing the process in the paragraph .	To answer the questions , we first predict a rich structure representing the process in the paragraph .	1<2	none	elab-addition	elab-addition
D14-1159	57-63	97-107	and the goal is to answer questions	Then , we map the question to a formal query ,	and the goal is to answer questions	Then , we map the question to a formal query ,	47-78	97-115	The input is a paragraph describing a biological process , and the goal is to answer questions that require an understanding of the relations between entities and events in the process .	Then , we map the question to a formal query , which is executed against the predicted structure .	1<2	none	elab-process_step	elab-process_step
D14-1159	97-107	108-115	Then , we map the question to a formal query ,	which is executed against the predicted structure .	Then , we map the question to a formal query ,	which is executed against the predicted structure .	97-115	97-115	Then , we map the question to a formal query , which is executed against the predicted structure .	Then , we map the question to a formal query , which is executed against the predicted structure .	1<2	none	elab-addition	elab-addition
D14-1159	116-117	118-128	We demonstrate	that answering questions via predicted structures substantially improves accuracy over baselines	We demonstrate	that answering questions via predicted structures substantially improves accuracy over baselines	116-133	116-133	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	1>2	none	attribution	attribution
D14-1159	26-37	118-128	In this paper , we focus on a new reading comprehension task	that answering questions via predicted structures substantially improves accuracy over baselines	In this paper , we focus on a new reading comprehension task	that answering questions via predicted structures substantially improves accuracy over baselines	26-46	116-133	In this paper , we focus on a new reading comprehension task that requires complex reasoning over a single document .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	1<2	none	evaluation	evaluation
D14-1159	118-128	129-133	that answering questions via predicted structures substantially improves accuracy over baselines	that use shallower representations .	that answering questions via predicted structures substantially improves accuracy over baselines	that use shallower representations .	116-133	116-133	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	We demonstrate that answering questions via predicted structures substantially improves accuracy over baselines that use shallower representations .	1<2	none	elab-addition	elab-addition
D14-1160	1-17,25-30	60-76	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	However , to the best of our knowledge , there is no systematic attempt in the literature	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	However , to the best of our knowledge , there is no systematic attempt in the literature	1-35	60-82	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	1>2	none	contrast	contrast
D14-1160	1-17,25-30	18-24	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	to comprehend the sensorial information in language	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	to comprehend the sensorial information in language	1-35	1-35	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	1<2	none	enablement	enablement
D14-1160	1-17,25-30	31-35	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	by using commonsense knowledge .	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	by using commonsense knowledge .	1-35	1-35	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	1<2	none	manner-means	manner-means
D14-1160	1-17,25-30	36-42,47-53	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	With this in mind , a lexicon <*> would be crucial for the computational tasks	Connecting words with senses , namely , sight , hearing , taste , smell and touch , <*> is a straightforward task for humans	With this in mind , a lexicon <*> would be crucial for the computational tasks	1-35	36-59	Connecting words with senses , namely , sight , hearing , taste , smell and touch , to comprehend the sensorial information in language is a straightforward task for humans by using commonsense knowledge .	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	1<2	none	elab-addition	elab-addition
D14-1160	36-42,47-53	43-46	With this in mind , a lexicon <*> would be crucial for the computational tasks	associating words with senses	With this in mind , a lexicon <*> would be crucial for the computational tasks	associating words with senses	36-59	36-59	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	1<2	none	elab-addition	elab-addition
D14-1160	47-53	54-59	would be crucial for the computational tasks	aiming at interpretation of language .	would be crucial for the computational tasks	aiming at interpretation of language .	36-59	36-59	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	With this in mind , a lexicon associating words with senses would be crucial for the computational tasks aiming at interpretation of language .	1<2	none	elab-addition	elab-addition
D14-1160	60-76	83-91	However , to the best of our knowledge , there is no systematic attempt in the literature	In this paper , we present a sensorial lexicon	However , to the best of our knowledge , there is no systematic attempt in the literature	In this paper , we present a sensorial lexicon	60-82	83-98	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	In this paper , we present a sensorial lexicon that associates English words with senses .	1>2	none	bg-goal	bg-goal
D14-1160	60-76	77-82	However , to the best of our knowledge , there is no systematic attempt in the literature	to build such a resource .	However , to the best of our knowledge , there is no systematic attempt in the literature	to build such a resource .	60-82	60-82	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	However , to the best of our knowledge , there is no systematic attempt in the literature to build such a resource .	1<2	none	elab-addition	elab-addition
D14-1160	83-91	92-98	In this paper , we present a sensorial lexicon	that associates English words with senses .	In this paper , we present a sensorial lexicon	that associates English words with senses .	83-98	83-98	In this paper , we present a sensorial lexicon that associates English words with senses .	In this paper , we present a sensorial lexicon that associates English words with senses .	1<2	none	elab-addition	elab-addition
D14-1160	99-103	104-108	To obtain this resource ,	we apply a computational method	To obtain this resource ,	we apply a computational method	99-115	99-115	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	1>2	none	enablement	enablement
D14-1160	83-91	104-108	In this paper , we present a sensorial lexicon	we apply a computational method	In this paper , we present a sensorial lexicon	we apply a computational method	83-98	99-115	In this paper , we present a sensorial lexicon that associates English words with senses .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	1<2	none	elab-addition	elab-addition
D14-1160	104-108	109-115	we apply a computational method	based on bootstrapping and corpus statistics .	we apply a computational method	based on bootstrapping and corpus statistics .	99-115	99-115	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	To obtain this resource , we apply a computational method based on bootstrapping and corpus statistics .	1<2	none	bg-general	bg-general
D14-1160	83-91	116-127	In this paper , we present a sensorial lexicon	The quality of the resulting lexicon is evaluated with a gold standard	In this paper , we present a sensorial lexicon	The quality of the resulting lexicon is evaluated with a gold standard	83-98	116-131	In this paper , we present a sensorial lexicon that associates English words with senses .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	1<2	none	evaluation	evaluation
D14-1160	116-127	128-131	The quality of the resulting lexicon is evaluated with a gold standard	created via crowdsourcing .	The quality of the resulting lexicon is evaluated with a gold standard	created via crowdsourcing .	116-131	116-131	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	1<2	none	elab-addition	elab-addition
D14-1160	132-134	135-138,143-158	The results show	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	The results show	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	132-182	132-182	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	1>2	none	attribution	attribution
D14-1160	116-127	135-138,143-158	The quality of the resulting lexicon is evaluated with a gold standard	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	The quality of the resulting lexicon is evaluated with a gold standard	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	116-131	132-182	The quality of the resulting lexicon is evaluated with a gold standard created via crowdsourcing .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	1<2	none	elab-addition	elab-addition
D14-1160	135-138,143-158	139-142	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	relying on the lexicon	that a simple classifier <*> outperforms two baselines on a sensory classification task , both at word and sentence level ,	relying on the lexicon	132-182	132-182	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	1<2	none	elab-addition	elab-addition
D14-1160	132-134	159-182	The results show	and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	The results show	and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	132-182	132-182	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	The results show that a simple classifier relying on the lexicon outperforms two baselines on a sensory classification task , both at word and sentence level , and confirm the soundness of the proposed approach for the construction of the lexicon and the usefulness of the resource for computational applications .	1<2	none	joint	joint
P06-1001	1-6	17-25	Statistical machine translation is quite robust	It only requires consistency between training and testing .	Statistical machine translation is quite robust	It only requires consistency between training and testing .	1-16	17-25	Statistical machine translation is quite robust when it comes to the choice of input representation .	It only requires consistency between training and testing .	1>2	none	contrast	contrast
P06-1001	1-6	7-16	Statistical machine translation is quite robust	when it comes to the choice of input representation .	Statistical machine translation is quite robust	when it comes to the choice of input representation .	1-16	1-16	Statistical machine translation is quite robust when it comes to the choice of input representation .	Statistical machine translation is quite robust when it comes to the choice of input representation .	1<2	none	temporal	temporal
P06-1001	17-25	26-40	It only requires consistency between training and testing .	As a result , there is a wide range of possible preprocessing choices for data	It only requires consistency between training and testing .	As a result , there is a wide range of possible preprocessing choices for data	17-25	26-46	It only requires consistency between training and testing .	As a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation .	1>2	none	result	result
P06-1001	26-40	60-83	As a result , there is a wide range of possible preprocessing choices for data	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	As a result , there is a wide range of possible preprocessing choices for data	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	26-46	60-83	As a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation .	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	1>2	none	bg-goal	bg-goal
P06-1001	26-40	41-46	As a result , there is a wide range of possible preprocessing choices for data	used in statistical machine translation .	As a result , there is a wide range of possible preprocessing choices for data	used in statistical machine translation .	26-46	26-46	As a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation .	As a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation .	1<2	none	elab-addition	elab-addition
P06-1001	26-40	47-55	As a result , there is a wide range of possible preprocessing choices for data	This is even more so for morphologically rich languages	As a result , there is a wide range of possible preprocessing choices for data	This is even more so for morphologically rich languages	26-46	47-59	As a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation .	This is even more so for morphologically rich languages such as Arabic .	1<2	none	elab-addition	elab-addition
P06-1001	47-55	56-59	This is even more so for morphologically rich languages	such as Arabic .	This is even more so for morphologically rich languages	such as Arabic .	47-59	47-59	This is even more so for morphologically rich languages such as Arabic .	This is even more so for morphologically rich languages such as Arabic .	1<2	none	elab-example	elab-example
P06-1001	60-83	84-90	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	We also present and evaluate different methods	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	We also present and evaluate different methods	60-83	84-100	In this paper , we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation .	We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .	1<2	none	elab-addition	elab-addition
P06-1001	84-90	91-94	We also present and evaluate different methods	for combining preprocessing schemes	We also present and evaluate different methods	for combining preprocessing schemes	84-100	84-100	We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .	We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .	1<2	none	elab-addition	elab-addition
P06-1001	91-94	95-100	for combining preprocessing schemes	resulting in improved translation quality .	for combining preprocessing schemes	resulting in improved translation quality .	84-100	84-100	We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .	We also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .	1<2	none	cause	cause
P06-1002	1-10	11-21	This paper presents an extensive evaluation of five different alignments	and investigates their impact on the corresponding MT system output .	This paper presents an extensive evaluation of five different alignments	and investigates their impact on the corresponding MT system output .	1-21	1-21	This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output .	This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output .	1<2	none	joint	joint
P06-1002	1-10	22-28	This paper presents an extensive evaluation of five different alignments	We introduce new measures for intrinsic evaluations	This paper presents an extensive evaluation of five different alignments	We introduce new measures for intrinsic evaluations	1-21	22-49	This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output .	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	1<2	none	elab-addition	elab-addition
P06-1002	22-28	29-37	We introduce new measures for intrinsic evaluations	and examine the distribution of phrases and untranslated words	We introduce new measures for intrinsic evaluations	and examine the distribution of phrases and untranslated words	22-49	22-49	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	1<2	none	progression	progression
P06-1002	29-37	38-39	and examine the distribution of phrases and untranslated words	during decoding	and examine the distribution of phrases and untranslated words	during decoding	22-49	22-49	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	1<2	none	temporal	temporal
P06-1002	29-37	40-49	and examine the distribution of phrases and untranslated words	to identify which characteristics of different alignments affect translation .	and examine the distribution of phrases and untranslated words	to identify which characteristics of different alignments affect translation .	22-49	22-49	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation .	1<2	none	enablement	enablement
P06-1002	50-51	52-58	We show	that precision-oriented alignments yield better MT output	We show	that precision-oriented alignments yield better MT output	50-71	50-71	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	1>2	none	attribution	attribution
P06-1002	1-10	52-58	This paper presents an extensive evaluation of five different alignments	that precision-oriented alignments yield better MT output	This paper presents an extensive evaluation of five different alignments	that precision-oriented alignments yield better MT output	1-21	50-71	This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output .	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	1<2	none	evaluation	evaluation
P06-1002	52-58	59-62	that precision-oriented alignments yield better MT output	( translating more words	that precision-oriented alignments yield better MT output	( translating more words	50-71	50-71	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	1<2	none	elab-definition	elab-definition
P06-1002	59-62	63-67	( translating more words	and using longer phrases )	( translating more words	and using longer phrases )	50-71	50-71	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	1<2	none	joint	joint
P06-1002	52-58	68-71	that precision-oriented alignments yield better MT output	than recalloriented alignments .	that precision-oriented alignments yield better MT output	than recalloriented alignments .	50-71	50-71	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	We show that precision-oriented alignments yield better MT output ( translating more words and using longer phrases ) than recalloriented alignments .	1<2	none	comparison	comparison
P06-1003	1-8	9-11	We present a method for unsupervised topic modelling	which adapts methods	We present a method for unsupervised topic modelling	which adapts methods	1-34	1-34	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	1<2	none	elab-addition	elab-addition
P06-1003	9-11	12-28	which adapts methods	used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 )	which adapts methods	used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 )	1-34	1-34	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	1<2	none	elab-addition	elab-addition
P06-1003	9-11	29-34	which adapts methods	to unsegmented multi-party discourse transcripts .	which adapts methods	to unsegmented multi-party discourse transcripts .	1-34	1-34	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	1<2	none	enablement	enablement
P06-1003	35-36	37-46	We show	how Bayesian inference in this generative model can be used	We show	how Bayesian inference in this generative model can be used	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1>2	none	attribution	attribution
P06-1003	1-8	37-46	We present a method for unsupervised topic modelling	how Bayesian inference in this generative model can be used	We present a method for unsupervised topic modelling	how Bayesian inference in this generative model can be used	1-34	35-98	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	elab-addition	elab-addition
P06-1003	37-46	47-58	how Bayesian inference in this generative model can be used	to simultaneously address the problems of topic segmentation and topic identification :	how Bayesian inference in this generative model can be used	to simultaneously address the problems of topic segmentation and topic identification :	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	enablement	enablement
P06-1003	47-58	59-68	to simultaneously address the problems of topic segmentation and topic identification :	automatically segmenting multi-party meetings into topically coherent segments with performance	to simultaneously address the problems of topic segmentation and topic identification :	automatically segmenting multi-party meetings into topically coherent segments with performance	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	elab-definition	elab-definition
P06-1003	59-68	69-83	automatically segmenting multi-party meetings into topically coherent segments with performance	which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 )	automatically segmenting multi-party meetings into topically coherent segments with performance	which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 )	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	comparison	comparison
P06-1003	59-68	84-87	automatically segmenting multi-party meetings into topically coherent segments with performance	while simultaneously extracting topics	automatically segmenting multi-party meetings into topically coherent segments with performance	while simultaneously extracting topics	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	joint	joint
P06-1003	84-87	88-90	while simultaneously extracting topics	which rate highly	while simultaneously extracting topics	which rate highly	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	elab-addition	elab-addition
P06-1003	88-90	91-98	which rate highly	when assessed for coherence by human judges .	which rate highly	when assessed for coherence by human judges .	35-98	35-98	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	We show how Bayesian inference in this generative model can be used to simultaneously address the problems of topic segmentation and topic identification : automatically segmenting multi-party meetings into topically coherent segments with performance which compares well with previous unsupervised segmentation-only methods ( Galley et al. , 2003 ) while simultaneously extracting topics which rate highly when assessed for coherence by human judges .	1<2	none	condition	condition
P06-1003	99-101	102-117	We also show	that this method appears robust in the face of off-topic dialogue and speech recognition errors .	We also show	that this method appears robust in the face of off-topic dialogue and speech recognition errors .	99-117	99-117	We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors .	We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors .	1>2	none	attribution	attribution
P06-1003	1-8	102-117	We present a method for unsupervised topic modelling	that this method appears robust in the face of off-topic dialogue and speech recognition errors .	We present a method for unsupervised topic modelling	that this method appears robust in the face of off-topic dialogue and speech recognition errors .	1-34	99-117	We present a method for unsupervised topic modelling which adapts methods used in document classification ( Blei et al. , 2003 ; Griffiths and Steyvers , 2004 ) to unsegmented multi-party discourse transcripts .	We also show that this method appears robust in the face of off-topic dialogue and speech recognition errors .	1<2	none	evaluation	evaluation
P06-1004	1-9	10-16	We consider the task of unsupervised lecture segmentation .	We formalize segmentation as a graph-partitioning task	We consider the task of unsupervised lecture segmentation .	We formalize segmentation as a graph-partitioning task	1-9	10-23	We consider the task of unsupervised lecture segmentation .	We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion .	1>2	none	bg-goal	bg-goal
P06-1004	10-16	17-23	We formalize segmentation as a graph-partitioning task	that optimizes the normalized cut criterion .	We formalize segmentation as a graph-partitioning task	that optimizes the normalized cut criterion .	10-23	10-23	We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion .	We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion .	1<2	none	elab-addition	elab-addition
P06-1004	10-16	24-29	We formalize segmentation as a graph-partitioning task	Our approach moves beyond localized comparisons	We formalize segmentation as a graph-partitioning task	Our approach moves beyond localized comparisons	10-23	24-37	We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion .	Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies .	1<2	none	elab-addition	elab-addition
P06-1004	24-29	30-37	Our approach moves beyond localized comparisons	and takes into account longrange cohesion dependencies .	Our approach moves beyond localized comparisons	and takes into account longrange cohesion dependencies .	24-37	24-37	Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies .	Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies .	1<2	none	joint	joint
P06-1004	38-40	41-47	Our results demonstrate	that global analysis improves the segmentation accuracy	Our results demonstrate	that global analysis improves the segmentation accuracy	38-58	38-58	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .	1>2	none	attribution	attribution
P06-1004	10-16	41-47	We formalize segmentation as a graph-partitioning task	that global analysis improves the segmentation accuracy	We formalize segmentation as a graph-partitioning task	that global analysis improves the segmentation accuracy	10-23	38-58	We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion .	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .	1<2	none	evaluation	evaluation
P06-1004	41-47	48-58	that global analysis improves the segmentation accuracy	and is robust in the presence of speech recognition errors .	that global analysis improves the segmentation accuracy	and is robust in the presence of speech recognition errors .	38-58	38-58	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .	Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors .	1<2	none	joint	joint
P06-1005	1-7	8-12	We present an approach to pronoun resolution	based on syntactic paths .	We present an approach to pronoun resolution	based on syntactic paths .	1-12	1-12	We present an approach to pronoun resolution based on syntactic paths .	We present an approach to pronoun resolution based on syntactic paths .	1<2	none	bg-general	bg-general
P06-1005	13-18	19-31	Through a simple bootstrapping procedure ,	we learn the likelihood of coreference between a pronoun and a candidate noun	Through a simple bootstrapping procedure ,	we learn the likelihood of coreference between a pronoun and a candidate noun	13-44	13-44	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	1>2	none	manner-means	manner-means
P06-1005	1-7	19-31	We present an approach to pronoun resolution	we learn the likelihood of coreference between a pronoun and a candidate noun	We present an approach to pronoun resolution	we learn the likelihood of coreference between a pronoun and a candidate noun	1-12	13-44	We present an approach to pronoun resolution based on syntactic paths .	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	1<2	none	elab-addition	elab-addition
P06-1005	19-31	32-44	we learn the likelihood of coreference between a pronoun and a candidate noun	based on the path in the parse tree between the two entities .	we learn the likelihood of coreference between a pronoun and a candidate noun	based on the path in the parse tree between the two entities .	13-44	13-44	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	1<2	none	bg-general	bg-general
P06-1005	32-44	45-56	based on the path in the parse tree between the two entities .	This path information enables us to handle previously challenging resolution instances ,	based on the path in the parse tree between the two entities .	This path information enables us to handle previously challenging resolution instances ,	13-44	45-65	Through a simple bootstrapping procedure , we learn the likelihood of coreference between a pronoun and a candidate noun based on the path in the parse tree between the two entities .	This path information enables us to handle previously challenging resolution instances , and also robustly addresses traditional syntactic coreference constraints .	1<2	none	elab-addition	elab-addition
P06-1005	45-56	57-65	This path information enables us to handle previously challenging resolution instances ,	and also robustly addresses traditional syntactic coreference constraints .	This path information enables us to handle previously challenging resolution instances ,	and also robustly addresses traditional syntactic coreference constraints .	45-65	45-65	This path information enables us to handle previously challenging resolution instances , and also robustly addresses traditional syntactic coreference constraints .	This path information enables us to handle previously challenging resolution instances , and also robustly addresses traditional syntactic coreference constraints .	1<2	none	joint	joint
P06-1005	45-56	66-77	This path information enables us to handle previously challenging resolution instances ,	Highly coreferent paths also allow mining of precise probabilistic gender/number information .	This path information enables us to handle previously challenging resolution instances ,	Highly coreferent paths also allow mining of precise probabilistic gender/number information .	45-65	66-77	This path information enables us to handle previously challenging resolution instances , and also robustly addresses traditional syntactic coreference constraints .	Highly coreferent paths also allow mining of precise probabilistic gender/number information .	1<2	none	elab-addition	elab-addition
P06-1005	78-94	95-104	We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier .	Significant gains in performance are observed on several datasets .	We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier .	Significant gains in performance are observed on several datasets .	78-94	95-104	We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier .	Significant gains in performance are observed on several datasets .	1>2	none	result	result
P06-1005	1-7	95-104	We present an approach to pronoun resolution	Significant gains in performance are observed on several datasets .	We present an approach to pronoun resolution	Significant gains in performance are observed on several datasets .	1-12	95-104	We present an approach to pronoun resolution based on syntactic paths .	Significant gains in performance are observed on several datasets .	1<2	none	evaluation	evaluation
P06-1006	1-8	32-40	Syntactic knowledge is important for pronoun resolution .	In the paper , we propose a kernel-based method	Syntactic knowledge is important for pronoun resolution .	In the paper , we propose a kernel-based method	1-8	32-55	Syntactic knowledge is important for pronoun resolution .	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	1>2	none	bg-goal	bg-goal
P06-1006	9-22	32-40	Traditionally , the syntactic information for pronoun resolution is represented in terms of features	In the paper , we propose a kernel-based method	Traditionally , the syntactic information for pronoun resolution is represented in terms of features	In the paper , we propose a kernel-based method	9-31	32-55	Traditionally , the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically .	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	1>2	none	bg-compare	bg-compare
P06-1006	9-22	23-31	Traditionally , the syntactic information for pronoun resolution is represented in terms of features	that have to be selected and defined heuristically .	Traditionally , the syntactic information for pronoun resolution is represented in terms of features	that have to be selected and defined heuristically .	9-31	9-31	Traditionally , the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically .	Traditionally , the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically .	1<2	none	elab-addition	elab-addition
P06-1006	32-40	41-55	In the paper , we propose a kernel-based method	that can automatically mine the syntactic information from the parse trees for pronoun resolution .	In the paper , we propose a kernel-based method	that can automatically mine the syntactic information from the parse trees for pronoun resolution .	32-55	32-55	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	1<2	none	elab-addition	elab-addition
P06-1006	56-67	68-82	Specifically , we utilize the parse trees directly as a structured feature	and apply kernel functions to this feature , as well as other normal features ,	Specifically , we utilize the parse trees directly as a structured feature	and apply kernel functions to this feature , as well as other normal features ,	56-88	56-88	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	1>2	none	progression	progression
P06-1006	32-40	68-82	In the paper , we propose a kernel-based method	and apply kernel functions to this feature , as well as other normal features ,	In the paper , we propose a kernel-based method	and apply kernel functions to this feature , as well as other normal features ,	32-55	56-88	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	1<2	none	elab-addition	elab-addition
P06-1006	68-82	83-88	and apply kernel functions to this feature , as well as other normal features ,	to learn the resolution classifier .	and apply kernel functions to this feature , as well as other normal features ,	to learn the resolution classifier .	56-88	56-88	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	1<2	none	enablement	enablement
P06-1006	68-82	89-97	and apply kernel functions to this feature , as well as other normal features ,	In this way , our approach avoids the efforts	and apply kernel functions to this feature , as well as other normal features ,	In this way , our approach avoids the efforts	56-88	89-110	Specifically , we utilize the parse trees directly as a structured feature and apply kernel functions to this feature , as well as other normal features , to learn the resolution classifier .	In this way , our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features .	1<2	none	elab-addition	elab-addition
P06-1006	89-97	98-110	In this way , our approach avoids the efforts	of decoding the parse trees into the set of flat syntactic features .	In this way , our approach avoids the efforts	of decoding the parse trees into the set of flat syntactic features .	89-110	89-110	In this way , our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features .	In this way , our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features .	1<2	none	elab-addition	elab-addition
P06-1006	111-114	115-122	The experimental results show	that our approach can bring significant performance improvement	The experimental results show	that our approach can bring significant performance improvement	111-132	111-132	The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .	The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .	1>2	none	attribution	attribution
P06-1006	32-40	115-122	In the paper , we propose a kernel-based method	that our approach can bring significant performance improvement	In the paper , we propose a kernel-based method	that our approach can bring significant performance improvement	32-55	111-132	In the paper , we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution .	The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .	1<2	none	evaluation	evaluation
P06-1006	115-122	123-132	that our approach can bring significant performance improvement	and is reliably effective for the pronoun resolution task .	that our approach can bring significant performance improvement	and is reliably effective for the pronoun resolution task .	111-132	111-132	The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .	The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task .	1<2	none	joint	joint
P06-1007	1-9	42-51,65-70	It has previously been assumed in the psycholinguistic literature	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	It has previously been assumed in the psycholinguistic literature	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	1-39	40-81	It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model .	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	1>2	none	bg-goal	bg-goal
P06-1007	1-9	10-34	It has previously been assumed in the psycholinguistic literature	that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information	It has previously been assumed in the psycholinguistic literature	that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information	1-39	1-39	It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model .	It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model .	1<2	none	elab-addition	elab-addition
P06-1007	10-34	35-39	that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information	used by the model .	that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information	used by the model .	1-39	1-39	It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model .	It has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model .	1<2	none	elab-addition	elab-addition
P06-1007	40-41	42-51,65-70	We show	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	We show	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	40-81	40-81	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	1>2	none	attribution	attribution
P06-1007	42-51,65-70	52-55	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	based on the design	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	based on the design	40-81	40-81	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	1<2	none	bg-general	bg-general
P06-1007	52-55	56-64	based on the design	used by Corley and Crocker ( 2000 ) )	based on the design	used by Corley and Crocker ( 2000 ) )	40-81	40-81	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	1<2	none	elab-addition	elab-addition
P06-1007	65-70	71-81	makes correct predictions on processing difficulty	observed in a wide range of empirical sentence processing data .	makes correct predictions on processing difficulty	observed in a wide range of empirical sentence processing data .	40-81	40-81	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	1<2	none	elab-addition	elab-addition
P06-1007	42-51,65-70	82-88	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	We use two modes of evaluation :	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	We use two modes of evaluation :	40-81	82-117	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	elab-addition	elab-addition
P06-1007	82-88	89	We use two modes of evaluation :	one	We use two modes of evaluation :	one	82-117	82-117	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	elab-enumember	elab-enumember
P06-1007	89	90-98	one	that relies on comparison with a control sentence ,	one	that relies on comparison with a control sentence ,	82-117	82-117	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	elab-addition	elab-addition
P06-1007	90-98	99-104	that relies on comparison with a control sentence ,	paralleling practice in human studies ;	that relies on comparison with a control sentence ,	paralleling practice in human studies ;	82-117	82-117	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	comparison	comparison
P06-1007	82-88	105	We use two modes of evaluation :	another	We use two modes of evaluation :	another	82-117	82-117	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	elab-enumember	elab-enumember
P06-1007	105	106-117	another	that measures probability drop in the disambiguating region of the sentence .	another	that measures probability drop in the disambiguating region of the sentence .	82-117	82-117	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	1<2	none	elab-addition	elab-addition
P06-1007	82-88	118-130	We use two modes of evaluation :	Both are surprisingly good indicators of the processing difficulty of garden-path sentences .	We use two modes of evaluation :	Both are surprisingly good indicators of the processing difficulty of garden-path sentences .	82-117	118-130	We use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence .	Both are surprisingly good indicators of the processing difficulty of garden-path sentences .	1<2	none	summary	summary
P06-1007	42-51,65-70	131-138	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	The sentences tested are drawn from published sources	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	The sentences tested are drawn from published sources	40-81	131-159	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	1<2	none	elab-addition	elab-addition
P06-1007	131-138	139-147	The sentences tested are drawn from published sources	and systematically explore five different types of ambiguity :	The sentences tested are drawn from published sources	and systematically explore five different types of ambiguity :	131-159	131-159	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	1<2	none	joint	joint
P06-1007	131-138	148-154	The sentences tested are drawn from published sources	previous studies have been narrower in scope	The sentences tested are drawn from published sources	previous studies have been narrower in scope	131-159	131-159	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	1<2	none	comparison	comparison
P06-1007	148-154	155-159	previous studies have been narrower in scope	and smaller in scale .	previous studies have been narrower in scope	and smaller in scale .	131-159	131-159	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	The sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale .	1<2	none	joint	joint
P06-1007	160-169	176-182	We do not deny the limitations of finite-state models ,	that their usefulness has been underestimated .	We do not deny the limitations of finite-state models ,	that their usefulness has been underestimated .	160-182	160-182	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	1>2	none	contrast	contrast
P06-1007	170-171	172-175	but argue	that our results show	but argue	that our results show	160-182	160-182	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	1>2	none	attribution	attribution
P06-1007	172-175	176-182	that our results show	that their usefulness has been underestimated .	that our results show	that their usefulness has been underestimated .	160-182	160-182	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	1>2	none	attribution	attribution
P06-1007	42-51,65-70	176-182	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	that their usefulness has been underestimated .	that a simple computational model ( a bigram part-of-speech tagger <*> makes correct predictions on processing difficulty	that their usefulness has been underestimated .	40-81	160-182	We show that a simple computational model ( a bigram part-of-speech tagger based on the design used by Corley and Crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data .	We do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .	1<2	none	evaluation	evaluation
P06-1008	1-7	8-12	We propose in this paper a method	for quantifying sentence grammaticality .	We propose in this paper a method	for quantifying sentence grammaticality .	1-12	1-12	We propose in this paper a method for quantifying sentence grammaticality .	We propose in this paper a method for quantifying sentence grammaticality .	1<2	none	elab-addition	elab-addition
P06-1008	1-7	13-14,25-38	We propose in this paper a method	The approach <*> makes it possible to evaluate a grammaticality index for any kind of sentence ,	We propose in this paper a method	The approach <*> makes it possible to evaluate a grammaticality index for any kind of sentence ,	1-12	13-42	We propose in this paper a method for quantifying sentence grammaticality .	The approach based on Property Grammars , a constraint-based syntactic formalism , makes it possible to evaluate a grammaticality index for any kind of sentence , including ill-formed ones .	1<2	none	elab-addition	elab-addition
P06-1008	13-14,25-38	15-24	The approach <*> makes it possible to evaluate a grammaticality index for any kind of sentence ,	based on Property Grammars , a constraint-based syntactic formalism ,	The approach <*> makes it possible to evaluate a grammaticality index for any kind of sentence ,	based on Property Grammars , a constraint-based syntactic formalism ,	13-42	13-42	The approach based on Property Grammars , a constraint-based syntactic formalism , makes it possible to evaluate a grammaticality index for any kind of sentence , including ill-formed ones .	The approach based on Property Grammars , a constraint-based syntactic formalism , makes it possible to evaluate a grammaticality index for any kind of sentence , including ill-formed ones .	1<2	none	bg-general	bg-general
P06-1008	25-38	39-42	makes it possible to evaluate a grammaticality index for any kind of sentence ,	including ill-formed ones .	makes it possible to evaluate a grammaticality index for any kind of sentence ,	including ill-formed ones .	13-42	13-42	The approach based on Property Grammars , a constraint-based syntactic formalism , makes it possible to evaluate a grammaticality index for any kind of sentence , including ill-formed ones .	The approach based on Property Grammars , a constraint-based syntactic formalism , makes it possible to evaluate a grammaticality index for any kind of sentence , including ill-formed ones .	1<2	none	elab-example	elab-example
P06-1008	1-7	43-52	We propose in this paper a method	We compare on a sample of sentences the grammaticality indices	We propose in this paper a method	We compare on a sample of sentences the grammaticality indices	1-12	43-68	We propose in this paper a method for quantifying sentence grammaticality .	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	1<2	none	elab-addition	elab-addition
P06-1008	43-52	53-56	We compare on a sample of sentences the grammaticality indices	obtained from PG formalism	We compare on a sample of sentences the grammaticality indices	obtained from PG formalism	43-68	43-68	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	1<2	none	elab-addition	elab-addition
P06-1008	43-52	57-60	We compare on a sample of sentences the grammaticality indices	and the acceptability judgements	We compare on a sample of sentences the grammaticality indices	and the acceptability judgements	43-68	43-68	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	1<2	none	joint	joint
P06-1008	57-60	61-68	and the acceptability judgements	measured by means of a psycholinguistic analysis .	and the acceptability judgements	measured by means of a psycholinguistic analysis .	43-68	43-68	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	We compare on a sample of sentences the grammaticality indices obtained from PG formalism and the acceptability judgements measured by means of a psycholinguistic analysis .	1<2	none	elab-addition	elab-addition
P06-1008	69-71	72-85	The results show	that the derived grammaticality index is a fairly good tracer of acceptability scores .	The results show	that the derived grammaticality index is a fairly good tracer of acceptability scores .	69-85	69-85	The results show that the derived grammaticality index is a fairly good tracer of acceptability scores .	The results show that the derived grammaticality index is a fairly good tracer of acceptability scores .	1>2	none	attribution	attribution
P06-1008	1-7	72-85	We propose in this paper a method	that the derived grammaticality index is a fairly good tracer of acceptability scores .	We propose in this paper a method	that the derived grammaticality index is a fairly good tracer of acceptability scores .	1-12	69-85	We propose in this paper a method for quantifying sentence grammaticality .	The results show that the derived grammaticality index is a fairly good tracer of acceptability scores .	1<2	none	evaluation	evaluation
P06-1009	1-8	9-17	In this paper we present a novel approach	for inducing word alignments from sentence aligned data .	In this paper we present a novel approach	for inducing word alignments from sentence aligned data .	1-17	1-17	In this paper we present a novel approach for inducing word alignments from sentence aligned data .	In this paper we present a novel approach for inducing word alignments from sentence aligned data .	1<2	none	elab-addition	elab-addition
P06-1009	1-8	18-31	In this paper we present a novel approach	We use a Conditional Random Field ( CRF ) , a discriminative model ,	In this paper we present a novel approach	We use a Conditional Random Field ( CRF ) , a discriminative model ,	1-17	18-41	In this paper we present a novel approach for inducing word alignments from sentence aligned data .	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	1<2	none	elab-addition	elab-addition
P06-1009	18-31	32-41	We use a Conditional Random Field ( CRF ) , a discriminative model ,	which is estimated on a small supervised training set .	We use a Conditional Random Field ( CRF ) , a discriminative model ,	which is estimated on a small supervised training set .	18-41	18-41	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	1<2	none	elab-addition	elab-addition
P06-1009	42-53	54-68	The CRF is conditioned on both the source and target texts ,	and thus allows for the use of arbitrary and overlapping features over these data .	The CRF is conditioned on both the source and target texts ,	and thus allows for the use of arbitrary and overlapping features over these data .	42-68	42-68	The CRF is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data .	The CRF is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data .	1>2	none	progression	progression
P06-1009	18-31	54-68	We use a Conditional Random Field ( CRF ) , a discriminative model ,	and thus allows for the use of arbitrary and overlapping features over these data .	We use a Conditional Random Field ( CRF ) , a discriminative model ,	and thus allows for the use of arbitrary and overlapping features over these data .	18-41	42-68	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	The CRF is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data .	1<2	none	elab-addition	elab-addition
P06-1009	18-31	69-78	We use a Conditional Random Field ( CRF ) , a discriminative model ,	Moreover , the CRF has efficient training and decoding processes	We use a Conditional Random Field ( CRF ) , a discriminative model ,	Moreover , the CRF has efficient training and decoding processes	18-41	69-85	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	Moreover , the CRF has efficient training and decoding processes which both find globally optimal solutions .	1<2	none	elab-addition	elab-addition
P06-1009	69-78	79-85	Moreover , the CRF has efficient training and decoding processes	which both find globally optimal solutions .	Moreover , the CRF has efficient training and decoding processes	which both find globally optimal solutions .	69-85	69-85	Moreover , the CRF has efficient training and decoding processes which both find globally optimal solutions .	Moreover , the CRF has efficient training and decoding processes which both find globally optimal solutions .	1<2	none	elab-addition	elab-addition
P06-1009	86-98	101-116	We apply this alignment model to both French-English and Romanian-English language pairs .	how a large number of highly predictive features can be easily incorporated into the CRF ,	We apply this alignment model to both French-English and Romanian-English language pairs .	how a large number of highly predictive features can be easily incorporated into the CRF ,	86-98	99-150	We apply this alignment model to both French-English and Romanian-English language pairs .	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	1>2	none	result	result
P06-1009	99-100	101-116	We show	how a large number of highly predictive features can be easily incorporated into the CRF ,	We show	how a large number of highly predictive features can be easily incorporated into the CRF ,	99-150	99-150	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	1>2	none	attribution	attribution
P06-1009	18-31	101-116	We use a Conditional Random Field ( CRF ) , a discriminative model ,	how a large number of highly predictive features can be easily incorporated into the CRF ,	We use a Conditional Random Field ( CRF ) , a discriminative model ,	how a large number of highly predictive features can be easily incorporated into the CRF ,	18-41	99-150	We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	1<2	none	evaluation	evaluation
P06-1009	117-118	119-150	and demonstrate	that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	and demonstrate	that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	99-150	99-150	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	1>2	none	attribution	attribution
P06-1009	101-116	119-150	how a large number of highly predictive features can be easily incorporated into the CRF ,	that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	how a large number of highly predictive features can be easily incorporated into the CRF ,	that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	99-150	99-150	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	We show how a large number of highly predictive features can be easily incorporated into the CRF , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .	1<2	none	joint	joint
P06-1010	1-8	9-13	In this paper we investigate ChineseEnglish name transliteration	using comparable corpora , corpora	In this paper we investigate ChineseEnglish name transliteration	using comparable corpora , corpora	1-43	1-43	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	1<2	none	manner-means	manner-means
P06-1010	9-13	14-27	using comparable corpora , corpora	where texts in the two languages deal in some of the same topics —	using comparable corpora , corpora	where texts in the two languages deal in some of the same topics —	1-43	1-43	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	1<2	none	elab-addition	elab-addition
P06-1010	14-27	28-35	where texts in the two languages deal in some of the same topics —	and therefore share references to named entities —	where texts in the two languages deal in some of the same topics —	and therefore share references to named entities —	1-43	1-43	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	1<2	none	progression	progression
P06-1010	28-35	36-43	and therefore share references to named entities —	but are not translations of each other .	and therefore share references to named entities —	but are not translations of each other .	1-43	1-43	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	1<2	none	contrast	contrast
P06-1010	1-8	44-51	In this paper we investigate ChineseEnglish name transliteration	We present two distinct methods for transliteration ,	In this paper we investigate ChineseEnglish name transliteration	We present two distinct methods for transliteration ,	1-43	44-68	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	1<2	none	elab-addition	elab-addition
P06-1010	44-51	52-53	We present two distinct methods for transliteration ,	one approach	We present two distinct methods for transliteration ,	one approach	44-68	44-68	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	1<2	none	elab-enumember	elab-enumember
P06-1010	52-53	54-57	one approach	using phonetic transliteration ,	one approach	using phonetic transliteration ,	44-68	44-68	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	1<2	none	manner-means	manner-means
P06-1010	44-51	58-60	We present two distinct methods for transliteration ,	and the second	We present two distinct methods for transliteration ,	and the second	44-68	44-68	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	1<2	none	elab-enumember	elab-enumember
P06-1010	58-60	61-68	and the second	using the temporal distribution of candidate pairs .	and the second	using the temporal distribution of candidate pairs .	44-68	44-68	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	1<2	none	manner-means	manner-means
P06-1010	69-76	82-88	Each of these approaches works quite well ,	one can achieve even better results .	Each of these approaches works quite well ,	one can achieve even better results .	69-88	69-88	Each of these approaches works quite well , but by combining the approaches one can achieve even better results .	Each of these approaches works quite well , but by combining the approaches one can achieve even better results .	1>2	none	contrast	contrast
P06-1010	77-81	82-88	but by combining the approaches	one can achieve even better results .	but by combining the approaches	one can achieve even better results .	69-88	69-88	Each of these approaches works quite well , but by combining the approaches one can achieve even better results .	Each of these approaches works quite well , but by combining the approaches one can achieve even better results .	1>2	none	manner-means	manner-means
P06-1010	44-51	82-88	We present two distinct methods for transliteration ,	one can achieve even better results .	We present two distinct methods for transliteration ,	one can achieve even better results .	44-68	69-88	We present two distinct methods for transliteration , one approach using phonetic transliteration , and the second using the temporal distribution of candidate pairs .	Each of these approaches works quite well , but by combining the approaches one can achieve even better results .	1<2	none	summary	summary
P06-1010	89-96	108-122	We then propose a novel score propagation method	This propagation method achieves further improvement over the best results from the previous step .	We then propose a novel score propagation method	This propagation method achieves further improvement over the best results from the previous step .	89-107	108-122	We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs .	This propagation method achieves further improvement over the best results from the previous step .	1>2	none	result	result
P06-1010	89-96	97-107	We then propose a novel score propagation method	that utilizes the co-occurrence of transliteration pairs within document pairs .	We then propose a novel score propagation method	that utilizes the co-occurrence of transliteration pairs within document pairs .	89-107	89-107	We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs .	We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs .	1<2	none	elab-addition	elab-addition
P06-1010	1-8	108-122	In this paper we investigate ChineseEnglish name transliteration	This propagation method achieves further improvement over the best results from the previous step .	In this paper we investigate ChineseEnglish name transliteration	This propagation method achieves further improvement over the best results from the previous step .	1-43	108-122	In this paper we investigate ChineseEnglish name transliteration using comparable corpora , corpora where texts in the two languages deal in some of the same topics — and therefore share references to named entities — but are not translations of each other .	This propagation method achieves further improvement over the best results from the previous step .	1<2	none	evaluation	evaluation
P06-1011	1-5	6-17	We present a novel method	for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	We present a novel method	for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	1-17	1-17	We present a novel method for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	We present a novel method for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	1<2	none	elab-addition	elab-addition
P06-1011	18-23	32-46	By analyzing potentially similar sentence pairs	which segments of the source sentence are translated into segments in the target sentence ,	By analyzing potentially similar sentence pairs	which segments of the source sentence are translated into segments in the target sentence ,	18-51	18-51	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	1>2	none	manner-means	manner-means
P06-1011	18-23	24-29	By analyzing potentially similar sentence pairs	using a signal processinginspired approach ,	By analyzing potentially similar sentence pairs	using a signal processinginspired approach ,	18-51	18-51	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	1<2	none	manner-means	manner-means
P06-1011	30-31	32-46	we detect	which segments of the source sentence are translated into segments in the target sentence ,	we detect	which segments of the source sentence are translated into segments in the target sentence ,	18-51	18-51	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	1>2	none	attribution	attribution
P06-1011	1-5	32-46	We present a novel method	which segments of the source sentence are translated into segments in the target sentence ,	We present a novel method	which segments of the source sentence are translated into segments in the target sentence ,	1-17	18-51	We present a novel method for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	1<2	none	elab-addition	elab-addition
P06-1011	32-46	47-51	which segments of the source sentence are translated into segments in the target sentence ,	and which are not .	which segments of the source sentence are translated into segments in the target sentence ,	and which are not .	18-51	18-51	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	By analyzing potentially similar sentence pairs using a signal processinginspired approach , we detect which segments of the source sentence are translated into segments in the target sentence , and which are not .	1<2	none	joint	joint
P06-1011	1-5	52-68	We present a novel method	This method enables us to extract useful machine translation training data even from very non-parallel corpora ,	We present a novel method	This method enables us to extract useful machine translation training data even from very non-parallel corpora ,	1-17	52-75	We present a novel method for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	This method enables us to extract useful machine translation training data even from very non-parallel corpora , which contain no parallel sentence pairs .	1<2	none	elab-addition	elab-addition
P06-1011	52-68	69-75	This method enables us to extract useful machine translation training data even from very non-parallel corpora ,	which contain no parallel sentence pairs .	This method enables us to extract useful machine translation training data even from very non-parallel corpora ,	which contain no parallel sentence pairs .	52-75	52-75	This method enables us to extract useful machine translation training data even from very non-parallel corpora , which contain no parallel sentence pairs .	This method enables us to extract useful machine translation training data even from very non-parallel corpora , which contain no parallel sentence pairs .	1<2	none	elab-addition	elab-addition
P06-1011	1-5	76-83	We present a novel method	We evaluate the quality of the extracted data	We present a novel method	We evaluate the quality of the extracted data	1-17	76-98	We present a novel method for extracting parallel sub-sentential fragments from comparable , non-parallel bilingual corpora .	We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .	1<2	none	evaluation	evaluation
P06-1011	84-85	86-98	by showing	that it improves the performance of a state-of-the-art statistical machine translation system .	by showing	that it improves the performance of a state-of-the-art statistical machine translation system .	76-98	76-98	We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .	We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .	1>2	none	attribution	attribution
P06-1011	76-83	86-98	We evaluate the quality of the extracted data	that it improves the performance of a state-of-the-art statistical machine translation system .	We evaluate the quality of the extracted data	that it improves the performance of a state-of-the-art statistical machine translation system .	76-98	76-98	We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .	We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system .	1<2	none	manner-means	manner-means
P06-1012	1-4,9-25	47-51	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	This paper presents a method	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	This paper presents a method	1-25	47-78	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1>2	none	bg-goal	bg-goal
P06-1012	1-4,9-25	5-8	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	drawn from different domains	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	drawn from different domains	1-25	1-25	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .	1<2	none	elab-addition	elab-addition
P06-1012	1-4,9-25	26-39	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems	Instances of a word <*> may have different sense priors ( the proportions of the different senses of a word ) .	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems	1-25	26-46	Instances of a word drawn from different domains may have different sense priors ( the proportions of the different senses of a word ) .	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems trained and applied on different domains .	1<2	none	elab-addition	elab-addition
P06-1012	26-39	40-46	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems	trained and applied on different domains .	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems	trained and applied on different domains .	26-46	26-46	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems trained and applied on different domains .	This in turn affects the accuracy of word sense disambiguation ( WSD ) systems trained and applied on different domains .	1<2	none	elab-addition	elab-addition
P06-1012	47-51	52-58	This paper presents a method	to estimate the sense priors of words	This paper presents a method	to estimate the sense priors of words	47-78	47-78	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1<2	none	elab-addition	elab-addition
P06-1012	52-58	59-64	to estimate the sense priors of words	drawn from a new domain ,	to estimate the sense priors of words	drawn from a new domain ,	47-78	47-78	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1<2	none	elab-addition	elab-addition
P06-1012	52-58	65-68	to estimate the sense priors of words	and highlights the importance	to estimate the sense priors of words	and highlights the importance	47-78	47-78	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1<2	none	joint	joint
P06-1012	65-68	69-73	and highlights the importance	of using well calibrated probabilities	and highlights the importance	of using well calibrated probabilities	47-78	47-78	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1<2	none	elab-addition	elab-addition
P06-1012	65-68	74-78	and highlights the importance	when performing these estimations .	and highlights the importance	when performing these estimations .	47-78	47-78	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	1<2	none	condition	condition
P06-1012	79-84	85-93	By using well calibrated probabilities ,	we are able to estimate the sense priors effectively	By using well calibrated probabilities ,	we are able to estimate the sense priors effectively	79-101	79-101	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .	1>2	none	manner-means	manner-means
P06-1012	47-51	85-93	This paper presents a method	we are able to estimate the sense priors effectively	This paper presents a method	we are able to estimate the sense priors effectively	47-78	79-101	This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .	1<2	none	evaluation	evaluation
P06-1012	85-93	94-101	we are able to estimate the sense priors effectively	to achieve significant improvements in WSD accuracy .	we are able to estimate the sense priors effectively	to achieve significant improvements in WSD accuracy .	79-101	79-101	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .	By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .	1<2	none	enablement	enablement
P06-1013	1-6	12-23	Combination methods are an effective way	This paper examines the benefits of system combination for unsupervised WSD .	Combination methods are an effective way	This paper examines the benefits of system combination for unsupervised WSD .	1-11	12-23	Combination methods are an effective way of improving system performance .	This paper examines the benefits of system combination for unsupervised WSD .	1>2	none	bg-goal	bg-goal
P06-1013	1-6	7-11	Combination methods are an effective way	of improving system performance .	Combination methods are an effective way	of improving system performance .	1-11	1-11	Combination methods are an effective way of improving system performance .	Combination methods are an effective way of improving system performance .	1<2	none	elab-addition	elab-addition
P06-1013	12-23	24-40	This paper examines the benefits of system combination for unsupervised WSD .	We investigate several voting- and arbiterbased combination strategies over a diverse pool of unsupervised WSD systems .	This paper examines the benefits of system combination for unsupervised WSD .	We investigate several voting- and arbiterbased combination strategies over a diverse pool of unsupervised WSD systems .	12-23	24-40	This paper examines the benefits of system combination for unsupervised WSD .	We investigate several voting- and arbiterbased combination strategies over a diverse pool of unsupervised WSD systems .	1<2	none	elab-addition	elab-addition
P06-1013	12-23	41-47	This paper examines the benefits of system combination for unsupervised WSD .	Our combination methods rely on predominant senses	This paper examines the benefits of system combination for unsupervised WSD .	Our combination methods rely on predominant senses	12-23	41-55	This paper examines the benefits of system combination for unsupervised WSD .	Our combination methods rely on predominant senses which are derived automatically from raw text .	1<2	none	elab-addition	elab-addition
P06-1013	41-47	48-55	Our combination methods rely on predominant senses	which are derived automatically from raw text .	Our combination methods rely on predominant senses	which are derived automatically from raw text .	41-55	41-55	Our combination methods rely on predominant senses which are derived automatically from raw text .	Our combination methods rely on predominant senses which are derived automatically from raw text .	1<2	none	elab-addition	elab-addition
P06-1013	12-23	56,65-71	This paper examines the benefits of system combination for unsupervised WSD .	Experiments <*> that our ensembles yield signifi-cantly better results	This paper examines the benefits of system combination for unsupervised WSD .	Experiments <*> that our ensembles yield signifi-cantly better results	12-23	56-76	This paper examines the benefits of system combination for unsupervised WSD .	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	1<2	none	evaluation	evaluation
P06-1013	56,65-71	57-63	Experiments <*> that our ensembles yield signifi-cantly better results	using the SemCor and Senseval-3 data sets	Experiments <*> that our ensembles yield signifi-cantly better results	using the SemCor and Senseval-3 data sets	56-76	56-76	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	1<2	none	elab-addition	elab-addition
P06-1013	64	65-71	demonstrate	that our ensembles yield signifi-cantly better results	demonstrate	that our ensembles yield signifi-cantly better results	56-76	56-76	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	1>2	none	attribution	attribution
P06-1013	65-71	72-76	that our ensembles yield signifi-cantly better results	when compared with state-of-the-art .	that our ensembles yield signifi-cantly better results	when compared with state-of-the-art .	56-76	56-76	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	Experiments using the SemCor and Senseval-3 data sets demonstrate that our ensembles yield signifi-cantly better results when compared with state-of-the-art .	1<2	none	comparison	comparison
P06-1014	1-15	16-23	Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation .	In this paper , we present a method	Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation .	In this paper , we present a method	1-15	16-52	Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation .	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	1>2	none	bg-goal	bg-goal
P06-1014	16-23	24-32	In this paper , we present a method	for reducing the granularity of the WordNet sense inventory	In this paper , we present a method	for reducing the granularity of the WordNet sense inventory	16-52	16-52	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	1<2	none	elab-addition	elab-addition
P06-1014	16-23	33-41	In this paper , we present a method	based on the mapping to a manually crafted dictionary	In this paper , we present a method	based on the mapping to a manually crafted dictionary	16-52	16-52	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	1<2	none	bg-general	bg-general
P06-1014	33-41	42-45	based on the mapping to a manually crafted dictionary	encoding sense hierarchies ,	based on the mapping to a manually crafted dictionary	encoding sense hierarchies ,	16-52	16-52	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	1<2	none	elab-addition	elab-addition
P06-1014	33-41	46-52	based on the mapping to a manually crafted dictionary	namely the Oxford Dictionary of English .	based on the mapping to a manually crafted dictionary	namely the Oxford Dictionary of English .	16-52	16-52	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	1<2	none	elab-addition	elab-addition
P06-1014	16-23	53-64	In this paper , we present a method	We assess the quality of the mapping and the induced clustering ,	In this paper , we present a method	We assess the quality of the mapping and the induced clustering ,	16-52	53-79	In this paper , we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies , namely the Oxford Dictionary of English .	We assess the quality of the mapping and the induced clustering , and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task .	1<2	none	evaluation	evaluation
P06-1014	53-64	65-79	We assess the quality of the mapping and the induced clustering ,	and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task .	We assess the quality of the mapping and the induced clustering ,	and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task .	53-79	53-79	We assess the quality of the mapping and the induced clustering , and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task .	We assess the quality of the mapping and the induced clustering , and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task .	1<2	none	joint	joint
P06-1015	1-16	17-21	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	for harvesting semantic relations .	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	for harvesting semantic relations .	1-21	1-21	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm for harvesting semantic relations .	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm for harvesting semantic relations .	1<2	none	enablement	enablement
P06-1015	1-16	22-26	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	The main contributions are :	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	The main contributions are :	1-21	22-58	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm for harvesting semantic relations .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	elab-addition	elab-addition
P06-1015	22-26	27-34	The main contributions are :	i ) a method for exploiting generic patterns	The main contributions are :	i ) a method for exploiting generic patterns	22-58	22-58	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	elab-enumember	elab-enumember
P06-1015	27-34	35-38	i ) a method for exploiting generic patterns	by filtering incorrect instances	i ) a method for exploiting generic patterns	by filtering incorrect instances	22-58	22-58	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	manner-means	manner-means
P06-1015	35-38	39-42	by filtering incorrect instances	using the Web ;	by filtering incorrect instances	using the Web ;	22-58	22-58	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	manner-means	manner-means
P06-1015	22-26	43-53	The main contributions are :	and ii ) a principled measure of pattern and instance reliability	The main contributions are :	and ii ) a principled measure of pattern and instance reliability	22-58	22-58	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	elab-enumember	elab-enumember
P06-1015	43-53	54-58	and ii ) a principled measure of pattern and instance reliability	enabling the filtering algorithm .	and ii ) a principled measure of pattern and instance reliability	enabling the filtering algorithm .	22-58	22-58	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	The main contributions are : i ) a method for exploiting generic patterns by filtering incorrect instances using the Web ; and ii ) a principled measure of pattern and instance reliability enabling the filtering algorithm .	1<2	none	elab-addition	elab-addition
P06-1015	1-16	59-80	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	1-21	59-88	In this paper , we present Espresso , a weakly-supervised , general-purpose , and accurate algorithm for harvesting semantic relations .	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations .	1<2	none	elab-addition	elab-addition
P06-1015	59-80	81-88	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	on extracting various general and specific relations .	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	on extracting various general and specific relations .	59-88	59-88	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations .	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations .	1<2	none	elab-addition	elab-addition
P06-1015	89-91	92-108	Experimental results show	that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	Experimental results show	that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	89-108	89-108	Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	1>2	none	attribution	attribution
P06-1015	59-80	92-108	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora ,	that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	59-88	89-108	We present an empirical comparison of Espresso with various state of the art systems , on different size and genre corpora , on extracting various general and specific relations .	Experimental results show that our exploitation of generic patterns substantially increases system recall with small effect on overall precision .	1<2	none	evaluation	evaluation
P06-1016	1-8	9-18	This paper proposes a novel hierarchical learning strategy	to deal with the data sparseness problem in relation extraction	This paper proposes a novel hierarchical learning strategy	to deal with the data sparseness problem in relation extraction	1-26	1-26	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	1<2	none	enablement	enablement
P06-1016	9-18	19-26	to deal with the data sparseness problem in relation extraction	by modeling the commonality among related classes .	to deal with the data sparseness problem in relation extraction	by modeling the commonality among related classes .	1-26	1-26	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	1<2	none	manner-means	manner-means
P06-1016	27-32	40-49	For each class in the hierarchy	a linear discriminative function is determined in a topdown way	For each class in the hierarchy	a linear discriminative function is determined in a topdown way	27-65	27-65	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1>2	none	bg-general	bg-general
P06-1016	27-32	33-35	For each class in the hierarchy	either manually predefined	For each class in the hierarchy	either manually predefined	27-65	27-65	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1<2	none	elab-addition	elab-addition
P06-1016	33-35	36-39	either manually predefined	or automatically clustered ,	either manually predefined	or automatically clustered ,	27-65	27-65	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1<2	none	joint	joint
P06-1016	1-8	40-49	This paper proposes a novel hierarchical learning strategy	a linear discriminative function is determined in a topdown way	This paper proposes a novel hierarchical learning strategy	a linear discriminative function is determined in a topdown way	1-26	27-65	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1<2	none	elab-addition	elab-addition
P06-1016	40-49	50-58	a linear discriminative function is determined in a topdown way	using a perceptron algorithm with the lower-level weight vector	a linear discriminative function is determined in a topdown way	using a perceptron algorithm with the lower-level weight vector	27-65	27-65	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1<2	none	manner-means	manner-means
P06-1016	50-58	59-65	using a perceptron algorithm with the lower-level weight vector	derived from the upper-level weight vector .	using a perceptron algorithm with the lower-level weight vector	derived from the upper-level weight vector .	27-65	27-65	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	1<2	none	elab-addition	elab-addition
P06-1016	66-81	82-92	As the upper-level class normally has much more positive training examples than the lower-level class ,	the corresponding linear discriminative function can be determined more reliably .	As the upper-level class normally has much more positive training examples than the lower-level class ,	the corresponding linear discriminative function can be determined more reliably .	66-92	66-92	As the upper-level class normally has much more positive training examples than the lower-level class , the corresponding linear discriminative function can be determined more reliably .	As the upper-level class normally has much more positive training examples than the lower-level class , the corresponding linear discriminative function can be determined more reliably .	1>2	none	exp-reason	exp-reason
P06-1016	40-49	82-92	a linear discriminative function is determined in a topdown way	the corresponding linear discriminative function can be determined more reliably .	a linear discriminative function is determined in a topdown way	the corresponding linear discriminative function can be determined more reliably .	27-65	66-92	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	As the upper-level class normally has much more positive training examples than the lower-level class , the corresponding linear discriminative function can be determined more reliably .	1<2	none	elab-addition	elab-addition
P06-1016	40-49	93-108	a linear discriminative function is determined in a topdown way	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level ,	a linear discriminative function is determined in a topdown way	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level ,	27-65	93-117	For each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector .	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level , which otherwise might suffer from limited training data .	1<2	none	elab-addition	elab-addition
P06-1016	93-108	109-117	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level ,	which otherwise might suffer from limited training data .	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level ,	which otherwise might suffer from limited training data .	93-117	93-117	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level , which otherwise might suffer from limited training data .	The upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level , which otherwise might suffer from limited training data .	1<2	none	elab-addition	elab-addition
P06-1016	118-125	126-147	Evaluation on the ACE RDC 2003 corpus shows	that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	Evaluation on the ACE RDC 2003 corpus shows	that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	118-147	118-147	Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	1>2	none	attribution	attribution
P06-1016	1-8	126-147	This paper proposes a novel hierarchical learning strategy	that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	This paper proposes a novel hierarchical learning strategy	that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	1-26	118-147	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	Evaluation on the ACE RDC 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in F-measure on least- and medium- frequent relations respectively .	1<2	none	evaluation	evaluation
P06-1016	148-150	151-166	It also shows	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	It also shows	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	148-172	148-172	It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set .	It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set .	1>2	none	attribution	attribution
P06-1016	1-8	151-166	This paper proposes a novel hierarchical learning strategy	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	This paper proposes a novel hierarchical learning strategy	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	1-26	148-172	This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes .	It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set .	1<2	none	evaluation	evaluation
P06-1016	151-166	167-172	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	using the same feature set .	that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes	using the same feature set .	148-172	148-172	It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set .	It also shows that our system outperforms the previous best-reported system by 2.7 in F-measure on the 24 subtypes using the same feature set .	1<2	none	manner-means	manner-means
P06-1017	1-14	15-38	Shortage of manually labeled data is an obstacle to supervised relation extraction methods .	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	Shortage of manually labeled data is an obstacle to supervised relation extraction methods .	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	1-14	15-38	Shortage of manually labeled data is an obstacle to supervised relation extraction methods .	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	1>2	none	bg-goal	bg-goal
P06-1017	15-38	39-59	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph ,	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph ,	15-38	39-93	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	1<2	none	elab-addition	elab-addition
P06-1017	39-59	60-66	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph ,	and tries to obtain a labeling function	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph ,	and tries to obtain a labeling function	39-93	39-93	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	1<2	none	joint	joint
P06-1017	60-66	67-71	and tries to obtain a labeling function	to satisfy two constraints :	and tries to obtain a labeling function	to satisfy two constraints :	39-93	39-93	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	1<2	none	enablement	enablement
P06-1017	67-71	72-82	to satisfy two constraints :	1 ) it should be fixed on the labeled nodes ,	to satisfy two constraints :	1 ) it should be fixed on the labeled nodes ,	39-93	39-93	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	1<2	none	elab-enumember	elab-enumember
P06-1017	67-71	83-93	to satisfy two constraints :	2 ) it should be smooth on the whole graph .	to satisfy two constraints :	2 ) it should be smooth on the whole graph .	39-93	39-93	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 ) it should be fixed on the labeled nodes , 2 ) it should be smooth on the whole graph .	1<2	none	elab-enumember	elab-enumember
P06-1017	94-100	101-109	Experiment results on the ACE corpus showed	that this LP algorithm achieves better performance than SVM	Experiment results on the ACE corpus showed	that this LP algorithm achieves better performance than SVM	94-131	94-131	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	1>2	none	attribution	attribution
P06-1017	15-38	101-109	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	that this LP algorithm achieves better performance than SVM	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	that this LP algorithm achieves better performance than SVM	15-38	94-131	In this paper we investigate a graph based semi-supervised learning algorithm , a label propagation ( LP ) algorithm , for relation extraction .	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	1<2	none	evaluation	evaluation
P06-1017	101-109	110-118	that this LP algorithm achieves better performance than SVM	when only very few labeled examples are available ,	that this LP algorithm achieves better performance than SVM	when only very few labeled examples are available ,	94-131	94-131	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	1<2	none	condition	condition
P06-1017	101-109	119-131	that this LP algorithm achieves better performance than SVM	and it also performs better than bootstrapping for the relation extraction task .	that this LP algorithm achieves better performance than SVM	and it also performs better than bootstrapping for the relation extraction task .	94-131	94-131	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	Experiment results on the ACE corpus showed that this LP algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the relation extraction task .	1<2	none	joint	joint
P06-1018	1-14	15-26	This paper proposes a generic mathematical formalism for the combination of various structures :	strings , trees , dags , graphs and products of them .	This paper proposes a generic mathematical formalism for the combination of various structures :	strings , trees , dags , graphs and products of them .	1-26	1-26	This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs and products of them .	This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs and products of them .	1<2	none	elab-enumember	elab-enumember
P06-1018	1-14	27-43	This paper proposes a generic mathematical formalism for the combination of various structures :	The polarization of the objects of the elementary structures controls the saturation of the final structure .	This paper proposes a generic mathematical formalism for the combination of various structures :	The polarization of the objects of the elementary structures controls the saturation of the final structure .	1-26	27-43	This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs and products of them .	The polarization of the objects of the elementary structures controls the saturation of the final structure .	1<2	none	elab-addition	elab-addition
P06-1018	1-14	44-51	This paper proposes a generic mathematical formalism for the combination of various structures :	This formalism is both elementary and powerful enough	This paper proposes a generic mathematical formalism for the combination of various structures :	This formalism is both elementary and powerful enough	1-26	44-72	This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , dags , graphs and products of them .	This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	1<2	none	evaluation	evaluation
P06-1018	44-51	52-58	This formalism is both elementary and powerful enough	to strongly simulate many grammar formalisms ,	This formalism is both elementary and powerful enough	to strongly simulate many grammar formalisms ,	44-72	44-72	This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	1<2	none	enablement	enablement
P06-1018	52-58	59-72	to strongly simulate many grammar formalisms ,	such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	to strongly simulate many grammar formalisms ,	such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	44-72	44-72	This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .	1<2	none	elab-example	elab-example
P06-1019	20-32	33-45	Much of the information in such grammars is encoded in the signature ,	and hence the key is facilitating a modularized development of type signatures .	Much of the information in such grammars is encoded in the signature ,	and hence the key is facilitating a modularized development of type signatures .	20-45	20-45	Much of the information in such grammars is encoded in the signature , and hence the key is facilitating a modularized development of type signatures .	Much of the information in such grammars is encoded in the signature , and hence the key is facilitating a modularized development of type signatures .	1>2	none	progression	progression
P06-1019	1-19	33-45	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	and hence the key is facilitating a modularized development of type signatures .	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	and hence the key is facilitating a modularized development of type signatures .	1-19	20-45	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	Much of the information in such grammars is encoded in the signature , and hence the key is facilitating a modularized development of type signatures .	1<2	none	elab-addition	elab-addition
P06-1019	46-52	55-59	We introduce a definition of signature modules	how two modules combine .	We introduce a definition of signature modules	how two modules combine .	46-59	46-59	We introduce a definition of signature modules and show how two modules combine .	We introduce a definition of signature modules and show how two modules combine .	1>2	none	progression	progression
P06-1019	53-54	55-59	and show	how two modules combine .	and show	how two modules combine .	46-59	46-59	We introduce a definition of signature modules and show how two modules combine .	We introduce a definition of signature modules and show how two modules combine .	1>2	none	attribution	attribution
P06-1019	1-19	55-59	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	how two modules combine .	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	how two modules combine .	1-19	46-59	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	We introduce a definition of signature modules and show how two modules combine .	1<2	none	elab-addition	elab-addition
P06-1019	1-19	60-70	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	Our definitions are motivated by the actual needs of grammar developers	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	Our definitions are motivated by the actual needs of grammar developers	1-19	60-80	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	Our definitions are motivated by the actual needs of grammar developers obtained through a careful examination of large scale grammars .	1<2	none	elab-addition	elab-addition
P06-1019	60-70	71-80	Our definitions are motivated by the actual needs of grammar developers	obtained through a careful examination of large scale grammars .	Our definitions are motivated by the actual needs of grammar developers	obtained through a careful examination of large scale grammars .	60-80	60-80	Our definitions are motivated by the actual needs of grammar developers obtained through a careful examination of large scale grammars .	Our definitions are motivated by the actual needs of grammar developers obtained through a careful examination of large scale grammars .	1<2	none	elab-addition	elab-addition
P06-1019	81-82	83-88	We show	that our definitions meet these needs	We show	that our definitions meet these needs	81-97	81-97	We show that our definitions meet these needs by conforming to a detailed set of desiderata .	We show that our definitions meet these needs by conforming to a detailed set of desiderata .	1>2	none	attribution	attribution
P06-1019	1-19	83-88	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	that our definitions meet these needs	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	that our definitions meet these needs	1-19	81-97	This work provides the essential foundations for modular construction of ( typed ) unification grammars for natural languages .	We show that our definitions meet these needs by conforming to a detailed set of desiderata .	1<2	none	evaluation	evaluation
P06-1019	83-88	89-97	that our definitions meet these needs	by conforming to a detailed set of desiderata .	that our definitions meet these needs	by conforming to a detailed set of desiderata .	81-97	81-97	We show that our definitions meet these needs by conforming to a detailed set of desiderata .	We show that our definitions meet these needs by conforming to a detailed set of desiderata .	1<2	none	manner-means	manner-means
P06-1020	1-11	12-21	This paper investigates the use of sublexical units as a solution	to handling the complex morphology with productive derivational processes ,	This paper investigates the use of sublexical units as a solution	to handling the complex morphology with productive derivational processes ,	1-32	1-32	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	1<2	none	elab-addition	elab-addition
P06-1020	12-21	22-32	to handling the complex morphology with productive derivational processes ,	in the development of a lexical functional grammar for Turkish .	to handling the complex morphology with productive derivational processes ,	in the development of a lexical functional grammar for Turkish .	1-32	1-32	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	1<2	none	elab-addition	elab-addition
P06-1020	1-11	33-57	This paper investigates the use of sublexical units as a solution	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	This paper investigates the use of sublexical units as a solution	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	1-32	33-57	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	1<2	none	elab-addition	elab-addition
P06-1020	33-57	58-68	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	This in turn leads to more succinct and manageable rules .	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	This in turn leads to more succinct and manageable rules .	33-57	58-68	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	This in turn leads to more succinct and manageable rules .	1<2	none	elab-addition	elab-addition
P06-1020	33-57	69-84	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	Further , the semantics of the derivations can also be systematically reflected in a compositional way	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	Further , the semantics of the derivations can also be systematically reflected in a compositional way	33-57	69-92	Such sublexical units make it possible to expose the internal structure of words with multiple derivations to the grammar rules in a uniform manner .	Further , the semantics of the derivations can also be systematically reflected in a compositional way by constructing PRED values on the fly .	1<2	none	elab-addition	elab-addition
P06-1020	69-84	85-92	Further , the semantics of the derivations can also be systematically reflected in a compositional way	by constructing PRED values on the fly .	Further , the semantics of the derivations can also be systematically reflected in a compositional way	by constructing PRED values on the fly .	69-92	69-92	Further , the semantics of the derivations can also be systematically reflected in a compositional way by constructing PRED values on the fly .	Further , the semantics of the derivations can also be systematically reflected in a compositional way by constructing PRED values on the fly .	1<2	none	manner-means	manner-means
P06-1020	93-94	95-99	We illustrate	how we use sublexical units	We illustrate	how we use sublexical units	93-120	93-120	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	1>2	none	attribution	attribution
P06-1020	1-11	95-99	This paper investigates the use of sublexical units as a solution	how we use sublexical units	This paper investigates the use of sublexical units as a solution	how we use sublexical units	1-32	93-120	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	1<2	none	elab-addition	elab-addition
P06-1020	95-99	100-109	how we use sublexical units	for handling simple productive derivational morphology and more interesting cases	how we use sublexical units	for handling simple productive derivational morphology and more interesting cases	93-120	93-120	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	1<2	none	enablement	enablement
P06-1020	100-109	110-115	for handling simple productive derivational morphology and more interesting cases	such as causativization , etc. ,	for handling simple productive derivational morphology and more interesting cases	such as causativization , etc. ,	93-120	93-120	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	1<2	none	elab-example	elab-example
P06-1020	100-109	116-120	for handling simple productive derivational morphology and more interesting cases	which change verb valency .	for handling simple productive derivational morphology and more interesting cases	which change verb valency .	93-120	93-120	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization , etc. , which change verb valency .	1<2	none	elab-addition	elab-addition
P06-1020	1-11	121-128	This paper investigates the use of sublexical units as a solution	Our priority is to handle several linguistic phenomena	This paper investigates the use of sublexical units as a solution	Our priority is to handle several linguistic phenomena	1-32	121-161	This paper investigates the use of sublexical units as a solution to handling the complex morphology with productive derivational processes , in the development of a lexical functional grammar for Turkish .	Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing , leaving the coverage and evaluation issues aside for the moment .	1<2	none	elab-addition	elab-addition
P06-1020	121-128	129-150	Our priority is to handle several linguistic phenomena	in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing ,	Our priority is to handle several linguistic phenomena	in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing ,	121-161	121-161	Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing , leaving the coverage and evaluation issues aside for the moment .	Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing , leaving the coverage and evaluation issues aside for the moment .	1<2	none	enablement	enablement
P06-1020	129-150	151-161	in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing ,	leaving the coverage and evaluation issues aside for the moment .	in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing ,	leaving the coverage and evaluation issues aside for the moment .	121-161	121-161	Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing , leaving the coverage and evaluation issues aside for the moment .	Our priority is to handle several linguistic phenomena in order to observe the effects of our approach on both the c-structure and the f-structure representation , and grammar writing , leaving the coverage and evaluation issues aside for the moment .	1<2	none	elab-addition	elab-addition
P06-1021	1-3,12-14	4-11	A grammatical method <*> is presented .	of combining two kinds of speech repair cues	A grammatical method <*> is presented .	of combining two kinds of speech repair cues	1-14	1-14	A grammatical method of combining two kinds of speech repair cues is presented .	A grammatical method of combining two kinds of speech repair cues is presented .	1<2	none	elab-addition	elab-addition
P06-1021	1-3,12-14	15-28	A grammatical method <*> is presented .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier	A grammatical method <*> is presented .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier	1-14	15-47	A grammatical method of combining two kinds of speech repair cues is presented .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	1<2	none	elab-aspect	elab-aspect
P06-1021	15-28	29-32	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier	that uses acoustic cues	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier	that uses acoustic cues	15-47	15-47	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	1<2	none	elab-addition	elab-addition
P06-1021	33-34	35-47	to identify	where normal prosody seems to be interrupted ( Lickley , 1996 ) .	to identify	where normal prosody seems to be interrupted ( Lickley , 1996 ) .	15-47	15-47	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	1>2	none	attribution	attribution
P06-1021	29-32	35-47	that uses acoustic cues	where normal prosody seems to be interrupted ( Lickley , 1996 ) .	that uses acoustic cues	where normal prosody seems to be interrupted ( Lickley , 1996 ) .	15-47	15-47	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	One cue , prosodic disjuncture , is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted ( Lickley , 1996 ) .	1<2	none	enablement	enablement
P06-1021	1-3,12-14	48-57	A grammatical method <*> is presented .	The other cue , syntactic parallelism , codifies the expectation	A grammatical method <*> is presented .	The other cue , syntactic parallelism , codifies the expectation	1-14	48-76	A grammatical method of combining two kinds of speech repair cues is presented .	The other cue , syntactic parallelism , codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum ( Levelt , 1983 ) .	1<2	none	elab-aspect	elab-aspect
P06-1021	48-57	58-63	The other cue , syntactic parallelism , codifies the expectation	that repairs continue a syntactic category	The other cue , syntactic parallelism , codifies the expectation	that repairs continue a syntactic category	48-76	48-76	The other cue , syntactic parallelism , codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum ( Levelt , 1983 ) .	The other cue , syntactic parallelism , codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum ( Levelt , 1983 ) .	1<2	none	elab-addition	elab-addition
P06-1021	58-63	64-76	that repairs continue a syntactic category	that was left unfinished in the reparandum ( Levelt , 1983 ) .	that repairs continue a syntactic category	that was left unfinished in the reparandum ( Levelt , 1983 ) .	48-76	48-76	The other cue , syntactic parallelism , codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum ( Levelt , 1983 ) .	The other cue , syntactic parallelism , codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum ( Levelt , 1983 ) .	1<2	none	elab-addition	elab-addition
P06-1021	1-3,12-14	77-85	A grammatical method <*> is presented .	The two cues are combined in a Treebank PCFG	A grammatical method <*> is presented .	The two cues are combined in a Treebank PCFG	1-14	77-96	A grammatical method of combining two kinds of speech repair cues is presented .	The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations .	1<2	none	elab-addition	elab-addition
P06-1021	77-85	86-89	The two cues are combined in a Treebank PCFG	whose states are split	The two cues are combined in a Treebank PCFG	whose states are split	77-96	77-96	The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations .	The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations .	1<2	none	elab-addition	elab-addition
P06-1021	86-89	90-96	whose states are split	using a few simple tree transformations .	whose states are split	using a few simple tree transformations .	77-96	77-96	The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations .	The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations .	1<2	none	manner-means	manner-means
P06-1021	97-105	106-119	Parsing performance on the Switchboard and Fisher corpora suggests	that these two cues help to locate speech repairs in a synergistic way .	Parsing performance on the Switchboard and Fisher corpora suggests	that these two cues help to locate speech repairs in a synergistic way .	97-119	97-119	Parsing performance on the Switchboard and Fisher corpora suggests that these two cues help to locate speech repairs in a synergistic way .	Parsing performance on the Switchboard and Fisher corpora suggests that these two cues help to locate speech repairs in a synergistic way .	1>2	none	attribution	attribution
P06-1021	1-3,12-14	106-119	A grammatical method <*> is presented .	that these two cues help to locate speech repairs in a synergistic way .	A grammatical method <*> is presented .	that these two cues help to locate speech repairs in a synergistic way .	1-14	97-119	A grammatical method of combining two kinds of speech repair cues is presented .	Parsing performance on the Switchboard and Fisher corpora suggests that these two cues help to locate speech repairs in a synergistic way .	1<2	none	evaluation	evaluation
P06-1022	1-9	41-51	Spoken monologues feature greater sentence length and structural complexity	This paper proposes a method for dependency parsing of Japanese monologues	Spoken monologues feature greater sentence length and structural complexity	This paper proposes a method for dependency parsing of Japanese monologues	1-14	41-56	Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues .	This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation .	1>2	none	bg-goal	bg-goal
P06-1022	1-9	10-14	Spoken monologues feature greater sentence length and structural complexity	than do spoken dialogues .	Spoken monologues feature greater sentence length and structural complexity	than do spoken dialogues .	1-14	1-14	Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues .	Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues .	1<2	none	comparison	comparison
P06-1022	15-23	24-31	To achieve high parsing performance for spoken monologues ,	it could prove effective to simplify the structure	To achieve high parsing performance for spoken monologues ,	it could prove effective to simplify the structure	15-40	15-40	To achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units .	To achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units .	1>2	none	enablement	enablement
P06-1022	1-9	24-31	Spoken monologues feature greater sentence length and structural complexity	it could prove effective to simplify the structure	Spoken monologues feature greater sentence length and structural complexity	it could prove effective to simplify the structure	1-14	15-40	Spoken monologues feature greater sentence length and structural complexity than do spoken dialogues .	To achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units .	1<2	none	elab-addition	elab-addition
P06-1022	24-31	32-40	it could prove effective to simplify the structure	by dividing a sentence into suitable language units .	it could prove effective to simplify the structure	by dividing a sentence into suitable language units .	15-40	15-40	To achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units .	To achieve high parsing performance for spoken monologues , it could prove effective to simplify the structure by dividing a sentence into suitable language units .	1<2	none	manner-means	manner-means
P06-1022	41-51	52-56	This paper proposes a method for dependency parsing of Japanese monologues	based on sentence segmentation .	This paper proposes a method for dependency parsing of Japanese monologues	based on sentence segmentation .	41-56	41-56	This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation .	This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation .	1<2	none	bg-general	bg-general
P06-1022	41-51	57-78	This paper proposes a method for dependency parsing of Japanese monologues	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	This paper proposes a method for dependency parsing of Japanese monologues	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	41-56	57-78	This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation .	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	1<2	none	elab-addition	elab-addition
P06-1022	57-78	79-87	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	First , the dependencies within a clause are identified	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	First , the dependencies within a clause are identified	57-78	79-102	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	First , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause .	1<2	none	elab-process_step	elab-process_step
P06-1022	79-87	88-93	First , the dependencies within a clause are identified	by dividing a sentence into clauses	First , the dependencies within a clause are identified	by dividing a sentence into clauses	79-102	79-102	First , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause .	First , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause .	1<2	none	manner-means	manner-means
P06-1022	88-93	94-102	by dividing a sentence into clauses	and executing stochastic dependency parsing for each clause .	by dividing a sentence into clauses	and executing stochastic dependency parsing for each clause .	79-102	79-102	First , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause .	First , the dependencies within a clause are identified by dividing a sentence into clauses and executing stochastic dependency parsing for each clause .	1<2	none	joint	joint
P06-1022	57-78	103-113	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	Next , the dependencies over clause boundaries are identified stochastically ,	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	Next , the dependencies over clause boundaries are identified stochastically ,	57-78	103-125	In this method , the dependency parsing is executed in two stages : at the clause level and the sentence level .	Next , the dependencies over clause boundaries are identified stochastically , and the dependency structure of the entire sentence is thus completed .	1<2	none	elab-process_step	elab-process_step
P06-1022	103-113	114-125	Next , the dependencies over clause boundaries are identified stochastically ,	and the dependency structure of the entire sentence is thus completed .	Next , the dependencies over clause boundaries are identified stochastically ,	and the dependency structure of the entire sentence is thus completed .	103-125	103-125	Next , the dependencies over clause boundaries are identified stochastically , and the dependency structure of the entire sentence is thus completed .	Next , the dependencies over clause boundaries are identified stochastically , and the dependency structure of the entire sentence is thus completed .	1<2	none	joint	joint
P06-1022	126-127	133-147	An experiment	shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	An experiment	shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	126-147	126-147	An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	1>2	none	attribution	attribution
P06-1022	126-127	128-132	An experiment	using a spoken monologue corpus	An experiment	using a spoken monologue corpus	126-147	126-147	An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	1<2	none	manner-means	manner-means
P06-1022	41-51	133-147	This paper proposes a method for dependency parsing of Japanese monologues	shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	This paper proposes a method for dependency parsing of Japanese monologues	shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	41-56	126-147	This paper proposes a method for dependency parsing of Japanese monologues based on sentence segmentation .	An experiment using a spoken monologue corpus shows this method to be effective for efficient dependency parsing of Japanese monologue sentences .	1<2	none	evaluation	evaluation
P06-1023	1-5	6-12	This paper describes a parser	which generates parse trees with empty elements	This paper describes a parser	which generates parse trees with empty elements	1-20	1-20	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	1<2	none	elab-addition	elab-addition
P06-1023	6-12	13-20	which generates parse trees with empty elements	in which traces and fillers are co-indexed .	which generates parse trees with empty elements	in which traces and fillers are co-indexed .	1-20	1-20	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	1<2	none	elab-addition	elab-addition
P06-1023	1-5	21-27	This paper describes a parser	The parser is an unlexicalized PCFG parser	This paper describes a parser	The parser is an unlexicalized PCFG parser	1-20	21-37	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse .	1<2	none	elab-addition	elab-addition
P06-1023	21-27	28-37	The parser is an unlexicalized PCFG parser	which is guaranteed to return the most probable parse .	The parser is an unlexicalized PCFG parser	which is guaranteed to return the most probable parse .	21-37	21-37	The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse .	The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse .	1<2	none	elab-addition	elab-addition
P06-1023	1-5	38-48	This paper describes a parser	The grammar is extracted from a version of the PENN treebank	This paper describes a parser	The grammar is extracted from a version of the PENN treebank	1-20	38-65	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning ( 2003 ) .	1<2	none	elab-addition	elab-addition
P06-1023	38-48	49-65	The grammar is extracted from a version of the PENN treebank	which was automatically annotated with features in the style of Klein and Manning ( 2003 ) .	The grammar is extracted from a version of the PENN treebank	which was automatically annotated with features in the style of Klein and Manning ( 2003 ) .	38-65	38-65	The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning ( 2003 ) .	The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning ( 2003 ) .	1<2	none	elab-addition	elab-addition
P06-1023	1-5	66-71	This paper describes a parser	The annotation includes GPSG-style slash features	This paper describes a parser	The annotation includes GPSG-style slash features	1-20	66-87	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	The annotation includes GPSG-style slash features which link traces and fillers , and other features which improve the general parsing accuracy .	1<2	none	elab-addition	elab-addition
P06-1023	66-71	72-80	The annotation includes GPSG-style slash features	which link traces and fillers , and other features	The annotation includes GPSG-style slash features	which link traces and fillers , and other features	66-87	66-87	The annotation includes GPSG-style slash features which link traces and fillers , and other features which improve the general parsing accuracy .	The annotation includes GPSG-style slash features which link traces and fillers , and other features which improve the general parsing accuracy .	1<2	none	elab-addition	elab-addition
P06-1023	72-80	81-87	which link traces and fillers , and other features	which improve the general parsing accuracy .	which link traces and fillers , and other features	which improve the general parsing accuracy .	66-87	66-87	The annotation includes GPSG-style slash features which link traces and fillers , and other features which improve the general parsing accuracy .	The annotation includes GPSG-style slash features which link traces and fillers , and other features which improve the general parsing accuracy .	1<2	none	elab-addition	elab-addition
P06-1023	88-116	117-143	In an evaluation on the PENN treebank ( Marcus et al. , 1993 ) , the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore .	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	In an evaluation on the PENN treebank ( Marcus et al. , 1993 ) , the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore .	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	88-116	117-143	In an evaluation on the PENN treebank ( Marcus et al. , 1993 ) , the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing fscore .	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	1>2	none	result	result
P06-1023	1-5	117-143	This paper describes a parser	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	This paper describes a parser	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	1-20	117-143	This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed .	Its results for the empty category prediction task and the trace-filler coindexation task exceed all previously reported results with 84.1 % and 77.4 % fscore , respectively	1<2	none	evaluation	evaluation
P06-1024	1-36	37-38,41-74	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	The contexts <*> are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	The contexts <*> are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	1-36	37-109	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	elab-addition	elab-addition
P06-1024	37-38,41-74	39-40	The contexts <*> are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	we use	The contexts <*> are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	we use	37-109	37-109	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	elab-addition	elab-addition
P06-1024	41-74	75-80	are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	which use only slot-based information ,	are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) ,	which use only slot-based information ,	37-109	37-109	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	elab-addition	elab-addition
P06-1024	75-80	81-91	which use only slot-based information ,	but are much less complex than the full dialogue "Information States"	which use only slot-based information ,	but are much less complex than the full dialogue "Information States"	37-109	37-109	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	contrast	contrast
P06-1024	81-91	92-101	but are much less complex than the full dialogue "Information States"	explored in ( Henderson et al. , 2005 ) ,	but are much less complex than the full dialogue "Information States"	explored in ( Henderson et al. , 2005 ) ,	37-109	37-109	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	elab-addition	elab-addition
P06-1024	81-91	102-109	but are much less complex than the full dialogue "Information States"	for which tractabe learning is an issue .	but are much less complex than the full dialogue "Information States"	for which tractabe learning is an issue .	37-109	37-109	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	The contexts we use are richer than previous research in this area , e.g. ( Levin and Pieraccini , 1997 ; Scheffler and Young , 2001 ; Singh et al. , 2002 ; Pietquin , 2004 ) , which use only slot-based information , but are much less complex than the full dialogue "Information States" explored in ( Henderson et al. , 2005 ) , for which tractabe learning is an issue .	1<2	none	elab-addition	elab-addition
P06-1024	110-111	112-124	We explore	how incrementally adding richer features allows learning of more effective dialogue strategies .	We explore	how incrementally adding richer features allows learning of more effective dialogue strategies .	110-124	110-124	We explore how incrementally adding richer features allows learning of more effective dialogue strategies .	We explore how incrementally adding richer features allows learning of more effective dialogue strategies .	1>2	none	attribution	attribution
P06-1024	1-36	112-124	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	how incrementally adding richer features allows learning of more effective dialogue strategies .	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	how incrementally adding richer features allows learning of more effective dialogue strategies .	1-36	110-124	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	We explore how incrementally adding richer features allows learning of more effective dialogue strategies .	1<2	none	elab-addition	elab-addition
P06-1024	1-36	125-129	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	We use 2 user simulations	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	We use 2 user simulations	1-36	125-158	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	We use 2 user simulations learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b ) to explore the effects of different features on learned dialogue strategies .	1<2	none	elab-addition	elab-addition
P06-1024	125-129	130-146	We use 2 user simulations	learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b )	We use 2 user simulations	learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b )	125-158	125-158	We use 2 user simulations learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b ) to explore the effects of different features on learned dialogue strategies .	We use 2 user simulations learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b ) to explore the effects of different features on learned dialogue strategies .	1<2	none	elab-addition	elab-addition
P06-1024	125-129	147-158	We use 2 user simulations	to explore the effects of different features on learned dialogue strategies .	We use 2 user simulations	to explore the effects of different features on learned dialogue strategies .	125-158	125-158	We use 2 user simulations learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b ) to explore the effects of different features on learned dialogue strategies .	We use 2 user simulations learned from COMMUNICATOR data ( Walker et al. , 2001 ; Georgila et al. , 2005b ) to explore the effects of different features on learned dialogue strategies .	1<2	none	enablement	enablement
P06-1024	159-161	162-203	Our results show	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	Our results show	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	159-209	159-209	Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy that uses only slot-status features .	Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy that uses only slot-status features .	1>2	none	attribution	attribution
P06-1024	1-36	162-203	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	1-36	159-209	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy that uses only slot-status features .	1<2	none	evaluation	evaluation
P06-1024	162-203	204-209	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	that uses only slot-status features .	that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy	that uses only slot-status features .	159-209	159-209	Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy that uses only slot-status features .	Our results show that adding the dialogue moves of the last system and user turns increases the average reward of the automatically learned strategies by 65.9 % over the original ( hand-coded ) COMMUNICATOR systems , and by 7.8 % over a baseline RL policy that uses only slot-status features .	1<2	none	elab-addition	elab-addition
P06-1024	210-211	212-230	We show	that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	We show	that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	210-230	210-230	We show that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	We show that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	1>2	none	attribution	attribution
P06-1024	1-36	212-230	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	1-36	210-230	We explore the use of restricted dialogue contexts in reinforcement learning ( RL ) of effective dialogue strategies for information seeking spoken dialogue systems ( e.g. COMMUNICATOR ( Walker et al. , 2001 ) ) .	We show that the learned strategies exhibit an emergent "focus switching" strategy and effective use of the "give help" action .	1<2	none	evaluation	evaluation
P06-1025	1-12	21-33	Speech recognition problems are a reality in current spoken dialogue systems .	we study dependencies between speech recognition problems and several higher level dialogue factors	Speech recognition problems are a reality in current spoken dialogue systems .	we study dependencies between speech recognition problems and several higher level dialogue factors	1-12	13-47	Speech recognition problems are a reality in current spoken dialogue systems .	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	1>2	none	bg-goal	bg-goal
P06-1025	13-20	21-33	In order to better understand these phenomena ,	we study dependencies between speech recognition problems and several higher level dialogue factors	In order to better understand these phenomena ,	we study dependencies between speech recognition problems and several higher level dialogue factors	13-47	13-47	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	1>2	none	elab-addition	elab-addition
P06-1025	21-33	34-41	we study dependencies between speech recognition problems and several higher level dialogue factors	that define our notion of student state :	we study dependencies between speech recognition problems and several higher level dialogue factors	that define our notion of student state :	13-47	13-47	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	1<2	none	elab-addition	elab-addition
P06-1025	34-41	42-47	that define our notion of student state :	frustration/anger , certainty and correctness .	that define our notion of student state :	frustration/anger , certainty and correctness .	13-47	13-47	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	1<2	none	elab-enumember	elab-enumember
P06-1025	21-33	48-63	we study dependencies between speech recognition problems and several higher level dialogue factors	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues	we study dependencies between speech recognition problems and several higher level dialogue factors	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues	13-47	48-73	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns .	1<2	none	elab-addition	elab-addition
P06-1025	48-63	64-73	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues	to discover these dependencies both within and across turns .	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues	to discover these dependencies both within and across turns .	48-73	48-73	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns .	We apply Chi Square ( χ2 ) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns .	1<2	none	enablement	enablement
P06-1025	21-33	74-77	we study dependencies between speech recognition problems and several higher level dialogue factors	Significant dependencies are combined	we study dependencies between speech recognition problems and several higher level dialogue factors	Significant dependencies are combined	13-47	74-95	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	1<2	none	elab-addition	elab-addition
P06-1025	74-77	78-85	Significant dependencies are combined	to produce interesting insights regarding speech recognition problems	Significant dependencies are combined	to produce interesting insights regarding speech recognition problems	74-95	74-95	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	1<2	none	enablement	enablement
P06-1025	78-85	86-90	to produce interesting insights regarding speech recognition problems	and to propose new strategies	to produce interesting insights regarding speech recognition problems	and to propose new strategies	74-95	74-95	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	1<2	none	joint	joint
P06-1025	86-90	91-95	and to propose new strategies	for handling these problems .	and to propose new strategies	for handling these problems .	74-95	74-95	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	Significant dependencies are combined to produce interesting insights regarding speech recognition problems and to propose new strategies for handling these problems .	1<2	none	enablement	enablement
P06-1025	96-98	99-115	We also find	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	We also find	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	96-122	96-122	We also find that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors to consider for spoken dialogue design .	We also find that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors to consider for spoken dialogue design .	1>2	none	attribution	attribution
P06-1025	21-33	99-115	we study dependencies between speech recognition problems and several higher level dialogue factors	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	we study dependencies between speech recognition problems and several higher level dialogue factors	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	13-47	96-122	In order to better understand these phenomena , we study dependencies between speech recognition problems and several higher level dialogue factors that define our notion of student state : frustration/anger , certainty and correctness .	We also find that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors to consider for spoken dialogue design .	1<2	none	evaluation	evaluation
P06-1025	99-115	116-122	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	to consider for spoken dialogue design .	that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors	to consider for spoken dialogue design .	96-122	96-122	We also find that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors to consider for spoken dialogue design .	We also find that tutoring , as a new domain for speech applications , exhibits interesting tradeoffs and new factors to consider for spoken dialogue design .	1<2	none	enablement	enablement
P06-1026	1-11	35-55	Data-driven techniques have been used for many computational linguistics tasks .	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	Data-driven techniques have been used for many computational linguistics tasks .	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	1-11	35-55	Data-driven techniques have been used for many computational linguistics tasks .	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	1>2	none	contrast	contrast
P06-1026	1-11	12,16-22	Data-driven techniques have been used for many computational linguistics tasks .	Models <*> are generally more robust than hand-crafted systems	Data-driven techniques have been used for many computational linguistics tasks .	Models <*> are generally more robust than hand-crafted systems	1-11	12-34	Data-driven techniques have been used for many computational linguistics tasks .	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	1<2	none	elab-addition	elab-addition
P06-1026	12,16-22	13-15	Models <*> are generally more robust than hand-crafted systems	derived from data	Models <*> are generally more robust than hand-crafted systems	derived from data	12-34	12-34	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	1<2	none	elab-addition	elab-addition
P06-1026	12,16-22	23-31	Models <*> are generally more robust than hand-crafted systems	since they better reflect the distribution of the phenomena	Models <*> are generally more robust than hand-crafted systems	since they better reflect the distribution of the phenomena	12-34	12-34	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	1<2	none	exp-reason	exp-reason
P06-1026	23-31	32-34	since they better reflect the distribution of the phenomena	being modeled .	since they better reflect the distribution of the phenomena	being modeled .	12-34	12-34	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	Models derived from data are generally more robust than hand-crafted systems since they better reflect the distribution of the phenomena being modeled .	1<2	none	elab-addition	elab-addition
P06-1026	35-55	56-63	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	In this paper , we compare two approaches	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	In this paper , we compare two approaches	35-55	56-86	With the availability of large corpora of spoken dialog , dialog management is now reaping the benefits of data-driven techniques .	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	1>2	none	bg-goal	bg-goal
P06-1026	56-63	64-70	In this paper , we compare two approaches	to modeling subtask structure in dialog :	In this paper , we compare two approaches	to modeling subtask structure in dialog :	56-86	56-86	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	1<2	none	enablement	enablement
P06-1026	64-70	71-86	to modeling subtask structure in dialog :	a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	to modeling subtask structure in dialog :	a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	56-86	56-86	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	1<2	none	elab-definition	elab-definition
P06-1026	56-63	87-90	In this paper , we compare two approaches	We evaluate these models	In this paper , we compare two approaches	We evaluate these models	56-86	87-100	In this paper , we compare two approaches to modeling subtask structure in dialog : a chunk-based model of subdialog sequences , and a parse-based , or hierarchical , model .	We evaluate these models using customer agent dialogs from a catalog service domain .	1<2	none	evaluation	evaluation
P06-1026	87-90	91-100	We evaluate these models	using customer agent dialogs from a catalog service domain .	We evaluate these models	using customer agent dialogs from a catalog service domain .	87-100	87-100	We evaluate these models using customer agent dialogs from a catalog service domain .	We evaluate these models using customer agent dialogs from a catalog service domain .	1<2	none	manner-means	manner-means
P06-1027	1-14	15-18	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	that can be used	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	that can be used	1-34	1-34	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	1<2	none	elab-addition	elab-addition
P06-1027	15-18	19-34	that can be used	to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	that can be used	to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	1-34	1-34	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	1<2	none	enablement	enablement
P06-1027	1-14	35-51	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	1-34	35-65	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	1<2	none	elab-addition	elab-addition
P06-1027	35-51	52-55	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	yielding a training objective	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	yielding a training objective	35-65	35-65	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	1<2	none	elab-addition	elab-addition
P06-1027	52-55	56-65	yielding a training objective	that combines unlabeled conditional entropy with labeled conditional likelihood .	yielding a training objective	that combines unlabeled conditional entropy with labeled conditional likelihood .	35-65	35-65	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	1<2	none	elab-addition	elab-addition
P06-1027	66-74	75-84	Although the training objective is no longer concave ,	it can still be used to improve an initial model	Although the training objective is no longer concave ,	it can still be used to improve an initial model	66-95	66-95	Although the training objective is no longer concave , it can still be used to improve an initial model ( e.g. obtained from supervised training ) by iterative ascent .	Although the training objective is no longer concave , it can still be used to improve an initial model ( e.g. obtained from supervised training ) by iterative ascent .	1>2	none	contrast	contrast
P06-1027	35-51	75-84	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	it can still be used to improve an initial model	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case ,	it can still be used to improve an initial model	35-65	66-95	Our approach is based on extending the minimum entropy regularization framework to the structured prediction case , yielding a training objective that combines unlabeled conditional entropy with labeled conditional likelihood .	Although the training objective is no longer concave , it can still be used to improve an initial model ( e.g. obtained from supervised training ) by iterative ascent .	1<2	none	elab-addition	elab-addition
P06-1027	75-84	85-95	it can still be used to improve an initial model	( e.g. obtained from supervised training ) by iterative ascent .	it can still be used to improve an initial model	( e.g. obtained from supervised training ) by iterative ascent .	66-95	66-95	Although the training objective is no longer concave , it can still be used to improve an initial model ( e.g. obtained from supervised training ) by iterative ascent .	Although the training objective is no longer concave , it can still be used to improve an initial model ( e.g. obtained from supervised training ) by iterative ascent .	1<2	none	manner-means	manner-means
P06-1027	96-104	117-131	We apply our new training algorithm to the problem	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	We apply our new training algorithm to the problem	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	96-131	96-131	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	1>2	none	joint	joint
P06-1027	96-104	105-114	We apply our new training algorithm to the problem	of identifying gene and protein mentions in biological texts ,	We apply our new training algorithm to the problem	of identifying gene and protein mentions in biological texts ,	96-131	96-131	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	1<2	none	elab-addition	elab-addition
P06-1027	115-116	117-131	and show	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	and show	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	96-131	96-131	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	1>2	none	attribution	attribution
P06-1027	1-14	117-131	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	We present a new semi-supervised training procedure for conditional random fields ( CRFs )	that incorporating unlabeled data improves the performance of the supervised CRF in this case .	1-34	96-131	We present a new semi-supervised training procedure for conditional random fields ( CRFs ) that can be used to train sequence segmentors and labelers from a combination of labeled and unlabeled training data .	We apply our new training algorithm to the problem of identifying gene and protein mentions in biological texts , and show that incorporating unlabeled data improves the performance of the supervised CRF in this case .	1<2	none	evaluation	evaluation
P06-1028	1-5	6-13	This paper proposes a framework	for training Conditional Random Fields ( CRFs )	This paper proposes a framework	for training Conditional Random Fields ( CRFs )	1-26	1-26	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	1<2	none	elab-addition	elab-addition
P06-1028	6-13	14-19	for training Conditional Random Fields ( CRFs )	to optimize multivariate evaluation measures ,	for training Conditional Random Fields ( CRFs )	to optimize multivariate evaluation measures ,	1-26	1-26	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	1<2	none	enablement	enablement
P06-1028	14-19	20-22	to optimize multivariate evaluation measures ,	including non-linear measures	to optimize multivariate evaluation measures ,	including non-linear measures	1-26	1-26	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	1<2	none	elab-addition	elab-addition
P06-1028	20-22	23-26	including non-linear measures	such as F-score .	including non-linear measures	such as F-score .	1-26	1-26	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	1<2	none	elab-example	elab-example
P06-1028	1-5	27-36	This paper proposes a framework	Our proposed framework is derived from an error minimization approach	This paper proposes a framework	Our proposed framework is derived from an error minimization approach	1-26	27-48	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure .	1<2	none	elab-addition	elab-addition
P06-1028	27-36	37-41	Our proposed framework is derived from an error minimization approach	that provides a simple solution	Our proposed framework is derived from an error minimization approach	that provides a simple solution	27-48	27-48	Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure .	Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure .	1<2	none	elab-addition	elab-addition
P06-1028	37-41	42-48	that provides a simple solution	for directly optimizing any evaluation measure .	that provides a simple solution	for directly optimizing any evaluation measure .	27-48	27-48	Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure .	Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure .	1<2	none	elab-addition	elab-addition
P06-1028	49-63	64-68	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition ,	we introduce a loss function	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition ,	we introduce a loss function	49-84	49-84	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	1>2	none	elab-addition	elab-addition
P06-1028	1-5	64-68	This paper proposes a framework	we introduce a loss function	This paper proposes a framework	we introduce a loss function	1-26	49-84	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	1<2	none	elab-addition	elab-addition
P06-1028	64-68	69-84	we introduce a loss function	that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	we introduce a loss function	that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	49-84	49-84	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	Specifically focusing on sequential segmentation tasks , i.e. text chunking and named entity recognition , we introduce a loss function that closely reflects the target evaluation measure for these tasks , namely , segmentation F-score .	1<2	none	elab-addition	elab-addition
P06-1028	85-87	88-97	Our experiments show	that our method performs better than standard CRF training .	Our experiments show	that our method performs better than standard CRF training .	85-97	85-97	Our experiments show that our method performs better than standard CRF training .	Our experiments show that our method performs better than standard CRF training .	1>2	none	attribution	attribution
P06-1028	1-5	88-97	This paper proposes a framework	that our method performs better than standard CRF training .	This paper proposes a framework	that our method performs better than standard CRF training .	1-26	85-97	This paper proposes a framework for training Conditional Random Fields ( CRFs ) to optimize multivariate evaluation measures , including non-linear measures such as F-score .	Our experiments show that our method performs better than standard CRF training .	1<2	none	evaluation	evaluation
P06-1029	1-12	29-43	Lasso is a regularization method for parameter estimation in linear models .	This paper explores the use of lasso for statistical language modeling for text input .	Lasso is a regularization method for parameter estimation in linear models .	This paper explores the use of lasso for statistical language modeling for text input .	1-12	29-43	Lasso is a regularization method for parameter estimation in linear models .	This paper explores the use of lasso for statistical language modeling for text input .	1>2	none	bg-goal	bg-goal
P06-1029	1-12	13-28	Lasso is a regularization method for parameter estimation in linear models .	It optimizes the model parameters with respect to a loss function subject to model complexities .	Lasso is a regularization method for parameter estimation in linear models .	It optimizes the model parameters with respect to a loss function subject to model complexities .	1-12	13-28	Lasso is a regularization method for parameter estimation in linear models .	It optimizes the model parameters with respect to a loss function subject to model complexities .	1<2	none	elab-addition	elab-addition
P06-1029	44-52	53-62	Owing to the very large number of parameters ,	directly optimizing the penalized lasso loss function is impossible .	Owing to the very large number of parameters ,	directly optimizing the penalized lasso loss function is impossible .	44-62	44-62	Owing to the very large number of parameters , directly optimizing the penalized lasso loss function is impossible .	Owing to the very large number of parameters , directly optimizing the penalized lasso loss function is impossible .	1>2	none	exp-reason	exp-reason
P06-1029	53-62	63-70	directly optimizing the penalized lasso loss function is impossible .	Therefore , we investigate two approximation methods ,	directly optimizing the penalized lasso loss function is impossible .	Therefore , we investigate two approximation methods ,	44-62	63-86	Owing to the very large number of parameters , directly optimizing the penalized lasso loss function is impossible .	Therefore , we investigate two approximation methods , the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	1>2	none	result	result
P06-1029	29-43	63-70	This paper explores the use of lasso for statistical language modeling for text input .	Therefore , we investigate two approximation methods ,	This paper explores the use of lasso for statistical language modeling for text input .	Therefore , we investigate two approximation methods ,	29-43	63-86	This paper explores the use of lasso for statistical language modeling for text input .	Therefore , we investigate two approximation methods , the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	1<2	none	elab-addition	elab-addition
P06-1029	63-70	71-86	Therefore , we investigate two approximation methods ,	the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	Therefore , we investigate two approximation methods ,	the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	63-86	63-86	Therefore , we investigate two approximation methods , the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	Therefore , we investigate two approximation methods , the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	1<2	none	elab-enumember	elab-enumember
P06-1029	63-70	87-89,98-104	Therefore , we investigate two approximation methods ,	Both methods , <*> bear strong resemblance to the boosting algorithm	Therefore , we investigate two approximation methods ,	Both methods , <*> bear strong resemblance to the boosting algorithm	63-86	87-117	Therefore , we investigate two approximation methods , the boosted lasso ( BLasso ) and the forward stagewise linear regression ( FSLR ) .	Both methods , when used with the exponential loss function , bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling .	1<2	none	elab-addition	elab-addition
P06-1029	87-89,98-104	90-97	Both methods , <*> bear strong resemblance to the boosting algorithm	when used with the exponential loss function ,	Both methods , <*> bear strong resemblance to the boosting algorithm	when used with the exponential loss function ,	87-117	87-117	Both methods , when used with the exponential loss function , bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling .	Both methods , when used with the exponential loss function , bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling .	1<2	none	condition	condition
P06-1029	98-104	105-117	bear strong resemblance to the boosting algorithm	which has been used as a discriminative training method for language modeling .	bear strong resemblance to the boosting algorithm	which has been used as a discriminative training method for language modeling .	87-117	87-117	Both methods , when used with the exponential loss function , bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling .	Both methods , when used with the exponential loss function , bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling .	1<2	none	elab-addition	elab-addition
P06-1029	118-126	127-140	Evaluations on the task of Japanese text input show	that BLasso is able to produce the best approximation to the lasso solution ,	Evaluations on the task of Japanese text input show	that BLasso is able to produce the best approximation to the lasso solution ,	118-163	118-163	Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution , and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution , and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	1>2	none	attribution	attribution
P06-1029	29-43	127-140	This paper explores the use of lasso for statistical language modeling for text input .	that BLasso is able to produce the best approximation to the lasso solution ,	This paper explores the use of lasso for statistical language modeling for text input .	that BLasso is able to produce the best approximation to the lasso solution ,	29-43	118-163	This paper explores the use of lasso for statistical language modeling for text input .	Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution , and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	1<2	none	evaluation	evaluation
P06-1029	127-140	141-163	that BLasso is able to produce the best approximation to the lasso solution ,	and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	that BLasso is able to produce the best approximation to the lasso solution ,	and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	118-163	118-163	Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution , and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution , and leads to a significant improvement , in terms of character error rate , over boosting and the traditional maximum likelihood estimation .	1<2	none	joint	joint
P06-1030	1-9	10-12	We have developed an automated Japanese essay scoring system	called Jess .	We have developed an automated Japanese essay scoring system	called Jess .	1-12	1-12	We have developed an automated Japanese essay scoring system called Jess .	We have developed an automated Japanese essay scoring system called Jess .	1<2	none	elab-addition	elab-addition
P06-1030	1-9	13-21	We have developed an automated Japanese essay scoring system	The system needs expert writings rather than expert raters	We have developed an automated Japanese essay scoring system	The system needs expert writings rather than expert raters	1-12	13-27	We have developed an automated Japanese essay scoring system called Jess .	The system needs expert writings rather than expert raters to build the evaluation model .	1<2	none	elab-addition	elab-addition
P06-1030	13-21	22-27	The system needs expert writings rather than expert raters	to build the evaluation model .	The system needs expert writings rather than expert raters	to build the evaluation model .	13-27	13-27	The system needs expert writings rather than expert raters to build the evaluation model .	The system needs expert writings rather than expert raters to build the evaluation model .	1<2	none	enablement	enablement
P06-1030	28-36	46-51	By detecting statistical outliers of predetermined aimed essay features	our system can evaluate essays .	By detecting statistical outliers of predetermined aimed essay features	our system can evaluate essays .	28-51	28-51	By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt , our system can evaluate essays .	By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt , our system can evaluate essays .	1>2	none	manner-means	manner-means
P06-1030	28-36	37-45	By detecting statistical outliers of predetermined aimed essay features	compared with many professional writings for each prompt ,	By detecting statistical outliers of predetermined aimed essay features	compared with many professional writings for each prompt ,	28-51	28-51	By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt , our system can evaluate essays .	By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt , our system can evaluate essays .	1<2	none	comparison	comparison
P06-1030	1-9	46-51	We have developed an automated Japanese essay scoring system	our system can evaluate essays .	We have developed an automated Japanese essay scoring system	our system can evaluate essays .	1-12	28-51	We have developed an automated Japanese essay scoring system called Jess .	By detecting statistical outliers of predetermined aimed essay features compared with many professional writings for each prompt , our system can evaluate essays .	1<2	none	elab-addition	elab-addition
P06-1030	1-9	52-58	We have developed an automated Japanese essay scoring system	The following three features are examined :	We have developed an automated Japanese essay scoring system	The following three features are examined :	1-12	52-127	We have developed an automated Japanese essay scoring system called Jess .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	evaluation	evaluation
P06-1030	52-58	59-83	The following three features are examined :	( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences ,	The following three features are examined :	( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences ,	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-enumember	elab-enumember
P06-1030	52-58	84-89	The following three features are examined :	( 2 ) organization — characteristics	The following three features are examined :	( 2 ) organization — characteristics	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-enumember	elab-enumember
P06-1030	84-89	90-97	( 2 ) organization — characteristics	associated with the orderly presentation of ideas ,	( 2 ) organization — characteristics	associated with the orderly presentation of ideas ,	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-addition	elab-addition
P06-1030	84-89	98-105	( 2 ) organization — characteristics	such as rhetorical features and linguistic cues ,	( 2 ) organization — characteristics	such as rhetorical features and linguistic cues ,	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-example	elab-example
P06-1030	52-58	106-112	The following three features are examined :	and ( 3 ) content — vocabulary	The following three features are examined :	and ( 3 ) content — vocabulary	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-enumember	elab-enumember
P06-1030	106-112	113-117	and ( 3 ) content — vocabulary	related to the topic ,	and ( 3 ) content — vocabulary	related to the topic ,	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-addition	elab-addition
P06-1030	106-112	118-127	and ( 3 ) content — vocabulary	such as relevant information and precise or specialized vocabulary .	and ( 3 ) content — vocabulary	such as relevant information and precise or specialized vocabulary .	52-127	52-127	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	The following three features are examined : ( 1 ) rhetoric — syntactic variety , or the use of various structures in the arrangement of phases , clauses , and sentences , ( 2 ) organization — characteristics associated with the orderly presentation of ideas , such as rhetorical features and linguistic cues , and ( 3 ) content — vocabulary related to the topic , such as relevant information and precise or specialized vocabulary .	1<2	none	elab-example	elab-example
P06-1030	1-9	128-133	We have developed an automated Japanese essay scoring system	The final evaluation score is calculated	We have developed an automated Japanese essay scoring system	The final evaluation score is calculated	1-12	128-155	We have developed an automated Japanese essay scoring system called Jess .	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	1<2	none	evaluation	evaluation
P06-1030	128-133	134-139	The final evaluation score is calculated	by deducting from a perfect score	The final evaluation score is calculated	by deducting from a perfect score	128-155	128-155	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	1<2	none	manner-means	manner-means
P06-1030	134-139	140-144	by deducting from a perfect score	assigned by a learning process	by deducting from a perfect score	assigned by a learning process	128-155	128-155	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	1<2	none	elab-addition	elab-addition
P06-1030	134-139	145-155	by deducting from a perfect score	using editorials and columns from the Mainichi Daily News newspaper .	by deducting from a perfect score	using editorials and columns from the Mainichi Daily News newspaper .	128-155	128-155	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	The final evaluation score is calculated by deducting from a perfect score assigned by a learning process using editorials and columns from the Mainichi Daily News newspaper .	1<2	none	manner-means	manner-means
P06-1030	1-9	156-164	We have developed an automated Japanese essay scoring system	A diagnosis for the essay is also given .	We have developed an automated Japanese essay scoring system	A diagnosis for the essay is also given .	1-12	156-164	We have developed an automated Japanese essay scoring system called Jess .	A diagnosis for the essay is also given .	1<2	none	elab-addition	elab-addition
P06-1031	1-5	6-15	This paper proposes a method	for detecting errors in article usage and singular plural usage	This paper proposes a method	for detecting errors in article usage and singular plural usage	1-22	1-22	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	1<2	none	elab-addition	elab-addition
P06-1031	1-5	16-22	This paper proposes a method	based on the mass count distinction .	This paper proposes a method	based on the mass count distinction .	1-22	1-22	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	1<2	none	bg-general	bg-general
P06-1031	1-5	23-31	This paper proposes a method	First , it learns decision lists from training data	This paper proposes a method	First , it learns decision lists from training data	1-22	23-40	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	First , it learns decision lists from training data generated automatically to distinguish mass and count nouns .	1<2	none	elab-process_step	elab-process_step
P06-1031	23-31	32-40	First , it learns decision lists from training data	generated automatically to distinguish mass and count nouns .	First , it learns decision lists from training data	generated automatically to distinguish mass and count nouns .	23-40	23-40	First , it learns decision lists from training data generated automatically to distinguish mass and count nouns .	First , it learns decision lists from training data generated automatically to distinguish mass and count nouns .	1<2	none	elab-addition	elab-addition
P06-1031	41-49	50-54	Then , in order to improve its performance ,	it is augmented by feedback	Then , in order to improve its performance ,	it is augmented by feedback	41-63	41-63	Then , in order to improve its performance , it is augmented by feedback that is obtained from the writing of learners .	Then , in order to improve its performance , it is augmented by feedback that is obtained from the writing of learners .	1>2	none	enablement	enablement
P06-1031	1-5	50-54	This paper proposes a method	it is augmented by feedback	This paper proposes a method	it is augmented by feedback	1-22	41-63	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	Then , in order to improve its performance , it is augmented by feedback that is obtained from the writing of learners .	1<2	none	elab-process_step	elab-process_step
P06-1031	50-54	55-63	it is augmented by feedback	that is obtained from the writing of learners .	it is augmented by feedback	that is obtained from the writing of learners .	41-63	41-63	Then , in order to improve its performance , it is augmented by feedback that is obtained from the writing of learners .	Then , in order to improve its performance , it is augmented by feedback that is obtained from the writing of learners .	1<2	none	elab-addition	elab-addition
P06-1031	1-5	64-68	This paper proposes a method	Finally , it detects errors	This paper proposes a method	Finally , it detects errors	1-22	64-77	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	Finally , it detects errors by applying rules to the mass count distinction .	1<2	none	elab-process_step	elab-process_step
P06-1031	64-68	69-77	Finally , it detects errors	by applying rules to the mass count distinction .	Finally , it detects errors	by applying rules to the mass count distinction .	64-77	64-77	Finally , it detects errors by applying rules to the mass count distinction .	Finally , it detects errors by applying rules to the mass count distinction .	1<2	none	manner-means	manner-means
P06-1031	78-79	80-91	Experiments show	that it achieves a recall of 0.71 and a precision of 0.72	Experiments show	that it achieves a recall of 0.71 and a precision of 0.72	78-103	78-103	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	1>2	none	attribution	attribution
P06-1031	1-5	80-91	This paper proposes a method	that it achieves a recall of 0.71 and a precision of 0.72	This paper proposes a method	that it achieves a recall of 0.71 and a precision of 0.72	1-22	78-103	This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction .	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	1<2	none	evaluation	evaluation
P06-1031	80-91	92-95	that it achieves a recall of 0.71 and a precision of 0.72	and outperforms other methods	that it achieves a recall of 0.71 and a precision of 0.72	and outperforms other methods	78-103	78-103	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	1<2	none	joint	joint
P06-1031	92-95	96-98	and outperforms other methods	used for comparison	and outperforms other methods	used for comparison	78-103	78-103	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	1<2	none	elab-addition	elab-addition
P06-1031	92-95	99-103	and outperforms other methods	when augmented by feedback .	and outperforms other methods	when augmented by feedback .	78-103	78-103	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	Experiments show that it achieves a recall of 0.71 and a precision of 0.72 and outperforms other methods used for comparison when augmented by feedback .	1<2	none	condition	condition
P06-1032	1-18	19-24	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	to identify and correct writing errors	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	to identify and correct writing errors	1-37	1-37	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	1<2	none	enablement	enablement
P06-1032	19-24	25-37	to identify and correct writing errors	made by learners of English as a Second Language ( ESL ) .	to identify and correct writing errors	made by learners of English as a Second Language ( ESL ) .	1-37	1-37	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	1<2	none	elab-addition	elab-addition
P06-1032	38-43	65-73	Using examples of mass noun errors	that application of the SMT paradigm can capture errors	Using examples of mass noun errors	that application of the SMT paradigm can capture errors	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1>2	none	manner-means	manner-means
P06-1032	38-43	44-53	Using examples of mass noun errors	found in the Chinese Learner Error Corpus ( CLEC )	Using examples of mass noun errors	found in the Chinese Learner Error Corpus ( CLEC )	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1<2	none	elab-addition	elab-addition
P06-1032	38-43	54-62	Using examples of mass noun errors	to guide creation of an engineered training set ,	Using examples of mass noun errors	to guide creation of an engineered training set ,	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1<2	none	enablement	enablement
P06-1032	63-64	65-73	we show	that application of the SMT paradigm can capture errors	we show	that application of the SMT paradigm can capture errors	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1>2	none	attribution	attribution
P06-1032	1-18	65-73	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	that application of the SMT paradigm can capture errors	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	that application of the SMT paradigm can capture errors	1-37	38-85	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1<2	none	elab-addition	elab-addition
P06-1032	65-73	74-80	that application of the SMT paradigm can capture errors	not well addressed by widely-used proofing tools	that application of the SMT paradigm can capture errors	not well addressed by widely-used proofing tools	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1<2	none	elab-addition	elab-addition
P06-1032	74-80	81-85	not well addressed by widely-used proofing tools	designed for native speakers .	not well addressed by widely-used proofing tools	designed for native speakers .	38-85	38-85	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	Using examples of mass noun errors found in the Chinese Learner Error Corpus ( CLEC ) to guide creation of an engineered training set , we show that application of the SMT paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers .	1<2	none	elab-addition	elab-addition
P06-1032	1-18	86-106	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	1-37	86-156	This paper presents a pilot study of the use of phrasal Statistical Machine Translation ( SMT ) techniques to identify and correct writing errors made by learners of English as a Second Language ( ESL ) .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	evaluation	evaluation
P06-1032	86-106	107-113	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	found on the World Wide Web ,	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	found on the World Wide Web ,	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	elab-addition	elab-addition
P06-1032	114	115-116,129-137	suggesting	that efforts <*> can enable the development of SMT-based writing assistance tools	suggesting	that efforts <*> can enable the development of SMT-based writing assistance tools	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1>2	none	attribution	attribution
P06-1032	86-106	115-116,129-137	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	that efforts <*> can enable the development of SMT-based writing assistance tools	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors	that efforts <*> can enable the development of SMT-based writing assistance tools	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	elab-addition	elab-addition
P06-1032	115-116,129-137	117-128	that efforts <*> can enable the development of SMT-based writing assistance tools	to collect alignable corpora of pre- and post-editing ESL writing samples offer	that efforts <*> can enable the development of SMT-based writing assistance tools	to collect alignable corpora of pre- and post-editing ESL writing samples offer	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	enablement	enablement
P06-1032	129-137	138-148	can enable the development of SMT-based writing assistance tools	capable of repairing many of the complex syntactic and lexical problems	can enable the development of SMT-based writing assistance tools	capable of repairing many of the complex syntactic and lexical problems	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	elab-addition	elab-addition
P06-1032	138-148	149-156	capable of repairing many of the complex syntactic and lexical problems	found in the writing of ESL learners .	capable of repairing many of the complex syntactic and lexical problems	found in the writing of ESL learners .	86-156	86-156	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	Our system was able to correct 61.81 % of mistakes in a set of naturally occurring examples of mass noun errors found on the World Wide Web , suggesting that efforts to collect alignable corpora of pre- and post-editing ESL writing samples offer can enable the development of SMT-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of ESL learners .	1<2	none	elab-addition	elab-addition
P06-1033	1-3,10-17	28-40	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	that similar transformations can give substantial improvements also in data-driven dependency parsing .	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	that similar transformations can give substantial improvements also in data-driven dependency parsing .	1-21	22-40	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations .	In this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing .	1>2	none	bg-compare	bg-compare
P06-1033	1-3,10-17	4-9	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	in order to improve parsing accuracy	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	in order to improve parsing accuracy	1-21	1-21	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations .	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations .	1<2	none	enablement	enablement
P06-1033	1-3,10-17	18-21	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	using constituency-based representations .	Transforming syntactic representations <*> has been exploited successfully in statistical parsing systems	using constituency-based representations .	1-21	1-21	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations .	Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations .	1<2	none	manner-means	manner-means
P06-1033	22-27	28-40	In this paper , we show	that similar transformations can give substantial improvements also in data-driven dependency parsing .	In this paper , we show	that similar transformations can give substantial improvements also in data-driven dependency parsing .	22-40	22-40	In this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing .	In this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing .	1>2	none	attribution	attribution
P06-1033	41-47	48-70	Experiments on the Prague Dependency Treebank show	that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	Experiments on the Prague Dependency Treebank show	that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	41-70	41-70	Experiments on the Prague Dependency Treebank show that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	Experiments on the Prague Dependency Treebank show that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	1>2	none	attribution	attribution
P06-1033	28-40	48-70	that similar transformations can give substantial improvements also in data-driven dependency parsing .	that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	that similar transformations can give substantial improvements also in data-driven dependency parsing .	that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	22-40	41-70	In this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing .	Experiments on the Prague Dependency Treebank show that systematic transformations of coordinate structures and verb groups result in a 10 % error reduction for a deterministic data-driven dependency parser .	1<2	none	evaluation	evaluation
P06-1033	28-40	71-77,82-91	that similar transformations can give substantial improvements also in data-driven dependency parsing .	Combining these transformations with previously proposed techniques <*> leads to state-of-the-art accuracy for the given data set .	that similar transformations can give substantial improvements also in data-driven dependency parsing .	Combining these transformations with previously proposed techniques <*> leads to state-of-the-art accuracy for the given data set .	22-40	71-91	In this paper , we show that similar transformations can give substantial improvements also in data-driven dependency parsing .	Combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-of-the-art accuracy for the given data set .	1<2	none	evaluation	evaluation
P06-1033	71-77,82-91	78-81	Combining these transformations with previously proposed techniques <*> leads to state-of-the-art accuracy for the given data set .	for recovering nonprojective dependencies	Combining these transformations with previously proposed techniques <*> leads to state-of-the-art accuracy for the given data set .	for recovering nonprojective dependencies	71-91	71-91	Combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-of-the-art accuracy for the given data set .	Combining these transformations with previously proposed techniques for recovering nonprojective dependencies leads to state-of-the-art accuracy for the given data set .	1<2	none	elab-addition	elab-addition
P06-1034	1-9	46-51	Spoken language generation for dialogue systems requires a dictionary	We propose a novel unsupervised method	Spoken language generation for dialogue systems requires a dictionary	We propose a novel unsupervised method	1-27	46-70	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	1>2	none	bg-goal	bg-goal
P06-1034	1-9	10-16	Spoken language generation for dialogue systems requires a dictionary	of mappings between semantic representations of concepts	Spoken language generation for dialogue systems requires a dictionary	of mappings between semantic representations of concepts	1-27	1-27	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	1<2	none	elab-addition	elab-addition
P06-1034	10-16	17-21	of mappings between semantic representations of concepts	the system wants to express	of mappings between semantic representations of concepts	the system wants to express	1-27	1-27	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	1<2	none	elab-addition	elab-addition
P06-1034	10-16	22-27	of mappings between semantic representations of concepts	and realizations of those concepts .	of mappings between semantic representations of concepts	and realizations of those concepts .	1-27	1-27	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	1<2	none	joint	joint
P06-1034	1-9	28-34	Spoken language generation for dialogue systems requires a dictionary	Dictionary creation is a costly process ;	Spoken language generation for dialogue systems requires a dictionary	Dictionary creation is a costly process ;	1-27	28-45	Spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts .	Dictionary creation is a costly process ; it is currently done by hand for each dialogue domain .	1<2	none	elab-addition	elab-addition
P06-1034	28-34	35-45	Dictionary creation is a costly process ;	it is currently done by hand for each dialogue domain .	Dictionary creation is a costly process ;	it is currently done by hand for each dialogue domain .	28-45	28-45	Dictionary creation is a costly process ; it is currently done by hand for each dialogue domain .	Dictionary creation is a costly process ; it is currently done by hand for each dialogue domain .	1<2	none	elab-addition	elab-addition
P06-1034	46-51	52-63	We propose a novel unsupervised method	for learning such mappings from user reviews in the target domain ,	We propose a novel unsupervised method	for learning such mappings from user reviews in the target domain ,	46-70	46-70	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	1<2	none	elab-addition	elab-addition
P06-1034	46-51	64-70	We propose a novel unsupervised method	and test it on restaurant reviews .	We propose a novel unsupervised method	and test it on restaurant reviews .	46-70	46-70	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	1<2	none	progression	progression
P06-1034	46-51	71-74	We propose a novel unsupervised method	We test the hypothesis	We propose a novel unsupervised method	We test the hypothesis	46-70	71-103	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision .	1<2	none	elab-addition	elab-addition
P06-1034	71-74	75-77,89-103	We test the hypothesis	that user reviews <*> make it possible to map review sentences to their semantic representation with high precision .	We test the hypothesis	that user reviews <*> make it possible to map review sentences to their semantic representation with high precision .	71-103	71-103	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision .	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision .	1<2	none	elab-addition	elab-addition
P06-1034	75-77,89-103	78-88	that user reviews <*> make it possible to map review sentences to their semantic representation with high precision .	that provide individual ratings for distinguished attributes of the domain entity	that user reviews <*> make it possible to map review sentences to their semantic representation with high precision .	that provide individual ratings for distinguished attributes of the domain entity	71-103	71-103	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision .	We test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision .	1<2	none	elab-addition	elab-addition
P06-1034	104-106	107-117	Experimental analyses show	that the mappings learned cover most of the domain ontology ,	Experimental analyses show	that the mappings learned cover most of the domain ontology ,	104-123	104-123	Experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation .	Experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation .	1>2	none	attribution	attribution
P06-1034	46-51	107-117	We propose a novel unsupervised method	that the mappings learned cover most of the domain ontology ,	We propose a novel unsupervised method	that the mappings learned cover most of the domain ontology ,	46-70	104-123	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	Experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation .	1<2	none	evaluation	evaluation
P06-1034	107-117	118-123	that the mappings learned cover most of the domain ontology ,	and provide good linguistic variation .	that the mappings learned cover most of the domain ontology ,	and provide good linguistic variation .	104-123	104-123	Experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation .	Experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation .	1<2	none	joint	joint
P06-1034	124-128	129-141	A subjective user evaluation shows	that the consistency between the semantic representations and the learned realizations is high	A subjective user evaluation shows	that the consistency between the semantic representations and the learned realizations is high	124-155	124-155	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .	1>2	none	attribution	attribution
P06-1034	46-51	129-141	We propose a novel unsupervised method	that the consistency between the semantic representations and the learned realizations is high	We propose a novel unsupervised method	that the consistency between the semantic representations and the learned realizations is high	46-70	124-155	We propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews .	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .	1<2	none	evaluation	evaluation
P06-1034	129-141	142-155	that the consistency between the semantic representations and the learned realizations is high	and that the naturalness of the realizations is higher than a hand-crafted baseline .	that the consistency between the semantic representations and the learned realizations is high	and that the naturalness of the realizations is higher than a hand-crafted baseline .	124-155	124-155	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .	A subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .	1<2	none	joint	joint
P06-1035	1-5	6-10	This paper presents a method	for building genetic language taxonomies	This paper presents a method	for building genetic language taxonomies	1-20	1-20	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	1<2	none	elab-addition	elab-addition
P06-1035	1-5	11-20	This paper presents a method	based on a new approach to comparing lexical forms .	This paper presents a method	based on a new approach to comparing lexical forms .	1-20	1-20	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	1<2	none	bg-general	bg-general
P06-1035	21-26	27-36	Instead of comparing forms cross-linguistically ,	a matrix of languageinternal similarities between forms is calculated .	Instead of comparing forms cross-linguistically ,	a matrix of languageinternal similarities between forms is calculated .	21-36	21-36	Instead of comparing forms cross-linguistically , a matrix of languageinternal similarities between forms is calculated .	Instead of comparing forms cross-linguistically , a matrix of languageinternal similarities between forms is calculated .	1>2	none	contrast	contrast
P06-1035	1-5	27-36	This paper presents a method	a matrix of languageinternal similarities between forms is calculated .	This paper presents a method	a matrix of languageinternal similarities between forms is calculated .	1-20	21-36	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	Instead of comparing forms cross-linguistically , a matrix of languageinternal similarities between forms is calculated .	1<2	none	elab-addition	elab-addition
P06-1035	27-36	37-41	a matrix of languageinternal similarities between forms is calculated .	These matrices are then compared	a matrix of languageinternal similarities between forms is calculated .	These matrices are then compared	21-36	37-47	Instead of comparing forms cross-linguistically , a matrix of languageinternal similarities between forms is calculated .	These matrices are then compared to give distances between languages .	1<2	none	elab-addition	elab-addition
P06-1035	37-41	42-47	These matrices are then compared	to give distances between languages .	These matrices are then compared	to give distances between languages .	37-47	37-47	These matrices are then compared to give distances between languages .	These matrices are then compared to give distances between languages .	1<2	none	enablement	enablement
P06-1035	48-49	50-61	We argue	that this coheres better with current thinking in linguistics and psycholinguistics .	We argue	that this coheres better with current thinking in linguistics and psycholinguistics .	48-61	48-61	We argue that this coheres better with current thinking in linguistics and psycholinguistics .	We argue that this coheres better with current thinking in linguistics and psycholinguistics .	1>2	none	attribution	attribution
P06-1035	1-5	50-61	This paper presents a method	that this coheres better with current thinking in linguistics and psycholinguistics .	This paper presents a method	that this coheres better with current thinking in linguistics and psycholinguistics .	1-20	48-61	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	We argue that this coheres better with current thinking in linguistics and psycholinguistics .	1<2	none	evaluation	evaluation
P06-1035	1-5	62-67,71-90	This paper presents a method	An implementation of this approach , <*> is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	This paper presents a method	An implementation of this approach , <*> is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	1-20	62-90	This paper presents a method for building genetic language taxonomies based on a new approach to comparing lexical forms .	An implementation of this approach , called PHILOLOGICON , is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	1<2	none	evaluation	evaluation
P06-1035	62-67,71-90	68-70	An implementation of this approach , <*> is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	called PHILOLOGICON ,	An implementation of this approach , <*> is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	called PHILOLOGICON ,	62-90	62-90	An implementation of this approach , called PHILOLOGICON , is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	An implementation of this approach , called PHILOLOGICON , is described , along with its application to Dyen et al.'s ( 1992 ) ninety-five wordlists from Indo-European languages .	1<2	none	elab-addition	elab-addition
P06-1036	1-13	20-23	A good dictionary contains not only many entries and a lot of information	but also adequate means	A good dictionary contains not only many entries and a lot of information	but also adequate means	1-29	1-29	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	1>2	none	progression	progression
P06-1036	1-13	14-19	A good dictionary contains not only many entries and a lot of information	concerning each one of them ,	A good dictionary contains not only many entries and a lot of information	concerning each one of them ,	1-29	1-29	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	1<2	none	elab-addition	elab-addition
P06-1036	20-23	41-46	but also adequate means	We will present here some ideas	but also adequate means	We will present here some ideas	1-29	41-66	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	1>2	none	bg-compare	bg-compare
P06-1036	20-23	24-29	but also adequate means	to reveal the stored information .	but also adequate means	to reveal the stored information .	1-29	1-29	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	1<2	none	enablement	enablement
P06-1036	20-23	30-40	but also adequate means	Information access depends crucially on the quality of the index .	but also adequate means	Information access depends crucially on the quality of the index .	1-29	30-40	A good dictionary contains not only many entries and a lot of information concerning each one of them , but also adequate means to reveal the stored information .	Information access depends crucially on the quality of the index .	1<2	none	elab-addition	elab-addition
P06-1036	41-46	47-53	We will present here some ideas	of how a dictionary could be enhanced	We will present here some ideas	of how a dictionary could be enhanced	41-66	41-66	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	1<2	none	elab-addition	elab-addition
P06-1036	47-53	54-66	of how a dictionary could be enhanced	to support a speaker/writer to find the word s/he is looking for .	of how a dictionary could be enhanced	to support a speaker/writer to find the word s/he is looking for .	41-66	41-66	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	1<2	none	enablement	enablement
P06-1036	41-46	67-80	We will present here some ideas	To this end we suggest to add to an existing electronic resource an index	We will present here some ideas	To this end we suggest to add to an existing electronic resource an index	41-66	67-87	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	To this end we suggest to add to an existing electronic resource an index based on the notion of association .	1<2	none	elab-addition	elab-addition
P06-1036	67-80	81-87	To this end we suggest to add to an existing electronic resource an index	based on the notion of association .	To this end we suggest to add to an existing electronic resource an index	based on the notion of association .	67-87	67-87	To this end we suggest to add to an existing electronic resource an index based on the notion of association .	To this end we suggest to add to an existing electronic resource an index based on the notion of association .	1<2	none	bg-general	bg-general
P06-1036	41-46	88-93	We will present here some ideas	We will also present preliminary work	We will present here some ideas	We will also present preliminary work	41-66	88-122	We will present here some ideas of how a dictionary could be enhanced to support a speaker/writer to find the word s/he is looking for .	We will also present preliminary work of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences extracted from a corpus .	1<2	none	elab-addition	elab-addition
P06-1036	88-93	94-117	We will also present preliminary work	of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences	We will also present preliminary work	of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences	88-122	88-122	We will also present preliminary work of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences extracted from a corpus .	We will also present preliminary work of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences extracted from a corpus .	1<2	none	elab-addition	elab-addition
P06-1036	94-117	118-122	of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences	extracted from a corpus .	of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences	extracted from a corpus .	88-122	88-122	We will also present preliminary work of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences extracted from a corpus .	We will also present preliminary work of how a subset of such associations , for example , topical associations , can be acquired by filtering a network of lexical co-occurrences extracted from a corpus .	1<2	none	elab-addition	elab-addition
P06-1037	1-7	8-16	We investigate the utility of supertag information	for guiding an existing dependency parser of German .	We investigate the utility of supertag information	for guiding an existing dependency parser of German .	1-16	1-16	We investigate the utility of supertag information for guiding an existing dependency parser of German .	We investigate the utility of supertag information for guiding an existing dependency parser of German .	1<2	none	elab-addition	elab-addition
P06-1037	17-26	27-34	Using weighted constraints to integrate the additionally available information ,	the decision process of the parser is influenced	Using weighted constraints to integrate the additionally available information ,	the decision process of the parser is influenced	17-48	17-48	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	1>2	none	manner-means	manner-means
P06-1037	1-7	27-34	We investigate the utility of supertag information	the decision process of the parser is influenced	We investigate the utility of supertag information	the decision process of the parser is influenced	1-16	17-48	We investigate the utility of supertag information for guiding an existing dependency parser of German .	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	1<2	none	elab-addition	elab-addition
P06-1037	27-34	35-39	the decision process of the parser is influenced	by changing its preferences ,	the decision process of the parser is influenced	by changing its preferences ,	17-48	17-48	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	1<2	none	manner-means	manner-means
P06-1037	35-39	40-48	by changing its preferences ,	without excluding alternative structural interpretations from being considered .	by changing its preferences ,	without excluding alternative structural interpretations from being considered .	17-48	17-48	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	Using weighted constraints to integrate the additionally available information , the decision process of the parser is influenced by changing its preferences , without excluding alternative structural interpretations from being considered .	1<2	none	condition	condition
P06-1037	1-7	49-56	We investigate the utility of supertag information	The paper reports on a series of experiments	We investigate the utility of supertag information	The paper reports on a series of experiments	1-16	49-68	We investigate the utility of supertag information for guiding an existing dependency parser of German .	The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy .	1<2	none	evaluation	evaluation
P06-1037	49-56	57-61	The paper reports on a series of experiments	using varying models of supertags	The paper reports on a series of experiments	using varying models of supertags	49-68	49-68	The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy .	The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy .	1<2	none	manner-means	manner-means
P06-1037	57-61	62-68	using varying models of supertags	that significantly increase the parsing accuracy .	using varying models of supertags	that significantly increase the parsing accuracy .	49-68	49-68	The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy .	The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy .	1<2	none	elab-addition	elab-addition
P06-1037	1-7	69-77,85-87	We investigate the utility of supertag information	In addition , an upper bound on the accuracy <*> is estimated .	We investigate the utility of supertag information	In addition , an upper bound on the accuracy <*> is estimated .	1-16	69-87	We investigate the utility of supertag information for guiding an existing dependency parser of German .	In addition , an upper bound on the accuracy that can be achieved with perfect supertags is estimated .	1<2	none	evaluation	evaluation
P06-1037	69-77,85-87	78-84	In addition , an upper bound on the accuracy <*> is estimated .	that can be achieved with perfect supertags	In addition , an upper bound on the accuracy <*> is estimated .	that can be achieved with perfect supertags	69-87	69-87	In addition , an upper bound on the accuracy that can be achieved with perfect supertags is estimated .	In addition , an upper bound on the accuracy that can be achieved with perfect supertags is estimated .	1<2	none	elab-addition	elab-addition
P06-1038	1-5	6-13	We present a novel approach	for discovering word categories , sets of words	We present a novel approach	for discovering word categories , sets of words	1-21	1-21	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	1<2	none	elab-addition	elab-addition
P06-1038	6-13	14-21	for discovering word categories , sets of words	sharing a significant aspect of their meaning .	for discovering word categories , sets of words	sharing a significant aspect of their meaning .	1-21	1-21	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	1<2	none	elab-addition	elab-addition
P06-1038	1-5	22-31	We present a novel approach	We utilize meta-patterns of high frequency words and content words	We present a novel approach	We utilize meta-patterns of high frequency words and content words	1-21	22-38	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	We utilize meta-patterns of high frequency words and content words in order to discover pattern candidates .	1<2	none	elab-addition	elab-addition
P06-1038	22-31	32-38	We utilize meta-patterns of high frequency words and content words	in order to discover pattern candidates .	We utilize meta-patterns of high frequency words and content words	in order to discover pattern candidates .	22-38	22-38	We utilize meta-patterns of high frequency words and content words in order to discover pattern candidates .	We utilize meta-patterns of high frequency words and content words in order to discover pattern candidates .	1<2	none	enablement	enablement
P06-1038	1-5	39-43	We present a novel approach	Symmetric patterns are then identified	We present a novel approach	Symmetric patterns are then identified	1-21	39-58	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	1<2	none	elab-addition	elab-addition
P06-1038	39-43	44-47	Symmetric patterns are then identified	using graph-based measures ,	Symmetric patterns are then identified	using graph-based measures ,	39-58	39-58	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	1<2	none	manner-means	manner-means
P06-1038	39-43	48-52	Symmetric patterns are then identified	and word categories are created	Symmetric patterns are then identified	and word categories are created	39-58	39-58	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	1<2	none	joint	joint
P06-1038	48-52	53-58	and word categories are created	based on graph clique sets .	and word categories are created	based on graph clique sets .	39-58	39-58	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	Symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets .	1<2	none	bg-general	bg-general
P06-1038	1-5	59-65	We present a novel approach	Our method is the first pattern-based method	We present a novel approach	Our method is the first pattern-based method	1-21	59-78	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words .	1<2	none	evaluation	evaluation
P06-1038	59-65	66-78	Our method is the first pattern-based method	that requires no corpus annotation or manually provided seed patterns or words .	Our method is the first pattern-based method	that requires no corpus annotation or manually provided seed patterns or words .	59-78	59-78	Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words .	Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words .	1<2	none	elab-addition	elab-addition
P06-1038	79-90	100-108	We evaluate our algorithm on very large corpora in two languages ,	Our fully unsupervised results are superior to previous work	We evaluate our algorithm on very large corpora in two languages ,	Our fully unsupervised results are superior to previous work	79-99	100-130	We evaluate our algorithm on very large corpora in two languages , using both human judgments and WordNet based evaluation .	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	1>2	none	result	result
P06-1038	79-90	91-99	We evaluate our algorithm on very large corpora in two languages ,	using both human judgments and WordNet based evaluation .	We evaluate our algorithm on very large corpora in two languages ,	using both human judgments and WordNet based evaluation .	79-99	79-99	We evaluate our algorithm on very large corpora in two languages , using both human judgments and WordNet based evaluation .	We evaluate our algorithm on very large corpora in two languages , using both human judgments and WordNet based evaluation .	1<2	none	manner-means	manner-means
P06-1038	1-5	100-108	We present a novel approach	Our fully unsupervised results are superior to previous work	We present a novel approach	Our fully unsupervised results are superior to previous work	1-21	100-130	We present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning .	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	1<2	none	evaluation	evaluation
P06-1038	100-108	109-126	Our fully unsupervised results are superior to previous work	that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster	Our fully unsupervised results are superior to previous work	that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster	100-130	100-130	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	1<2	none	elab-addition	elab-addition
P06-1038	109-126	127-130	that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster	than previously reported .	that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster	than previously reported .	100-130	100-130	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	Our fully unsupervised results are superior to previous work that used a POS tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .	1<2	none	comparison	comparison
P06-1039	1-18	19-23	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	BAYESUM leverages the common case	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	BAYESUM leverages the common case	1-18	19-34	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	BAYESUM leverages the common case in which multiple documents are relevant to a single query .	1<2	none	elab-addition	elab-addition
P06-1039	19-23	24-34	BAYESUM leverages the common case	in which multiple documents are relevant to a single query .	BAYESUM leverages the common case	in which multiple documents are relevant to a single query .	19-34	19-34	BAYESUM leverages the common case in which multiple documents are relevant to a single query .	BAYESUM leverages the common case in which multiple documents are relevant to a single query .	1<2	none	elab-addition	elab-addition
P06-1039	35-43	44-56	Using these documents as reinforcement for query terms ,	BAYESUM is not afflicted by the paucity of information in short queries .	Using these documents as reinforcement for query terms ,	BAYESUM is not afflicted by the paucity of information in short queries .	35-56	35-56	Using these documents as reinforcement for query terms , BAYESUM is not afflicted by the paucity of information in short queries .	Using these documents as reinforcement for query terms , BAYESUM is not afflicted by the paucity of information in short queries .	1>2	none	manner-means	manner-means
P06-1039	1-18	44-56	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	BAYESUM is not afflicted by the paucity of information in short queries .	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	BAYESUM is not afflicted by the paucity of information in short queries .	1-18	35-56	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	Using these documents as reinforcement for query terms , BAYESUM is not afflicted by the paucity of information in short queries .	1<2	none	elab-addition	elab-addition
P06-1039	57-58	59-69	We show	that approximate inference in BAYESUM is possible on large data sets	We show	that approximate inference in BAYESUM is possible on large data sets	57-77	57-77	We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system .	We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system .	1>2	none	attribution	attribution
P06-1039	1-18	59-69	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	that approximate inference in BAYESUM is possible on large data sets	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	that approximate inference in BAYESUM is possible on large data sets	1-18	57-77	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system .	1<2	none	elab-addition	elab-addition
P06-1039	59-69	70-77	that approximate inference in BAYESUM is possible on large data sets	and results in a state-of-the-art summarization system .	that approximate inference in BAYESUM is possible on large data sets	and results in a state-of-the-art summarization system .	57-77	57-77	We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system .	We show that approximate inference in BAYESUM is possible on large data sets and results in a state-of-the-art summarization system .	1<2	none	progression	progression
P06-1039	78-81	82-100	Furthermore , we show	how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	Furthermore , we show	how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	78-100	78-100	Furthermore , we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	Furthermore , we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	1>2	none	attribution	attribution
P06-1039	1-18	82-100	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	1-18	78-100	We present BAYESUM ( for "Bayesian summarization" ) , a model for sentence extraction in query-focused summarization .	Furthermore , we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework .	1<2	none	evaluation	evaluation
P06-1040	1-6	7-13	We present an unsupervised learning algorithm	that mines large text corpora for patterns	We present an unsupervised learning algorithm	that mines large text corpora for patterns	1-19	1-19	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	1<2	none	elab-addition	elab-addition
P06-1040	7-13	14-19	that mines large text corpora for patterns	that express implicit semantic relations .	that mines large text corpora for patterns	that express implicit semantic relations .	1-19	1-19	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	1<2	none	elab-addition	elab-addition
P06-1040	1-6	20-47	We present an unsupervised learning algorithm	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	We present an unsupervised learning algorithm	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	1-19	20-62	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y .	1<2	none	elab-addition	elab-addition
P06-1040	20-47	48-62	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	according to how well each pattern Pi expresses the relations between X and Y .	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	according to how well each pattern Pi expresses the relations between X and Y .	20-62	20-62	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y .	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y .	1<2	none	bg-general	bg-general
P06-1040	63-74	75-93	For example , given X = ostrich and Y = bird ,	the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	For example , given X = ostrich and Y = bird ,	the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	63-93	63-93	For example , given X = ostrich and Y = bird , the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	For example , given X = ostrich and Y = bird , the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	1>2	none	condition	condition
P06-1040	20-47	75-93	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked	the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	20-62	63-93	For a given input word pair X : Y with some unspecified semantic relations , the corresponding output list of patterns <P1 , ... , Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y .	For example , given X = ostrich and Y = bird , the two highest ranking output patterns are "X is the largest Y" and "Y such as the X" .	1<2	none	elab-example	elab-example
P06-1040	1-6	94-101	We present an unsupervised learning algorithm	The output patterns are intended to be useful	We present an unsupervised learning algorithm	The output patterns are intended to be useful	1-19	94-123	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	1<2	none	elab-addition	elab-addition
P06-1040	94-101	102-110	The output patterns are intended to be useful	for finding further pairs with the same relations ,	The output patterns are intended to be useful	for finding further pairs with the same relations ,	94-123	94-123	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	1<2	none	elab-addition	elab-addition
P06-1040	102-110	111-123	for finding further pairs with the same relations ,	to support the construction of lexicons , ontologies , and semantic networks .	for finding further pairs with the same relations ,	to support the construction of lexicons , ontologies , and semantic networks .	94-123	94-123	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	1<2	none	enablement	enablement
P06-1040	94-101	124-130	The output patterns are intended to be useful	The patterns are sorted by pertinence ,	The output patterns are intended to be useful	The patterns are sorted by pertinence ,	94-123	124-159	The output patterns are intended to be useful for finding further pairs with the same relations , to support the construction of lexicons , ontologies , and semantic networks .	The patterns are sorted by pertinence , where the pertinence of a pattern Pi for a word pair X : Y is the expected relational similarity between the given pair and typical pairs for Pi .	1<2	none	elab-addition	elab-addition
P06-1040	124-130	131-159	The patterns are sorted by pertinence ,	where the pertinence of a pattern Pi for a word pair X : Y is the expected relational similarity between the given pair and typical pairs for Pi .	The patterns are sorted by pertinence ,	where the pertinence of a pattern Pi for a word pair X : Y is the expected relational similarity between the given pair and typical pairs for Pi .	124-159	124-159	The patterns are sorted by pertinence , where the pertinence of a pattern Pi for a word pair X : Y is the expected relational similarity between the given pair and typical pairs for Pi .	The patterns are sorted by pertinence , where the pertinence of a pattern Pi for a word pair X : Y is the expected relational similarity between the given pair and typical pairs for Pi .	1<2	none	elab-addition	elab-addition
P06-1040	160-168	183-192	The algorithm is empirically evaluated on two tasks ,	On both tasks , the algorithm achieves state-of-the-art results ,	The algorithm is empirically evaluated on two tasks ,	On both tasks , the algorithm achieves state-of-the-art results ,	160-182	183-206	The algorithm is empirically evaluated on two tasks , solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs .	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	1>2	none	result	result
P06-1040	160-168	169-182	The algorithm is empirically evaluated on two tasks ,	solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs .	The algorithm is empirically evaluated on two tasks ,	solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs .	160-182	160-182	The algorithm is empirically evaluated on two tasks , solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs .	The algorithm is empirically evaluated on two tasks , solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs .	1<2	none	elab-addition	elab-addition
P06-1040	1-6	183-192	We present an unsupervised learning algorithm	On both tasks , the algorithm achieves state-of-the-art results ,	We present an unsupervised learning algorithm	On both tasks , the algorithm achieves state-of-the-art results ,	1-19	183-206	We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations .	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	1<2	none	evaluation	evaluation
P06-1040	183-192	193-202	On both tasks , the algorithm achieves state-of-the-art results ,	performing significantly better than several alternative pattern ranking algorithms ,	On both tasks , the algorithm achieves state-of-the-art results ,	performing significantly better than several alternative pattern ranking algorithms ,	183-206	183-206	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	1<2	none	comparison	comparison
P06-1040	193-202	203-206	performing significantly better than several alternative pattern ranking algorithms ,	based on tf-idf .	performing significantly better than several alternative pattern ranking algorithms ,	based on tf-idf .	183-206	183-206	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	On both tasks , the algorithm achieves state-of-the-art results , performing significantly better than several alternative pattern ranking algorithms , based on tf-idf .	1<2	none	bg-general	bg-general
P06-1041	1-15	16-25	In this paper we investigate the benefit of stochastic predictor components for the parsing quality	which can be obtained with a rule-based dependency grammar .	In this paper we investigate the benefit of stochastic predictor components for the parsing quality	which can be obtained with a rule-based dependency grammar .	1-25	1-25	In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar .	In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar .	1<2	none	elab-addition	elab-addition
P06-1041	26-42	43-54	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser	we were able to improve upon the baseline by 3.2 % ,	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser	we were able to improve upon the baseline by 3.2 % ,	26-68	26-68	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	1>2	none	manner-means	manner-means
P06-1041	1-15	43-54	In this paper we investigate the benefit of stochastic predictor components for the parsing quality	we were able to improve upon the baseline by 3.2 % ,	In this paper we investigate the benefit of stochastic predictor components for the parsing quality	we were able to improve upon the baseline by 3.2 % ,	1-25	26-68	In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar .	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	1<2	none	evaluation	evaluation
P06-1041	43-54	55-68	we were able to improve upon the baseline by 3.2 % ,	bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	we were able to improve upon the baseline by 3.2 % ,	bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	26-68	26-68	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	1<2	none	elab-addition	elab-addition
P06-1041	43-54	69-81	we were able to improve upon the baseline by 3.2 % ,	We attribute the successful integration to the ability of the underlying grammar model	we were able to improve upon the baseline by 3.2 % ,	We attribute the successful integration to the ability of the underlying grammar model	26-68	69-98	By including a chunker , a supertagger , a PP attacher , and a fast probabilistic parser we were able to improve upon the baseline by 3.2 % , bringing the overall labelled accuracy to 91.1 % on the German NEGRA corpus .	We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .	1<2	none	elab-addition	elab-addition
P06-1041	69-81	82-90	We attribute the successful integration to the ability of the underlying grammar model	to combine uncertain evidence in a soft manner ,	We attribute the successful integration to the ability of the underlying grammar model	to combine uncertain evidence in a soft manner ,	69-98	69-98	We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .	We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .	1<2	none	enablement	enablement
P06-1041	82-90	91-98	to combine uncertain evidence in a soft manner ,	thus avoiding the problem of error propagation .	to combine uncertain evidence in a soft manner ,	thus avoiding the problem of error propagation .	69-98	69-98	We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .	We attribute the successful integration to the ability of the underlying grammar model to combine uncertain evidence in a soft manner , thus avoiding the problem of error propagation .	1<2	none	cause	cause
P06-1042	1-6	7-12	We introduce an error mining technique	for automatically detecting errors in resources	We introduce an error mining technique	for automatically detecting errors in resources	1-19	1-19	We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems .	We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems .	1<2	none	elab-addition	elab-addition
P06-1042	7-12	13-19	for automatically detecting errors in resources	that are used in parsing systems .	for automatically detecting errors in resources	that are used in parsing systems .	1-19	1-19	We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems .	We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems .	1<2	none	elab-addition	elab-addition
P06-1042	1-6	20-26	We introduce an error mining technique	We applied this technique on parsing results	We introduce an error mining technique	We applied this technique on parsing results	1-19	20-48	We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems .	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	1<2	none	elab-addition	elab-addition
P06-1042	20-26	27-37	We applied this technique on parsing results	produced on several million words by two distinct parsing systems ,	We applied this technique on parsing results	produced on several million words by two distinct parsing systems ,	20-48	20-48	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	1<2	none	elab-addition	elab-addition
P06-1042	27-37	38-48	produced on several million words by two distinct parsing systems ,	which share the syntactic lexicon and the pre-parsing processing chain .	produced on several million words by two distinct parsing systems ,	which share the syntactic lexicon and the pre-parsing processing chain .	20-48	20-48	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	1<2	none	elab-addition	elab-addition
P06-1042	20-26	49-62	We applied this technique on parsing results	We were thus able to identify missing and erroneous information in these resources .	We applied this technique on parsing results	We were thus able to identify missing and erroneous information in these resources .	20-48	49-62	We applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain .	We were thus able to identify missing and erroneous information in these resources .	1<2	none	cause	cause
P06-1043	1-2,16-25	26-40	Statistical parsers <*> have shown vast improvements over the last 10 years .	Much of this improvement , however , is based upon an ever-increasing number of features	Statistical parsers <*> have shown vast improvements over the last 10 years .	Much of this improvement , however , is based upon an ever-increasing number of features	1-25	26-52	Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .	Much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the WSJ treebank data .	1>2	none	elab-addition	elab-addition
P06-1043	1-2,16-25	3-15	Statistical parsers <*> have shown vast improvements over the last 10 years .	trained and tested on the Penn Wall Street Journal ( WSJ ) treebank	Statistical parsers <*> have shown vast improvements over the last 10 years .	trained and tested on the Penn Wall Street Journal ( WSJ ) treebank	1-25	1-25	Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .	Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .	1<2	none	elab-addition	elab-addition
P06-1043	26-40	53-57	Much of this improvement , however , is based upon an ever-increasing number of features	This has led to concern	Much of this improvement , however , is based upon an ever-increasing number of features	This has led to concern	26-52	53-77	Much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the WSJ treebank data .	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	1>2	none	elab-addition	elab-addition
P06-1043	26-40	41-52	Much of this improvement , however , is based upon an ever-increasing number of features	to be trained on ( typically ) the WSJ treebank data .	Much of this improvement , however , is based upon an ever-increasing number of features	to be trained on ( typically ) the WSJ treebank data .	26-52	26-52	Much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the WSJ treebank data .	Much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the WSJ treebank data .	1<2	none	elab-addition	elab-addition
P06-1043	53-57	118-124	This has led to concern	This paper should allay these fears .	This has led to concern	This paper should allay these fears .	53-77	118-124	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	This paper should allay these fears .	1>2	none	bg-goal	bg-goal
P06-1043	53-57	58-77	This has led to concern	that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	This has led to concern	that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	53-77	53-77	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	1<2	none	elab-addition	elab-addition
P06-1043	53-57	78-82	This has led to concern	Such worries have merit .	This has led to concern	Such worries have merit .	53-77	78-82	This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres .	Such worries have merit .	1<2	none	elab-addition	elab-addition
P06-1043	83-103	104-117	The standard "Charniak parser" checks in at a labeled precisionrecall f-measure of 89.7 % on the Penn WSJ test set ,	but only 82.9 % on the test set from the Brown treebank corpus .	The standard "Charniak parser" checks in at a labeled precisionrecall f-measure of 89.7 % on the Penn WSJ test set ,	but only 82.9 % on the test set from the Brown treebank corpus .	83-117	83-117	The standard "Charniak parser" checks in at a labeled precisionrecall f-measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .	The standard "Charniak parser" checks in at a labeled precisionrecall f-measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .	1>2	none	contrast	contrast
P06-1043	78-82	104-117	Such worries have merit .	but only 82.9 % on the test set from the Brown treebank corpus .	Such worries have merit .	but only 82.9 % on the test set from the Brown treebank corpus .	78-82	83-117	Such worries have merit .	The standard "Charniak parser" checks in at a labeled precisionrecall f-measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .	1<2	none	exp-evidence	exp-evidence
P06-1043	125-129	130-133,142-152	In particular , we show	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	In particular , we show	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	125-152	125-152	In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .	In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .	1>2	none	attribution	attribution
P06-1043	118-124	130-133,142-152	This paper should allay these fears .	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	This paper should allay these fears .	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	118-124	125-152	This paper should allay these fears .	In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .	1<2	none	evaluation	evaluation
P06-1043	130-133,142-152	134-141	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	described in Charniak and Johnson ( 2005 )	that the reranking parser <*> improves performance of the parser on Brown to 85.2 % .	described in Charniak and Johnson ( 2005 )	125-152	125-152	In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .	In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .	1<2	none	elab-addition	elab-addition
P06-1043	118-124	153-182	This paper should allay these fears .	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	This paper should allay these fears .	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	118-124	153-190	This paper should allay these fears .	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .	1<2	none	evaluation	evaluation
P06-1043	153-182	183-190	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	without any use of labeled Brown data .	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	without any use of labeled Brown data .	153-190	153-190	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .	1<2	none	condition	condition
P06-1043	153-182	191-193	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	This is remarkable	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again	This is remarkable	153-190	191-208	Furthermore , use of the self-training techniques described in ( McClosky et al. , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .	This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4 % .	1<2	none	elab-addition	elab-addition
P06-1043	191-193	194-208	This is remarkable	since training the parser and reranker on labeled Brown data achieves only 88.4 % .	This is remarkable	since training the parser and reranker on labeled Brown data achieves only 88.4 % .	191-208	191-208	This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4 % .	This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4 % .	1<2	none	exp-reason	exp-reason
P06-1044	1-3,14-18	61-65	Lexical classes , <*> can provide an effective means	We report a novel experiment	Lexical classes , <*> can provide an effective means	We report a novel experiment	1-32	61-79	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	We report a novel experiment where similar technology is applied to the important , challenging domain of biomedicine .	1>2	none	bg-goal	bg-goal
P06-1044	1-3,14-18	4-13	Lexical classes , <*> can provide an effective means	when tailored to the application and domain in question ,	Lexical classes , <*> can provide an effective means	when tailored to the application and domain in question ,	1-32	1-32	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	1<2	none	condition	condition
P06-1044	14-18	19-32	can provide an effective means	to deal with a number of natural language processing ( NLP ) tasks .	can provide an effective means	to deal with a number of natural language processing ( NLP ) tasks .	1-32	1-32	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	1<2	none	enablement	enablement
P06-1044	33-41	45-60	While manual construction of such classes is difficult ,	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	While manual construction of such classes is difficult ,	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	33-60	33-60	While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	1>2	none	contrast	contrast
P06-1044	42-44	45-60	recent research shows	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	recent research shows	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	33-60	33-60	While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	1>2	none	attribution	attribution
P06-1044	1-3,14-18	45-60	Lexical classes , <*> can provide an effective means	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	Lexical classes , <*> can provide an effective means	that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	1-32	33-60	Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing ( NLP ) tasks .	While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy .	1<2	none	elab-addition	elab-addition
P06-1044	61-65	66-79	We report a novel experiment	where similar technology is applied to the important , challenging domain of biomedicine .	We report a novel experiment	where similar technology is applied to the important , challenging domain of biomedicine .	61-79	61-79	We report a novel experiment where similar technology is applied to the important , challenging domain of biomedicine .	We report a novel experiment where similar technology is applied to the important , challenging domain of biomedicine .	1<2	none	elab-addition	elab-addition
P06-1044	80-81	82-86,96-102	We show	that the resulting classification , <*> is highly accurate and strongly domainspecific .	We show	that the resulting classification , <*> is highly accurate and strongly domainspecific .	80-102	80-102	We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .	We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .	1>2	none	attribution	attribution
P06-1044	61-65	82-86,96-102	We report a novel experiment	that the resulting classification , <*> is highly accurate and strongly domainspecific .	We report a novel experiment	that the resulting classification , <*> is highly accurate and strongly domainspecific .	61-79	80-102	We report a novel experiment where similar technology is applied to the important , challenging domain of biomedicine .	We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .	1<2	none	elab-addition	elab-addition
P06-1044	82-86,96-102	87-95	that the resulting classification , <*> is highly accurate and strongly domainspecific .	acquired from a corpus of biomedical journal articles ,	that the resulting classification , <*> is highly accurate and strongly domainspecific .	acquired from a corpus of biomedical journal articles ,	80-102	80-102	We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .	We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .	1<2	none	elab-addition	elab-addition
P06-1044	61-65	103-106	We report a novel experiment	It can be used	We report a novel experiment	It can be used	61-79	103-126	We report a novel experiment where similar technology is applied to the important , challenging domain of biomedicine .	It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	1<2	none	elab-addition	elab-addition
P06-1044	103-106	107-114	It can be used	to aid BIO-NLP directly or as useful material	It can be used	to aid BIO-NLP directly or as useful material	103-126	103-126	It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	1<2	none	enablement	enablement
P06-1044	107-114	115-126	to aid BIO-NLP directly or as useful material	for investigating the syntax and semantics of verbs in biomedical texts .	to aid BIO-NLP directly or as useful material	for investigating the syntax and semantics of verbs in biomedical texts .	103-126	103-126	It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .	1<2	none	elab-addition	elab-addition
P06-1045	1-10	33-51	Various methods have been proposed for automatic synonym acquisition ,	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	Various methods have been proposed for automatic synonym acquisition ,	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	1-21	22-51	Various methods have been proposed for automatic synonym acquisition , as synonyms are one of the most fundamental lexical knowledge .	Whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contextual information are useful for the purpose .	1>2	none	elab-addition	elab-addition
P06-1045	1-10	11-21	Various methods have been proposed for automatic synonym acquisition ,	as synonyms are one of the most fundamental lexical knowledge .	Various methods have been proposed for automatic synonym acquisition ,	as synonyms are one of the most fundamental lexical knowledge .	1-21	1-21	Various methods have been proposed for automatic synonym acquisition , as synonyms are one of the most fundamental lexical knowledge .	Various methods have been proposed for automatic synonym acquisition , as synonyms are one of the most fundamental lexical knowledge .	1<2	none	exp-reason	exp-reason
P06-1045	22-32	33-51	Whereas many methods are based on contextual clues of words ,	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	Whereas many methods are based on contextual clues of words ,	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	22-51	22-51	Whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contextual information are useful for the purpose .	Whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contextual information are useful for the purpose .	1>2	none	contrast	contrast
P06-1045	33-51	52-63	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	This study has experimentally investigated the impact of contextual information selection ,	little attention has been paid to what kind of categories of contextual information are useful for the purpose .	This study has experimentally investigated the impact of contextual information selection ,	22-51	52-81	Whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contextual information are useful for the purpose .	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	1>2	none	bg-compare	bg-compare
P06-1045	52-63	64-73	This study has experimentally investigated the impact of contextual information selection ,	by extracting three kinds of word relationships from corpora :	This study has experimentally investigated the impact of contextual information selection ,	by extracting three kinds of word relationships from corpora :	52-81	52-81	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	1<2	none	manner-means	manner-means
P06-1045	64-73	74-81	by extracting three kinds of word relationships from corpora :	dependency , sentence co-occurrence , and proximity .	by extracting three kinds of word relationships from corpora :	dependency , sentence co-occurrence , and proximity .	52-81	52-81	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	1<2	none	elab-enumember	elab-enumember
P06-1045	82-85	97-110	The evaluation result shows	combination of two or more kinds of contextual information gives more stable performance .	The evaluation result shows	combination of two or more kinds of contextual information gives more stable performance .	82-110	82-110	The evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance .	The evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance .	1>2	none	attribution	attribution
P06-1045	86-96	97-110	that while dependency and proximity perform relatively well by themselves ,	combination of two or more kinds of contextual information gives more stable performance .	that while dependency and proximity perform relatively well by themselves ,	combination of two or more kinds of contextual information gives more stable performance .	82-110	82-110	The evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance .	The evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance .	1>2	none	progression	progression
P06-1045	52-63	97-110	This study has experimentally investigated the impact of contextual information selection ,	combination of two or more kinds of contextual information gives more stable performance .	This study has experimentally investigated the impact of contextual information selection ,	combination of two or more kinds of contextual information gives more stable performance .	52-81	82-110	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	The evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance .	1<2	none	evaluation	evaluation
P06-1045	111-122	127-142	We've further investigated useful selection of dependency relations and modification categories ,	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	We've further investigated useful selection of dependency relations and modification categories ,	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	111-142	111-142	We've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	We've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	1>2	none	result	result
P06-1045	123-126	127-142	and it is found	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	and it is found	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	111-142	111-142	We've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	We've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	1>2	none	attribution	attribution
P06-1045	52-63	127-142	This study has experimentally investigated the impact of contextual information selection ,	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	This study has experimentally investigated the impact of contextual information selection ,	that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	52-81	111-142	This study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity .	We've further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .	1<2	none	evaluation	evaluation
P06-1046	1-3,7-11	18-24,33-45	Accurately representing synonymy <*> requires large volumes of data	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	Accurately representing synonymy <*> requires large volumes of data	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	1-17	18-45	Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words .	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	1>2	none	contrast	contrast
P06-1046	1-3,7-11	4-6	Accurately representing synonymy <*> requires large volumes of data	using distributional similarity	Accurately representing synonymy <*> requires large volumes of data	using distributional similarity	1-17	1-17	Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words .	Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words .	1<2	none	manner-means	manner-means
P06-1046	7-11	12-17	requires large volumes of data	to reliably represent infrequent words .	requires large volumes of data	to reliably represent infrequent words .	1-17	1-17	Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words .	Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words .	1<2	none	enablement	enablement
P06-1046	18-24,33-45	46-54	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	In this paper , we compare several existing approaches	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	In this paper , we compare several existing approaches	18-45	46-63	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	In this paper , we compare several existing approaches to approximating the nearestneighbour search for distributional similarity .	1>2	none	bg-compare	bg-compare
P06-1046	18-24,33-45	25-28	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	to comparing context vectors	However , the naive nearest neighbour approach <*> scales poorly ( O ( n2 ) in the vocabulary size ) .	to comparing context vectors	18-45	18-45	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	1<2	none	elab-addition	elab-addition
P06-1046	25-28	29-32	to comparing context vectors	extracted from large corpora	to comparing context vectors	extracted from large corpora	18-45	18-45	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	However , the naive nearest neighbour approach to comparing context vectors extracted from large corpora scales poorly ( O ( n2 ) in the vocabulary size ) .	1<2	none	elab-addition	elab-addition
P06-1046	46-54	55-63	In this paper , we compare several existing approaches	to approximating the nearestneighbour search for distributional similarity .	In this paper , we compare several existing approaches	to approximating the nearestneighbour search for distributional similarity .	46-63	46-63	In this paper , we compare several existing approaches to approximating the nearestneighbour search for distributional similarity .	In this paper , we compare several existing approaches to approximating the nearestneighbour search for distributional similarity .	1<2	none	elab-addition	elab-addition
P06-1046	64-72	75-88	We investigate the trade-off between efficiency and accuracy ,	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	We investigate the trade-off between efficiency and accuracy ,	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	64-88	64-88	We investigate the trade-off between efficiency and accuracy , and find that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	We investigate the trade-off between efficiency and accuracy , and find that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	1>2	none	result	result
P06-1046	73-74	75-88	and find	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	and find	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	64-88	64-88	We investigate the trade-off between efficiency and accuracy , and find that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	We investigate the trade-off between efficiency and accuracy , and find that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	1>2	none	attribution	attribution
P06-1046	46-54	75-88	In this paper , we compare several existing approaches	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	In this paper , we compare several existing approaches	that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	46-63	64-88	In this paper , we compare several existing approaches to approximating the nearestneighbour search for distributional similarity .	We investigate the trade-off between efficiency and accuracy , and find that SASH ( Houle and Sakuma , 2005 ) provides the best balance .	1<2	none	evaluation	evaluation
P06-1047	1-20	51-71	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	In this paper , we define an event as one or more event terms along with the named entities associated ,	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	In this paper , we define an event as one or more event terms along with the named entities associated ,	1-25	51-100	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe .	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	1>2	none	bg-compare	bg-compare
P06-1047	1-20	21-25	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	that the sentences describe .	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	that the sentences describe .	1-25	1-25	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe .	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe .	1<2	none	elab-addition	elab-addition
P06-1047	1-20	26-33	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	Each event has its own internal structure ,	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events	Each event has its own internal structure ,	1-25	26-50	Event-based summarization attempts to select and organize the sentences in a summary with respect to the events or the sub-events that the sentences describe .	Each event has its own internal structure , and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally .	1<2	none	elab-addition	elab-addition
P06-1047	26-33	34-50	Each event has its own internal structure ,	and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally .	Each event has its own internal structure ,	and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally .	26-50	26-50	Each event has its own internal structure , and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally .	Each event has its own internal structure , and meanwhile often relates to other events semantically , temporally , spatially , causally or conditionally .	1<2	none	joint	joint
P06-1047	51-71	72-76	In this paper , we define an event as one or more event terms along with the named entities associated ,	and present a novel approach	In this paper , we define an event as one or more event terms along with the named entities associated ,	and present a novel approach	51-100	51-100	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	1<2	none	joint	joint
P06-1047	72-76	77-83	and present a novel approach	to derive intra- and inter- event relevance	and present a novel approach	to derive intra- and inter- event relevance	51-100	51-100	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	1<2	none	enablement	enablement
P06-1047	77-83	84-100	to derive intra- and inter- event relevance	using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	to derive intra- and inter- event relevance	using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	51-100	51-100	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	1<2	none	manner-means	manner-means
P06-1047	51-71	101-106	In this paper , we define an event as one or more event terms along with the named entities associated ,	We then apply PageRank ranking algorithm	In this paper , we define an event as one or more event terms along with the named entities associated ,	We then apply PageRank ranking algorithm	51-100	101-124	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	We then apply PageRank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived .	1<2	none	elab-addition	elab-addition
P06-1047	101-106	107-124	We then apply PageRank ranking algorithm	to estimate the significance of an event for inclusion in a summary from the event relevance derived .	We then apply PageRank ranking algorithm	to estimate the significance of an event for inclusion in a summary from the event relevance derived .	101-124	101-124	We then apply PageRank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived .	We then apply PageRank ranking algorithm to estimate the significance of an event for inclusion in a summary from the event relevance derived .	1<2	none	enablement	enablement
P06-1047	125-132	133-139,143-145	Experiments on the DUC 2001 test data shows	that the relevance of the named entities <*> achieves better result	Experiments on the DUC 2001 test data shows	that the relevance of the named entities <*> achieves better result	125-157	125-157	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	1>2	none	attribution	attribution
P06-1047	51-71	133-139,143-145	In this paper , we define an event as one or more event terms along with the named entities associated ,	that the relevance of the named entities <*> achieves better result	In this paper , we define an event as one or more event terms along with the named entities associated ,	that the relevance of the named entities <*> achieves better result	51-100	125-157	In this paper , we define an event as one or more event terms along with the named entities associated , and present a novel approach to derive intra- and inter- event relevance using the information of internal association , semantic relatedness , distributional similarity and named entity clustering .	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	1<2	none	evaluation	evaluation
P06-1047	133-139,143-145	140-142	that the relevance of the named entities <*> achieves better result	involved in events	that the relevance of the named entities <*> achieves better result	involved in events	125-157	125-157	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	1<2	none	elab-addition	elab-addition
P06-1047	133-139,143-145	146-154	that the relevance of the named entities <*> achieves better result	when their relevance is derived from the event terms	that the relevance of the named entities <*> achieves better result	when their relevance is derived from the event terms	125-157	125-157	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	1<2	none	condition	condition
P06-1047	146-154	155-157	when their relevance is derived from the event terms	they associate .	when their relevance is derived from the event terms	they associate .	125-157	125-157	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	1<2	none	elab-addition	elab-addition
P06-1047	158-160	161-180	It also reveals	that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	It also reveals	that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	158-180	158-180	It also reveals that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	It also reveals that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	1>2	none	attribution	attribution
P06-1047	133-139,143-145	161-180	that the relevance of the named entities <*> achieves better result	that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	that the relevance of the named entities <*> achieves better result	that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	125-157	158-180	Experiments on the DUC 2001 test data shows that the relevance of the named entities involved in events achieves better result when their relevance is derived from the event terms they associate .	It also reveals that the topic-specific relevance from documents themselves outperforms the semantic relevance from a general purpose knowledge base like Word-Net .	1<2	none	elab-addition	elab-addition
P06-1048	1-5	15-23	Sentence compression is the task	This paper focuses on three aspects of this task	Sentence compression is the task	This paper focuses on three aspects of this task	1-14	15-42	Sentence compression is the task of producing a summary at the sentence level .	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	1>2	none	bg-goal	bg-goal
P06-1048	1-5	6-14	Sentence compression is the task	of producing a summary at the sentence level .	Sentence compression is the task	of producing a summary at the sentence level .	1-14	1-14	Sentence compression is the task of producing a summary at the sentence level .	Sentence compression is the task of producing a summary at the sentence level .	1<2	none	elab-addition	elab-addition
P06-1048	15-23	24-33	This paper focuses on three aspects of this task	which have not received detailed treatment in the literature :	This paper focuses on three aspects of this task	which have not received detailed treatment in the literature :	15-42	15-42	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	1<2	none	elab-addition	elab-addition
P06-1048	15-23	34-42	This paper focuses on three aspects of this task	training requirements , scalability , and automatic evaluation .	This paper focuses on three aspects of this task	training requirements , scalability , and automatic evaluation .	15-42	15-42	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	1<2	none	elab-enumember	elab-enumember
P06-1048	15-23	43-58	This paper focuses on three aspects of this task	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	This paper focuses on three aspects of this task	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	15-42	43-74	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	1<2	none	elab-addition	elab-addition
P06-1048	59-60	61-74	and examine	how these models port to different domains ( written vs. spoken text ) .	and examine	how these models port to different domains ( written vs. spoken text ) .	43-74	43-74	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	1>2	none	attribution	attribution
P06-1048	43-58	61-74	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	how these models port to different domains ( written vs. spoken text ) .	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	how these models port to different domains ( written vs. spoken text ) .	43-74	43-74	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	1<2	none	progression	progression
P06-1048	75-78	79-85	To achieve this ,	a human-authored compression corpus has been created	To achieve this ,	a human-authored compression corpus has been created	75-100	75-100	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	1>2	none	enablement	enablement
P06-1048	43-58	79-85	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	a human-authored compression corpus has been created	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm	a human-authored compression corpus has been created	43-74	75-100	We provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) .	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	1<2	none	elab-addition	elab-addition
P06-1048	79-85	86-97	a human-authored compression corpus has been created	and our study highlights potential problems with the automatically gathered compression corpora	a human-authored compression corpus has been created	and our study highlights potential problems with the automatically gathered compression corpora	75-100	75-100	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	1<2	none	joint	joint
P06-1048	86-97	98-100	and our study highlights potential problems with the automatically gathered compression corpora	currently used .	and our study highlights potential problems with the automatically gathered compression corpora	currently used .	75-100	75-100	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	To achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .	1<2	none	elab-addition	elab-addition
P06-1048	101-104	105-111	Finally , we assess	whether automatic evaluation measures can be used	Finally , we assess	whether automatic evaluation measures can be used	101-116	101-116	Finally , we assess whether automatic evaluation measures can be used to determine compression quality .	Finally , we assess whether automatic evaluation measures can be used to determine compression quality .	1>2	none	attribution	attribution
P06-1048	15-23	105-111	This paper focuses on three aspects of this task	whether automatic evaluation measures can be used	This paper focuses on three aspects of this task	whether automatic evaluation measures can be used	15-42	101-116	This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .	Finally , we assess whether automatic evaluation measures can be used to determine compression quality .	1<2	none	elab-addition	elab-addition
P06-1048	105-111	112-116	whether automatic evaluation measures can be used	to determine compression quality .	whether automatic evaluation measures can be used	to determine compression quality .	101-116	101-116	Finally , we assess whether automatic evaluation measures can be used to determine compression quality .	Finally , we assess whether automatic evaluation measures can be used to determine compression quality .	1<2	none	enablement	enablement
P06-1049	1-10	15-19	Ordering information is a difficult but important task for applications	We present a bottom-up approach	Ordering information is a difficult but important task for applications	We present a bottom-up approach	1-14	15-27	Ordering information is a difficult but important task for applications generating natural-language text .	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	1>2	none	bg-goal	bg-goal
P06-1049	1-10	11-14	Ordering information is a difficult but important task for applications	generating natural-language text .	Ordering information is a difficult but important task for applications	generating natural-language text .	1-14	1-14	Ordering information is a difficult but important task for applications generating natural-language text .	Ordering information is a difficult but important task for applications generating natural-language text .	1<2	none	elab-addition	elab-addition
P06-1049	15-19	20-22	We present a bottom-up approach	to arranging sentences	We present a bottom-up approach	to arranging sentences	15-27	15-27	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	1<2	none	elab-addition	elab-addition
P06-1049	20-22	23-27	to arranging sentences	extracted for multi-document summarization .	to arranging sentences	extracted for multi-document summarization .	15-27	15-27	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	1<2	none	elab-addition	elab-addition
P06-1049	28-43	44-57	To capture the association and order of two textual segments ( eg , sentences ) ,	we define four criteria , chronology , topical-closeness , precedence , and succession .	To capture the association and order of two textual segments ( eg , sentences ) ,	we define four criteria , chronology , topical-closeness , precedence , and succession .	28-57	28-57	To capture the association and order of two textual segments ( eg , sentences ) , we define four criteria , chronology , topical-closeness , precedence , and succession .	To capture the association and order of two textual segments ( eg , sentences ) , we define four criteria , chronology , topical-closeness , precedence , and succession .	1>2	none	enablement	enablement
P06-1049	15-19	44-57	We present a bottom-up approach	we define four criteria , chronology , topical-closeness , precedence , and succession .	We present a bottom-up approach	we define four criteria , chronology , topical-closeness , precedence , and succession .	15-27	28-57	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	To capture the association and order of two textual segments ( eg , sentences ) , we define four criteria , chronology , topical-closeness , precedence , and succession .	1<2	none	elab-addition	elab-addition
P06-1049	44-57	58-70	we define four criteria , chronology , topical-closeness , precedence , and succession .	These criteria are integrated into a criterion by a supervised learning approach .	we define four criteria , chronology , topical-closeness , precedence , and succession .	These criteria are integrated into a criterion by a supervised learning approach .	28-57	58-70	To capture the association and order of two textual segments ( eg , sentences ) , we define four criteria , chronology , topical-closeness , precedence , and succession .	These criteria are integrated into a criterion by a supervised learning approach .	1<2	none	elab-addition	elab-addition
P06-1049	15-19	71-79	We present a bottom-up approach	We repeatedly concatenate two textual segments into one segment	We present a bottom-up approach	We repeatedly concatenate two textual segments into one segment	15-27	71-94	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged .	1<2	none	elab-addition	elab-addition
P06-1049	71-79	80-83	We repeatedly concatenate two textual segments into one segment	based on the criterion	We repeatedly concatenate two textual segments into one segment	based on the criterion	71-94	71-94	We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged .	We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged .	1<2	none	bg-general	bg-general
P06-1049	71-79	84-94	We repeatedly concatenate two textual segments into one segment	until we obtain the overall segment with all sentences arranged .	We repeatedly concatenate two textual segments into one segment	until we obtain the overall segment with all sentences arranged .	71-94	71-94	We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged .	We repeatedly concatenate two textual segments into one segment based on the criterion until we obtain the overall segment with all sentences arranged .	1<2	none	temporal	temporal
P06-1049	15-19	95-107	We present a bottom-up approach	Our experimental results show a significant improvement over existing sentence ordering strategies .	We present a bottom-up approach	Our experimental results show a significant improvement over existing sentence ordering strategies .	15-27	95-107	We present a bottom-up approach to arranging sentences extracted for multi-document summarization .	Our experimental results show a significant improvement over existing sentence ordering strategies .	1<2	none	evaluation	evaluation
P06-1050	1-8	9-20	We have constructed a corpus of news articles	in which events are annotated for estimated bounds on their duration .	We have constructed a corpus of news articles	in which events are annotated for estimated bounds on their duration .	1-20	1-20	We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration .	We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration .	1<2	none	elab-addition	elab-addition
P06-1050	1-8	21-25	We have constructed a corpus of news articles	Here we describe a method	We have constructed a corpus of news articles	Here we describe a method	1-20	21-35	We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration .	Here we describe a method for measuring inter-annotator agreement for these event duration distributions .	1<2	none	elab-addition	elab-addition
P06-1050	21-25	26-35	Here we describe a method	for measuring inter-annotator agreement for these event duration distributions .	Here we describe a method	for measuring inter-annotator agreement for these event duration distributions .	21-35	21-35	Here we describe a method for measuring inter-annotator agreement for these event duration distributions .	Here we describe a method for measuring inter-annotator agreement for these event duration distributions .	1<2	none	elab-addition	elab-addition
P06-1050	36-38	39-42,47-52	We then show	that machine learning techniques <*> yield coarse-grained event duration information ,	We then show	that machine learning techniques <*> yield coarse-grained event duration information ,	36-61	36-61	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	1>2	none	attribution	attribution
P06-1050	1-8	39-42,47-52	We have constructed a corpus of news articles	that machine learning techniques <*> yield coarse-grained event duration information ,	We have constructed a corpus of news articles	that machine learning techniques <*> yield coarse-grained event duration information ,	1-20	36-61	We have constructed a corpus of news articles in which events are annotated for estimated bounds on their duration .	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	1<2	none	elab-addition	elab-addition
P06-1050	39-42,47-52	43-46	that machine learning techniques <*> yield coarse-grained event duration information ,	applied to this data	that machine learning techniques <*> yield coarse-grained event duration information ,	applied to this data	36-61	36-61	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	1<2	none	elab-addition	elab-addition
P06-1050	39-42,47-52	53-56	that machine learning techniques <*> yield coarse-grained event duration information ,	considerably outperforming a baseline	that machine learning techniques <*> yield coarse-grained event duration information ,	considerably outperforming a baseline	36-61	36-61	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	1<2	none	elab-addition	elab-addition
P06-1050	53-56	57-61	considerably outperforming a baseline	and approaching human performance .	considerably outperforming a baseline	and approaching human performance .	36-61	36-61	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	We then show that machine learning techniques applied to this data yield coarse-grained event duration information , considerably outperforming a baseline and approaching human performance .	1<2	none	joint	joint
P06-1051	1-14	15-30	In this paper we define a novel similarity measure between examples of textual entailments	and we use it as a kernel function in Support Vector Machines ( SVMs ) .	In this paper we define a novel similarity measure between examples of textual entailments	and we use it as a kernel function in Support Vector Machines ( SVMs ) .	1-30	1-30	In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines ( SVMs ) .	In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines ( SVMs ) .	1<2	none	joint	joint
P06-1051	1-14	31-39	In this paper we define a novel similarity measure between examples of textual entailments	This allows us to automatically learn the rewrite rules	In this paper we define a novel similarity measure between examples of textual entailments	This allows us to automatically learn the rewrite rules	1-30	31-49	In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines ( SVMs ) .	This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases .	1<2	none	elab-addition	elab-addition
P06-1051	31-39	40-49	This allows us to automatically learn the rewrite rules	that describe a non trivial set of entailment cases .	This allows us to automatically learn the rewrite rules	that describe a non trivial set of entailment cases .	31-49	31-49	This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases .	This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases .	1<2	none	elab-addition	elab-addition
P06-1051	1-14	50-71	In this paper we define a novel similarity measure between examples of textual entailments	The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4 % over the state-of-the-art methods .	In this paper we define a novel similarity measure between examples of textual entailments	The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4 % over the state-of-the-art methods .	1-30	50-71	In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines ( SVMs ) .	The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4 % over the state-of-the-art methods .	1<2	none	evaluation	evaluation
P06-1052	12-24	25-33	Given an underspecified semantic representation ( USR ) of a scope ambiguity ,	compute an USR with fewer mutually equivalent readings .	Given an underspecified semantic representation ( USR ) of a scope ambiguity ,	compute an USR with fewer mutually equivalent readings .	1-33	1-33	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	1>2	none	condition	condition
P06-1052	1-11	25-33	We present an efficient algorithm for the redundancy elimination problem :	compute an USR with fewer mutually equivalent readings .	We present an efficient algorithm for the redundancy elimination problem :	compute an USR with fewer mutually equivalent readings .	1-33	1-33	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	1<2	none	elab-addition	elab-addition
P06-1052	1-11	34-40	We present an efficient algorithm for the redundancy elimination problem :	The algorithm operates on underspecified chart representations	We present an efficient algorithm for the redundancy elimination problem :	The algorithm operates on underspecified chart representations	1-33	34-59	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	1<2	none	elab-addition	elab-addition
P06-1052	34-40	41-47	The algorithm operates on underspecified chart representations	which are derived from dominance graphs ;	The algorithm operates on underspecified chart representations	which are derived from dominance graphs ;	34-59	34-59	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	1<2	none	elab-addition	elab-addition
P06-1052	34-40	48-54	The algorithm operates on underspecified chart representations	it can be applied to the USRs	The algorithm operates on underspecified chart representations	it can be applied to the USRs	34-59	34-59	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	1<2	none	elab-addition	elab-addition
P06-1052	48-54	55-59	it can be applied to the USRs	computed by large-scale grammars .	it can be applied to the USRs	computed by large-scale grammars .	34-59	34-59	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .	1<2	none	elab-addition	elab-addition
P06-1052	60-67	70-77	We evaluate the algorithm on a corpus ,	that it reduces the degree of ambiguity significantly	We evaluate the algorithm on a corpus ,	that it reduces the degree of ambiguity significantly	60-82	60-82	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	1>2	none	progression	progression
P06-1052	68-69	70-77	and show	that it reduces the degree of ambiguity significantly	and show	that it reduces the degree of ambiguity significantly	60-82	60-82	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	1>2	none	attribution	attribution
P06-1052	1-11	70-77	We present an efficient algorithm for the redundancy elimination problem :	that it reduces the degree of ambiguity significantly	We present an efficient algorithm for the redundancy elimination problem :	that it reduces the degree of ambiguity significantly	1-33	60-82	We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation ( USR ) of a scope ambiguity , compute an USR with fewer mutually equivalent readings .	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	1<2	none	evaluation	evaluation
P06-1052	70-77	78-82	that it reduces the degree of ambiguity significantly	while taking negligible runtime .	that it reduces the degree of ambiguity significantly	while taking negligible runtime .	60-82	60-82	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .	1<2	none	joint	joint
P06-1053	1-13	18-22	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency	This paper describes a method	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency	This paper describes a method	1-17	18-31	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .	This paper describes a method for incorporating priming into an incremental probabilistic parser .	1>2	none	bg-goal	bg-goal
P06-1053	1-13	14-17	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency	to repeat structures .	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency	to repeat structures .	1-17	1-17	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .	The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .	1<2	none	enablement	enablement
P06-1053	18-22	23-31	This paper describes a method	for incorporating priming into an incremental probabilistic parser .	This paper describes a method	for incorporating priming into an incremental probabilistic parser .	18-31	18-31	This paper describes a method for incorporating priming into an incremental probabilistic parser .	This paper describes a method for incorporating priming into an incremental probabilistic parser .	1<2	none	elab-addition	elab-addition
P06-1053	18-22	32-36	This paper describes a method	Three models are compared ,	This paper describes a method	Three models are compared ,	18-31	32-52	This paper describes a method for incorporating priming into an incremental probabilistic parser .	Three models are compared , which involve priming of rules between sentences , within sentences , and within coordinate structures .	1<2	none	elab-addition	elab-addition
P06-1053	32-36	37-52	Three models are compared ,	which involve priming of rules between sentences , within sentences , and within coordinate structures .	Three models are compared ,	which involve priming of rules between sentences , within sentences , and within coordinate structures .	32-52	32-52	Three models are compared , which involve priming of rules between sentences , within sentences , and within coordinate structures .	Three models are compared , which involve priming of rules between sentences , within sentences , and within coordinate structures .	1<2	none	elab-addition	elab-addition
P06-1053	32-36	53-62	Three models are compared ,	These models simulate the reading time advantage for parallel structures	Three models are compared ,	These models simulate the reading time advantage for parallel structures	32-52	53-78	Three models are compared , which involve priming of rules between sentences , within sentences , and within coordinate structures .	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .	1<2	none	evaluation	evaluation
P06-1053	53-62	63-67	These models simulate the reading time advantage for parallel structures	found in human data ,	These models simulate the reading time advantage for parallel structures	found in human data ,	53-78	53-78	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .	1<2	none	elab-addition	elab-addition
P06-1053	53-62	68-78	These models simulate the reading time advantage for parallel structures	and also yield a small increase in overall parsing accuracy .	These models simulate the reading time advantage for parallel structures	and also yield a small increase in overall parsing accuracy .	53-78	53-78	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .	These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall parsing accuracy .	1<2	none	joint	joint
P06-1054	1-12	13-24	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our parser computes parse trees from bottom up in one pass ,	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our parser computes parse trees from bottom up in one pass ,	1-12	13-32	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions .	1<2	none	elab-addition	elab-addition
P06-1054	13-24	25-27	Our parser computes parse trees from bottom up in one pass ,	and uses classifiers	Our parser computes parse trees from bottom up in one pass ,	and uses classifiers	13-32	13-32	Our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions .	Our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions .	1<2	none	joint	joint
P06-1054	25-27	28-32	and uses classifiers	to make shift-reduce decisions .	and uses classifiers	to make shift-reduce decisions .	13-32	13-32	Our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions .	Our parser computes parse trees from bottom up in one pass , and uses classifiers to make shift-reduce decisions .	1<2	none	enablement	enablement
P06-1054	33-43	44-46,52-55	Trained and evaluated on the standard training and test sets ,	our best model <*> runs in linear time	Trained and evaluated on the standard training and test sets ,	our best model <*> runs in linear time	33-75	33-75	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1>2	none	elab-addition	elab-addition
P06-1054	1-12	44-46,52-55	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	our best model <*> runs in linear time	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	our best model <*> runs in linear time	1-12	33-75	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1<2	none	evaluation	evaluation
P06-1054	44-46,52-55	47-51	our best model <*> runs in linear time	( using stacked classifiers )	our best model <*> runs in linear time	( using stacked classifiers )	33-75	33-75	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1<2	none	manner-means	manner-means
P06-1054	52-55	56-64	runs in linear time	and has labeled precision and recall above 88 %	runs in linear time	and has labeled precision and recall above 88 %	33-75	33-75	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1<2	none	joint	joint
P06-1054	56-64	65-69	and has labeled precision and recall above 88 %	using gold-standard part-of-speech tags ,	and has labeled precision and recall above 88 %	using gold-standard part-of-speech tags ,	33-75	33-75	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1<2	none	manner-means	manner-means
P06-1054	56-64	70-75	and has labeled precision and recall above 88 %	surpassing the best published results .	and has labeled precision and recall above 88 %	surpassing the best published results .	33-75	33-75	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	Trained and evaluated on the standard training and test sets , our best model ( using stacked classifiers ) runs in linear time and has labeled precision and recall above 88 % using gold-standard part-of-speech tags , surpassing the best published results .	1<2	none	elab-addition	elab-addition
P06-1054	1-12	76-86	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our SVM parser is 2-13 times faster than state-of-the-art parsers ,	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our SVM parser is 2-13 times faster than state-of-the-art parsers ,	1-12	76-92	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our SVM parser is 2-13 times faster than state-of-the-art parsers , while producing more accurate results .	1<2	none	evaluation	evaluation
P06-1054	76-86	87-92	Our SVM parser is 2-13 times faster than state-of-the-art parsers ,	while producing more accurate results .	Our SVM parser is 2-13 times faster than state-of-the-art parsers ,	while producing more accurate results .	76-92	76-92	Our SVM parser is 2-13 times faster than state-of-the-art parsers , while producing more accurate results .	Our SVM parser is 2-13 times faster than state-of-the-art parsers , while producing more accurate results .	1<2	none	joint	joint
P06-1054	1-12	93-115	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers , but with 5-6 % losses in accuracy .	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers , but with 5-6 % losses in accuracy .	1-12	93-115	We present a novel classifier-based deterministic parser for Chinese constituency parsing .	Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers , but with 5-6 % losses in accuracy .	1<2	none	evaluation	evaluation
P06-1055	1-8	9-18	We present an automatic approach to tree annotation	in which basic nonterminal symbols are alternately split and merged	We present an automatic approach to tree annotation	in which basic nonterminal symbols are alternately split and merged	1-27	1-27	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	1<2	none	elab-addition	elab-addition
P06-1055	9-18	19-27	in which basic nonterminal symbols are alternately split and merged	to maximize the likelihood of a training treebank .	in which basic nonterminal symbols are alternately split and merged	to maximize the likelihood of a training treebank .	1-27	1-27	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	1<2	none	enablement	enablement
P06-1055	28-34	35-39	Starting with a simple Xbar grammar ,	we learn a new grammar	Starting with a simple Xbar grammar ,	we learn a new grammar	28-48	28-48	Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .	Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .	1>2	none	elab-addition	elab-addition
P06-1055	1-8	35-39	We present an automatic approach to tree annotation	we learn a new grammar	We present an automatic approach to tree annotation	we learn a new grammar	1-27	28-48	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .	1<2	none	elab-addition	elab-addition
P06-1055	35-39	40-48	we learn a new grammar	whose nonterminals are subsymbols of the original nonterminals .	we learn a new grammar	whose nonterminals are subsymbols of the original nonterminals .	28-48	28-48	Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .	Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .	1<2	none	elab-addition	elab-addition
P06-1055	49-54	55-65	In contrast with previous work ,	we are able to split various terminals to different degrees ,	In contrast with previous work ,	we are able to split various terminals to different degrees ,	49-75	49-75	In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .	In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .	1>2	none	contrast	contrast
P06-1055	1-8	55-65	We present an automatic approach to tree annotation	we are able to split various terminals to different degrees ,	We present an automatic approach to tree annotation	we are able to split various terminals to different degrees ,	1-27	49-75	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .	1<2	none	elab-addition	elab-addition
P06-1055	55-65	66-75	we are able to split various terminals to different degrees ,	as appropriate to the actual complexity in the data .	we are able to split various terminals to different degrees ,	as appropriate to the actual complexity in the data .	49-75	49-75	In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .	In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .	1<2	none	comparison	comparison
P06-1055	1-8	76-84	We present an automatic approach to tree annotation	Our grammars automatically learn the kinds of linguistic distinctions	We present an automatic approach to tree annotation	Our grammars automatically learn the kinds of linguistic distinctions	1-27	76-93	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .	1<2	none	evaluation	evaluation
P06-1055	76-84	85-93	Our grammars automatically learn the kinds of linguistic distinctions	exhibited in previous work on manual tree annotation .	Our grammars automatically learn the kinds of linguistic distinctions	exhibited in previous work on manual tree annotation .	76-93	76-93	Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .	Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .	1<2	none	elab-addition	elab-addition
P06-1055	1-8	94-115	We present an automatic approach to tree annotation	On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .	We present an automatic approach to tree annotation	On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .	1-27	94-115	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .	1<2	none	evaluation	evaluation
P06-1055	116-119	120-133	Despite its simplicity ,	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	Despite its simplicity ,	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	116-139	116-139	Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems .	Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems .	1>2	none	contrast	contrast
P06-1055	1-8	120-133	We present an automatic approach to tree annotation	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	We present an automatic approach to tree annotation	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	1-27	116-139	We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .	Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems .	1<2	none	evaluation	evaluation
P06-1055	120-133	134-139	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	higher than fully lexicalized systems .	our best grammar achieves an F1 of 90.2 % on the Penn Treebank ,	higher than fully lexicalized systems .	116-139	116-139	Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems .	Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems .	1<2	none	comparison	comparison
P06-1056	1-9	23-46	Partial cognates are pairs of words in two languages	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	Partial cognates are pairs of words in two languages	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	1-22	23-46	Partial cognates are pairs of words in two languages that have the same meaning in some , but not all contexts .	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	1>2	none	elab-addition	elab-addition
P06-1056	1-9	10-22	Partial cognates are pairs of words in two languages	that have the same meaning in some , but not all contexts .	Partial cognates are pairs of words in two languages	that have the same meaning in some , but not all contexts .	1-22	1-22	Partial cognates are pairs of words in two languages that have the same meaning in some , but not all contexts .	Partial cognates are pairs of words in two languages that have the same meaning in some , but not all contexts .	1<2	none	elab-addition	elab-addition
P06-1056	23-46	47-57	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	In this paper we propose a supervised and a semisupervised method	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	In this paper we propose a supervised and a semisupervised method	23-46	47-69	Detecting the actual meaning of a partial cognate in context can be useful for Machine Translation tools and for Computer-Assisted Language Learning tools .	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	1>2	none	bg-goal	bg-goal
P06-1056	47-57	58-65	In this paper we propose a supervised and a semisupervised method	to disambiguate partial cognates between two languages :	In this paper we propose a supervised and a semisupervised method	to disambiguate partial cognates between two languages :	47-69	47-69	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	1<2	none	enablement	enablement
P06-1056	58-65	66-69	to disambiguate partial cognates between two languages :	French and English .	to disambiguate partial cognates between two languages :	French and English .	47-69	47-69	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	1<2	none	elab-enumember	elab-enumember
P06-1056	47-57	70-76	In this paper we propose a supervised and a semisupervised method	The methods use only automatically-labeled data ;	In this paper we propose a supervised and a semisupervised method	The methods use only automatically-labeled data ;	47-69	70-89	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	The methods use only automatically-labeled data ; therefore they can be applied for other pairs of languages as well .	1<2	none	elab-addition	elab-addition
P06-1056	70-76	77-89	The methods use only automatically-labeled data ;	therefore they can be applied for other pairs of languages as well .	The methods use only automatically-labeled data ;	therefore they can be applied for other pairs of languages as well .	70-89	70-89	The methods use only automatically-labeled data ; therefore they can be applied for other pairs of languages as well .	The methods use only automatically-labeled data ; therefore they can be applied for other pairs of languages as well .	1<2	none	cause	cause
P06-1056	90-92	93-97	We also show	that our methods perform well	We also show	that our methods perform well	90-104	90-104	We also show that our methods perform well when using corpora from different domains .	We also show that our methods perform well when using corpora from different domains .	1>2	none	attribution	attribution
P06-1056	47-57	93-97	In this paper we propose a supervised and a semisupervised method	that our methods perform well	In this paper we propose a supervised and a semisupervised method	that our methods perform well	47-69	90-104	In this paper we propose a supervised and a semisupervised method to disambiguate partial cognates between two languages : French and English .	We also show that our methods perform well when using corpora from different domains .	1<2	none	evaluation	evaluation
P06-1056	93-97	98-104	that our methods perform well	when using corpora from different domains .	that our methods perform well	when using corpora from different domains .	90-104	90-104	We also show that our methods perform well when using corpora from different domains .	We also show that our methods perform well when using corpora from different domains .	1<2	none	condition	condition
P06-1057	13-16	17-27	which requires to recognize	whether the senses of two synonymous words match in context .	which requires to recognize	whether the senses of two synonymous words match in context .	1-27	1-27	This paper investigates conceptually and empirically the novel sense matching task , which requires to recognize whether the senses of two synonymous words match in context .	This paper investigates conceptually and empirically the novel sense matching task , which requires to recognize whether the senses of two synonymous words match in context .	1>2	none	attribution	attribution
P06-1057	1-12	17-27	This paper investigates conceptually and empirically the novel sense matching task ,	whether the senses of two synonymous words match in context .	This paper investigates conceptually and empirically the novel sense matching task ,	whether the senses of two synonymous words match in context .	1-27	1-27	This paper investigates conceptually and empirically the novel sense matching task , which requires to recognize whether the senses of two synonymous words match in context .	This paper investigates conceptually and empirically the novel sense matching task , which requires to recognize whether the senses of two synonymous words match in context .	1<2	none	elab-addition	elab-addition
P06-1057	1-12	28-35	This paper investigates conceptually and empirically the novel sense matching task ,	We suggest direct approaches to the problem ,	This paper investigates conceptually and empirically the novel sense matching task ,	We suggest direct approaches to the problem ,	1-27	28-58	This paper investigates conceptually and empirically the novel sense matching task , which requires to recognize whether the senses of two synonymous words match in context .	We suggest direct approaches to the problem , which avoid the intermediate step of explicit word sense disambiguation , and demonstrate their appealing advantages and stimulating potential for future research .	1<2	none	evaluation	evaluation
P06-1057	28-35	36-46	We suggest direct approaches to the problem ,	which avoid the intermediate step of explicit word sense disambiguation ,	We suggest direct approaches to the problem ,	which avoid the intermediate step of explicit word sense disambiguation ,	28-58	28-58	We suggest direct approaches to the problem , which avoid the intermediate step of explicit word sense disambiguation , and demonstrate their appealing advantages and stimulating potential for future research .	We suggest direct approaches to the problem , which avoid the intermediate step of explicit word sense disambiguation , and demonstrate their appealing advantages and stimulating potential for future research .	1<2	none	elab-addition	elab-addition
P06-1057	28-35	47-58	We suggest direct approaches to the problem ,	and demonstrate their appealing advantages and stimulating potential for future research .	We suggest direct approaches to the problem ,	and demonstrate their appealing advantages and stimulating potential for future research .	28-58	28-58	We suggest direct approaches to the problem , which avoid the intermediate step of explicit word sense disambiguation , and demonstrate their appealing advantages and stimulating potential for future research .	We suggest direct approaches to the problem , which avoid the intermediate step of explicit word sense disambiguation , and demonstrate their appealing advantages and stimulating potential for future research .	1<2	none	joint	joint
P06-1058	1-6	7-13	This paper presents a new approach	based on Equivalent Pseudowords ( EPs )	This paper presents a new approach	based on Equivalent Pseudowords ( EPs )	1-25	1-25	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	1<2	none	bg-general	bg-general
P06-1058	1-6	14-25	This paper presents a new approach	to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	This paper presents a new approach	to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	1-25	1-25	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	1<2	none	enablement	enablement
P06-1058	7-13	26-32	based on Equivalent Pseudowords ( EPs )	EPs are particular artificial ambiguous words ,	based on Equivalent Pseudowords ( EPs )	EPs are particular artificial ambiguous words ,	1-25	26-41	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	1<2	none	elab-addition	elab-addition
P06-1058	26-32	33-36	EPs are particular artificial ambiguous words ,	which can be used	EPs are particular artificial ambiguous words ,	which can be used	26-41	26-41	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	1<2	none	elab-addition	elab-addition
P06-1058	33-36	37-41	which can be used	to realize unsupervised WSD .	which can be used	to realize unsupervised WSD .	26-41	26-41	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	1<2	none	enablement	enablement
P06-1058	26-32	42-46	EPs are particular artificial ambiguous words ,	A Bayesian classifier is implemented	EPs are particular artificial ambiguous words ,	A Bayesian classifier is implemented	26-41	42-60	EPs are particular artificial ambiguous words , which can be used to realize unsupervised WSD .	A Bayesian classifier is implemented to test the efficacy of the EP solution on Senseval-3 Chinese test set .	1<2	none	elab-addition	elab-addition
P06-1058	42-46	47-60	A Bayesian classifier is implemented	to test the efficacy of the EP solution on Senseval-3 Chinese test set .	A Bayesian classifier is implemented	to test the efficacy of the EP solution on Senseval-3 Chinese test set .	42-60	42-60	A Bayesian classifier is implemented to test the efficacy of the EP solution on Senseval-3 Chinese test set .	A Bayesian classifier is implemented to test the efficacy of the EP solution on Senseval-3 Chinese test set .	1<2	none	enablement	enablement
P06-1058	1-6	61-74	This paper presents a new approach	The performance is better than state-of-the-art results with an average F-measure of 0.80 .	This paper presents a new approach	The performance is better than state-of-the-art results with an average F-measure of 0.80 .	1-25	61-74	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	The performance is better than state-of-the-art results with an average F-measure of 0.80 .	1<2	none	evaluation	evaluation
P06-1058	1-6	75-85	This paper presents a new approach	The experiment verifies the value of EP for unsupervised WSD .	This paper presents a new approach	The experiment verifies the value of EP for unsupervised WSD .	1-25	75-85	This paper presents a new approach based on Equivalent Pseudowords ( EPs ) to tackle Word Sense Disambiguation ( WSD ) in Chinese language .	The experiment verifies the value of EP for unsupervised WSD .	1<2	none	evaluation	evaluation
P06-1059	1-4	5-18	This paper presents techniques	to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	This paper presents techniques	to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	1-18	1-18	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	1<2	none	enablement	enablement
P06-1059	1-4	19-25	This paper presents techniques	Our framework can handle an NER task	This paper presents techniques	Our framework can handle an NER task	1-18	19-39	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost .	1<2	none	elab-addition	elab-addition
P06-1059	19-25	26-33	Our framework can handle an NER task	that has long named entities and many labels	Our framework can handle an NER task	that has long named entities and many labels	19-39	19-39	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost .	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost .	1<2	none	elab-addition	elab-addition
P06-1059	26-33	34-39	that has long named entities and many labels	which increase the computational cost .	that has long named entities and many labels	which increase the computational cost .	19-39	19-39	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost .	Our framework can handle an NER task that has long named entities and many labels which increase the computational cost .	1<2	none	elab-addition	elab-addition
P06-1059	40-45	46-50	To reduce the computational cost ,	we propose two techniques :	To reduce the computational cost ,	we propose two techniques :	40-86	40-86	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1>2	none	enablement	enablement
P06-1059	1-4	46-50	This paper presents techniques	we propose two techniques :	This paper presents techniques	we propose two techniques :	1-18	40-86	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1<2	none	elab-addition	elab-addition
P06-1059	46-50	51-59	we propose two techniques :	the first is the use of feature forests ,	we propose two techniques :	the first is the use of feature forests ,	40-86	40-86	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1<2	none	elab-enumember	elab-enumember
P06-1059	51-59	60-67	the first is the use of feature forests ,	which enables us to pack feature-equivalent states ,	the first is the use of feature forests ,	which enables us to pack feature-equivalent states ,	40-86	40-86	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1<2	none	elab-addition	elab-addition
P06-1059	46-50	68-77	we propose two techniques :	and the second is the introduction of a filtering process	we propose two techniques :	and the second is the introduction of a filtering process	40-86	40-86	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1<2	none	elab-enumember	elab-enumember
P06-1059	68-77	78-86	and the second is the introduction of a filtering process	which significantly reduces the number of candidate states .	and the second is the introduction of a filtering process	which significantly reduces the number of candidate states .	40-86	40-86	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	To reduce the computational cost , we propose two techniques : the first is the use of feature forests , which enables us to pack feature-equivalent states , and the second is the introduction of a filtering process which significantly reduces the number of candidate states .	1<2	none	elab-addition	elab-addition
P06-1059	1-4	87-97	This paper presents techniques	This framework allows us to use a rich set of features	This paper presents techniques	This framework allows us to use a rich set of features	1-18	87-110	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities .	1<2	none	elab-addition	elab-addition
P06-1059	87-97	98-102	This framework allows us to use a rich set of features	extracted from the chunk-based representation	This framework allows us to use a rich set of features	extracted from the chunk-based representation	87-110	87-110	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities .	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities .	1<2	none	elab-addition	elab-addition
P06-1059	98-102	103-110	extracted from the chunk-based representation	that can capture informative characteristics of entities .	extracted from the chunk-based representation	that can capture informative characteristics of entities .	87-110	87-110	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities .	This framework allows us to use a rich set of features extracted from the chunk-based representation that can capture informative characteristics of entities .	1<2	none	elab-addition	elab-addition
P06-1059	1-4	111-116	This paper presents techniques	We also introduce a simple trick	This paper presents techniques	We also introduce a simple trick	1-18	111-130	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels .	1<2	none	elab-addition	elab-addition
P06-1059	111-116	117-122	We also introduce a simple trick	to transfer information about distant entities	We also introduce a simple trick	to transfer information about distant entities	111-130	111-130	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels .	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels .	1<2	none	enablement	enablement
P06-1059	117-122	123-130	to transfer information about distant entities	by embedding label information into non-entity labels .	to transfer information about distant entities	by embedding label information into non-entity labels .	111-130	111-130	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels .	We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels .	1<2	none	manner-means	manner-means
P06-1059	131-133	134-148	Experimental results show	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	Experimental results show	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	131-157	131-157	Experimental results show that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task without using any external resources or post-processing techniques .	Experimental results show that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task without using any external resources or post-processing techniques .	1>2	none	attribution	attribution
P06-1059	1-4	134-148	This paper presents techniques	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	This paper presents techniques	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	1-18	131-157	This paper presents techniques to apply semi-CRFs to Named Entity Recognition tasks with a tractable computational cost .	Experimental results show that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task without using any external resources or post-processing techniques .	1<2	none	evaluation	evaluation
P06-1059	134-148	149-157	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	without using any external resources or post-processing techniques .	that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task	without using any external resources or post-processing techniques .	131-157	131-157	Experimental results show that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task without using any external resources or post-processing techniques .	Experimental results show that our model achieves an F-score of 71.48 % on the JNLPBA 2004 shared task without using any external resources or post-processing techniques .	1<2	none	condition	condition
P06-1060	1-11	12-19	As natural language understanding research advances towards deeper knowledge modeling ,	the tasks become more and more complex :	As natural language understanding research advances towards deeper knowledge modeling ,	the tasks become more and more complex :	1-38	1-38	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	1>2	none	temporal	temporal
P06-1060	12-19	94-101	the tasks become more and more complex :	In this article , we investigate three methods	the tasks become more and more complex :	In this article , we investigate three methods	1-38	94-114	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	1>2	none	bg-goal	bg-goal
P06-1060	12-19	20-38	the tasks become more and more complex :	we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	the tasks become more and more complex :	we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	1-38	1-38	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	1<2	none	elab-addition	elab-addition
P06-1060	12-19	39-42,48-61	the tasks become more and more complex :	One such example , <*> is the mention detection and recognition task in the Automatic Content Extraction project ,	the tasks become more and more complex :	One such example , <*> is the mention detection and recognition task in the Automatic Content Extraction project ,	1-38	39-93	As natural language understanding research advances towards deeper knowledge modeling , the tasks become more and more complex : we are interested in more nuanced word characteristics , more linguistic properties , deeper semantic and syntactic features .	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	1<2	none	elab-example	elab-example
P06-1060	39-42,48-61	43-47	One such example , <*> is the mention detection and recognition task in the Automatic Content Extraction project ,	explored in this article ,	One such example , <*> is the mention detection and recognition task in the Automatic Content Extraction project ,	explored in this article ,	39-93	39-93	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	1<2	none	elab-addition	elab-addition
P06-1060	48-61	62-75	is the mention detection and recognition task in the Automatic Content Extraction project ,	with the goal of identifying named , nominal or pronominal references to real-world entities—mentions—	is the mention detection and recognition task in the Automatic Content Extraction project ,	with the goal of identifying named , nominal or pronominal references to real-world entities—mentions—	39-93	39-93	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	1<2	none	elab-addition	elab-addition
P06-1060	62-75	76-84	with the goal of identifying named , nominal or pronominal references to real-world entities—mentions—	and labeling them with three types of information :	with the goal of identifying named , nominal or pronominal references to real-world entities—mentions—	and labeling them with three types of information :	39-93	39-93	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	1<2	none	joint	joint
P06-1060	76-84	85-93	and labeling them with three types of information :	entity type , entity subtype and mention type .	and labeling them with three types of information :	entity type , entity subtype and mention type .	39-93	39-93	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	One such example , explored in this article , is the mention detection and recognition task in the Automatic Content Extraction project , with the goal of identifying named , nominal or pronominal references to real-world entities—mentions— and labeling them with three types of information : entity type , entity subtype and mention type .	1<2	none	elab-enumember	elab-enumember
P06-1060	94-101	102-106	In this article , we investigate three methods	of assigning these related tags	In this article , we investigate three methods	of assigning these related tags	94-114	94-114	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	1<2	none	elab-addition	elab-addition
P06-1060	94-101	107-114	In this article , we investigate three methods	and compare them on several data sets .	In this article , we investigate three methods	and compare them on several data sets .	94-114	94-114	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	1<2	none	joint	joint
P06-1060	94-101	115-116,125-134	In this article , we investigate three methods	A system <*> participated and ranked very competitively in the ACE'04 evaluation .	In this article , we investigate three methods	A system <*> participated and ranked very competitively in the ACE'04 evaluation .	94-114	115-134	In this article , we investigate three methods of assigning these related tags and compare them on several data sets .	A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation .	1<2	none	evaluation	evaluation
P06-1060	115-116,125-134	117-120	A system <*> participated and ranked very competitively in the ACE'04 evaluation .	based on the methods	A system <*> participated and ranked very competitively in the ACE'04 evaluation .	based on the methods	115-134	115-134	A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation .	A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation .	1<2	none	bg-general	bg-general
P06-1060	117-120	121-124	based on the methods	presented in this article	based on the methods	presented in this article	115-134	115-134	A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation .	A system based on the methods presented in this article participated and ranked very competitively in the ACE'04 evaluation .	1<2	none	elab-addition	elab-addition
P06-1061	1-10	32-35	Hidden Markov models ( HMMs ) are powerful statistical models	an HMM is used	Hidden Markov models ( HMMs ) are powerful statistical models	an HMM is used	1-22	23-43	Hidden Markov models ( HMMs ) are powerful statistical models that have found successful applications in Information Extraction ( IE ) .	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	1>2	none	elab-addition	elab-addition
P06-1061	1-10	11-22	Hidden Markov models ( HMMs ) are powerful statistical models	that have found successful applications in Information Extraction ( IE ) .	Hidden Markov models ( HMMs ) are powerful statistical models	that have found successful applications in Information Extraction ( IE ) .	1-22	1-22	Hidden Markov models ( HMMs ) are powerful statistical models that have found successful applications in Information Extraction ( IE ) .	Hidden Markov models ( HMMs ) are powerful statistical models that have found successful applications in Information Extraction ( IE ) .	1<2	none	elab-addition	elab-addition
P06-1061	23-25	32-35	In current approaches	an HMM is used	In current approaches	an HMM is used	23-43	23-43	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	1>2	none	elab-addition	elab-addition
P06-1061	23-25	26-31	In current approaches	to applying HMMs to IE ,	In current approaches	to applying HMMs to IE ,	23-43	23-43	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	1<2	none	enablement	enablement
P06-1061	32-35	65-69	an HMM is used	We propose to use HMMs	an HMM is used	We propose to use HMMs	23-43	65-97	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	1>2	none	bg-compare	bg-compare
P06-1061	32-35	36-43	an HMM is used	to model text at the document level .	an HMM is used	to model text at the document level .	23-43	23-43	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	1<2	none	enablement	enablement
P06-1061	32-35	44-54	an HMM is used	This modelling might cause undesired redundancy in extraction in the sense	an HMM is used	This modelling might cause undesired redundancy in extraction in the sense	23-43	44-64	In current approaches to applying HMMs to IE , an HMM is used to model text at the document level .	This modelling might cause undesired redundancy in extraction in the sense that more than one filler is identified and extracted .	1<2	none	elab-addition	elab-addition
P06-1061	44-54	55-64	This modelling might cause undesired redundancy in extraction in the sense	that more than one filler is identified and extracted .	This modelling might cause undesired redundancy in extraction in the sense	that more than one filler is identified and extracted .	44-64	44-64	This modelling might cause undesired redundancy in extraction in the sense that more than one filler is identified and extracted .	This modelling might cause undesired redundancy in extraction in the sense that more than one filler is identified and extracted .	1<2	none	elab-addition	elab-addition
P06-1061	65-69	70-77	We propose to use HMMs	to model text at the segment level ,	We propose to use HMMs	to model text at the segment level ,	65-97	65-97	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	1<2	none	enablement	enablement
P06-1061	70-77	78-87	to model text at the segment level ,	in which the extraction process consists of two steps :	to model text at the segment level ,	in which the extraction process consists of two steps :	65-97	65-97	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	1<2	none	elab-addition	elab-addition
P06-1061	78-87	88-91	in which the extraction process consists of two steps :	a segment retrieval step	in which the extraction process consists of two steps :	a segment retrieval step	65-97	65-97	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	1<2	none	elab-enumember	elab-enumember
P06-1061	88-91	92-97	a segment retrieval step	followed by an extraction step .	a segment retrieval step	followed by an extraction step .	65-97	65-97	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	1<2	none	elab-addition	elab-addition
P06-1061	98-106	107-110	In order to retrieve extractionrelevant segments from documents ,	we introduce a method	In order to retrieve extractionrelevant segments from documents ,	we introduce a method	98-119	98-119	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	1>2	none	enablement	enablement
P06-1061	65-69	107-110	We propose to use HMMs	we introduce a method	We propose to use HMMs	we introduce a method	65-97	98-119	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	1<2	none	elab-addition	elab-addition
P06-1061	107-110	111-113	we introduce a method	to use HMMs	we introduce a method	to use HMMs	98-119	98-119	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	1<2	none	enablement	enablement
P06-1061	111-113	114-119	to use HMMs	to model and retrieve segments .	to use HMMs	to model and retrieve segments .	98-119	98-119	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	In order to retrieve extractionrelevant segments from documents , we introduce a method to use HMMs to model and retrieve segments .	1<2	none	enablement	enablement
P06-1061	120-123	139-152	Our experimental results show	but also has better overall extraction performance than traditional document HMM IE systems .	Our experimental results show	but also has better overall extraction performance than traditional document HMM IE systems .	120-152	120-152	Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy , but also has better overall extraction performance than traditional document HMM IE systems .	Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy , but also has better overall extraction performance than traditional document HMM IE systems .	1>2	none	attribution	attribution
P06-1061	124-138	139-152	that the resulting segment HMM IE system not only achieves near zero extraction redundancy ,	but also has better overall extraction performance than traditional document HMM IE systems .	that the resulting segment HMM IE system not only achieves near zero extraction redundancy ,	but also has better overall extraction performance than traditional document HMM IE systems .	120-152	120-152	Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy , but also has better overall extraction performance than traditional document HMM IE systems .	Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy , but also has better overall extraction performance than traditional document HMM IE systems .	1>2	none	progression	progression
P06-1061	65-69	139-152	We propose to use HMMs	but also has better overall extraction performance than traditional document HMM IE systems .	We propose to use HMMs	but also has better overall extraction performance than traditional document HMM IE systems .	65-97	120-152	We propose to use HMMs to model text at the segment level , in which the extraction process consists of two steps : a segment retrieval step followed by an extraction step .	Our experimental results show that the resulting segment HMM IE system not only achieves near zero extraction redundancy , but also has better overall extraction performance than traditional document HMM IE systems .	1<2	none	evaluation	evaluation
P06-1062	14-23	24-33	Based on the Document Object Model ( DOM ) ,	a web page is represented as a DOM tree .	Based on the Document Object Model ( DOM ) ,	a web page is represented as a DOM tree .	14-33	14-33	Based on the Document Object Model ( DOM ) , a web page is represented as a DOM tree .	Based on the Document Object Model ( DOM ) , a web page is represented as a DOM tree .	1>2	none	bg-general	bg-general
P06-1062	1-13	24-33	This paper presents a new web mining scheme for parallel data acquisition .	a web page is represented as a DOM tree .	This paper presents a new web mining scheme for parallel data acquisition .	a web page is represented as a DOM tree .	1-13	14-33	This paper presents a new web mining scheme for parallel data acquisition .	Based on the Document Object Model ( DOM ) , a web page is represented as a DOM tree .	1<2	none	elab-process_step	elab-process_step
P06-1062	1-13	34-41	This paper presents a new web mining scheme for parallel data acquisition .	Then a DOM tree alignment model is proposed	This paper presents a new web mining scheme for parallel data acquisition .	Then a DOM tree alignment model is proposed	1-13	34-55	This paper presents a new web mining scheme for parallel data acquisition .	Then a DOM tree alignment model is proposed to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees .	1<2	none	elab-process_step	elab-process_step
P06-1062	34-41	42-55	Then a DOM tree alignment model is proposed	to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees .	Then a DOM tree alignment model is proposed	to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees .	34-55	34-55	Then a DOM tree alignment model is proposed to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees .	Then a DOM tree alignment model is proposed to identify the translationally equivalent texts and hyperlinks between two parallel DOM trees .	1<2	none	enablement	enablement
P06-1062	56-62	63-69	By tracing the identified parallel hyperlinks ,	parallel web documents are recursively mined .	By tracing the identified parallel hyperlinks ,	parallel web documents are recursively mined .	56-69	56-69	By tracing the identified parallel hyperlinks , parallel web documents are recursively mined .	By tracing the identified parallel hyperlinks , parallel web documents are recursively mined .	1>2	none	manner-means	manner-means
P06-1062	1-13	63-69	This paper presents a new web mining scheme for parallel data acquisition .	parallel web documents are recursively mined .	This paper presents a new web mining scheme for parallel data acquisition .	parallel web documents are recursively mined .	1-13	56-69	This paper presents a new web mining scheme for parallel data acquisition .	By tracing the identified parallel hyperlinks , parallel web documents are recursively mined .	1<2	none	elab-process_step	elab-process_step
P06-1062	70-75	79-88	Compared with previous mining schemes ,	that this new mining scheme improves the mining coverage ,	Compared with previous mining schemes ,	that this new mining scheme improves the mining coverage ,	70-101	70-101	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	1>2	none	comparison	comparison
P06-1062	76-78	79-88	the benchmarks show	that this new mining scheme improves the mining coverage ,	the benchmarks show	that this new mining scheme improves the mining coverage ,	70-101	70-101	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	1>2	none	attribution	attribution
P06-1062	1-13	79-88	This paper presents a new web mining scheme for parallel data acquisition .	that this new mining scheme improves the mining coverage ,	This paper presents a new web mining scheme for parallel data acquisition .	that this new mining scheme improves the mining coverage ,	1-13	70-101	This paper presents a new web mining scheme for parallel data acquisition .	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	1<2	none	evaluation	evaluation
P06-1062	79-88	89-92	that this new mining scheme improves the mining coverage ,	reduces mining bandwidth ,	that this new mining scheme improves the mining coverage ,	reduces mining bandwidth ,	70-101	70-101	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	1<2	none	joint	joint
P06-1062	79-88	93-101	that this new mining scheme improves the mining coverage ,	and enhances the quality of mined parallel sentences .	that this new mining scheme improves the mining coverage ,	and enhances the quality of mined parallel sentences .	70-101	70-101	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	Compared with previous mining schemes , the benchmarks show that this new mining scheme improves the mining coverage , reduces mining bandwidth , and enhances the quality of mined parallel sentences .	1<2	none	joint	joint
P06-1063	1-15	16-22	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	( i ) use in training parsers	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	( i ) use in training parsers	1-35	1-35	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	1<2	none	elab-addition	elab-addition
P06-1063	16-22	23-26	( i ) use in training parsers	employed in QA ,	( i ) use in training parsers	employed in QA ,	1-35	1-35	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	1<2	none	elab-addition	elab-addition
P06-1063	1-15	27-35	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	and ( ii ) evaluation of question parsing .	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	and ( ii ) evaluation of question parsing .	1-35	1-35	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	1<2	none	elab-addition	elab-addition
P06-1063	1-15	36-41	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	We present a series of experiments	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	We present a series of experiments	1-35	36-68	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets .	1<2	none	elab-addition	elab-addition
P06-1063	36-41	42-59	We present a series of experiments	to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser	We present a series of experiments	to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser	36-68	36-68	We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets .	We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets .	1<2	none	enablement	enablement
P06-1063	42-59	60-68	to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser	in parsing both question and non-question test sets .	to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser	in parsing both question and non-question test sets .	36-68	36-68	We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets .	We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets .	1<2	none	elab-addition	elab-addition
P06-1063	1-15	69-80	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	We introduce a new method for recovering empty nodes and their antecedents	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for	We introduce a new method for recovering empty nodes and their antecedents	1-35	69-97	This paper describes the development of QuestionBank , a corpus of 4000 parseannotated questions for ( i ) use in training parsers employed in QA , and ( ii ) evaluation of question parsing .	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	1<2	none	elab-addition	elab-addition
P06-1063	69-80	81-86	We introduce a new method for recovering empty nodes and their antecedents	( capturing long distance dependencies )	We introduce a new method for recovering empty nodes and their antecedents	( capturing long distance dependencies )	69-97	69-97	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	1<2	none	elab-addition	elab-addition
P06-1063	69-80	87-92	We introduce a new method for recovering empty nodes and their antecedents	from parser output in CFG trees	We introduce a new method for recovering empty nodes and their antecedents	from parser output in CFG trees	69-97	69-97	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	1<2	none	elab-addition	elab-addition
P06-1063	69-80	93-97	We introduce a new method for recovering empty nodes and their antecedents	using LFG f-structure reentrancies .	We introduce a new method for recovering empty nodes and their antecedents	using LFG f-structure reentrancies .	69-97	69-97	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	We introduce a new method for recovering empty nodes and their antecedents ( capturing long distance dependencies ) from parser output in CFG trees using LFG f-structure reentrancies .	1<2	none	manner-means	manner-means
P06-1064	1-4	5-9	We present an algorithm	which creates a German CCGbank	We present an algorithm	which creates a German CCGbank	1-24	1-24	We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	1<2	none	elab-addition	elab-addition
P06-1064	5-9	10-24	which creates a German CCGbank	by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	which creates a German CCGbank	by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	1-24	1-24	We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	1<2	none	manner-means	manner-means
P06-1064	1-4	25-31	We present an algorithm	The resulting corpus contains 46,628 derivations ,	We present an algorithm	The resulting corpus contains 46,628 derivations ,	1-24	25-41	We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees .	The resulting corpus contains 46,628 derivations , covering 95 % of all complete sentences in Tiger .	1<2	none	evaluation	evaluation
P06-1064	25-31	32-41	The resulting corpus contains 46,628 derivations ,	covering 95 % of all complete sentences in Tiger .	The resulting corpus contains 46,628 derivations ,	covering 95 % of all complete sentences in Tiger .	25-41	25-41	The resulting corpus contains 46,628 derivations , covering 95 % of all complete sentences in Tiger .	The resulting corpus contains 46,628 derivations , covering 95 % of all complete sentences in Tiger .	1<2	none	elab-addition	elab-addition
P06-1064	25-31	42,47-61	The resulting corpus contains 46,628 derivations ,	Lexicons <*> contain correct lexical entries for 94 % of all known tokens in unseen text .	The resulting corpus contains 46,628 derivations ,	Lexicons <*> contain correct lexical entries for 94 % of all known tokens in unseen text .	25-41	42-61	The resulting corpus contains 46,628 derivations , covering 95 % of all complete sentences in Tiger .	Lexicons extracted from this corpus contain correct lexical entries for 94 % of all known tokens in unseen text .	1<2	none	elab-addition	elab-addition
P06-1064	42,47-61	43-46	Lexicons <*> contain correct lexical entries for 94 % of all known tokens in unseen text .	extracted from this corpus	Lexicons <*> contain correct lexical entries for 94 % of all known tokens in unseen text .	extracted from this corpus	42-61	42-61	Lexicons extracted from this corpus contain correct lexical entries for 94 % of all known tokens in unseen text .	Lexicons extracted from this corpus contain correct lexical entries for 94 % of all known tokens in unseen text .	1<2	none	elab-addition	elab-addition
P06-1065	1-11	25-30	For many years , statistical machine translation relied on generative models	that discriminative models could be used	For many years , statistical machine translation relied on generative models	that discriminative models could be used	1-17	18-39	For many years , statistical machine translation relied on generative models to provide bilingual word alignments .	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	1>2	none	elab-addition	elab-addition
P06-1065	1-11	12-17	For many years , statistical machine translation relied on generative models	to provide bilingual word alignments .	For many years , statistical machine translation relied on generative models	to provide bilingual word alignments .	1-17	1-17	For many years , statistical machine translation relied on generative models to provide bilingual word alignments .	For many years , statistical machine translation relied on generative models to provide bilingual word alignments .	1<2	none	enablement	enablement
P06-1065	18-24	25-30	In 2005 , several independent efforts showed	that discriminative models could be used	In 2005 , several independent efforts showed	that discriminative models could be used	18-39	18-39	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	1>2	none	attribution	attribution
P06-1065	25-30	45-58	that discriminative models could be used	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	that discriminative models could be used	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	18-39	40-68	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	1>2	none	bg-goal	bg-goal
P06-1065	25-30	31-39	that discriminative models could be used	to enhance or replace the standard generative approach .	that discriminative models could be used	to enhance or replace the standard generative approach .	18-39	18-39	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .	1<2	none	enablement	enablement
P06-1065	40-44	45-58	Building on this work ,	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	Building on this work ,	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	40-68	40-68	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	1>2	none	bg-general	bg-general
P06-1065	45-58	59-68	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	but predominantly through selection of more and better features .	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	but predominantly through selection of more and better features .	40-68	40-68	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	1<2	none	contrast	contrast
P06-1065	45-58	69-77	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	Our best model produces the lowest alignment error rate	we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods ,	Our best model produces the lowest alignment error rate	40-68	69-85	Building on this work , we demonstrate substantial improvement in word-alignment accuracy , partly though improved training methods , but predominantly through selection of more and better features .	Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data .	1<2	none	evaluation	evaluation
P06-1065	69-77	78-85	Our best model produces the lowest alignment error rate	yet reported on Canadian Hansards bilingual data .	Our best model produces the lowest alignment error rate	yet reported on Canadian Hansards bilingual data .	69-85	69-85	Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data .	Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data .	1<2	none	elab-addition	elab-addition
P06-1066	1-14	15-23	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	that uses a maximum entropy ( MaxEnt ) model	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	that uses a maximum entropy ( MaxEnt ) model	1-34	1-34	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	1<2	none	elab-addition	elab-addition
P06-1066	15-23	24-34	that uses a maximum entropy ( MaxEnt ) model	to predicate reorderings of neighbor blocks ( phrase pairs ) .	that uses a maximum entropy ( MaxEnt ) model	to predicate reorderings of neighbor blocks ( phrase pairs ) .	1-34	1-34	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	1<2	none	enablement	enablement
P06-1066	1-14	35-47	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features	1-34	35-54	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext .	1<2	none	elab-addition	elab-addition
P06-1066	35-47	48-54	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features	automatically learned from a real-world bitext .	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features	automatically learned from a real-world bitext .	35-54	35-54	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext .	The model provides content-dependent , hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext .	1<2	none	elab-addition	elab-addition
P06-1066	1-14	55-58	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	We present an algorithm	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	We present an algorithm	1-34	55-70	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	We present an algorithm to extract all reordering events of neighbor blocks from bilingual data .	1<2	none	elab-addition	elab-addition
P06-1066	55-58	59-70	We present an algorithm	to extract all reordering events of neighbor blocks from bilingual data .	We present an algorithm	to extract all reordering events of neighbor blocks from bilingual data .	55-70	55-70	We present an algorithm to extract all reordering events of neighbor blocks from bilingual data .	We present an algorithm to extract all reordering events of neighbor blocks from bilingual data .	1<2	none	enablement	enablement
P06-1066	1-14	71-95	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	In our experiments on Chineseto-English translation , this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks .	We propose a novel reordering model for phrase-based statistical machine translation ( SMT )	In our experiments on Chineseto-English translation , this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks .	1-34	71-95	We propose a novel reordering model for phrase-based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .	In our experiments on Chineseto-English translation , this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks .	1<2	none	evaluation	evaluation
P06-1067	1-6	7-13	In this paper , we argue	that n-gram language models are not sufficient	In this paper , we argue	that n-gram language models are not sufficient	1-22	1-22	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	1>2	none	attribution	attribution
P06-1067	7-13	14-17	that n-gram language models are not sufficient	to address word reordering	that n-gram language models are not sufficient	to address word reordering	1-22	1-22	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	1<2	none	enablement	enablement
P06-1067	14-17	18-22	to address word reordering	required for Machine Translation .	to address word reordering	required for Machine Translation .	1-22	1-22	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	1<2	none	elab-addition	elab-addition
P06-1067	7-13	23-28	that n-gram language models are not sufficient	We propose a new distortion model	that n-gram language models are not sufficient	We propose a new distortion model	1-22	23-45	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	1<2	none	elab-addition	elab-addition
P06-1067	23-28	29-37	We propose a new distortion model	that can be used with existing phrase-based SMT decoders	We propose a new distortion model	that can be used with existing phrase-based SMT decoders	23-45	23-45	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	1<2	none	elab-addition	elab-addition
P06-1067	29-37	38-45	that can be used with existing phrase-based SMT decoders	to address those n-gram language model limitations .	that can be used with existing phrase-based SMT decoders	to address those n-gram language model limitations .	23-45	23-45	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	1<2	none	enablement	enablement
P06-1067	23-28	46-55	We propose a new distortion model	We present empirical results in Arabic to English Machine Translation	We propose a new distortion model	We present empirical results in Arabic to English Machine Translation	23-45	46-67	We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations .	We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used .	1<2	none	evaluation	evaluation
P06-1067	46-55	56-60	We present empirical results in Arabic to English Machine Translation	that show statistically significant improvements	We present empirical results in Arabic to English Machine Translation	that show statistically significant improvements	46-67	46-67	We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used .	We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used .	1<2	none	elab-addition	elab-addition
P06-1067	56-60	61-67	that show statistically significant improvements	when our proposed model is used .	that show statistically significant improvements	when our proposed model is used .	46-67	46-67	We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used .	We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used .	1<2	none	condition	condition
P06-1067	7-13	68-73	that n-gram language models are not sufficient	We also propose a novel metric	that n-gram language models are not sufficient	We also propose a novel metric	1-22	68-92	In this paper , we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation .	We also propose a novel metric to measure word order similarity ( or difference ) between any pair of languages based on word alignments .	1<2	none	elab-addition	elab-addition
P06-1067	68-73	74-87	We also propose a novel metric	to measure word order similarity ( or difference ) between any pair of languages	We also propose a novel metric	to measure word order similarity ( or difference ) between any pair of languages	68-92	68-92	We also propose a novel metric to measure word order similarity ( or difference ) between any pair of languages based on word alignments .	We also propose a novel metric to measure word order similarity ( or difference ) between any pair of languages based on word alignments .	1<2	none	enablement	enablement
P06-1067	74-87	88-92	to measure word order similarity ( or difference ) between any pair of languages	based on word alignments .	to measure word order similarity ( or difference ) between any pair of languages	based on word alignments .	68-92	68-92	We also propose a novel metric to measure word order similarity ( or difference ) between any pair of languages based on word alignments .	We also propose a novel metric to measure word order similarity ( or difference ) between any pair of languages based on word alignments .	1<2	none	bg-general	bg-general
P06-1068	1-6	7-15	This paper presents a study on	if and how automatically extracted keywords can be used	This paper presents a study on	if and how automatically extracted keywords can be used	1-20	1-20	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	1<2	none	elab-addition	elab-addition
P06-1068	7-15	16-20	if and how automatically extracted keywords can be used	to improve text categorization .	if and how automatically extracted keywords can be used	to improve text categorization .	1-20	1-20	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	1<2	none	enablement	enablement
P06-1068	21-24	25-28,42-43	In summary we show	that a higher performance <*> is achieved	In summary we show	that a higher performance <*> is achieved	21-55	21-55	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	1>2	none	attribution	attribution
P06-1068	1-6	25-28,42-43	This paper presents a study on	that a higher performance <*> is achieved	This paper presents a study on	that a higher performance <*> is achieved	1-20	21-55	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	1<2	none	summary	summary
P06-1068	25-28,42-43	29-41	that a higher performance <*> is achieved	— as measured by micro-averaged F-measure on a standard text categorization collection —	that a higher performance <*> is achieved	— as measured by micro-averaged F-measure on a standard text categorization collection —	21-55	21-55	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	1<2	none	elab-addition	elab-addition
P06-1068	25-28,42-43	44-55	that a higher performance <*> is achieved	when the full-text representation is combined with the automatically extracted keywords .	that a higher performance <*> is achieved	when the full-text representation is combined with the automatically extracted keywords .	21-55	21-55	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	1<2	none	condition	condition
P06-1068	44-55	56-59	when the full-text representation is combined with the automatically extracted keywords .	The combination is obtained	when the full-text representation is combined with the automatically extracted keywords .	The combination is obtained	21-55	56-75	In summary we show that a higher performance — as measured by micro-averaged F-measure on a standard text categorization collection — is achieved when the full-text representation is combined with the automatically extracted keywords .	The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords .	1<2	none	elab-addition	elab-addition
P06-1068	56-59	60-68	The combination is obtained	by giving higher weights to words in the full-texts	The combination is obtained	by giving higher weights to words in the full-texts	56-75	56-75	The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords .	The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords .	1<2	none	manner-means	manner-means
P06-1068	60-68	69-75	by giving higher weights to words in the full-texts	that are also extracted as keywords .	by giving higher weights to words in the full-texts	that are also extracted as keywords .	56-75	56-75	The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords .	The combination is obtained by giving higher weights to words in the full-texts that are also extracted as keywords .	1<2	none	elab-addition	elab-addition
P06-1068	76-81	101-112	We also present results for experiments	Of these two experiments , the unigrams have the best performance ,	We also present results for experiments	Of these two experiments , the unigrams have the best performance ,	76-100	101-121	We also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact .	Of these two experiments , the unigrams have the best performance , although neither performs as well as headlines only .	1>2	none	result	result
P06-1068	76-81	82-93	We also present results for experiments	in which the keywords are the only input to the categorizer ,	We also present results for experiments	in which the keywords are the only input to the categorizer ,	76-100	76-100	We also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact .	We also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact .	1<2	none	elab-addition	elab-addition
P06-1068	82-93	94-100	in which the keywords are the only input to the categorizer ,	either represented as unigrams or intact .	in which the keywords are the only input to the categorizer ,	either represented as unigrams or intact .	76-100	76-100	We also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact .	We also present results for experiments in which the keywords are the only input to the categorizer , either represented as unigrams or intact .	1<2	none	elab-addition	elab-addition
P06-1068	1-6	101-112	This paper presents a study on	Of these two experiments , the unigrams have the best performance ,	This paper presents a study on	Of these two experiments , the unigrams have the best performance ,	1-20	101-121	This paper presents a study on if and how automatically extracted keywords can be used to improve text categorization .	Of these two experiments , the unigrams have the best performance , although neither performs as well as headlines only .	1<2	none	evaluation	evaluation
P06-1068	101-112	113-121	Of these two experiments , the unigrams have the best performance ,	although neither performs as well as headlines only .	Of these two experiments , the unigrams have the best performance ,	although neither performs as well as headlines only .	101-121	101-121	Of these two experiments , the unigrams have the best performance , although neither performs as well as headlines only .	Of these two experiments , the unigrams have the best performance , although neither performs as well as headlines only .	1<2	none	contrast	contrast
P06-1069	1-14	15-34	Words and character-bigrams are both used as features in Chinese text processing tasks ,	but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	Words and character-bigrams are both used as features in Chinese text processing tasks ,	but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	1-34	1-34	Words and character-bigrams are both used as features in Chinese text processing tasks , but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	Words and character-bigrams are both used as features in Chinese text processing tasks , but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	1>2	none	contrast	contrast
P06-1069	15-34	35-50	but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	We carry out here a full performance comparison between them by experiments on various document collections	but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	We carry out here a full performance comparison between them by experiments on various document collections	1-34	35-104	Words and character-bigrams are both used as features in Chinese text processing tasks , but no systematic comparison or analysis of their values as features for Chinese text categorization has been reported heretofore .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1>2	none	bg-compare	bg-compare
P06-1069	35-50	51-62	We carry out here a full performance comparison between them by experiments on various document collections	( including a manually word-segmented corpus as a golden standard ) ,	We carry out here a full performance comparison between them by experiments on various document collections	( including a manually word-segmented corpus as a golden standard ) ,	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	elab-example	elab-example
P06-1069	35-50	63-66	We carry out here a full performance comparison between them by experiments on various document collections	and a semi-quantitative analysis	We carry out here a full performance comparison between them by experiments on various document collections	and a semi-quantitative analysis	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	joint	joint
P06-1069	35-50	67-74	We carry out here a full performance comparison between them by experiments on various document collections	to elucidate the characteristics of their behavior ;	We carry out here a full performance comparison between them by experiments on various document collections	to elucidate the characteristics of their behavior ;	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	enablement	enablement
P06-1069	35-50	75-85	We carry out here a full performance comparison between them by experiments on various document collections	and try to provide some preliminary clue for feature term choice	We carry out here a full performance comparison between them by experiments on various document collections	and try to provide some preliminary clue for feature term choice	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	joint	joint
P06-1069	75-85	86-96	and try to provide some preliminary clue for feature term choice	( in most cases , character-bigrams are better than words )	and try to provide some preliminary clue for feature term choice	( in most cases , character-bigrams are better than words )	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	elab-addition	elab-addition
P06-1069	75-85	97-104	and try to provide some preliminary clue for feature term choice	and dimensionality setting in text categorization systems .	and try to provide some preliminary clue for feature term choice	and dimensionality setting in text categorization systems .	35-104	35-104	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	We carry out here a full performance comparison between them by experiments on various document collections ( including a manually word-segmented corpus as a golden standard ) , and a semi-quantitative analysis to elucidate the characteristics of their behavior ; and try to provide some preliminary clue for feature term choice ( in most cases , character-bigrams are better than words ) and dimensionality setting in text categorization systems .	1<2	none	joint	joint
P06-1070	1-6	39-45	Cross-language Text Categorization is the task	In this work we present many solutions	Cross-language Text Categorization is the task	In this work we present many solutions	1-38	39-73	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	1>2	none	bg-goal	bg-goal
P06-1070	1-6	7-12	Cross-language Text Categorization is the task	of assigning semantic classes to documents	Cross-language Text Categorization is the task	of assigning semantic classes to documents	1-38	1-38	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	1<2	none	elab-addition	elab-addition
P06-1070	7-12	13-21	of assigning semantic classes to documents	written in a target language ( e.g. English )	of assigning semantic classes to documents	written in a target language ( e.g. English )	1-38	1-38	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	1<2	none	elab-addition	elab-addition
P06-1070	7-12	22-38	of assigning semantic classes to documents	while the system is trained using labeled documents in a source language ( e.g. Italian ) .	of assigning semantic classes to documents	while the system is trained using labeled documents in a source language ( e.g. Italian ) .	1-38	1-38	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	Cross-language Text Categorization is the task of assigning semantic classes to documents written in a target language ( e.g. English ) while the system is trained using labeled documents in a source language ( e.g. Italian ) .	1<2	none	temporal	temporal
P06-1070	39-45	46-53	In this work we present many solutions	according to the availability of bilingual resources ,	In this work we present many solutions	according to the availability of bilingual resources ,	39-73	39-73	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	1<2	none	elab-addition	elab-addition
P06-1070	54-56	57-65	and we show	that it is possible to deal with the problem	and we show	that it is possible to deal with the problem	39-73	39-73	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	1>2	none	attribution	attribution
P06-1070	39-45	57-65	In this work we present many solutions	that it is possible to deal with the problem	In this work we present many solutions	that it is possible to deal with the problem	39-73	39-73	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	1<2	none	joint	joint
P06-1070	57-65	66-73	that it is possible to deal with the problem	even when no such resources are accessible .	that it is possible to deal with the problem	even when no such resources are accessible .	39-73	39-73	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	1<2	none	condition	condition
P06-1070	39-45	74-89	In this work we present many solutions	The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora .	In this work we present many solutions	The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora .	39-73	74-89	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	The core technique relies on the automatic acquisition of Multilingual Domain Models from comparable corpora .	1<2	none	elab-addition	elab-addition
P06-1070	39-45	90-97	In this work we present many solutions	Experiments show the effectiveness of our approach ,	In this work we present many solutions	Experiments show the effectiveness of our approach ,	39-73	90-110	In this work we present many solutions according to the availability of bilingual resources , and we show that it is possible to deal with the problem even when no such resources are accessible .	Experiments show the effectiveness of our approach , providing a low cost solution for the Cross Language Text Categorization task .	1<2	none	evaluation	evaluation
P06-1070	90-97	98-110	Experiments show the effectiveness of our approach ,	providing a low cost solution for the Cross Language Text Categorization task .	Experiments show the effectiveness of our approach ,	providing a low cost solution for the Cross Language Text Categorization task .	90-110	90-110	Experiments show the effectiveness of our approach , providing a low cost solution for the Cross Language Text Categorization task .	Experiments show the effectiveness of our approach , providing a low cost solution for the Cross Language Text Categorization task .	1<2	none	elab-addition	elab-addition
P06-1070	111-118	119-132	In particular , when bilingual dictionaries are available	the performance of the categorization gets close to that of monolingual text categorization .	In particular , when bilingual dictionaries are available	the performance of the categorization gets close to that of monolingual text categorization .	111-132	111-132	In particular , when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization .	In particular , when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization .	1>2	none	condition	condition
P06-1070	90-97	119-132	Experiments show the effectiveness of our approach ,	the performance of the categorization gets close to that of monolingual text categorization .	Experiments show the effectiveness of our approach ,	the performance of the categorization gets close to that of monolingual text categorization .	90-110	111-132	Experiments show the effectiveness of our approach , providing a low cost solution for the Cross Language Text Categorization task .	In particular , when bilingual dictionaries are available the performance of the categorization gets close to that of monolingual text categorization .	1<2	none	elab-addition	elab-addition
P06-1071	1-11	12-19	Recent developments in statistical modeling of various linguistic phenomena have shown	that additional features give consistent performance improvements .	Recent developments in statistical modeling of various linguistic phenomena have shown	that additional features give consistent performance improvements .	1-19	1-19	Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements .	Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements .	1>2	none	attribution	attribution
P06-1071	12-19	20-30	that additional features give consistent performance improvements .	Quite often , improvements are limited by the number of features	that additional features give consistent performance improvements .	Quite often , improvements are limited by the number of features	1-19	20-37	Recent developments in statistical modeling of various linguistic phenomena have shown that additional features give consistent performance improvements .	Quite often , improvements are limited by the number of features a system is able to explore .	1>2	none	elab-addition	elab-addition
P06-1071	20-30	38-45	Quite often , improvements are limited by the number of features	This paper describes a novel progressive training algorithm	Quite often , improvements are limited by the number of features	This paper describes a novel progressive training algorithm	20-37	38-62	Quite often , improvements are limited by the number of features a system is able to explore .	This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	1>2	none	bg-compare	bg-compare
P06-1071	20-30	31-37	Quite often , improvements are limited by the number of features	a system is able to explore .	Quite often , improvements are limited by the number of features	a system is able to explore .	20-37	20-37	Quite often , improvements are limited by the number of features a system is able to explore .	Quite often , improvements are limited by the number of features a system is able to explore .	1<2	none	elab-addition	elab-addition
P06-1071	38-45	46-62	This paper describes a novel progressive training algorithm	that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	This paper describes a novel progressive training algorithm	that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	38-62	38-62	This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	1<2	none	elab-addition	elab-addition
P06-1071	38-45	63-81	This paper describes a novel progressive training algorithm	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm :	This paper describes a novel progressive training algorithm	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm :	38-62	63-112	This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm : the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 ) when the same feature spaces are used .	1<2	none	evaluation	evaluation
P06-1071	63-81	82-104	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm :	the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 )	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm :	the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 )	63-112	63-112	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm : the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 ) when the same feature spaces are used .	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm : the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 ) when the same feature spaces are used .	1<2	none	elab-addition	elab-addition
P06-1071	82-104	105-112	the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 )	when the same feature spaces are used .	the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 )	when the same feature spaces are used .	63-112	63-112	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm : the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 ) when the same feature spaces are used .	Experimental results in edit region identification demonstrate the benefits of the progressive feature selection ( PFS ) algorithm : the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms ( e.g. , Zhou et al. , 2003 ) when the same feature spaces are used .	1<2	none	condition	condition
P06-1071	113-121	122-149	When additional features and their combinations are used ,	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	When additional features and their combinations are used ,	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	113-173	113-173	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	1>2	none	condition	condition
P06-1071	38-45	122-149	This paper describes a novel progressive training algorithm	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	This paper describes a novel progressive training algorithm	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	38-62	113-173	This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy ( CME ) modeling .	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	1<2	none	evaluation	evaluation
P06-1071	122-149	150-158	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	which leads to a 20 % relative error reduction	the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) ,	which leads to a 20 % relative error reduction	113-173	113-173	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	1<2	none	elab-addition	elab-addition
P06-1071	150-158	159-163	which leads to a 20 % relative error reduction	in parsing the Switchboard corpus	which leads to a 20 % relative error reduction	in parsing the Switchboard corpus	113-173	113-173	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	1<2	none	elab-addition	elab-addition
P06-1071	159-163	164-173	in parsing the Switchboard corpus	when gold edits are used as the upper bound .	in parsing the Switchboard corpus	when gold edits are used as the upper bound .	113-173	113-173	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	When additional features and their combinations are used , the PFS gives 17.66 % relative improvement over the previously reported best result in edit region identification on Switchboard corpus ( Kahn et al. , 2005 ) , which leads to a 20 % relative error reduction in parsing the Switchboard corpus when gold edits are used as the upper bound .	1<2	none	condition	condition
P06-1072	1-3	4-18	We first show	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	We first show	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	1-32	1-32	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	1>2	none	attribution	attribution
P06-1072	4-18	19-32	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	1-32	1-32	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	1<2	none	elab-addition	elab-addition
P06-1072	33-39	45-49	Next , by annealing the free parameter	we achieve further improvements .	Next , by annealing the free parameter	we achieve further improvements .	33-49	33-49	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	1>2	none	manner-means	manner-means
P06-1072	33-39	40-44	Next , by annealing the free parameter	that controls this bias ,	Next , by annealing the free parameter	that controls this bias ,	33-49	33-49	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	1<2	none	elab-addition	elab-addition
P06-1072	4-18	45-49	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	we achieve further improvements .	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	we achieve further improvements .	1-32	33-49	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	1<2	none	elab-process_step	elab-process_step
P06-1072	45-49	50-62	we achieve further improvements .	We then describe an alternative kind of structural bias , toward "broken" hypotheses	we achieve further improvements .	We then describe an alternative kind of structural bias , toward "broken" hypotheses	33-49	50-78	Next , by annealing the free parameter that controls this bias , we achieve further improvements .	We then describe an alternative kind of structural bias , toward "broken" hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .	1<2	none	elab-process_step	elab-process_step
P06-1072	50-62	63-70	We then describe an alternative kind of structural bias , toward "broken" hypotheses	consisting of partial structures over segmented sentences ,	We then describe an alternative kind of structural bias , toward "broken" hypotheses	consisting of partial structures over segmented sentences ,	50-78	50-78	We then describe an alternative kind of structural bias , toward "broken" hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .	We then describe an alternative kind of structural bias , toward "broken" hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .	1<2	none	elab-addition	elab-addition
P06-1072	50-62	71-78	We then describe an alternative kind of structural bias , toward "broken" hypotheses	and show a similar pattern of improvement .	We then describe an alternative kind of structural bias , toward "broken" hypotheses	and show a similar pattern of improvement .	50-78	50-78	We then describe an alternative kind of structural bias , toward "broken" hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .	We then describe an alternative kind of structural bias , toward "broken" hypotheses consisting of partial structures over segmented sentences , and show a similar pattern of improvement .	1<2	none	joint	joint
P06-1072	4-18	79-93	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) ,	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) ,	1-32	79-140	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	1<2	none	evaluation	evaluation
P06-1072	79-93	94-103	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) ,	apply the latter to grammar induction in six languages ,	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) ,	apply the latter to grammar induction in six languages ,	79-140	79-140	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	1<2	none	progression	progression
P06-1072	104-105	106-127	and show	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	and show	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	79-140	79-140	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	1>2	none	attribution	attribution
P06-1072	94-103	106-127	apply the latter to grammar induction in six languages ,	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	apply the latter to grammar induction in six languages ,	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	79-140	79-140	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	1<2	none	progression	progression
P06-1072	106-127	128-140	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	achieving to our knowledge the best results on this task to date .	that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) ,	achieving to our knowledge the best results on this task to date .	79-140	79-140	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	We relate this approach to contrastive estimation ( Smith and Eisner , 2005a ) , apply the latter to grammar induction in six languages , and show that our new approach improves accuracy by 1-17 % ( absolute ) over CE ( and 8-30 % over EM ) , achieving to our knowledge the best results on this task to date .	1<2	none	elab-addition	elab-addition
P06-1072	4-18	141-158	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	Our method , structural annealing , is a general technique with broad applicability to hidden-structure discovery problems .	how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models	Our method , structural annealing , is a general technique with broad applicability to hidden-structure discovery problems .	1-32	141-158	We first show how a structural locality bias can improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples ( Klein and Manning , 2004 ) .	Our method , structural annealing , is a general technique with broad applicability to hidden-structure discovery problems .	1<2	none	evaluation	evaluation
P06-1073	1-13	52-60	Short vowels and other diacritics are not part of written Arabic scripts .	We propose in this paper a maximum entropy approach	Short vowels and other diacritics are not part of written Arabic scripts .	We propose in this paper a maximum entropy approach	1-13	52-67	Short vowels and other diacritics are not part of written Arabic scripts .	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	1>2	none	bg-goal	bg-goal
P06-1073	1-13	14-31	Short vowels and other diacritics are not part of written Arabic scripts .	Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic .	Short vowels and other diacritics are not part of written Arabic scripts .	Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic .	1-13	14-31	Short vowels and other diacritics are not part of written Arabic scripts .	Exceptions are made for important political and religious texts and in scripts for beginning students of Arabic .	1<2	none	elab-addition	elab-addition
P06-1073	1-13	32-37	Short vowels and other diacritics are not part of written Arabic scripts .	Script without diacritics have considerable ambiguity	Short vowels and other diacritics are not part of written Arabic scripts .	Script without diacritics have considerable ambiguity	1-13	32-51	Short vowels and other diacritics are not part of written Arabic scripts .	Script without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting .	1<2	none	elab-addition	elab-addition
P06-1073	32-37	38-51	Script without diacritics have considerable ambiguity	because many words with different diacritic patterns appear identical in a diacritic-less setting .	Script without diacritics have considerable ambiguity	because many words with different diacritic patterns appear identical in a diacritic-less setting .	32-51	32-51	Script without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting .	Script without diacritics have considerable ambiguity because many words with different diacritic patterns appear identical in a diacritic-less setting .	1<2	none	exp-reason	exp-reason
P06-1073	52-60	61-67	We propose in this paper a maximum entropy approach	for restoring diacritics in a document .	We propose in this paper a maximum entropy approach	for restoring diacritics in a document .	52-67	52-67	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	1<2	none	elab-addition	elab-addition
P06-1073	52-60	68-82	We propose in this paper a maximum entropy approach	The approach can easily integrate and make effective use of diverse types of information ;	We propose in this paper a maximum entropy approach	The approach can easily integrate and make effective use of diverse types of information ;	52-67	68-99	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	The approach can easily integrate and make effective use of diverse types of information ; the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	1<2	none	elab-addition	elab-addition
P06-1073	68-82	83-99	The approach can easily integrate and make effective use of diverse types of information ;	the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	The approach can easily integrate and make effective use of diverse types of information ;	the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	68-99	68-99	The approach can easily integrate and make effective use of diverse types of information ; the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	The approach can easily integrate and make effective use of diverse types of information ; the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	1<2	none	elab-addition	elab-addition
P06-1073	68-82	100-112	The approach can easily integrate and make effective use of diverse types of information ;	The combination of these feature types leads to a state-of-the-art diacritization model .	The approach can easily integrate and make effective use of diverse types of information ;	The combination of these feature types leads to a state-of-the-art diacritization model .	68-99	100-112	The approach can easily integrate and make effective use of diverse types of information ; the model we propose integrates a wide array of lexical , segmentbased and part-of-speech tag features .	The combination of these feature types leads to a state-of-the-art diacritization model .	1<2	none	elab-addition	elab-addition
P06-1073	113-125	126-151	Using a publicly available corpus ( LDC's Arabic Treebank Part 3 ) ,	we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	Using a publicly available corpus ( LDC's Arabic Treebank Part 3 ) ,	we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	113-151	113-151	Using a publicly available corpus ( LDC's Arabic Treebank Part 3 ) , we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	Using a publicly available corpus ( LDC's Arabic Treebank Part 3 ) , we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	1>2	none	manner-means	manner-means
P06-1073	52-60	126-151	We propose in this paper a maximum entropy approach	we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	We propose in this paper a maximum entropy approach	we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	52-67	113-151	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	Using a publicly available corpus ( LDC's Arabic Treebank Part 3 ) , we achieve a diacritic error rate of 5.1 % , a segment error rate 8.5 % , and a word error rate of 17.3 % .	1<2	none	evaluation	evaluation
P06-1073	52-60	152-181	We propose in this paper a maximum entropy approach	In case-ending-less setting , we obtain a diacritic error rate of 2.2 % , a segment error rate 4.0 % , and a word error rate of 7.2 % .	We propose in this paper a maximum entropy approach	In case-ending-less setting , we obtain a diacritic error rate of 2.2 % , a segment error rate 4.0 % , and a word error rate of 7.2 % .	52-67	152-181	We propose in this paper a maximum entropy approach for restoring diacritics in a document .	In case-ending-less setting , we obtain a diacritic error rate of 2.2 % , a segment error rate 4.0 % , and a word error rate of 7.2 % .	1<2	none	evaluation	evaluation
P06-1074	1-10	16-28	General information retrieval systems are designed to serve all users	In this paper , we propose a novel approach to personalized search .	General information retrieval systems are designed to serve all users	In this paper , we propose a novel approach to personalized search .	1-15	16-28	General information retrieval systems are designed to serve all users without considering individual needs .	In this paper , we propose a novel approach to personalized search .	1>2	none	bg-compare	bg-compare
P06-1074	1-10	11-15	General information retrieval systems are designed to serve all users	without considering individual needs .	General information retrieval systems are designed to serve all users	without considering individual needs .	1-15	1-15	General information retrieval systems are designed to serve all users without considering individual needs .	General information retrieval systems are designed to serve all users without considering individual needs .	1<2	none	condition	condition
P06-1074	16-28	29-52	In this paper , we propose a novel approach to personalized search .	It can , in a unified way , exploit and utilize implicit feedback information , such as query logs and immediately viewed documents .	In this paper , we propose a novel approach to personalized search .	It can , in a unified way , exploit and utilize implicit feedback information , such as query logs and immediately viewed documents .	16-28	29-52	In this paper , we propose a novel approach to personalized search .	It can , in a unified way , exploit and utilize implicit feedback information , such as query logs and immediately viewed documents .	1<2	none	elab-addition	elab-addition
P06-1074	16-28	53-67	In this paper , we propose a novel approach to personalized search .	Moreover , our approach can implement result re-ranking and query expansion simultaneously and collaboratively .	In this paper , we propose a novel approach to personalized search .	Moreover , our approach can implement result re-ranking and query expansion simultaneously and collaboratively .	16-28	53-67	In this paper , we propose a novel approach to personalized search .	Moreover , our approach can implement result re-ranking and query expansion simultaneously and collaboratively .	1<2	none	elab-addition	elab-addition
P06-1074	68-72	73-89	Based on this approach ,	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	Based on this approach ,	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	68-96	68-96	Based on this approach , we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) , which supports both English and Chinese .	Based on this approach , we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) , which supports both English and Chinese .	1>2	none	bg-general	bg-general
P06-1074	16-28	73-89	In this paper , we propose a novel approach to personalized search .	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	In this paper , we propose a novel approach to personalized search .	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	16-28	68-96	In this paper , we propose a novel approach to personalized search .	Based on this approach , we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) , which supports both English and Chinese .	1<2	none	elab-addition	elab-addition
P06-1074	73-89	90-96	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	which supports both English and Chinese .	we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) ,	which supports both English and Chinese .	68-96	68-96	Based on this approach , we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) , which supports both English and Chinese .	Based on this approach , we develop a client-side personalized web search agent PAIR ( Personalized Assistant for Information Retrieval ) , which supports both English and Chinese .	1<2	none	elab-addition	elab-addition
P06-1074	97-105	106-115	Our experiments on TREC and HTRDP collections clearly show	that the new approach is both effective and efficient .	Our experiments on TREC and HTRDP collections clearly show	that the new approach is both effective and efficient .	97-115	97-115	Our experiments on TREC and HTRDP collections clearly show that the new approach is both effective and efficient .	Our experiments on TREC and HTRDP collections clearly show that the new approach is both effective and efficient .	1>2	none	attribution	attribution
P06-1074	16-28	106-115	In this paper , we propose a novel approach to personalized search .	that the new approach is both effective and efficient .	In this paper , we propose a novel approach to personalized search .	that the new approach is both effective and efficient .	16-28	97-115	In this paper , we propose a novel approach to personalized search .	Our experiments on TREC and HTRDP collections clearly show that the new approach is both effective and efficient .	1<2	none	evaluation	evaluation
P06-1075	28-36	37-42	To obtain MT systems of different translation quality ,	we degrade a rule-based MT system	To obtain MT systems of different translation quality ,	we degrade a rule-based MT system	28-57	28-57	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	1>2	none	enablement	enablement
P06-1075	1-27	37-42	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	we degrade a rule-based MT system	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	we degrade a rule-based MT system	1-27	28-57	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	1<2	none	elab-addition	elab-addition
P06-1075	37-42	43-57	we degrade a rule-based MT system	by decreasing the size of the rule base and the size of the dictionary .	we degrade a rule-based MT system	by decreasing the size of the rule base and the size of the dictionary .	28-57	28-57	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	1<2	none	manner-means	manner-means
P06-1075	37-42	58-63	we degrade a rule-based MT system	We use the degraded MT systems	we degrade a rule-based MT system	We use the degraded MT systems	28-57	58-79	To obtain MT systems of different translation quality , we degrade a rule-based MT system by decreasing the size of the rule base and the size of the dictionary .	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	1<2	none	elab-addition	elab-addition
P06-1075	58-63	64-66	We use the degraded MT systems	to translate queries	We use the degraded MT systems	to translate queries	58-79	58-79	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	1<2	none	enablement	enablement
P06-1075	64-66	67-71	to translate queries	and submit the translated queries	to translate queries	and submit the translated queries	58-79	58-79	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	1<2	none	joint	joint
P06-1075	67-71	72-79	and submit the translated queries	of varying quality to the IR system .	and submit the translated queries	of varying quality to the IR system .	58-79	58-79	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	We use the degraded MT systems to translate queries and submit the translated queries of varying quality to the IR system .	1<2	none	elab-addition	elab-addition
P06-1075	80-94	100-105	Retrieval effectiveness is found to correlate highly with the translation quality of the queries .	that affect the retrieval effectiveness .	Retrieval effectiveness is found to correlate highly with the translation quality of the queries .	that affect the retrieval effectiveness .	80-94	95-105	Retrieval effectiveness is found to correlate highly with the translation quality of the queries .	We further analyze the factors that affect the retrieval effectiveness .	1>2	none	elab-addition	elab-addition
P06-1075	95-99	106-116	We further analyze the factors	Title queries are found to be preferred in MT-based CLIR .	We further analyze the factors	Title queries are found to be preferred in MT-based CLIR .	95-105	106-116	We further analyze the factors that affect the retrieval effectiveness .	Title queries are found to be preferred in MT-based CLIR .	1>2	none	result	result
P06-1075	95-99	100-105	We further analyze the factors	that affect the retrieval effectiveness .	We further analyze the factors	that affect the retrieval effectiveness .	95-105	95-105	We further analyze the factors that affect the retrieval effectiveness .	We further analyze the factors that affect the retrieval effectiveness .	1<2	none	elab-addition	elab-addition
P06-1075	1-27	106-116	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	Title queries are found to be preferred in MT-based CLIR .	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	Title queries are found to be preferred in MT-based CLIR .	1-27	106-116	This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation ( MT ) based Cross-Language Information Retrieval ( CLIR ) .	Title queries are found to be preferred in MT-based CLIR .	1<2	none	evaluation	evaluation
P06-1075	106-116	117-134	Title queries are found to be preferred in MT-based CLIR .	In addition , dictionary-based degradation is shown to have stronger impact than rule-based degradation in MT-based CLIR .	Title queries are found to be preferred in MT-based CLIR .	In addition , dictionary-based degradation is shown to have stronger impact than rule-based degradation in MT-based CLIR .	106-116	117-134	Title queries are found to be preferred in MT-based CLIR .	In addition , dictionary-based degradation is shown to have stronger impact than rule-based degradation in MT-based CLIR .	1<2	none	elab-addition	elab-addition
P06-1076	1-28	29-32	The trend in information retrieval systems is from document to sub-document retrieval , such as sentences in a summarization system and words or phrases in question-answering system .	Despite this trend ,	The trend in information retrieval systems is from document to sub-document retrieval , such as sentences in a summarization system and words or phrases in question-answering system .	Despite this trend ,	1-28	29-50	The trend in information retrieval systems is from document to sub-document retrieval , such as sentences in a summarization system and words or phrases in question-answering system .	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	1>2	none	elab-addition	elab-addition
P06-1076	29-32	33-41	Despite this trend ,	systems continue to model language at a document level	Despite this trend ,	systems continue to model language at a document level	29-50	29-50	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	1>2	none	contrast	contrast
P06-1076	33-41	51-74	systems continue to model language at a document level	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	systems continue to model language at a document level	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	29-50	51-74	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	1>2	none	bg-goal	bg-goal
P06-1076	33-41	42-50	systems continue to model language at a document level	using the inverse document frequency ( IDF ) .	systems continue to model language at a document level	using the inverse document frequency ( IDF ) .	29-50	29-50	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	Despite this trend , systems continue to model language at a document level using the inverse document frequency ( IDF ) .	1<2	none	manner-means	manner-means
P06-1076	75-78	79-86	A direct comparison reveals	that all language models are highly correlated ;	A direct comparison reveals	that all language models are highly correlated ;	75-102	75-102	A direct comparison reveals that all language models are highly correlated ; however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	A direct comparison reveals that all language models are highly correlated ; however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	1>2	none	attribution	attribution
P06-1076	51-74	79-86	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	that all language models are highly correlated ;	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	that all language models are highly correlated ;	51-74	75-102	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	A direct comparison reveals that all language models are highly correlated ; however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	1<2	none	elab-addition	elab-addition
P06-1076	79-86	87-102	that all language models are highly correlated ;	however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	that all language models are highly correlated ;	however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	75-102	75-102	A direct comparison reveals that all language models are highly correlated ; however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	A direct comparison reveals that all language models are highly correlated ; however , the average ISF and ITF values are 5.5 and 10.4 higher than IDF .	1<2	none	contrast	contrast
P06-1076	51-74	103-127	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	All language models appeared to follow a power law distribution with a slope coefficient of 1.6 for documents and 1.7 for sentences and terms .	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	All language models appeared to follow a power law distribution with a slope coefficient of 1.6 for documents and 1.7 for sentences and terms .	51-74	103-127	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	All language models appeared to follow a power law distribution with a slope coefficient of 1.6 for documents and 1.7 for sentences and terms .	1<2	none	elab-addition	elab-addition
P06-1076	51-74	128-156	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	We conclude with an analysis of IDF stability with respect to random , journal , and section partitions of the 100,830 full-text scientific articles in our experimental corpus .	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	We conclude with an analysis of IDF stability with respect to random , journal , and section partitions of the 100,830 full-text scientific articles in our experimental corpus .	51-74	128-156	In this paper , we compare and contrast IDF with inverse sentence frequency ( ISF ) and inverse term frequency ( ITF ) .	We conclude with an analysis of IDF stability with respect to random , journal , and section partitions of the 100,830 full-text scientific articles in our experimental corpus .	1<2	none	evaluation	evaluation
P06-1077	1-6	7-14	We present a novel translation model	based on tree-to-string alignment template ( TAT )	We present a novel translation model	based on tree-to-string alignment template ( TAT )	1-28	1-28	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	1<2	none	bg-general	bg-general
P06-1077	7-14	15-28	based on tree-to-string alignment template ( TAT )	which describes the alignment between a source parse tree and a target string .	based on tree-to-string alignment template ( TAT )	which describes the alignment between a source parse tree and a target string .	1-28	1-28	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	1<2	none	elab-addition	elab-addition
P06-1077	1-6	29-38	We present a novel translation model	A TAT is capable of generating both terminals and non-terminals	We present a novel translation model	A TAT is capable of generating both terminals and non-terminals	1-28	29-48	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels .	1<2	none	elab-aspect	elab-aspect
P06-1077	29-38	39-48	A TAT is capable of generating both terminals and non-terminals	and performing reordering at both low and high levels .	A TAT is capable of generating both terminals and non-terminals	and performing reordering at both low and high levels .	29-48	29-48	A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels .	A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels .	1<2	none	joint	joint
P06-1077	1-6	49-53	We present a novel translation model	The model is linguistically syntaxbased	We present a novel translation model	The model is linguistically syntaxbased	1-28	49-67	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned , source side parsed parallel texts .	1<2	none	elab-aspect	elab-aspect
P06-1077	49-53	54-67	The model is linguistically syntaxbased	because TATs are extracted automatically from word-aligned , source side parsed parallel texts .	The model is linguistically syntaxbased	because TATs are extracted automatically from word-aligned , source side parsed parallel texts .	49-67	49-67	The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned , source side parsed parallel texts .	The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned , source side parsed parallel texts .	1<2	none	exp-reason	exp-reason
P06-1077	68-73	74-78	To translate a source sentence ,	we first employ a parser	To translate a source sentence ,	we first employ a parser	68-97	68-97	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	1>2	none	enablement	enablement
P06-1077	1-6	74-78	We present a novel translation model	we first employ a parser	We present a novel translation model	we first employ a parser	1-28	68-97	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	1<2	none	elab-aspect	elab-aspect
P06-1077	74-78	79-84	we first employ a parser	to produce a source parse tree	we first employ a parser	to produce a source parse tree	68-97	68-97	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	1<2	none	enablement	enablement
P06-1077	74-78	85-88	we first employ a parser	and then apply TATs	we first employ a parser	and then apply TATs	68-97	68-97	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	1<2	none	progression	progression
P06-1077	85-88	89-97	and then apply TATs	to transform the tree into a target string .	and then apply TATs	to transform the tree into a target string .	68-97	68-97	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	To translate a source sentence , we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string .	1<2	none	enablement	enablement
P06-1077	98-100	101-115	Our experiments show	that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	Our experiments show	that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	98-115	98-115	Our experiments show that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	Our experiments show that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	1>2	none	attribution	attribution
P06-1077	1-6	101-115	We present a novel translation model	that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	We present a novel translation model	that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	1-28	98-115	We present a novel translation model based on tree-to-string alignment template ( TAT ) which describes the alignment between a source parse tree and a target string .	Our experiments show that the TAT-based model significantly outperforms Pharaoh , a state-of-the-art decoder for phrase-based models .	1<2	none	evaluation	evaluation
P06-1078	1-15	16-29	This paper proposes a named entity recognition ( NER ) method for speech recognition results	that uses confidence on automatic speech recognition ( ASR ) as a feature .	This paper proposes a named entity recognition ( NER ) method for speech recognition results	that uses confidence on automatic speech recognition ( ASR ) as a feature .	1-29	1-29	This paper proposes a named entity recognition ( NER ) method for speech recognition results that uses confidence on automatic speech recognition ( ASR ) as a feature .	This paper proposes a named entity recognition ( NER ) method for speech recognition results that uses confidence on automatic speech recognition ( ASR ) as a feature .	1<2	none	elab-addition	elab-addition
P06-1078	30-34	35-42	The ASR confidence feature indicates	whether each word has been correctly recognized .	The ASR confidence feature indicates	whether each word has been correctly recognized .	30-42	30-42	The ASR confidence feature indicates whether each word has been correctly recognized .	The ASR confidence feature indicates whether each word has been correctly recognized .	1>2	none	attribution	attribution
P06-1078	1-15	35-42	This paper proposes a named entity recognition ( NER ) method for speech recognition results	whether each word has been correctly recognized .	This paper proposes a named entity recognition ( NER ) method for speech recognition results	whether each word has been correctly recognized .	1-29	30-42	This paper proposes a named entity recognition ( NER ) method for speech recognition results that uses confidence on automatic speech recognition ( ASR ) as a feature .	The ASR confidence feature indicates whether each word has been correctly recognized .	1<2	none	elab-aspect	elab-aspect
P06-1078	1-15	43-47	This paper proposes a named entity recognition ( NER ) method for speech recognition results	The NER model is trained	This paper proposes a named entity recognition ( NER ) method for speech recognition results	The NER model is trained	1-29	43-67	This paper proposes a named entity recognition ( NER ) method for speech recognition results that uses confidence on automatic speech recognition ( ASR ) as a feature .	The NER model is trained using ASR results with named entity ( NE ) labels as well as the corresponding transcriptions with NE labels .	1<2	none	elab-aspect	elab-aspect
P06-1078	43-47	48-67	The NER model is trained	using ASR results with named entity ( NE ) labels as well as the corresponding transcriptions with NE labels .	The NER model is trained	using ASR results with named entity ( NE ) labels as well as the corresponding transcriptions with NE labels .	43-67	43-67	The NER model is trained using ASR results with named entity ( NE ) labels as well as the corresponding transcriptions with NE labels .	The NER model is trained using ASR results with named entity ( NE ) labels as well as the corresponding transcriptions with NE labels .	1<2	none	manner-means	manner-means
P06-1078	68-69	85-100	In experiments	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	In experiments	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	68-104	68-104	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	1>2	none	elab-addition	elab-addition
P06-1078	68-69	70-84	In experiments	using support vector machines ( SVMs ) and speech data from Japanese newspaper articles ,	In experiments	using support vector machines ( SVMs ) and speech data from Japanese newspaper articles ,	68-104	68-104	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	1<2	none	elab-addition	elab-addition
P06-1078	1-15	85-100	This paper proposes a named entity recognition ( NER ) method for speech recognition results	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	This paper proposes a named entity recognition ( NER ) method for speech recognition results	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	1-29	68-104	This paper proposes a named entity recognition ( NER ) method for speech recognition results that uses confidence on automatic speech recognition ( ASR ) as a feature .	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	1<2	none	evaluation	evaluation
P06-1078	85-100	101-104	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	by improving precision .	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	by improving precision .	68-104	68-104	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	1<2	none	manner-means	manner-means
P06-1078	105-107	108-119	These results show	that the proposed method is effective in NER for noisy inputs .	These results show	that the proposed method is effective in NER for noisy inputs .	105-119	105-119	These results show that the proposed method is effective in NER for noisy inputs .	These results show that the proposed method is effective in NER for noisy inputs .	1>2	none	attribution	attribution
P06-1078	85-100	108-119	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	that the proposed method is effective in NER for noisy inputs .	the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure	that the proposed method is effective in NER for noisy inputs .	68-104	105-119	In experiments using support vector machines ( SVMs ) and speech data from Japanese newspaper articles , the proposed method outperformed a simple application of textbased NER to ASR results in NER Fmeasure by improving precision .	These results show that the proposed method is effective in NER for noisy inputs .	1<2	none	summary	summary
P06-1079	1-6	7-16	We approach the zero-anaphora resolution problem	by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution .	We approach the zero-anaphora resolution problem	by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution .	1-16	1-16	We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution .	We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution .	1<2	none	manner-means	manner-means
P06-1079	1-6	17-35	We approach the zero-anaphora resolution problem	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	We approach the zero-anaphora resolution problem	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	1-16	17-35	We approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution .	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	1<2	none	elab-addition	elab-addition
P06-1079	36-42	46-66	Taking Japanese as a target language ,	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	Taking Japanese as a target language ,	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	36-76	36-76	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	1>2	none	elab-addition	elab-addition
P06-1079	43-45	46-66	we empirically demonstrate	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	we empirically demonstrate	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	36-76	36-76	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	1>2	none	attribution	attribution
P06-1079	17-35	46-66	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	17-35	36-76	For the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues .	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	1<2	none	elab-example	elab-example
P06-1079	46-66	67-76	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	which consequently improves the overall performance of zeroanaphora resolution .	that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora ,	which consequently improves the overall performance of zeroanaphora resolution .	36-76	36-76	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	Taking Japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .	1<2	none	elab-addition	elab-addition
P06-1080	1-17	18-31	An automatic word spacing is one of the important tasks in Korean language processing and information retrieval.	Since there are a number of confusing cases in word spacing of Korean ,	An automatic word spacing is one of the important tasks in Korean language processing and information retrieval.	Since there are a number of confusing cases in word spacing of Korean ,	1-17	18-42	An automatic word spacing is one of the important tasks in Korean language processing and information retrieval.	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	1>2	none	elab-addition	elab-addition
P06-1080	18-31	32-38	Since there are a number of confusing cases in word spacing of Korean ,	there are some mistakes in many texts	Since there are a number of confusing cases in word spacing of Korean ,	there are some mistakes in many texts	18-42	18-42	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	1>2	none	exp-reason	exp-reason
P06-1080	32-38	43-52	there are some mistakes in many texts	This paper presents a high-accurate method for automatic word spacing	there are some mistakes in many texts	This paper presents a high-accurate method for automatic word spacing	18-42	43-58	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	1>2	none	bg-compare	bg-compare
P06-1080	32-38	39-42	there are some mistakes in many texts	including news articles .	there are some mistakes in many texts	including news articles .	18-42	18-42	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	Since there are a number of confusing cases in word spacing of Korean , there are some mistakes in many texts including news articles .	1<2	none	elab-example	elab-example
P06-1080	43-52	53-58	This paper presents a high-accurate method for automatic word spacing	based on self-organizing -gram model .	This paper presents a high-accurate method for automatic word spacing	based on self-organizing -gram model .	43-58	43-58	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	1<2	none	bg-general	bg-general
P06-1080	43-52	59-68	This paper presents a high-accurate method for automatic word spacing	This method is basically a variant of -gram model ,	This paper presents a high-accurate method for automatic word spacing	This method is basically a variant of -gram model ,	43-58	59-78	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	This method is basically a variant of -gram model , but achieves high accuracy by automatically adapting context size .	1<2	none	elab-addition	elab-addition
P06-1080	59-68	69-72	This method is basically a variant of -gram model ,	but achieves high accuracy	This method is basically a variant of -gram model ,	but achieves high accuracy	59-78	59-78	This method is basically a variant of -gram model , but achieves high accuracy by automatically adapting context size .	This method is basically a variant of -gram model , but achieves high accuracy by automatically adapting context size .	1<2	none	contrast	contrast
P06-1080	69-72	73-78	but achieves high accuracy	by automatically adapting context size .	but achieves high accuracy	by automatically adapting context size .	59-78	59-78	This method is basically a variant of -gram model , but achieves high accuracy by automatically adapting context size .	This method is basically a variant of -gram model , but achieves high accuracy by automatically adapting context size .	1<2	none	manner-means	manner-means
P06-1080	79-87	88-95	In order to find the optimal context size ,	the proposed method automatically increases the context size	In order to find the optimal context size ,	the proposed method automatically increases the context size	79-112	79-112	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	1>2	none	enablement	enablement
P06-1080	43-52	88-95	This paper presents a high-accurate method for automatic word spacing	the proposed method automatically increases the context size	This paper presents a high-accurate method for automatic word spacing	the proposed method automatically increases the context size	43-58	79-112	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	1<2	none	manner-means	manner-means
P06-1080	88-95	96-101	the proposed method automatically increases the context size	when the contextual distribution after increasing	the proposed method automatically increases the context size	when the contextual distribution after increasing	79-112	79-112	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	1<2	none	temporal	temporal
P06-1080	96-101	102-112	when the contextual distribution after increasing	it dose not agree with that of the current context .	when the contextual distribution after increasing	it dose not agree with that of the current context .	79-112	79-112	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	In order to find the optimal context size , the proposed method automatically increases the context size when the contextual distribution after increasing it dose not agree with that of the current context .	1<2	none	elab-addition	elab-addition
P06-1080	43-52	113-118	This paper presents a high-accurate method for automatic word spacing	It also decreases the context size	This paper presents a high-accurate method for automatic word spacing	It also decreases the context size	43-58	113-133	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	It also decreases the context size when the distribution of reduced context is similar to that of the current context .	1<2	none	manner-means	manner-means
P06-1080	113-118	119-133	It also decreases the context size	when the distribution of reduced context is similar to that of the current context .	It also decreases the context size	when the distribution of reduced context is similar to that of the current context .	113-133	113-133	It also decreases the context size when the distribution of reduced context is similar to that of the current context .	It also decreases the context size when the distribution of reduced context is similar to that of the current context .	1<2	none	temporal	temporal
P06-1080	43-52	134-138	This paper presents a high-accurate method for automatic word spacing	This approach achieves high accuracy	This paper presents a high-accurate method for automatic word spacing	This approach achieves high accuracy	43-58	134-161	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	This approach achieves high accuracy by considering higher dimensional data in case of necessity , and the increased computational cost are compensated by the reduced context size .	1<2	none	manner-means	manner-means
P06-1080	134-138	139-148	This approach achieves high accuracy	by considering higher dimensional data in case of necessity ,	This approach achieves high accuracy	by considering higher dimensional data in case of necessity ,	134-161	134-161	This approach achieves high accuracy by considering higher dimensional data in case of necessity , and the increased computational cost are compensated by the reduced context size .	This approach achieves high accuracy by considering higher dimensional data in case of necessity , and the increased computational cost are compensated by the reduced context size .	1<2	none	manner-means	manner-means
P06-1080	134-138	149-161	This approach achieves high accuracy	and the increased computational cost are compensated by the reduced context size .	This approach achieves high accuracy	and the increased computational cost are compensated by the reduced context size .	134-161	134-161	This approach achieves high accuracy by considering higher dimensional data in case of necessity , and the increased computational cost are compensated by the reduced context size .	This approach achieves high accuracy by considering higher dimensional data in case of necessity , and the increased computational cost are compensated by the reduced context size .	1<2	none	manner-means	manner-means
P06-1080	162-165	166-177	The experimental results show	that the self-organizing structure of -gram model enhances the basic model .	The experimental results show	that the self-organizing structure of -gram model enhances the basic model .	162-177	162-177	The experimental results show that the self-organizing structure of -gram model enhances the basic model .	The experimental results show that the self-organizing structure of -gram model enhances the basic model .	1>2	none	attribution	attribution
P06-1080	43-52	166-177	This paper presents a high-accurate method for automatic word spacing	that the self-organizing structure of -gram model enhances the basic model .	This paper presents a high-accurate method for automatic word spacing	that the self-organizing structure of -gram model enhances the basic model .	43-58	162-177	This paper presents a high-accurate method for automatic word spacing based on self-organizing -gram model .	The experimental results show that the self-organizing structure of -gram model enhances the basic model .	1<2	none	evaluation	evaluation
P06-1081	1-8	9-24	Due to the historical and cultural reasons ,	English phases , especially the proper nouns and new words , frequently appear in Web pages	Due to the historical and cultural reasons ,	English phases , especially the proper nouns and new words , frequently appear in Web pages	1-35	1-35	Due to the historical and cultural reasons , English phases , especially the proper nouns and new words , frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean .	Due to the historical and cultural reasons , English phases , especially the proper nouns and new words , frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean .	1>2	none	cause	cause
P06-1081	9-24	53-68	English phases , especially the proper nouns and new words , frequently appear in Web pages	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	English phases , especially the proper nouns and new words , frequently appear in Web pages	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	1-35	36-68	Due to the historical and cultural reasons , English phases , especially the proper nouns and new words , frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean .	Although these English terms and their equivalences in the Asian languages refer to the same concept , they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	1>2	none	elab-addition	elab-addition
P06-1081	9-24	25-35	English phases , especially the proper nouns and new words , frequently appear in Web pages	written primarily in Asian languages such as Chinese and Korean .	English phases , especially the proper nouns and new words , frequently appear in Web pages	written primarily in Asian languages such as Chinese and Korean .	1-35	1-35	Due to the historical and cultural reasons , English phases , especially the proper nouns and new words , frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean .	Due to the historical and cultural reasons , English phases , especially the proper nouns and new words , frequently appear in Web pages written primarily in Asian languages such as Chinese and Korean .	1<2	none	elab-addition	elab-addition
P06-1081	36-52	53-68	Although these English terms and their equivalences in the Asian languages refer to the same concept ,	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	Although these English terms and their equivalences in the Asian languages refer to the same concept ,	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	36-68	36-68	Although these English terms and their equivalences in the Asian languages refer to the same concept , they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	Although these English terms and their equivalences in the Asian languages refer to the same concept , they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	1>2	none	contrast	contrast
P06-1081	53-68	69-73	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	This paper describes the degree	they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	This paper describes the degree	36-68	69-89	Although these English terms and their equivalences in the Asian languages refer to the same concept , they are erroneously treated as independent index units in traditional Information Retrieval ( IR ) .	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	1>2	none	bg-compare	bg-compare
P06-1081	69-73	74-80	This paper describes the degree	to which the problem arises in IR	This paper describes the degree	to which the problem arises in IR	69-89	69-89	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	1<2	none	elab-addition	elab-addition
P06-1081	74-80	81-85	to which the problem arises in IR	and suggests a novel technique	to which the problem arises in IR	and suggests a novel technique	69-89	69-89	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	1<2	none	joint	joint
P06-1081	81-85	86-89	and suggests a novel technique	to solve it .	and suggests a novel technique	to solve it .	69-89	69-89	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	1<2	none	enablement	enablement
P06-1081	69-73	90-102	This paper describes the degree	Our method firstly extracts an English phrase from Asian language Web pages ,	This paper describes the degree	Our method firstly extracts an English phrase from Asian language Web pages ,	69-89	90-122	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	Our method firstly extracts an English phrase from Asian language Web pages , and then unifies the extracted phrase and its equivalence ( s ) in the language as one index unit .	1<2	none	elab-addition	elab-addition
P06-1081	90-102	103-122	Our method firstly extracts an English phrase from Asian language Web pages ,	and then unifies the extracted phrase and its equivalence ( s ) in the language as one index unit .	Our method firstly extracts an English phrase from Asian language Web pages ,	and then unifies the extracted phrase and its equivalence ( s ) in the language as one index unit .	90-122	90-122	Our method firstly extracts an English phrase from Asian language Web pages , and then unifies the extracted phrase and its equivalence ( s ) in the language as one index unit .	Our method firstly extracts an English phrase from Asian language Web pages , and then unifies the extracted phrase and its equivalence ( s ) in the language as one index unit .	1<2	none	progression	progression
P06-1081	123-125	126-140	Experimental results show	that the high precision of our conceptual unification approach greatly improves the IR performance .	Experimental results show	that the high precision of our conceptual unification approach greatly improves the IR performance .	123-140	123-140	Experimental results show that the high precision of our conceptual unification approach greatly improves the IR performance .	Experimental results show that the high precision of our conceptual unification approach greatly improves the IR performance .	1>2	none	attribution	attribution
P06-1081	69-73	126-140	This paper describes the degree	that the high precision of our conceptual unification approach greatly improves the IR performance .	This paper describes the degree	that the high precision of our conceptual unification approach greatly improves the IR performance .	69-89	123-140	This paper describes the degree to which the problem arises in IR and suggests a novel technique to solve it .	Experimental results show that the high precision of our conceptual unification approach greatly improves the IR performance .	1<2	none	evaluation	evaluation
P06-1082	1-2,7-11	49-63	Word alignment <*> has recently become popular .	In this work we studied the performance of two very popular recency-vector based approaches ,	Word alignment <*> has recently become popular .	In this work we studied the performance of two very popular recency-vector based approaches ,	1-11	49-89	Word alignment using recency-vector based approach has recently become popular .	In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	1>2	none	bg-goal	bg-goal
P06-1082	1-2,7-11	3-6	Word alignment <*> has recently become popular .	using recency-vector based approach	Word alignment <*> has recently become popular .	using recency-vector based approach	1-11	1-11	Word alignment using recency-vector based approach has recently become popular .	Word alignment using recency-vector based approach has recently become popular .	1<2	none	elab-addition	elab-addition
P06-1082	12-19	23-25	One major advantage of these techniques is that	they perform well	One major advantage of these techniques is that	they perform well	12-36	12-36	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	1>2	none	attribution	attribution
P06-1082	20-22	23-25	unlike other approaches	they perform well	unlike other approaches	they perform well	12-36	12-36	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	1>2	none	contrast	contrast
P06-1082	1-2,7-11	23-25	Word alignment <*> has recently become popular .	they perform well	Word alignment <*> has recently become popular .	they perform well	1-11	12-36	Word alignment using recency-vector based approach has recently become popular .	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	1<2	none	elab-addition	elab-addition
P06-1082	23-25	26-36	they perform well	even if the size of the parallel corpora is small .	they perform well	even if the size of the parallel corpora is small .	12-36	12-36	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small .	1<2	none	contrast	contrast
P06-1082	1-2,7-11	37-43	Word alignment <*> has recently become popular .	This makes these algorithms worth-studying for languages	Word alignment <*> has recently become popular .	This makes these algorithms worth-studying for languages	1-11	37-48	Word alignment using recency-vector based approach has recently become popular .	This makes these algorithms worth-studying for languages where resources are scarce .	1<2	none	progression	progression
P06-1082	37-43	44-48	This makes these algorithms worth-studying for languages	where resources are scarce .	This makes these algorithms worth-studying for languages	where resources are scarce .	37-48	37-48	This makes these algorithms worth-studying for languages where resources are scarce .	This makes these algorithms worth-studying for languages where resources are scarce .	1<2	none	elab-addition	elab-addition
P06-1082	49-63	64-89	In this work we studied the performance of two very popular recency-vector based approaches ,	proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	In this work we studied the performance of two very popular recency-vector based approaches ,	proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	49-89	49-89	In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	1<2	none	elab-enumember	elab-enumember
P06-1082	64-89	90-102	proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	But performance of the above algorithms was not found to be satisfactory .	proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	But performance of the above algorithms was not found to be satisfactory .	49-89	90-102	In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	But performance of the above algorithms was not found to be satisfactory .	1<2	none	contrast	contrast
P06-1082	90-102	103-125	But performance of the above algorithms was not found to be satisfactory .	However , subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus .	But performance of the above algorithms was not found to be satisfactory .	However , subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus .	90-102	103-125	But performance of the above algorithms was not found to be satisfactory .	However , subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus .	1<2	none	contrast	contrast
P06-1082	49-63	126-141	In this work we studied the performance of two very popular recency-vector based approaches ,	The present paper discusses the new version of the algorithm and its performance in detail .	In this work we studied the performance of two very popular recency-vector based approaches ,	The present paper discusses the new version of the algorithm and its performance in detail .	49-89	126-141	In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown , 1994 ) and ( Somers , 1998 ) , respectively , for word alignment in English-Hindi parallel corpus .	The present paper discusses the new version of the algorithm and its performance in detail .	1<2	none	elab-addition	elab-addition
P06-1083	1-4	5-11	This paper proposes methods	for extracting loanwords from Cyrillic Mongolian corpora	This paper proposes methods	for extracting loanwords from Cyrillic Mongolian corpora	1-18	1-18	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	1<2	none	enablement	enablement
P06-1083	5-11	12-18	for extracting loanwords from Cyrillic Mongolian corpora	and producing a Japanese-Mongolian bilingual dictionary .	for extracting loanwords from Cyrillic Mongolian corpora	and producing a Japanese-Mongolian bilingual dictionary .	1-18	1-18	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	1<2	none	joint	joint
P06-1083	1-4	19-24	This paper proposes methods	We extract loanwords from Mongolian corpora	This paper proposes methods	We extract loanwords from Mongolian corpora	1-18	19-30	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	We extract loanwords from Mongolian corpora using our own handcrafted rules .	1<2	none	manner-means	manner-means
P06-1083	19-24	25-30	We extract loanwords from Mongolian corpora	using our own handcrafted rules .	We extract loanwords from Mongolian corpora	using our own handcrafted rules .	19-30	19-30	We extract loanwords from Mongolian corpora using our own handcrafted rules .	We extract loanwords from Mongolian corpora using our own handcrafted rules .	1<2	none	manner-means	manner-means
P06-1083	31-36	37-43	To complement the rule-based extraction ,	we also extract words in Mongolian corpora	To complement the rule-based extraction ,	we also extract words in Mongolian corpora	31-54	31-54	To complement the rule-based extraction , we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords .	To complement the rule-based extraction , we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords .	1>2	none	enablement	enablement
P06-1083	1-4	37-43	This paper proposes methods	we also extract words in Mongolian corpora	This paper proposes methods	we also extract words in Mongolian corpora	1-18	31-54	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	To complement the rule-based extraction , we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords .	1<2	none	manner-means	manner-means
P06-1083	37-43	44-54	we also extract words in Mongolian corpora	that are phonetically similar to Japanese Katakana words as loanwords .	we also extract words in Mongolian corpora	that are phonetically similar to Japanese Katakana words as loanwords .	31-54	31-54	To complement the rule-based extraction , we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords .	To complement the rule-based extraction , we also extract words in Mongolian corpora that are phonetically similar to Japanese Katakana words as loanwords .	1<2	none	elab-addition	elab-addition
P06-1083	1-4	55-65	This paper proposes methods	In addition , we correspond the extracted loanwords to Japanese words	This paper proposes methods	In addition , we correspond the extracted loanwords to Japanese words	1-18	55-71	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	In addition , we correspond the extracted loanwords to Japanese words and produce a bilingual dictionary .	1<2	none	progression	progression
P06-1083	55-65	66-71	In addition , we correspond the extracted loanwords to Japanese words	and produce a bilingual dictionary .	In addition , we correspond the extracted loanwords to Japanese words	and produce a bilingual dictionary .	55-71	55-71	In addition , we correspond the extracted loanwords to Japanese words and produce a bilingual dictionary .	In addition , we correspond the extracted loanwords to Japanese words and produce a bilingual dictionary .	1<2	none	joint	joint
P06-1083	1-4	72-78	This paper proposes methods	We propose a stemming method for Mongolian	This paper proposes methods	We propose a stemming method for Mongolian	1-18	72-83	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	We propose a stemming method for Mongolian to extract loanwords correctly .	1<2	none	progression	progression
P06-1083	72-78	79-83	We propose a stemming method for Mongolian	to extract loanwords correctly .	We propose a stemming method for Mongolian	to extract loanwords correctly .	72-83	72-83	We propose a stemming method for Mongolian to extract loanwords correctly .	We propose a stemming method for Mongolian to extract loanwords correctly .	1<2	none	enablement	enablement
P06-1083	1-4	84-92	This paper proposes methods	We verify the effectiveness of our methods experimentally .	This paper proposes methods	We verify the effectiveness of our methods experimentally .	1-18	84-92	This paper proposes methods for extracting loanwords from Cyrillic Mongolian corpora and producing a Japanese-Mongolian bilingual dictionary .	We verify the effectiveness of our methods experimentally .	1<2	none	evaluation	evaluation
P06-1084	1-5	73-79	Morphological disambiguation is the process	We present an unsupervised stochastic model -	Morphological disambiguation is the process	We present an unsupervised stochastic model -	1-20	73-106	Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	1>2	none	elab-addition	elab-addition
P06-1084	1-5	6-20	Morphological disambiguation is the process	of assigning one set of morphological features to each individual word in a text .	Morphological disambiguation is the process	of assigning one set of morphological features to each individual word in a text .	1-20	1-20	Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .	Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .	1<2	none	elab-addition	elab-addition
P06-1084	21-25	37-39,45-48	When the word is ambiguous	a disambiguation procedure <*> must be applied .	When the word is ambiguous	a disambiguation procedure <*> must be applied .	21-48	21-48	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	1>2	none	temporal	temporal
P06-1084	21-25	26-36	When the word is ambiguous	( there are several possible analyses for the word ) ,	When the word is ambiguous	( there are several possible analyses for the word ) ,	21-48	21-48	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	1<2	none	elab-definition	elab-definition
P06-1084	1-5	37-39,45-48	Morphological disambiguation is the process	a disambiguation procedure <*> must be applied .	Morphological disambiguation is the process	a disambiguation procedure <*> must be applied .	1-20	21-48	Morphological disambiguation is the process of assigning one set of morphological features to each individual word in a text .	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	1<2	none	elab-addition	elab-addition
P06-1084	37-39,45-48	40-44	a disambiguation procedure <*> must be applied .	based on the word context	a disambiguation procedure <*> must be applied .	based on the word context	21-48	21-48	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	When the word is ambiguous ( there are several possible analyses for the word ) , a disambiguation procedure based on the word context must be applied .	1<2	none	bg-general	bg-general
P06-1084	49-59	73-79	This paper deals with morphological disambiguation of the Hebrew language ,	We present an unsupervised stochastic model -	This paper deals with morphological disambiguation of the Hebrew language ,	We present an unsupervised stochastic model -	49-72	73-106	This paper deals with morphological disambiguation of the Hebrew language , which combines morphemes into a word in both agglutinative and fusional ways .	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	1>2	none	bg-goal	bg-goal
P06-1084	49-59	60-72	This paper deals with morphological disambiguation of the Hebrew language ,	which combines morphemes into a word in both agglutinative and fusional ways .	This paper deals with morphological disambiguation of the Hebrew language ,	which combines morphemes into a word in both agglutinative and fusional ways .	49-72	49-72	This paper deals with morphological disambiguation of the Hebrew language , which combines morphemes into a word in both agglutinative and fusional ways .	This paper deals with morphological disambiguation of the Hebrew language , which combines morphemes into a word in both agglutinative and fusional ways .	1<2	none	elab-addition	elab-addition
P06-1084	73-79	80-89	We present an unsupervised stochastic model -	the only resource we use is a morphological analyzer -	We present an unsupervised stochastic model -	the only resource we use is a morphological analyzer -	73-106	73-106	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	1<2	none	elab-addition	elab-addition
P06-1084	73-79	90-96	We present an unsupervised stochastic model -	which deals with the data sparseness problem	We present an unsupervised stochastic model -	which deals with the data sparseness problem	73-106	73-106	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	1<2	none	elab-addition	elab-addition
P06-1084	90-96	97-106	which deals with the data sparseness problem	caused by the affixational morphology of the Hebrew language .	which deals with the data sparseness problem	caused by the affixational morphology of the Hebrew language .	73-106	73-106	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	1<2	none	elab-addition	elab-addition
P06-1084	73-79	107-117	We present an unsupervised stochastic model -	We present a text encoding method for languages with affixational morphology	We present an unsupervised stochastic model -	We present a text encoding method for languages with affixational morphology	73-106	107-138	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules ( which are quite restricted in Hebrew ) helps in the disambiguation .	1<2	none	elab-addition	elab-addition
P06-1084	107-117	118-125,134-138	We present a text encoding method for languages with affixational morphology	in which the knowledge of word formation rules <*> helps in the disambiguation .	We present a text encoding method for languages with affixational morphology	in which the knowledge of word formation rules <*> helps in the disambiguation .	107-138	107-138	We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules ( which are quite restricted in Hebrew ) helps in the disambiguation .	We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules ( which are quite restricted in Hebrew ) helps in the disambiguation .	1<2	none	elab-addition	elab-addition
P06-1084	118-125,134-138	126-133	in which the knowledge of word formation rules <*> helps in the disambiguation .	( which are quite restricted in Hebrew )	in which the knowledge of word formation rules <*> helps in the disambiguation .	( which are quite restricted in Hebrew )	107-138	107-138	We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules ( which are quite restricted in Hebrew ) helps in the disambiguation .	We present a text encoding method for languages with affixational morphology in which the knowledge of word formation rules ( which are quite restricted in Hebrew ) helps in the disambiguation .	1<2	none	elab-addition	elab-addition
P06-1084	73-79	139-142	We present an unsupervised stochastic model -	We adapt HMM algorithms	We present an unsupervised stochastic model -	We adapt HMM algorithms	73-106	139-167	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .	1<2	none	manner-means	manner-means
P06-1084	139-142	143-150	We adapt HMM algorithms	for learning and searching this text representation ,	We adapt HMM algorithms	for learning and searching this text representation ,	139-167	139-167	We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .	We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .	1<2	none	enablement	enablement
P06-1084	139-142	151-167	We adapt HMM algorithms	in such a way that segmentation and tagging can be learned in parallel in one step .	We adapt HMM algorithms	in such a way that segmentation and tagging can be learned in parallel in one step .	139-167	139-167	We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .	We adapt HMM algorithms for learning and searching this text representation , in such a way that segmentation and tagging can be learned in parallel in one step .	1<2	none	elab-addition	elab-addition
P06-1084	168-174	175-184	Results on a large scale evaluation indicate	that this learning improves disambiguation for complex tag sets .	Results on a large scale evaluation indicate	that this learning improves disambiguation for complex tag sets .	168-184	168-184	Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets .	Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets .	1>2	none	attribution	attribution
P06-1084	175-184	185-195	that this learning improves disambiguation for complex tag sets .	Our method is applicable to other languages with affix morphology .	that this learning improves disambiguation for complex tag sets .	Our method is applicable to other languages with affix morphology .	168-184	185-195	Results on a large scale evaluation indicate that this learning improves disambiguation for complex tag sets .	Our method is applicable to other languages with affix morphology .	1>2	none	manner-means	manner-means
P06-1084	73-79	185-195	We present an unsupervised stochastic model -	Our method is applicable to other languages with affix morphology .	We present an unsupervised stochastic model -	Our method is applicable to other languages with affix morphology .	73-106	185-195	We present an unsupervised stochastic model - the only resource we use is a morphological analyzer - which deals with the data sparseness problem caused by the affixational morphology of the Hebrew language .	Our method is applicable to other languages with affix morphology .	1<2	none	evaluation	evaluation
P06-1085	1-11	32-39	Developing better methods for segmenting continuous text into words is important	We propose two new Bayesian word segmentation methods	Developing better methods for segmenting continuous text into words is important	We propose two new Bayesian word segmentation methods	1-31	32-50	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages , and may shed light on how humans learn to segment speech .	We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively .	1>2	none	bg-goal	bg-goal
P06-1085	1-11	12-19	Developing better methods for segmenting continuous text into words is important	for improving the processing of Asian languages ,	Developing better methods for segmenting continuous text into words is important	for improving the processing of Asian languages ,	1-31	1-31	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages , and may shed light on how humans learn to segment speech .	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages , and may shed light on how humans learn to segment speech .	1<2	none	enablement	enablement
P06-1085	1-11	20-31	Developing better methods for segmenting continuous text into words is important	and may shed light on how humans learn to segment speech .	Developing better methods for segmenting continuous text into words is important	and may shed light on how humans learn to segment speech .	1-31	1-31	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages , and may shed light on how humans learn to segment speech .	Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages , and may shed light on how humans learn to segment speech .	1<2	none	progression	progression
P06-1085	32-39	40-50	We propose two new Bayesian word segmentation methods	that assume unigram and bigram models of word dependencies respectively .	We propose two new Bayesian word segmentation methods	that assume unigram and bigram models of word dependencies respectively .	32-50	32-50	We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively .	We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively .	1<2	none	elab-addition	elab-addition
P06-1085	32-39	51-65	We propose two new Bayesian word segmentation methods	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) ,	We propose two new Bayesian word segmentation methods	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) ,	32-50	51-75	We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively .	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) , demonstrating the importance of such dependencies for word segmentation .	1<2	none	evaluation	evaluation
P06-1085	51-65	66-75	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) ,	demonstrating the importance of such dependencies for word segmentation .	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) ,	demonstrating the importance of such dependencies for word segmentation .	51-75	51-75	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) , demonstrating the importance of such dependencies for word segmentation .	The bigram model greatly outperforms the unigram model ( and previous probabilistic models ) , demonstrating the importance of such dependencies for word segmentation .	1<2	none	elab-addition	elab-addition
P06-1085	76-78	79-89	We also show	that previous probabilistic models rely crucially on suboptimal search procedures .	We also show	that previous probabilistic models rely crucially on suboptimal search procedures .	76-89	76-89	We also show that previous probabilistic models rely crucially on suboptimal search procedures .	We also show that previous probabilistic models rely crucially on suboptimal search procedures .	1>2	none	attribution	attribution
P06-1085	32-39	79-89	We propose two new Bayesian word segmentation methods	that previous probabilistic models rely crucially on suboptimal search procedures .	We propose two new Bayesian word segmentation methods	that previous probabilistic models rely crucially on suboptimal search procedures .	32-50	76-89	We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively .	We also show that previous probabilistic models rely crucially on suboptimal search procedures .	1<2	none	evaluation	evaluation
P06-1086	1-15	16-19	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	Our work is novel	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	Our work is novel	1-15	16-34	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects .	1<2	none	elab-addition	elab-addition
P06-1086	16-19	20-26	Our work is novel	in that it explicitly addresses the need	Our work is novel	in that it explicitly addresses the need	16-34	16-34	Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects .	Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects .	1<2	none	exp-reason	exp-reason
P06-1086	20-26	27-34	in that it explicitly addresses the need	for processing the morphology of the dialects .	in that it explicitly addresses the need	for processing the morphology of the dialects .	16-34	16-34	Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects .	Our work is novel in that it explicitly addresses the need for processing the morphology of the dialects .	1<2	none	elab-addition	elab-addition
P06-1086	1-15	35-47	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation ,	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation ,	1-15	35-65	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .	1<2	none	elab-addition	elab-addition
P06-1086	35-47	48-55	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation ,	it has separate phonological and orthographic representations ,	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation ,	it has separate phonological and orthographic representations ,	35-65	35-65	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .	1<2	none	joint	joint
P06-1086	48-55	56-65	it has separate phonological and orthographic representations ,	and it allows for combining morphemes from different dialects .	it has separate phonological and orthographic representations ,	and it allows for combining morphemes from different dialects .	35-65	35-65	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .	MAGEAD performs an on-line analysis to or generation from a root+pattern+features representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .	1<2	none	joint	joint
P06-1086	1-15	66-73	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	We present a detailed evaluation of MAGEAD .	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	We present a detailed evaluation of MAGEAD .	1-15	66-73	We present MAGEAD , a morphological analyzer and generator for the Arabic language family .	We present a detailed evaluation of MAGEAD .	1<2	none	evaluation	evaluation
P06-1087	12-13	14-29	We show	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	We show	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	12-38	12-38	We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew , and propose an alternative definition of Simple NPs .	We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew , and propose an alternative definition of Simple NPs .	1>2	none	attribution	attribution
P06-1087	1-11	14-29	We present a method for Noun Phrase chunking in Hebrew .	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	We present a method for Noun Phrase chunking in Hebrew .	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	1-11	12-38	We present a method for Noun Phrase chunking in Hebrew .	We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew , and propose an alternative definition of Simple NPs .	1<2	none	elab-process_step	elab-process_step
P06-1087	14-29	30-38	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	and propose an alternative definition of Simple NPs .	that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew ,	and propose an alternative definition of Simple NPs .	12-38	12-38	We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew , and propose an alternative definition of Simple NPs .	We show that the traditional definition of base-NPs as nonrecursive noun phrases does not apply in Hebrew , and propose an alternative definition of Simple NPs .	1<2	none	progression	progression
P06-1087	1-11	39-44	We present a method for Noun Phrase chunking in Hebrew .	We review syntactic properties of Hebrew	We present a method for Noun Phrase chunking in Hebrew .	We review syntactic properties of Hebrew	1-11	39-66	We present a method for Noun Phrase chunking in Hebrew .	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	1<2	none	elab-process_step	elab-process_step
P06-1087	39-44	45-49	We review syntactic properties of Hebrew	related to noun phrases ,	We review syntactic properties of Hebrew	related to noun phrases ,	39-66	39-66	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	1<2	none	elab-addition	elab-addition
P06-1087	50-51	52-66	which indicate	that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	which indicate	that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	39-66	39-66	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	1>2	none	attribution	attribution
P06-1087	39-44	52-66	We review syntactic properties of Hebrew	that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	We review syntactic properties of Hebrew	that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	39-66	39-66	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	1<2	none	elab-addition	elab-addition
P06-1087	67-73	84-98	As a confirmation , we apply methods	These methods give low results ( F from 76 to 86 ) in Hebrew .	As a confirmation , we apply methods	These methods give low results ( F from 76 to 86 ) in Hebrew .	67-83	84-98	As a confirmation , we apply methods known to work well for English to Hebrew data .	These methods give low results ( F from 76 to 86 ) in Hebrew .	1>2	none	manner-means	manner-means
P06-1087	67-73	74-83	As a confirmation , we apply methods	known to work well for English to Hebrew data .	As a confirmation , we apply methods	known to work well for English to Hebrew data .	67-83	67-83	As a confirmation , we apply methods known to work well for English to Hebrew data .	As a confirmation , we apply methods known to work well for English to Hebrew data .	1<2	none	elab-addition	elab-addition
P06-1087	39-44	84-98	We review syntactic properties of Hebrew	These methods give low results ( F from 76 to 86 ) in Hebrew .	We review syntactic properties of Hebrew	These methods give low results ( F from 76 to 86 ) in Hebrew .	39-66	84-98	We review syntactic properties of Hebrew related to noun phrases , which indicate that the task of Hebrew SimpleNP chunking is harder than base-NP chunking in English .	These methods give low results ( F from 76 to 86 ) in Hebrew .	1<2	none	evaluation	evaluation
P06-1087	1-11	99-104	We present a method for Noun Phrase chunking in Hebrew .	We then discuss our method ,	We present a method for Noun Phrase chunking in Hebrew .	We then discuss our method ,	1-11	99-114	We present a method for Noun Phrase chunking in Hebrew .	We then discuss our method , which applies SVM induction over lexical and morphological features .	1<2	none	elab-process_step	elab-process_step
P06-1087	99-104	105-114	We then discuss our method ,	which applies SVM induction over lexical and morphological features .	We then discuss our method ,	which applies SVM induction over lexical and morphological features .	99-114	99-114	We then discuss our method , which applies SVM induction over lexical and morphological features .	We then discuss our method , which applies SVM induction over lexical and morphological features .	1<2	none	elab-addition	elab-addition
P06-1087	1-11	115-134	We present a method for Noun Phrase chunking in Hebrew .	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 ,	We present a method for Noun Phrase chunking in Hebrew .	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 ,	1-11	115-153	We present a method for Noun Phrase chunking in Hebrew .	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 , resulting in a system with average performance of 93 % precision , 93.4 % recall and 93.2 Fmeasure .	1<2	none	evaluation	evaluation
P06-1087	115-134	135-153	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 ,	resulting in a system with average performance of 93 % precision , 93.4 % recall and 93.2 Fmeasure .	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 ,	resulting in a system with average performance of 93 % precision , 93.4 % recall and 93.2 Fmeasure .	115-153	115-153	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 , resulting in a system with average performance of 93 % precision , 93.4 % recall and 93.2 Fmeasure .	Morphological features improve the average precision by ~0.5 % , recall by ~1 % , and F-measure by ~0.75 , resulting in a system with average performance of 93 % precision , 93.4 % recall and 93.2 Fmeasure .	1<2	none	result	result
P06-1088	1-10	11-24	With performance above 97 % accuracy for newspaper text ,	part of speech ( POS ) tagging might be considered a solved problem .	With performance above 97 % accuracy for newspaper text ,	part of speech ( POS ) tagging might be considered a solved problem .	1-24	1-24	With performance above 97 % accuracy for newspaper text , part of speech ( POS ) tagging might be considered a solved problem .	With performance above 97 % accuracy for newspaper text , part of speech ( POS ) tagging might be considered a solved problem .	1>2	none	exp-evidence	exp-evidence
P06-1088	11-24	61-66	part of speech ( POS ) tagging might be considered a solved problem .	tagging accuracy is much lower .	part of speech ( POS ) tagging might be considered a solved problem .	tagging accuracy is much lower .	1-24	43-66	With performance above 97 % accuracy for newspaper text , part of speech ( POS ) tagging might be considered a solved problem .	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	1>2	none	contrast	contrast
P06-1088	25-28	29-42	Previous studies have shown	that allowing the parser to resolve POS tag ambiguity does not improve performance .	Previous studies have shown	that allowing the parser to resolve POS tag ambiguity does not improve performance .	25-42	25-42	Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance .	Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance .	1>2	none	attribution	attribution
P06-1088	11-24	29-42	part of speech ( POS ) tagging might be considered a solved problem .	that allowing the parser to resolve POS tag ambiguity does not improve performance .	part of speech ( POS ) tagging might be considered a solved problem .	that allowing the parser to resolve POS tag ambiguity does not improve performance .	1-24	25-42	With performance above 97 % accuracy for newspaper text , part of speech ( POS ) tagging might be considered a solved problem .	Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance .	1<2	none	elab-addition	elab-addition
P06-1088	43-47	61-66	However , for grammar formalisms	tagging accuracy is much lower .	However , for grammar formalisms	tagging accuracy is much lower .	43-66	43-66	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	1>2	none	elab-addition	elab-addition
P06-1088	43-47	48-54	However , for grammar formalisms	which use more fine-grained grammatical categories ,	However , for grammar formalisms	which use more fine-grained grammatical categories ,	43-66	43-66	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	1<2	none	elab-addition	elab-addition
P06-1088	48-54	55-60	which use more fine-grained grammatical categories ,	for example TAG and CCG ,	which use more fine-grained grammatical categories ,	for example TAG and CCG ,	43-66	43-66	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	1<2	none	elab-example	elab-example
P06-1088	61-66	81-85	tagging accuracy is much lower .	We describe a multi-tagging approach	tagging accuracy is much lower .	We describe a multi-tagging approach	43-66	81-101	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	1>2	none	bg-compare	bg-compare
P06-1088	61-66	67-80	tagging accuracy is much lower .	In fact , for these formalisms , premature ambiguity resolution makes parsing infeasible .	tagging accuracy is much lower .	In fact , for these formalisms , premature ambiguity resolution makes parsing infeasible .	43-66	67-80	However , for grammar formalisms which use more fine-grained grammatical categories , for example TAG and CCG , tagging accuracy is much lower .	In fact , for these formalisms , premature ambiguity resolution makes parsing infeasible .	1<2	none	elab-addition	elab-addition
P06-1088	81-85	86-101	We describe a multi-tagging approach	which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	We describe a multi-tagging approach	which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	81-101	81-101	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	1<2	none	elab-addition	elab-addition
P06-1088	81-85	102-110	We describe a multi-tagging approach	We extend this multitagging approach to the POS level	We describe a multi-tagging approach	We extend this multitagging approach to the POS level	81-101	102-120	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags .	1<2	none	progression	progression
P06-1088	102-110	111-113	We extend this multitagging approach to the POS level	to overcome errors	We extend this multitagging approach to the POS level	to overcome errors	102-120	102-120	We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags .	We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags .	1<2	none	enablement	enablement
P06-1088	111-113	114-120	to overcome errors	introduced by automatically assigned POS tags .	to overcome errors	introduced by automatically assigned POS tags .	102-120	102-120	We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags .	We extend this multitagging approach to the POS level to overcome errors introduced by automatically assigned POS tags .	1<2	none	elab-addition	elab-addition
P06-1088	121-127	128-144	Although POS tagging accuracy seems high ,	maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	Although POS tagging accuracy seems high ,	maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	121-144	121-144	Although POS tagging accuracy seems high , maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	Although POS tagging accuracy seems high , maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	1>2	none	contrast	contrast
P06-1088	81-85	128-144	We describe a multi-tagging approach	maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	We describe a multi-tagging approach	maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	81-101	121-144	We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient CCG parsing .	Although POS tagging accuracy seems high , maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging .	1<2	none	evaluation	evaluation
P06-1089	1-8	9-15	In this paper , we present a method	for guessing POS tags of unknown words	In this paper , we present a method	for guessing POS tags of unknown words	1-21	1-21	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	1<2	none	enablement	enablement
P06-1089	9-15	16-21	for guessing POS tags of unknown words	using local and global information .	for guessing POS tags of unknown words	using local and global information .	1-21	1-21	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	1<2	none	elab-addition	elab-addition
P06-1089	22-29	40-48	Although many existing methods use only local information	global information ( extra-sentential features ) provides valuable clues	Although many existing methods use only local information	global information ( extra-sentential features ) provides valuable clues	22-56	22-56	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	1>2	none	contrast	contrast
P06-1089	22-29	30-39	Although many existing methods use only local information	( i.e. limited window size or intra-sentential features ) ,	Although many existing methods use only local information	( i.e. limited window size or intra-sentential features ) ,	22-56	22-56	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	1<2	none	elab-example	elab-example
P06-1089	1-8	40-48	In this paper , we present a method	global information ( extra-sentential features ) provides valuable clues	In this paper , we present a method	global information ( extra-sentential features ) provides valuable clues	1-21	22-56	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	1<2	none	exp-reason	exp-reason
P06-1089	40-48	49-56	global information ( extra-sentential features ) provides valuable clues	for predicting POS tags of unknown words .	global information ( extra-sentential features ) provides valuable clues	for predicting POS tags of unknown words .	22-56	22-56	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	Although many existing methods use only local information ( i.e. limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting POS tags of unknown words .	1<2	none	enablement	enablement
P06-1089	1-8	57-67	In this paper , we present a method	We propose a probabilistic model for POS guessing of unknown words	In this paper , we present a method	We propose a probabilistic model for POS guessing of unknown words	1-21	57-84	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	1<2	none	manner-means	manner-means
P06-1089	57-67	68-76	We propose a probabilistic model for POS guessing of unknown words	using global information as well as local information ,	We propose a probabilistic model for POS guessing of unknown words	using global information as well as local information ,	57-84	57-84	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	1<2	none	elab-addition	elab-addition
P06-1089	57-67	77-80	We propose a probabilistic model for POS guessing of unknown words	and estimate its parameters	We propose a probabilistic model for POS guessing of unknown words	and estimate its parameters	57-84	57-84	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	1<2	none	joint	joint
P06-1089	77-80	81-84	and estimate its parameters	using Gibbs sampling .	and estimate its parameters	using Gibbs sampling .	57-84	57-84	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	We propose a probabilistic model for POS guessing of unknown words using global information as well as local information , and estimate its parameters using Gibbs sampling .	1<2	none	elab-addition	elab-addition
P06-1089	1-8	85-95	In this paper , we present a method	We also attempt to apply the model to semisupervised learning ,	In this paper , we present a method	We also attempt to apply the model to semisupervised learning ,	1-21	85-102	In this paper , we present a method for guessing POS tags of unknown words using local and global information .	We also attempt to apply the model to semisupervised learning , and conduct experiments on multiple corpora .	1<2	none	evaluation	evaluation
P06-1089	85-95	96-102	We also attempt to apply the model to semisupervised learning ,	and conduct experiments on multiple corpora .	We also attempt to apply the model to semisupervised learning ,	and conduct experiments on multiple corpora .	85-102	85-102	We also attempt to apply the model to semisupervised learning , and conduct experiments on multiple corpora .	We also attempt to apply the model to semisupervised learning , and conduct experiments on multiple corpora .	1<2	none	joint	joint
P06-1090	1-11	12-22	In this paper , we present a novel global reordering model	that can be incorporated into standard phrase-based statistical machine translation .	In this paper , we present a novel global reordering model	that can be incorporated into standard phrase-based statistical machine translation .	1-22	1-22	In this paper , we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation .	In this paper , we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation .	1<2	none	elab-addition	elab-addition
P06-1090	23-27	44-52	Unlike previous local reordering models	our model explicitly models the reordering of long distances	Unlike previous local reordering models	our model explicitly models the reordering of long distances	23-66	23-66	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	1>2	none	contrast	contrast
P06-1090	23-27	28-43	Unlike previous local reordering models	that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) ,	Unlike previous local reordering models	that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) ,	23-66	23-66	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	1<2	none	elab-addition	elab-addition
P06-1090	1-11	44-52	In this paper , we present a novel global reordering model	our model explicitly models the reordering of long distances	In this paper , we present a novel global reordering model	our model explicitly models the reordering of long distances	1-22	23-66	In this paper , we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation .	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	1<2	none	elab-addition	elab-addition
P06-1090	44-52	53-66	our model explicitly models the reordering of long distances	by directly estimating the parameters from the phrase alignments of bilingual training sentences .	our model explicitly models the reordering of long distances	by directly estimating the parameters from the phrase alignments of bilingual training sentences .	23-66	23-66	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	1<2	none	manner-means	manner-means
P06-1090	44-52	67-82	our model explicitly models the reordering of long distances	In principle , the global phrase reordering model is conditioned on the source and target phrases	our model explicitly models the reordering of long distances	In principle , the global phrase reordering model is conditioned on the source and target phrases	23-66	67-97	Unlike previous local reordering models that emphasize the reordering of adjacent phrase pairs ( Tillmann and Zhang , 2005 ) , our model explicitly models the reordering of long distances by directly estimating the parameters from the phrase alignments of bilingual training sentences .	In principle , the global phrase reordering model is conditioned on the source and target phrases that are currently being translated , and the previously translated source and target phrases .	1<2	none	elab-addition	elab-addition
P06-1090	67-82	83-88	In principle , the global phrase reordering model is conditioned on the source and target phrases	that are currently being translated ,	In principle , the global phrase reordering model is conditioned on the source and target phrases	that are currently being translated ,	67-97	67-97	In principle , the global phrase reordering model is conditioned on the source and target phrases that are currently being translated , and the previously translated source and target phrases .	In principle , the global phrase reordering model is conditioned on the source and target phrases that are currently being translated , and the previously translated source and target phrases .	1<2	none	elab-addition	elab-addition
P06-1090	67-82	89-97	In principle , the global phrase reordering model is conditioned on the source and target phrases	and the previously translated source and target phrases .	In principle , the global phrase reordering model is conditioned on the source and target phrases	and the previously translated source and target phrases .	67-97	67-97	In principle , the global phrase reordering model is conditioned on the source and target phrases that are currently being translated , and the previously translated source and target phrases .	In principle , the global phrase reordering model is conditioned on the source and target phrases that are currently being translated , and the previously translated source and target phrases .	1<2	none	joint	joint
P06-1090	98-102	103-112	To cope with sparseness ,	we use N-best phrase alignments and bilingual phrase clustering ,	To cope with sparseness ,	we use N-best phrase alignments and bilingual phrase clustering ,	98-122	98-122	To cope with sparseness , we use N-best phrase alignments and bilingual phrase clustering , and investigate a variety of combinations of conditioning factors .	To cope with sparseness , we use N-best phrase alignments and bilingual phrase clustering , and investigate a variety of combinations of conditioning factors .	1>2	none	enablement	enablement
P06-1090	1-11	103-112	In this paper , we present a novel global reordering model	we use N-best phrase alignments and bilingual phrase clustering ,	In this paper , we present a novel global reordering model	we use N-best phrase alignments and bilingual phrase clustering ,	1-22	98-122	In this paper , we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation .	To cope with sparseness , we use N-best phrase alignments and bilingual phrase clustering , and investigate a variety of combinations of conditioning factors .	1<2	none	manner-means	manner-means
P06-1090	103-112	113-122	we use N-best phrase alignments and bilingual phrase clustering ,	and investigate a variety of combinations of conditioning factors .	we use N-best phrase alignments and bilingual phrase clustering ,	and investigate a variety of combinations of conditioning factors .	98-122	98-122	To cope with sparseness , we use N-best phrase alignments and bilingual phrase clustering , and investigate a variety of combinations of conditioning factors .	To cope with sparseness , we use N-best phrase alignments and bilingual phrase clustering , and investigate a variety of combinations of conditioning factors .	1<2	none	joint	joint
P06-1090	123-125	129-145	Through experiments ,	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	Through experiments ,	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	123-145	123-145	Through experiments , we show , that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	Through experiments , we show , that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	1>2	none	result	result
P06-1090	126-128	129-145	we show ,	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	we show ,	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	123-145	123-145	Through experiments , we show , that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	Through experiments , we show , that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	1>2	none	attribution	attribution
P06-1090	1-11	129-145	In this paper , we present a novel global reordering model	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	In this paper , we present a novel global reordering model	that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	1-22	123-145	In this paper , we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation .	Through experiments , we show , that the global reordering model significantly improves the translation accuracy of a standard Japanese-English translation task .	1<2	none	evaluation	evaluation
P06-1091	1-15	16-22	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The key component is a new procedure	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The key component is a new procedure	1-15	16-35	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .	1<2	none	elab-addition	elab-addition
P06-1091	16-22	23-29	The key component is a new procedure	to directly optimize the global scoring function	The key component is a new procedure	to directly optimize the global scoring function	16-35	16-35	The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .	The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .	1<2	none	enablement	enablement
P06-1091	23-29	30-35	to directly optimize the global scoring function	used by a SMT decoder .	to directly optimize the global scoring function	used by a SMT decoder .	16-35	16-35	The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .	The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder .	1<2	none	elab-addition	elab-addition
P06-1091	36-53	54-57,65-74	No translation , language , or distortion model probabilities are used as in earlier work on SMT .	Therefore our method , <*> is both simpler and more extensible than previous approaches .	No translation , language , or distortion model probabilities are used as in earlier work on SMT .	Therefore our method , <*> is both simpler and more extensible than previous approaches .	36-53	54-74	No translation , language , or distortion model probabilities are used as in earlier work on SMT .	Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .	1>2	none	result	result
P06-1091	1-15	54-57,65-74	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	Therefore our method , <*> is both simpler and more extensible than previous approaches .	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	Therefore our method , <*> is both simpler and more extensible than previous approaches .	1-15	54-74	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .	1<2	none	elab-addition	elab-addition
P06-1091	54-57,65-74	58-64	Therefore our method , <*> is both simpler and more extensible than previous approaches .	which employs less domain specific knowledge ,	Therefore our method , <*> is both simpler and more extensible than previous approaches .	which employs less domain specific knowledge ,	54-74	54-74	Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .	Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .	1<2	none	elab-addition	elab-addition
P06-1091	54-57,65-74	75-86	Therefore our method , <*> is both simpler and more extensible than previous approaches .	Moreover , the training procedure treats the decoder as a black-box ,	Therefore our method , <*> is both simpler and more extensible than previous approaches .	Moreover , the training procedure treats the decoder as a black-box ,	54-74	75-97	Therefore our method , which employs less domain specific knowledge , is both simpler and more extensible than previous approaches .	Moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme .	1<2	none	progression	progression
P06-1091	75-86	87-91	Moreover , the training procedure treats the decoder as a black-box ,	and thus can be used	Moreover , the training procedure treats the decoder as a black-box ,	and thus can be used	75-97	75-97	Moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme .	Moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme .	1<2	none	result	result
P06-1091	87-91	92-97	and thus can be used	to optimize any decoding scheme .	and thus can be used	to optimize any decoding scheme .	75-97	75-97	Moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme .	Moreover , the training procedure treats the decoder as a black-box , and thus can be used to optimize any decoding scheme .	1<2	none	enablement	enablement
P06-1091	1-15	98-109	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The training algorithm is evaluated on a standard Arabic-English translation task .	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The training algorithm is evaluated on a standard Arabic-English translation task .	1-15	98-109	This paper presents a novel training algorithm for a linearly-scored block sequence translation model .	The training algorithm is evaluated on a standard Arabic-English translation task .	1<2	none	evaluation	evaluation
P06-1092	1-15	47-53	The noisy channel model approach is successfully applied to various natural language processing tasks .	As a solution we describe a method	The noisy channel model approach is successfully applied to various natural language processing tasks .	As a solution we describe a method	1-15	47-71	The noisy channel model approach is successfully applied to various natural language processing tasks .	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	1>2	none	bg-compare	bg-compare
P06-1092	1-15	16-27	The noisy channel model approach is successfully applied to various natural language processing tasks .	Currently the main research focus of this approach is adaptation methods ,	The noisy channel model approach is successfully applied to various natural language processing tasks .	Currently the main research focus of this approach is adaptation methods ,	1-15	16-46	The noisy channel model approach is successfully applied to various natural language processing tasks .	Currently the main research focus of this approach is adaptation methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .	1<2	none	elab-addition	elab-addition
P06-1092	16-27	28-39	Currently the main research focus of this approach is adaptation methods ,	how to capture characteristics of words and expressions in a target domain	Currently the main research focus of this approach is adaptation methods ,	how to capture characteristics of words and expressions in a target domain	16-46	16-46	Currently the main research focus of this approach is adaptation methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .	Currently the main research focus of this approach is adaptation methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .	1<2	none	elab-definition	elab-definition
P06-1092	28-39	40-46	how to capture characteristics of words and expressions in a target domain	given example sentences in that domain .	how to capture characteristics of words and expressions in a target domain	given example sentences in that domain .	16-46	16-46	Currently the main research focus of this approach is adaptation methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .	Currently the main research focus of this approach is adaptation methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .	1<2	none	elab-addition	elab-addition
P06-1092	47-53	54-65	As a solution we describe a method	enlarging the vocabulary of a language model to an almost infinite size	As a solution we describe a method	enlarging the vocabulary of a language model to an almost infinite size	47-71	47-71	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	1<2	none	elab-addition	elab-addition
P06-1092	54-65	66-71	enlarging the vocabulary of a language model to an almost infinite size	and capturing their context information .	enlarging the vocabulary of a language model to an almost infinite size	and capturing their context information .	47-71	47-71	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	1<2	none	joint	joint
P06-1092	47-53	72-79	As a solution we describe a method	Especially the new method is suitable for languages	As a solution we describe a method	Especially the new method is suitable for languages	47-71	72-88	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	Especially the new method is suitable for languages in which words are not delimited by whitespace .	1<2	none	progression	progression
P06-1092	72-79	80-88	Especially the new method is suitable for languages	in which words are not delimited by whitespace .	Especially the new method is suitable for languages	in which words are not delimited by whitespace .	72-88	72-88	Especially the new method is suitable for languages in which words are not delimited by whitespace .	Especially the new method is suitable for languages in which words are not delimited by whitespace .	1<2	none	elab-addition	elab-addition
P06-1092	89-99	100-115	We applied our method to a phoneme-to-text transcription task in Japanese	and reduced about 10 % of the errors in the results of an existing method .	We applied our method to a phoneme-to-text transcription task in Japanese	and reduced about 10 % of the errors in the results of an existing method .	89-115	89-115	We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10 % of the errors in the results of an existing method .	We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10 % of the errors in the results of an existing method .	1>2	none	progression	progression
P06-1092	47-53	100-115	As a solution we describe a method	and reduced about 10 % of the errors in the results of an existing method .	As a solution we describe a method	and reduced about 10 % of the errors in the results of an existing method .	47-71	89-115	As a solution we describe a method enlarging the vocabulary of a language model to an almost infinite size and capturing their context information .	We applied our method to a phoneme-to-text transcription task in Japanese and reduced about 10 % of the errors in the results of an existing method .	1<2	none	evaluation	evaluation
P06-1093	1-8	66-68	Call centers handle customer queries from various domains	Towards this ,	Call centers handle customer queries from various domains	Towards this ,	1-23	66-82	Call centers handle customer queries from various domains such as computer sales and support , mobile phones , car rental , etc. .	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	1>2	none	elab-addition	elab-addition
P06-1093	1-8	9-23	Call centers handle customer queries from various domains	such as computer sales and support , mobile phones , car rental , etc. .	Call centers handle customer queries from various domains	such as computer sales and support , mobile phones , car rental , etc. .	1-23	1-23	Call centers handle customer queries from various domains such as computer sales and support , mobile phones , car rental , etc. .	Call centers handle customer queries from various domains such as computer sales and support , mobile phones , car rental , etc. .	1<2	none	elab-example	elab-example
P06-1093	1-8	24-31	Call centers handle customer queries from various domains	Each such domain generally has a domain model	Call centers handle customer queries from various domains	Each such domain generally has a domain model	1-23	24-39	Call centers handle customer queries from various domains such as computer sales and support , mobile phones , car rental , etc. .	Each such domain generally has a domain model which is essential to handle customer complaints .	1<2	none	elab-addition	elab-addition
P06-1093	24-31	32-39	Each such domain generally has a domain model	which is essential to handle customer complaints .	Each such domain generally has a domain model	which is essential to handle customer complaints .	24-39	24-39	Each such domain generally has a domain model which is essential to handle customer complaints .	Each such domain generally has a domain model which is essential to handle customer complaints .	1<2	none	elab-addition	elab-addition
P06-1093	24-31	40-56	Each such domain generally has a domain model	These models contain common problem categories , typical customer issues and their solutions , greeting styles .	Each such domain generally has a domain model	These models contain common problem categories , typical customer issues and their solutions , greeting styles .	24-39	40-56	Each such domain generally has a domain model which is essential to handle customer complaints .	These models contain common problem categories , typical customer issues and their solutions , greeting styles .	1<2	none	elab-addition	elab-addition
P06-1093	24-31	57-65	Each such domain generally has a domain model	Currently these models are manually created over time .	Each such domain generally has a domain model	Currently these models are manually created over time .	24-39	57-65	Each such domain generally has a domain model which is essential to handle customer complaints .	Currently these models are manually created over time .	1<2	none	elab-addition	elab-addition
P06-1093	66-68	69-73	Towards this ,	we propose an unsupervised technique	Towards this ,	we propose an unsupervised technique	66-82	66-82	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	1>2	none	elab-addition	elab-addition
P06-1093	69-73	74-82	we propose an unsupervised technique	to generate domain models automatically from call transcriptions .	we propose an unsupervised technique	to generate domain models automatically from call transcriptions .	66-82	66-82	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	1<2	none	enablement	enablement
P06-1093	69-73	83-93	we propose an unsupervised technique	We use a state of the art Automatic Speech Recognition system	we propose an unsupervised technique	We use a state of the art Automatic Speech Recognition system	66-82	83-132	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	1<2	none	manner-means	manner-means
P06-1093	83-93	94-102	We use a state of the art Automatic Speech Recognition system	to transcribe the calls between agents and customers ,	We use a state of the art Automatic Speech Recognition system	to transcribe the calls between agents and customers ,	83-132	83-132	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	1<2	none	enablement	enablement
P06-1093	94-102	103-114	to transcribe the calls between agents and customers ,	which still results in high word error rates ( 40 % )	to transcribe the calls between agents and customers ,	which still results in high word error rates ( 40 % )	83-132	83-132	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	1<2	none	elab-addition	elab-addition
P06-1093	115-116	117-132	and show	that even from these noisy transcriptions of calls we can automatically build a domain model .	and show	that even from these noisy transcriptions of calls we can automatically build a domain model .	83-132	83-132	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	1>2	none	attribution	attribution
P06-1093	83-93	117-132	We use a state of the art Automatic Speech Recognition system	that even from these noisy transcriptions of calls we can automatically build a domain model .	We use a state of the art Automatic Speech Recognition system	that even from these noisy transcriptions of calls we can automatically build a domain model .	83-132	83-132	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	We use a state of the art Automatic Speech Recognition system to transcribe the calls between agents and customers , which still results in high word error rates ( 40 % ) and show that even from these noisy transcriptions of calls we can automatically build a domain model .	1<2	none	result	result
P06-1093	69-73	133-142	we propose an unsupervised technique	The domain model is comprised of primarily a topic taxonomy	we propose an unsupervised technique	The domain model is comprised of primarily a topic taxonomy	66-82	133-165	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic ( s ) , typical Questions-Answers ( Q&As ) , typical actions and call statistics .	1<2	none	elab-addition	elab-addition
P06-1093	133-142	143-165	The domain model is comprised of primarily a topic taxonomy	where every node is characterized by topic ( s ) , typical Questions-Answers ( Q&As ) , typical actions and call statistics .	The domain model is comprised of primarily a topic taxonomy	where every node is characterized by topic ( s ) , typical Questions-Answers ( Q&As ) , typical actions and call statistics .	133-165	133-165	The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic ( s ) , typical Questions-Answers ( Q&As ) , typical actions and call statistics .	The domain model is comprised of primarily a topic taxonomy where every node is characterized by topic ( s ) , typical Questions-Answers ( Q&As ) , typical actions and call statistics .	1<2	none	elab-addition	elab-addition
P06-1093	166-167	168-182	We show	how such a domain model can be used for topic identification of unseen calls .	We show	how such a domain model can be used for topic identification of unseen calls .	166-182	166-182	We show how such a domain model can be used for topic identification of unseen calls .	We show how such a domain model can be used for topic identification of unseen calls .	1>2	none	attribution	attribution
P06-1093	69-73	168-182	we propose an unsupervised technique	how such a domain model can be used for topic identification of unseen calls .	we propose an unsupervised technique	how such a domain model can be used for topic identification of unseen calls .	66-82	166-182	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	We show how such a domain model can be used for topic identification of unseen calls .	1<2	none	elab-addition	elab-addition
P06-1093	69-73	183-189	we propose an unsupervised technique	We also propose applications for aiding agents	we propose an unsupervised technique	We also propose applications for aiding agents	66-82	183-202	Towards this , we propose an unsupervised technique to generate domain models automatically from call transcriptions .	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	1<2	none	progression	progression
P06-1093	183-189	190-192	We also propose applications for aiding agents	while handling calls	We also propose applications for aiding agents	while handling calls	183-202	183-202	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	1<2	none	temporal	temporal
P06-1093	183-189	193-196	We also propose applications for aiding agents	and for agent monitoring	We also propose applications for aiding agents	and for agent monitoring	183-202	183-202	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	1<2	none	joint	joint
P06-1093	193-196	197-202	and for agent monitoring	based on the domain model .	and for agent monitoring	based on the domain model .	183-202	183-202	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	We also propose applications for aiding agents while handling calls and for agent monitoring based on the domain model .	1<2	none	bg-general	bg-general
P06-1094	1-22	23-29	The paper presents a new model for contextdependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene .	The paper discusses novel psycholinguistic experimental data	The paper presents a new model for contextdependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene .	The paper discusses novel psycholinguistic experimental data	1-22	23-36	The paper presents a new model for contextdependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene .	The paper discusses novel psycholinguistic experimental data that tests and verifies the model .	1<2	none	elab-addition	elab-addition
P06-1094	23-29	30-36	The paper discusses novel psycholinguistic experimental data	that tests and verifies the model .	The paper discusses novel psycholinguistic experimental data	that tests and verifies the model .	23-36	23-36	The paper discusses novel psycholinguistic experimental data that tests and verifies the model .	The paper discusses novel psycholinguistic experimental data that tests and verifies the model .	1<2	none	elab-addition	elab-addition
P06-1094	23-29	37-42	The paper discusses novel psycholinguistic experimental data	The model has been implemented ,	The paper discusses novel psycholinguistic experimental data	The model has been implemented ,	23-36	37-64	The paper discusses novel psycholinguistic experimental data that tests and verifies the model .	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	1<2	none	evaluation	evaluation
P06-1094	37-42	43-47	The model has been implemented ,	and enables a conversational robot	The model has been implemented ,	and enables a conversational robot	37-64	37-64	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	1<2	none	joint	joint
P06-1094	43-47	48-57	and enables a conversational robot	to identify objects in a scene through topological spatial relations	and enables a conversational robot	to identify objects in a scene through topological spatial relations	37-64	37-64	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	1<2	none	enablement	enablement
P06-1094	48-57	58-64	to identify objects in a scene through topological spatial relations	( e.g. "X near Y" ) .	to identify objects in a scene through topological spatial relations	( e.g. "X near Y" ) .	37-64	37-64	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	The model has been implemented , and enables a conversational robot to identify objects in a scene through topological spatial relations ( e.g. "X near Y" ) .	1<2	none	elab-example	elab-example
P06-1094	23-29	65-77	The paper discusses novel psycholinguistic experimental data	The model can help motivate the choice between topological and projective prepositions .	The paper discusses novel psycholinguistic experimental data	The model can help motivate the choice between topological and projective prepositions .	23-36	65-77	The paper discusses novel psycholinguistic experimental data that tests and verifies the model .	The model can help motivate the choice between topological and projective prepositions .	1<2	none	evaluation	evaluation
P06-1095	19-23	24-31	To address data sparseness ,	we used temporal reasoning as an oversampling method	To address data sparseness ,	we used temporal reasoning as an oversampling method	19-62	19-62	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	1>2	none	enablement	enablement
P06-1095	1-18	24-31	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	we used temporal reasoning as an oversampling method	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	we used temporal reasoning as an oversampling method	1-18	19-62	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	1<2	none	manner-means	manner-means
P06-1095	24-31	32-40	we used temporal reasoning as an oversampling method	to dramatically expand the amount of training data ,	we used temporal reasoning as an oversampling method	to dramatically expand the amount of training data ,	19-62	19-62	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	1<2	none	enablement	enablement
P06-1095	24-31	41-52	we used temporal reasoning as an oversampling method	resulting in predictive accuracy on link labeling as high as 93 %	we used temporal reasoning as an oversampling method	resulting in predictive accuracy on link labeling as high as 93 %	19-62	19-62	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	1<2	none	result	result
P06-1095	41-52	53-62	resulting in predictive accuracy on link labeling as high as 93 %	using a Maximum Entropy classifier on human annotated data .	resulting in predictive accuracy on link labeling as high as 93 %	using a Maximum Entropy classifier on human annotated data .	19-62	19-62	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	To address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a Maximum Entropy classifier on human annotated data .	1<2	none	manner-means	manner-means
P06-1095	1-18	63-73	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	This method compared favorably against a series of increasingly sophisticated baselines	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	This method compared favorably against a series of increasingly sophisticated baselines	1-18	63-82	This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts .	This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .	1<2	none	evaluation	evaluation
P06-1095	63-73	74-77	This method compared favorably against a series of increasingly sophisticated baselines	involving expansion of rules	This method compared favorably against a series of increasingly sophisticated baselines	involving expansion of rules	63-82	63-82	This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .	This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .	1<2	none	elab-addition	elab-addition
P06-1095	74-77	78-82	involving expansion of rules	derived from human intuitions .	involving expansion of rules	derived from human intuitions .	63-82	63-82	This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .	This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .	1<2	none	elab-addition	elab-addition
P06-1096	1-9	10-18	We present a perceptron-style discriminative approach to machine translation	in which large feature sets can be exploited .	We present a perceptron-style discriminative approach to machine translation	in which large feature sets can be exploited .	1-18	1-18	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited .	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited .	1<2	none	elab-addition	elab-addition
P06-1096	19-23	24-37	Unlike discriminative reranking approaches ,	our system can take advantage of learned features in all stages of decoding .	Unlike discriminative reranking approaches ,	our system can take advantage of learned features in all stages of decoding .	19-37	19-37	Unlike discriminative reranking approaches , our system can take advantage of learned features in all stages of decoding .	Unlike discriminative reranking approaches , our system can take advantage of learned features in all stages of decoding .	1>2	none	contrast	contrast
P06-1096	1-9	24-37	We present a perceptron-style discriminative approach to machine translation	our system can take advantage of learned features in all stages of decoding .	We present a perceptron-style discriminative approach to machine translation	our system can take advantage of learned features in all stages of decoding .	1-18	19-37	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited .	Unlike discriminative reranking approaches , our system can take advantage of learned features in all stages of decoding .	1<2	none	elab-addition	elab-addition
P06-1096	1-9	38-47	We present a perceptron-style discriminative approach to machine translation	We first discuss several challenges to error-driven discriminative approaches .	We present a perceptron-style discriminative approach to machine translation	We first discuss several challenges to error-driven discriminative approaches .	1-18	38-47	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited .	We first discuss several challenges to error-driven discriminative approaches .	1<2	none	elab-process_step	elab-process_step
P06-1096	38-47	48-57	We first discuss several challenges to error-driven discriminative approaches .	In particular , we explore different ways of updating parameters	We first discuss several challenges to error-driven discriminative approaches .	In particular , we explore different ways of updating parameters	38-47	48-62	We first discuss several challenges to error-driven discriminative approaches .	In particular , we explore different ways of updating parameters given a training example .	1<2	none	elab-addition	elab-addition
P06-1096	48-57	58-62	In particular , we explore different ways of updating parameters	given a training example .	In particular , we explore different ways of updating parameters	given a training example .	48-62	48-62	In particular , we explore different ways of updating parameters given a training example .	In particular , we explore different ways of updating parameters given a training example .	1<2	none	condition	condition
P06-1096	63-64	65-79	We find	that making frequent but smaller updates is preferable to making fewer but larger updates .	We find	that making frequent but smaller updates is preferable to making fewer but larger updates .	63-79	63-79	We find that making frequent but smaller updates is preferable to making fewer but larger updates .	We find that making frequent but smaller updates is preferable to making fewer but larger updates .	1>2	none	attribution	attribution
P06-1096	48-57	65-79	In particular , we explore different ways of updating parameters	that making frequent but smaller updates is preferable to making fewer but larger updates .	In particular , we explore different ways of updating parameters	that making frequent but smaller updates is preferable to making fewer but larger updates .	48-62	63-79	In particular , we explore different ways of updating parameters given a training example .	We find that making frequent but smaller updates is preferable to making fewer but larger updates .	1<2	none	result	result
P06-1096	1-9	80-87	We present a perceptron-style discriminative approach to machine translation	Then , we discuss an array of features	We present a perceptron-style discriminative approach to machine translation	Then , we discuss an array of features	1-18	80-105	We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited .	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	1<2	none	elab-process_step	elab-process_step
P06-1096	88-90	91-96	and show both	how they quantitatively increase BLEU score	and show both	how they quantitatively increase BLEU score	80-105	80-105	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	1>2	none	attribution	attribution
P06-1096	80-87	91-96	Then , we discuss an array of features	how they quantitatively increase BLEU score	Then , we discuss an array of features	how they quantitatively increase BLEU score	80-105	80-105	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	1<2	none	result	result
P06-1096	91-96	97-105	how they quantitatively increase BLEU score	and how they qualitatively interact on specific examples .	how they quantitatively increase BLEU score	and how they qualitatively interact on specific examples .	80-105	80-105	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	1<2	none	joint	joint
P06-1096	80-87	106-108,111-114	Then , we discuss an array of features	One particular feature <*> is a novel way	Then , we discuss an array of features	One particular feature <*> is a novel way	80-105	106-131	Then , we discuss an array of features and show both how they quantitatively increase BLEU score and how they qualitatively interact on specific examples .	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	1<2	none	elab-addition	elab-addition
P06-1096	106-108,111-114	109-110	One particular feature <*> is a novel way	we investigate	One particular feature <*> is a novel way	we investigate	106-131	106-131	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	1<2	none	elab-addition	elab-addition
P06-1096	111-114	115-124	is a novel way	to introduce learning into the initial phrase extraction process ,	is a novel way	to introduce learning into the initial phrase extraction process ,	106-131	106-131	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	1<2	none	enablement	enablement
P06-1096	115-124	125-131	to introduce learning into the initial phrase extraction process ,	which has previously been entirely heuristic .	to introduce learning into the initial phrase extraction process ,	which has previously been entirely heuristic .	106-131	106-131	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	One particular feature we investigate is a novel way to introduce learning into the initial phrase extraction process , which has previously been entirely heuristic .	1<2	none	elab-addition	elab-addition
P06-1097	1-5	6-11	We introduce a semi-supervised approach	to training for statistical machine translation	We introduce a semi-supervised approach	to training for statistical machine translation	1-43	1-43	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	1<2	none	enablement	enablement
P06-1097	6-11	12-18	to training for statistical machine translation	that alternates the traditional Expectation Maximization step	to training for statistical machine translation	that alternates the traditional Expectation Maximization step	1-43	1-43	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	1<2	none	elab-addition	elab-addition
P06-1097	12-18	19-30	that alternates the traditional Expectation Maximization step	that is applied on a large training corpus with a discriminative step	that alternates the traditional Expectation Maximization step	that is applied on a large training corpus with a discriminative step	1-43	1-43	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	1<2	none	elab-addition	elab-addition
P06-1097	19-30	31-43	that is applied on a large training corpus with a discriminative step	aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	that is applied on a large training corpus with a discriminative step	aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	1-43	1-43	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	1<2	none	elab-addition	elab-addition
P06-1097	44-45	46-64	We show	that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	We show	that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	44-64	44-64	We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	1>2	none	attribution	attribution
P06-1097	1-5	46-64	We introduce a semi-supervised approach	that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	We introduce a semi-supervised approach	that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	1-43	44-64	We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small , manually word-aligned sub-corpus .	We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .	1<2	none	evaluation	evaluation
P06-1098	1-8	9-20	We present a hierarchical phrase-based statistical machine translation	in which a target sentence is efficiently generated in left-to-right order .	We present a hierarchical phrase-based statistical machine translation	in which a target sentence is efficiently generated in left-to-right order .	1-20	1-20	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	1<2	none	elab-addition	elab-addition
P06-1098	1-8	21-39	We present a hierarchical phrase-based statistical machine translation	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule :	We present a hierarchical phrase-based statistical machine translation	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule :	1-20	21-52	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule : The paired target-side of a production rule takes a phrase prefixed form .	1<2	none	elab-aspect	elab-aspect
P06-1098	21-39	40-52	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule :	The paired target-side of a production rule takes a phrase prefixed form .	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule :	The paired target-side of a production rule takes a phrase prefixed form .	21-52	21-52	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule : The paired target-side of a production rule takes a phrase prefixed form .	The model is a class of synchronous-CFG with a Greibach Normal Form-like structure for the projected production rule : The paired target-side of a production rule takes a phrase prefixed form .	1<2	none	elab-addition	elab-addition
P06-1098	1-8	53-71	We present a hierarchical phrase-based statistical machine translation	The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side .	We present a hierarchical phrase-based statistical machine translation	The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side .	1-20	53-71	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	The decoder for the targetnormalized form is based on an Earlystyle top down parser on the source side .	1<2	none	elab-aspect	elab-aspect
P06-1098	1-8	72-74,81-86	We present a hierarchical phrase-based statistical machine translation	The target-normalized form <*> implies a left-toright generation of translations	We present a hierarchical phrase-based statistical machine translation	The target-normalized form <*> implies a left-toright generation of translations	1-20	72-97	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models .	1<2	none	elab-aspect	elab-aspect
P06-1098	72-74,81-86	75-80	The target-normalized form <*> implies a left-toright generation of translations	coupled with our top down parser	The target-normalized form <*> implies a left-toright generation of translations	coupled with our top down parser	72-97	72-97	The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models .	The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models .	1<2	none	elab-addition	elab-addition
P06-1098	81-86	87-97	implies a left-toright generation of translations	which enables us a straightforward integration with ngram language models .	implies a left-toright generation of translations	which enables us a straightforward integration with ngram language models .	72-97	72-97	The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models .	The target-normalized form coupled with our top down parser implies a left-toright generation of translations which enables us a straightforward integration with ngram language models .	1<2	none	elab-addition	elab-addition
P06-1098	98-108	109-120	Our model was experimented on a Japanese-to-English newswire translation task ,	and showed statistically significant performance improvements against a phrase-based translation system .	Our model was experimented on a Japanese-to-English newswire translation task ,	and showed statistically significant performance improvements against a phrase-based translation system .	98-120	98-120	Our model was experimented on a Japanese-to-English newswire translation task , and showed statistically significant performance improvements against a phrase-based translation system .	Our model was experimented on a Japanese-to-English newswire translation task , and showed statistically significant performance improvements against a phrase-based translation system .	1>2	none	result	result
P06-1098	1-8	109-120	We present a hierarchical phrase-based statistical machine translation	and showed statistically significant performance improvements against a phrase-based translation system .	We present a hierarchical phrase-based statistical machine translation	and showed statistically significant performance improvements against a phrase-based translation system .	1-20	98-120	We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order .	Our model was experimented on a Japanese-to-English newswire translation task , and showed statistically significant performance improvements against a phrase-based translation system .	1<2	none	evaluation	evaluation
P06-1099	1-14	50-56	In the past years , a number of lexical association measures have been studied	We here explicitly test this assumption .	In the past years , a number of lexical association measures have been studied	We here explicitly test this assumption .	1-24	50-56	In the past years , a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations .	We here explicitly test this assumption .	1>2	none	elab-addition	elab-addition
P06-1099	1-14	15-24	In the past years , a number of lexical association measures have been studied	to help extract new scientific terminology or general-language collocations .	In the past years , a number of lexical association measures have been studied	to help extract new scientific terminology or general-language collocations .	1-24	1-24	In the past years , a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations .	In the past years , a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations .	1<2	none	enablement	enablement
P06-1099	1-14	25-36,42-49	In the past years , a number of lexical association measures have been studied	The implicit assumption of this research was that newly designed term measures <*> would outperform simple counts of cooccurrence frequencies .	In the past years , a number of lexical association measures have been studied	The implicit assumption of this research was that newly designed term measures <*> would outperform simple counts of cooccurrence frequencies .	1-24	25-49	In the past years , a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations .	The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies .	1<2	none	elab-addition	elab-addition
P06-1099	25-36,42-49	37-41	The implicit assumption of this research was that newly designed term measures <*> would outperform simple counts of cooccurrence frequencies .	involving more sophisticated statistical criteria	The implicit assumption of this research was that newly designed term measures <*> would outperform simple counts of cooccurrence frequencies .	involving more sophisticated statistical criteria	25-49	25-49	The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies .	The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies .	1<2	none	elab-addition	elab-addition
P06-1099	57-63	66-73	By way of four qualitative criteria ,	that purely statistics-based measures reveal virtually no difference	By way of four qualitative criteria ,	that purely statistics-based measures reveal virtually no difference	57-92	57-92	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	1>2	none	manner-means	manner-means
P06-1099	64-65	66-73	we show	that purely statistics-based measures reveal virtually no difference	we show	that purely statistics-based measures reveal virtually no difference	57-92	57-92	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	1>2	none	attribution	attribution
P06-1099	50-56	66-73	We here explicitly test this assumption .	that purely statistics-based measures reveal virtually no difference	We here explicitly test this assumption .	that purely statistics-based measures reveal virtually no difference	50-56	57-92	We here explicitly test this assumption .	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	1<2	none	evaluation	evaluation
P06-1099	66-73	74-80	that purely statistics-based measures reveal virtually no difference	compared with frequency of occurrence counts ,	that purely statistics-based measures reveal virtually no difference	compared with frequency of occurrence counts ,	57-92	57-92	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	1<2	none	comparison	comparison
P06-1099	66-73	81-92	that purely statistics-based measures reveal virtually no difference	while linguistically more informed metrics do reveal such a marked difference .	that purely statistics-based measures reveal virtually no difference	while linguistically more informed metrics do reveal such a marked difference .	57-92	57-92	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts , while linguistically more informed metrics do reveal such a marked difference .	1<2	none	temporal	temporal
P06-1100	1-5	24-31	Many algorithms have been developed	In this paper , we propose two algorithms	Many algorithms have been developed	In this paper , we propose two algorithms	1-23	24-42	Many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories .	In this paper , we propose two algorithms for automatically ontologizing ( attaching ) semantic relations into WordNet .	1>2	none	elab-addition	elab-addition
P06-1100	1-5	6-11	Many algorithms have been developed	to harvest lexical semantic resources ,	Many algorithms have been developed	to harvest lexical semantic resources ,	1-23	1-23	Many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories .	Many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories .	1<2	none	enablement	enablement
P06-1100	1-5	12-23	Many algorithms have been developed	however few have linked the mined knowledge into formal knowledge repositories .	Many algorithms have been developed	however few have linked the mined knowledge into formal knowledge repositories .	1-23	1-23	Many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories .	Many algorithms have been developed to harvest lexical semantic resources , however few have linked the mined knowledge into formal knowledge repositories .	1<2	none	contrast	contrast
P06-1100	24-31	32-42	In this paper , we propose two algorithms	for automatically ontologizing ( attaching ) semantic relations into WordNet .	In this paper , we propose two algorithms	for automatically ontologizing ( attaching ) semantic relations into WordNet .	24-42	24-42	In this paper , we propose two algorithms for automatically ontologizing ( attaching ) semantic relations into WordNet .	In this paper , we propose two algorithms for automatically ontologizing ( attaching ) semantic relations into WordNet .	1<2	none	enablement	enablement
P06-1100	43-50	58-67	We present an empirical evaluation on the task	showing an improvement on F-score over a baseline model .	We present an empirical evaluation on the task	showing an improvement on F-score over a baseline model .	43-67	43-67	We present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on F-score over a baseline model .	We present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on F-score over a baseline model .	1>2	none	result	result
P06-1100	43-50	51-57	We present an empirical evaluation on the task	of attaching partof and causation relations ,	We present an empirical evaluation on the task	of attaching partof and causation relations ,	43-67	43-67	We present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on F-score over a baseline model .	We present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on F-score over a baseline model .	1<2	none	elab-addition	elab-addition
P06-1100	24-31	58-67	In this paper , we propose two algorithms	showing an improvement on F-score over a baseline model .	In this paper , we propose two algorithms	showing an improvement on F-score over a baseline model .	24-42	43-67	In this paper , we propose two algorithms for automatically ontologizing ( attaching ) semantic relations into WordNet .	We present an empirical evaluation on the task of attaching partof and causation relations , showing an improvement on F-score over a baseline model .	1<2	none	evaluation	evaluation
P06-1101	1-5	6-10	We propose a novel algorithm	for inducing semantic taxonomies .	We propose a novel algorithm	for inducing semantic taxonomies .	1-10	1-10	We propose a novel algorithm for inducing semantic taxonomies .	We propose a novel algorithm for inducing semantic taxonomies .	1<2	none	enablement	enablement
P06-1101	1-5	11-21	We propose a novel algorithm	Previous algorithms for taxonomy induction have typically focused on independent classifiers	We propose a novel algorithm	Previous algorithms for taxonomy induction have typically focused on independent classifiers	1-10	11-35	We propose a novel algorithm for inducing semantic taxonomies .	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	1<2	none	bg-compare	bg-compare
P06-1101	11-21	22-26	Previous algorithms for taxonomy induction have typically focused on independent classifiers	for discovering new single relationships	Previous algorithms for taxonomy induction have typically focused on independent classifiers	for discovering new single relationships	11-35	11-35	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	1<2	none	enablement	enablement
P06-1101	22-26	27-35	for discovering new single relationships	based on hand-constructed or automatically discovered textual patterns .	for discovering new single relationships	based on hand-constructed or automatically discovered textual patterns .	11-35	11-35	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	1<2	none	bg-general	bg-general
P06-1101	11-21	36-49	Previous algorithms for taxonomy induction have typically focused on independent classifiers	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships	Previous algorithms for taxonomy induction have typically focused on independent classifiers	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships	11-35	36-76	Previous algorithms for taxonomy induction have typically focused on independent classifiers for discovering new single relationships based on hand-constructed or automatically discovered textual patterns .	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	1<2	none	contrast	contrast
P06-1101	36-49	50-58	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships	to optimize the entire structure of the taxonomy ,	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships	to optimize the entire structure of the taxonomy ,	36-76	36-76	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	1<2	none	enablement	enablement
P06-1101	50-58	59-65	to optimize the entire structure of the taxonomy ,	using knowledge of a word's coordinate terms	to optimize the entire structure of the taxonomy ,	using knowledge of a word's coordinate terms	36-76	36-76	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	1<2	none	manner-means	manner-means
P06-1101	59-65	66-76	using knowledge of a word's coordinate terms	to help in determining its hypernyms , and vice versa .	using knowledge of a word's coordinate terms	to help in determining its hypernyms , and vice versa .	36-76	36-76	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	By contrast , our algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word's coordinate terms to help in determining its hypernyms , and vice versa .	1<2	none	enablement	enablement
P06-1101	1-5	77-89	We propose a novel algorithm	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition ,	We propose a novel algorithm	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition ,	1-10	77-113	We propose a novel algorithm for inducing semantic taxonomies .	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition , where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .	1<2	none	elab-addition	elab-addition
P06-1101	77-89	90-113	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition ,	where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition ,	where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .	77-113	77-113	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition , where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .	We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition , where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .	1<2	none	elab-addition	elab-addition
P06-1101	114-137	148-150,155-172	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	114-143	144-172	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm using the same component classifiers .	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	1>2	none	manner-means	manner-means
P06-1101	114-137	138-143	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm	using the same component classifiers .	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm	using the same component classifiers .	114-143	114-143	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm using the same component classifiers .	We add 10,000 novel synsets to WordNet 2.1 at 84 % precision , a relative error reduction of 70 % over a non-joint algorithm using the same component classifiers .	1<2	none	manner-means	manner-means
P06-1101	144-147	148-150,155-172	Finally , we show	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	Finally , we show	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	144-172	144-172	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	1>2	none	attribution	attribution
P06-1101	1-5	148-150,155-172	We propose a novel algorithm	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	We propose a novel algorithm	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	1-10	144-172	We propose a novel algorithm for inducing semantic taxonomies .	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	1<2	none	evaluation	evaluation
P06-1101	148-150,155-172	151-154	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	built using our algorithm	that a taxonomy <*> shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	built using our algorithm	144-172	144-172	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	Finally , we show that a taxonomy built using our algorithm shows a 23 % relative F-score improvement over WordNet 2.1 on an independent testset of hypernym pairs .	1<2	none	elab-addition	elab-addition
P06-1102	1-39	40-48	In a new approach to large-scale extraction of facts from unstructured text , distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns , and the validation and ranking of candidate facts .	The evaluation measures the quality and coverage of facts	In a new approach to large-scale extraction of facts from unstructured text , distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns , and the validation and ranking of candidate facts .	The evaluation measures the quality and coverage of facts	1-39	40-72	In a new approach to large-scale extraction of facts from unstructured text , distributional similarities become an integral part of both the iterative acquisition of high-coverage contextual extraction patterns , and the validation and ranking of candidate facts .	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	1<2	none	evaluation	evaluation
P06-1102	40-48	49-56	The evaluation measures the quality and coverage of facts	extracted from one hundred million Web documents ,	The evaluation measures the quality and coverage of facts	extracted from one hundred million Web documents ,	40-72	40-72	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	1<2	none	elab-addition	elab-addition
P06-1102	40-48	57-61	The evaluation measures the quality and coverage of facts	starting from ten seed facts	The evaluation measures the quality and coverage of facts	starting from ten seed facts	40-72	40-72	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	1<2	none	elab-addition	elab-addition
P06-1102	57-61	62-72	starting from ten seed facts	and using no additional knowledge , lexicons or complex tools .	starting from ten seed facts	and using no additional knowledge , lexicons or complex tools .	40-72	40-72	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	The evaluation measures the quality and coverage of facts extracted from one hundred million Web documents , starting from ten seed facts and using no additional knowledge , lexicons or complex tools .	1<2	none	joint	joint
P06-1103	1-17	38-62	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	1-17	38-79	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	1>2	none	bg-compare	bg-compare
P06-1103	1-17	18-24	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	Current approaches often employ machine learning techniques	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	Current approaches often employ machine learning techniques	1-17	18-29	Named Entity recognition ( NER ) is an important part of many natural language processing tasks .	Current approaches often employ machine learning techniques and require supervised data .	1<2	none	elab-addition	elab-addition
P06-1103	18-24	25-29	Current approaches often employ machine learning techniques	and require supervised data .	Current approaches often employ machine learning techniques	and require supervised data .	18-29	18-29	Current approaches often employ machine learning techniques and require supervised data .	Current approaches often employ machine learning techniques and require supervised data .	1<2	none	joint	joint
P06-1103	25-29	30-37	and require supervised data .	However , many languages lack such resources .	and require supervised data .	However , many languages lack such resources .	18-29	30-37	Current approaches often employ machine learning techniques and require supervised data .	However , many languages lack such resources .	1<2	none	contrast	contrast
P06-1103	38-62	63-66	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	given a bilingual corpora	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	given a bilingual corpora	38-79	38-79	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	1<2	none	condition	condition
P06-1103	63-66	67-79	given a bilingual corpora	in which it is weakly temporally aligned with a resource rich language .	given a bilingual corpora	in which it is weakly temporally aligned with a resource rich language .	38-79	38-79	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	1<2	none	elab-addition	elab-addition
P06-1103	38-62	80-88	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	NEs have similar time distributions across such corpora ,	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	NEs have similar time distributions across such corpora ,	38-79	80-101	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	NEs have similar time distributions across such corpora , and often some of the tokens in a multi-word NE are transliterated .	1<2	none	elab-aspect	elab-aspect
P06-1103	80-88	89-101	NEs have similar time distributions across such corpora ,	and often some of the tokens in a multi-word NE are transliterated .	NEs have similar time distributions across such corpora ,	and often some of the tokens in a multi-word NE are transliterated .	80-101	80-101	NEs have similar time distributions across such corpora , and often some of the tokens in a multi-word NE are transliterated .	NEs have similar time distributions across such corpora , and often some of the tokens in a multi-word NE are transliterated .	1<2	none	joint	joint
P06-1103	38-62	102-105	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	We develop an algorithm	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	We develop an algorithm	38-79	102-111	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	We develop an algorithm that exploits both observations iteratively .	1<2	none	manner-means	manner-means
P06-1103	102-105	106-111	We develop an algorithm	that exploits both observations iteratively .	We develop an algorithm	that exploits both observations iteratively .	102-111	102-111	We develop an algorithm that exploits both observations iteratively .	We develop an algorithm that exploits both observations iteratively .	1<2	none	elab-addition	elab-addition
P06-1103	102-105	112-135	We develop an algorithm	The algorithm makes use of a new , frequency based , metric for time distributions and a resource free discriminative approach to transliteration .	We develop an algorithm	The algorithm makes use of a new , frequency based , metric for time distributions and a resource free discriminative approach to transliteration .	102-111	112-135	We develop an algorithm that exploits both observations iteratively .	The algorithm makes use of a new , frequency based , metric for time distributions and a resource free discriminative approach to transliteration .	1<2	none	elab-addition	elab-addition
P06-1103	136-144	145-150	Seeded with a small number of transliteration pairs ,	our algorithm discovers multi-word NEs ,	Seeded with a small number of transliteration pairs ,	our algorithm discovers multi-word NEs ,	136-170	136-170	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	1>2	none	elab-addition	elab-addition
P06-1103	102-105	145-150	We develop an algorithm	our algorithm discovers multi-word NEs ,	We develop an algorithm	our algorithm discovers multi-word NEs ,	102-111	136-170	We develop an algorithm that exploits both observations iteratively .	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	1<2	none	elab-addition	elab-addition
P06-1103	145-150	151-156	our algorithm discovers multi-word NEs ,	and takes advantage of a dictionary	our algorithm discovers multi-word NEs ,	and takes advantage of a dictionary	136-170	136-170	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	1<2	none	joint	joint
P06-1103	151-156	157-161	and takes advantage of a dictionary	( if one exists )	and takes advantage of a dictionary	( if one exists )	136-170	136-170	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	1<2	none	elab-addition	elab-addition
P06-1103	151-156	162-170	and takes advantage of a dictionary	to account for translated or partially translated NEs .	and takes advantage of a dictionary	to account for translated or partially translated NEs .	136-170	136-170	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	Seeded with a small number of transliteration pairs , our algorithm discovers multi-word NEs , and takes advantage of a dictionary ( if one exists ) to account for translated or partially translated NEs .	1<2	none	enablement	enablement
P06-1103	171-179	180-189	We evaluate the algorithm on an English-Russian corpus ,	and show high level of NEs discovery in Russian .	We evaluate the algorithm on an English-Russian corpus ,	and show high level of NEs discovery in Russian .	171-189	171-189	We evaluate the algorithm on an English-Russian corpus , and show high level of NEs discovery in Russian .	We evaluate the algorithm on an English-Russian corpus , and show high level of NEs discovery in Russian .	1>2	none	result	result
P06-1103	38-62	180-189	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	and show high level of NEs discovery in Russian .	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language ,	and show high level of NEs discovery in Russian .	38-79	171-189	This paper presents an ( almost ) unsupervised learning algorithm for automatic discovery of Named Entities ( NEs ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language .	We evaluate the algorithm on an English-Russian corpus , and show high level of NEs discovery in Russian .	1<2	none	evaluation	evaluation
P06-1104	1-11	12-20	This paper proposes a novel composite kernel for relation extraction .	The composite kernel consists of two individual kernels :	This paper proposes a novel composite kernel for relation extraction .	The composite kernel consists of two individual kernels :	1-11	12-42	This paper proposes a novel composite kernel for relation extraction .	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	1<2	none	elab-addition	elab-addition
P06-1104	12-20	21-23	The composite kernel consists of two individual kernels :	an entity kernel	The composite kernel consists of two individual kernels :	an entity kernel	12-42	12-42	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	1<2	none	elab-enumember	elab-enumember
P06-1104	21-23	24-28	an entity kernel	that allows for entity-related features	an entity kernel	that allows for entity-related features	12-42	12-42	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	1<2	none	elab-addition	elab-addition
P06-1104	21-23	29-34	an entity kernel	and a convolution parse tree kernel	an entity kernel	and a convolution parse tree kernel	12-42	12-42	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	1<2	none	joint	joint
P06-1104	29-34	35-42	and a convolution parse tree kernel	that models syntactic information of relation examples .	and a convolution parse tree kernel	that models syntactic information of relation examples .	12-42	12-42	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	The composite kernel consists of two individual kernels : an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples .	1<2	none	elab-addition	elab-addition
P06-1104	1-11	43-57	This paper proposes a novel composite kernel for relation extraction .	The motivation of our method is to fully utilize the nice properties of kernel methods	This paper proposes a novel composite kernel for relation extraction .	The motivation of our method is to fully utilize the nice properties of kernel methods	1-11	43-65	This paper proposes a novel composite kernel for relation extraction .	The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction .	1<2	none	elab-addition	elab-addition
P06-1104	43-57	58-65	The motivation of our method is to fully utilize the nice properties of kernel methods	to explore diverse knowledge for relation extraction .	The motivation of our method is to fully utilize the nice properties of kernel methods	to explore diverse knowledge for relation extraction .	43-65	43-65	The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction .	The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction .	1<2	none	enablement	enablement
P06-1104	66-68	69-80	Our study illustrates	that the composite kernel can effectively capture both flat and structured features	Our study illustrates	that the composite kernel can effectively capture both flat and structured features	66-98	66-98	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	1>2	none	attribution	attribution
P06-1104	43-57	69-80	The motivation of our method is to fully utilize the nice properties of kernel methods	that the composite kernel can effectively capture both flat and structured features	The motivation of our method is to fully utilize the nice properties of kernel methods	that the composite kernel can effectively capture both flat and structured features	43-65	66-98	The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction .	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	1<2	none	result	result
P06-1104	69-80	81-88	that the composite kernel can effectively capture both flat and structured features	without the need for extensive feature engineering ,	that the composite kernel can effectively capture both flat and structured features	without the need for extensive feature engineering ,	66-98	66-98	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	1<2	none	elab-addition	elab-addition
P06-1104	69-80	89-98	that the composite kernel can effectively capture both flat and structured features	and can also easily scale to include more features .	that the composite kernel can effectively capture both flat and structured features	and can also easily scale to include more features .	66-98	66-98	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering , and can also easily scale to include more features .	1<2	none	joint	joint
P06-1104	99-104	105-112	Evaluation on the ACE corpus shows	that our method outperforms the previous best-reported methods	Evaluation on the ACE corpus shows	that our method outperforms the previous best-reported methods	99-124	99-124	Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .	Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .	1>2	none	attribution	attribution
P06-1104	1-11	105-112	This paper proposes a novel composite kernel for relation extraction .	that our method outperforms the previous best-reported methods	This paper proposes a novel composite kernel for relation extraction .	that our method outperforms the previous best-reported methods	1-11	99-124	This paper proposes a novel composite kernel for relation extraction .	Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .	1<2	none	evaluation	evaluation
P06-1104	105-112	113-124	that our method outperforms the previous best-reported methods	and significantly outperforms previous two dependency tree kernels for relation extraction .	that our method outperforms the previous best-reported methods	and significantly outperforms previous two dependency tree kernels for relation extraction .	99-124	99-124	Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .	Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction .	1<2	none	joint	joint
P06-1105	1-8	9-13	In this paper , we present a method	that improves Japanese dependency parsing	In this paper , we present a method	that improves Japanese dependency parsing	1-19	1-19	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	1<2	none	elab-addition	elab-addition
P06-1105	9-13	14-19	that improves Japanese dependency parsing	by using large-scale statistical information .	that improves Japanese dependency parsing	by using large-scale statistical information .	1-19	1-19	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	1<2	none	manner-means	manner-means
P06-1105	1-8	20-27	In this paper , we present a method	It takes into account two kinds of information	In this paper , we present a method	It takes into account two kinds of information	1-19	20-65	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	1<2	none	elab-addition	elab-addition
P06-1105	20-27	28-40	It takes into account two kinds of information	not considered in previous statistical ( machine learning based ) parsing methods :	It takes into account two kinds of information	not considered in previous statistical ( machine learning based ) parsing methods :	20-65	20-65	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	1<2	none	elab-addition	elab-addition
P06-1105	28-40	41-65	not considered in previous statistical ( machine learning based ) parsing methods :	information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	not considered in previous statistical ( machine learning based ) parsing methods :	information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	20-65	20-65	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	1<2	none	elab-enumember	elab-enumember
P06-1105	20-27	66-81	It takes into account two kinds of information	This information can be collected from the results of automatic dependency parsing of large-scale corpora .	It takes into account two kinds of information	This information can be collected from the results of automatic dependency parsing of large-scale corpora .	20-65	66-81	It takes into account two kinds of information not considered in previous statistical ( machine learning based ) parsing methods : information about dependency relations among the case elements of a verb , and information about co-occurrence relations between a verb and its case element .	This information can be collected from the results of automatic dependency parsing of large-scale corpora .	1<2	none	elab-addition	elab-addition
P06-1105	82-86,106	107-116	The results of an experiment <*> showed	that our method can improve the accuracy of the results	The results of an experiment <*> showed	that our method can improve the accuracy of the results	82-122	82-122	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1>2	none	attribution	attribution
P06-1105	82-86,106	87-92	The results of an experiment <*> showed	in which our method was used	The results of an experiment <*> showed	in which our method was used	82-122	82-122	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1<2	none	elab-addition	elab-addition
P06-1105	87-92	93-96	in which our method was used	to rerank the results	in which our method was used	to rerank the results	82-122	82-122	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1<2	none	enablement	enablement
P06-1105	93-96	97-105	to rerank the results	obtained using an existing machine learning based parsing method	to rerank the results	obtained using an existing machine learning based parsing method	82-122	82-122	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1<2	none	elab-addition	elab-addition
P06-1105	1-8	107-116	In this paper , we present a method	that our method can improve the accuracy of the results	In this paper , we present a method	that our method can improve the accuracy of the results	1-19	82-122	In this paper , we present a method that improves Japanese dependency parsing by using large-scale statistical information .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1<2	none	evaluation	evaluation
P06-1105	107-116	117-122	that our method can improve the accuracy of the results	obtained using the existing method .	that our method can improve the accuracy of the results	obtained using the existing method .	82-122	82-122	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	The results of an experiment in which our method was used to rerank the results obtained using an existing machine learning based parsing method showed that our method can improve the accuracy of the results obtained using the existing method .	1<2	none	elab-addition	elab-addition
P06-1106	1-13	14-21	This paper presents a hybrid approach to question answering in the clinical domain	that combines techniques from summarization and information retrieval.	This paper presents a hybrid approach to question answering in the clinical domain	that combines techniques from summarization and information retrieval.	1-21	1-21	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	1<2	none	elab-addition	elab-addition
P06-1106	1-13	22-28	This paper presents a hybrid approach to question answering in the clinical domain	We tackle a frequently-occurring class of questions	This paper presents a hybrid approach to question answering in the clinical domain	We tackle a frequently-occurring class of questions	1-21	22-42	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	We tackle a frequently-occurring class of questions that takes the form "What is the best drug treatment for X ? "	1<2	none	elab-addition	elab-addition
P06-1106	22-28	29-42	We tackle a frequently-occurring class of questions	that takes the form "What is the best drug treatment for X ? "	We tackle a frequently-occurring class of questions	that takes the form "What is the best drug treatment for X ? "	22-42	22-42	We tackle a frequently-occurring class of questions that takes the form "What is the best drug treatment for X ? "	We tackle a frequently-occurring class of questions that takes the form "What is the best drug treatment for X ? "	1<2	none	elab-addition	elab-addition
P06-1106	43-51	52-60	Starting from an initial set of MEDLINE citations ,	our system first identifies the drugs under study .	Starting from an initial set of MEDLINE citations ,	our system first identifies the drugs under study .	43-60	43-60	Starting from an initial set of MEDLINE citations , our system first identifies the drugs under study .	Starting from an initial set of MEDLINE citations , our system first identifies the drugs under study .	1>2	none	elab-addition	elab-addition
P06-1106	1-13	52-60	This paper presents a hybrid approach to question answering in the clinical domain	our system first identifies the drugs under study .	This paper presents a hybrid approach to question answering in the clinical domain	our system first identifies the drugs under study .	1-21	43-60	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	Starting from an initial set of MEDLINE citations , our system first identifies the drugs under study .	1<2	none	elab-process_step	elab-process_step
P06-1106	1-13	61-64	This paper presents a hybrid approach to question answering in the clinical domain	Abstracts are then clustered	This paper presents a hybrid approach to question answering in the clinical domain	Abstracts are then clustered	1-21	61-72	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	Abstracts are then clustered using semantic classes from the UMLS ontology .	1<2	none	elab-process_step	elab-process_step
P06-1106	61-64	65-72	Abstracts are then clustered	using semantic classes from the UMLS ontology .	Abstracts are then clustered	using semantic classes from the UMLS ontology .	61-72	61-72	Abstracts are then clustered using semantic classes from the UMLS ontology .	Abstracts are then clustered using semantic classes from the UMLS ontology .	1<2	none	manner-means	manner-means
P06-1106	1-13	73-83	This paper presents a hybrid approach to question answering in the clinical domain	Finally , a short extractive summary is generated for each abstract	This paper presents a hybrid approach to question answering in the clinical domain	Finally , a short extractive summary is generated for each abstract	1-21	73-88	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	Finally , a short extractive summary is generated for each abstract to populate the clusters .	1<2	none	elab-process_step	elab-process_step
P06-1106	73-83	84-88	Finally , a short extractive summary is generated for each abstract	to populate the clusters .	Finally , a short extractive summary is generated for each abstract	to populate the clusters .	73-88	73-88	Finally , a short extractive summary is generated for each abstract to populate the clusters .	Finally , a short extractive summary is generated for each abstract to populate the clusters .	1<2	none	enablement	enablement
P06-1106	89-90,107	108-125	Two evaluations <*> demonstrate	that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations <*> demonstrate	that our system compares favorably to PubMed , the search system most widely used by physicians today .	89-125	89-125	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1>2	none	attribution	attribution
P06-1106	89-90,107	91-93	Two evaluations <*> demonstrate	—a manual one	Two evaluations <*> demonstrate	—a manual one	89-125	89-125	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1<2	none	elab-enumember	elab-enumember
P06-1106	91-93	94-97	—a manual one	focused on short answers	—a manual one	focused on short answers	89-125	89-125	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1<2	none	elab-addition	elab-addition
P06-1106	91-93	98-101	—a manual one	and an automatic one	—a manual one	and an automatic one	89-125	89-125	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1<2	none	joint	joint
P06-1106	98-101	102-106	and an automatic one	focused on the supporting abstracts—	and an automatic one	focused on the supporting abstracts—	89-125	89-125	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1<2	none	elab-addition	elab-addition
P06-1106	1-13	108-125	This paper presents a hybrid approach to question answering in the clinical domain	that our system compares favorably to PubMed , the search system most widely used by physicians today .	This paper presents a hybrid approach to question answering in the clinical domain	that our system compares favorably to PubMed , the search system most widely used by physicians today .	1-21	89-125	This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.	Two evaluations —a manual one focused on short answers and an automatic one focused on the supporting abstracts— demonstrate that our system compares favorably to PubMed , the search system most widely used by physicians today .	1<2	none	evaluation	evaluation
P06-1107	1-8	9-16	In this paper we investigate a novel method	to detect asymmetric entailment relations between verbs .	In this paper we investigate a novel method	to detect asymmetric entailment relations between verbs .	1-16	1-16	In this paper we investigate a novel method to detect asymmetric entailment relations between verbs .	In this paper we investigate a novel method to detect asymmetric entailment relations between verbs .	1<2	none	enablement	enablement
P06-1107	1-8	17-22	In this paper we investigate a novel method	Our starting point is the idea	In this paper we investigate a novel method	Our starting point is the idea	1-16	17-33	In this paper we investigate a novel method to detect asymmetric entailment relations between verbs .	Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information .	1<2	none	elab-addition	elab-addition
P06-1107	17-22	23-33	Our starting point is the idea	that some point-wise verb selectional preferences carry relevant semantic information .	Our starting point is the idea	that some point-wise verb selectional preferences carry relevant semantic information .	17-33	17-33	Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information .	Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information .	1<2	none	elab-addition	elab-addition
P06-1107	34,41-44	48-50,58-65	Experiments <*> show promising results .	our method , <*> significantly increases the performance of entailment detection .	Experiments <*> show promising results .	our method , <*> significantly increases the performance of entailment detection .	34-44	45-65	Experiments using WordNet as a gold standard show promising results .	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	1>2	none	manner-means	manner-means
P06-1107	34,41-44	35-40	Experiments <*> show promising results .	using WordNet as a gold standard	Experiments <*> show promising results .	using WordNet as a gold standard	34-44	34-44	Experiments using WordNet as a gold standard show promising results .	Experiments using WordNet as a gold standard show promising results .	1<2	none	elab-addition	elab-addition
P06-1107	45-47	48-50,58-65	Where applicable ,	our method , <*> significantly increases the performance of entailment detection .	Where applicable ,	our method , <*> significantly increases the performance of entailment detection .	45-65	45-65	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	1>2	none	elab-addition	elab-addition
P06-1107	1-8	48-50,58-65	In this paper we investigate a novel method	our method , <*> significantly increases the performance of entailment detection .	In this paper we investigate a novel method	our method , <*> significantly increases the performance of entailment detection .	1-16	45-65	In this paper we investigate a novel method to detect asymmetric entailment relations between verbs .	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	1<2	none	evaluation	evaluation
P06-1107	48-50,58-65	51-57	our method , <*> significantly increases the performance of entailment detection .	used in combination with other approaches ,	our method , <*> significantly increases the performance of entailment detection .	used in combination with other approaches ,	45-65	45-65	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	1<2	none	elab-addition	elab-addition
P06-1107	48-50,58-65	66-68,72-85	our method , <*> significantly increases the performance of entailment detection .	A combined approach <*> improves the AROC of 5 % absolute points with respect to standard models .	our method , <*> significantly increases the performance of entailment detection .	A combined approach <*> improves the AROC of 5 % absolute points with respect to standard models .	45-65	66-85	Where applicable , our method , used in combination with other approaches , significantly increases the performance of entailment detection .	A combined approach including our model improves the AROC of 5 % absolute points with respect to standard models .	1<2	none	exp-evidence	exp-evidence
P06-1107	66-68,72-85	69-71	A combined approach <*> improves the AROC of 5 % absolute points with respect to standard models .	including our model	A combined approach <*> improves the AROC of 5 % absolute points with respect to standard models .	including our model	66-85	66-85	A combined approach including our model improves the AROC of 5 % absolute points with respect to standard models .	A combined approach including our model improves the AROC of 5 % absolute points with respect to standard models .	1<2	none	elab-addition	elab-addition
P06-1108	1-5	6-16	In this paper we present	how the automatic extraction of events from text can be used	In this paper we present	how the automatic extraction of events from text can be used	1-41	1-41	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	1>2	none	attribution	attribution
P06-1108	6-16	17-25	how the automatic extraction of events from text can be used	to both classify narrative texts according to plot quality	how the automatic extraction of events from text can be used	to both classify narrative texts according to plot quality	1-41	1-41	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	1<2	none	enablement	enablement
P06-1108	17-25	26-33	to both classify narrative texts according to plot quality	and produce advice in an interactive learning environment	to both classify narrative texts according to plot quality	and produce advice in an interactive learning environment	1-41	1-41	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	1<2	none	joint	joint
P06-1108	26-33	34-41	and produce advice in an interactive learning environment	intended to help students with story writing .	and produce advice in an interactive learning environment	intended to help students with story writing .	1-41	1-41	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	1<2	none	elab-addition	elab-addition
P06-1108	6-16	42-49	how the automatic extraction of events from text can be used	We focus on the story rewriting task ,	how the automatic extraction of events from text can be used	We focus on the story rewriting task ,	1-41	42-70	In this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing .	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	1<2	none	elab-addition	elab-addition
P06-1108	42-49	50-59	We focus on the story rewriting task ,	in which an exemplar story is read to the students	We focus on the story rewriting task ,	in which an exemplar story is read to the students	42-70	42-70	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	1<2	none	elab-addition	elab-addition
P06-1108	50-59	60-70	in which an exemplar story is read to the students	and the students rewrite the story in their own words .	in which an exemplar story is read to the students	and the students rewrite the story in their own words .	42-70	42-70	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	1<2	none	joint	joint
P06-1108	42-49	71-80	We focus on the story rewriting task ,	The system automatically extracts events from the raw text ,	We focus on the story rewriting task ,	The system automatically extracts events from the raw text ,	42-70	71-89	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	The system automatically extracts events from the raw text , formalized as a sequence of temporally ordered predicate-arguments .	1<2	none	elab-process_step	elab-process_step
P06-1108	71-80	81-89	The system automatically extracts events from the raw text ,	formalized as a sequence of temporally ordered predicate-arguments .	The system automatically extracts events from the raw text ,	formalized as a sequence of temporally ordered predicate-arguments .	71-89	71-89	The system automatically extracts events from the raw text , formalized as a sequence of temporally ordered predicate-arguments .	The system automatically extracts events from the raw text , formalized as a sequence of temporally ordered predicate-arguments .	1<2	none	elab-addition	elab-addition
P06-1108	42-49	90-96	We focus on the story rewriting task ,	These events are given to a machine-learner	We focus on the story rewriting task ,	These events are given to a machine-learner	42-70	90-105	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	These events are given to a machine-learner that produces a coarse-grained rating of the story .	1<2	none	elab-process_step	elab-process_step
P06-1108	90-96	97-105	These events are given to a machine-learner	that produces a coarse-grained rating of the story .	These events are given to a machine-learner	that produces a coarse-grained rating of the story .	90-105	90-105	These events are given to a machine-learner that produces a coarse-grained rating of the story .	These events are given to a machine-learner that produces a coarse-grained rating of the story .	1<2	none	elab-addition	elab-addition
P06-1108	42-49	106-117	We focus on the story rewriting task ,	The results of the machine-learner and the extracted events are then used	We focus on the story rewriting task ,	The results of the machine-learner and the extracted events are then used	42-70	106-125	We focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words .	The results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students .	1<2	none	elab-process_step	elab-process_step
P06-1108	106-117	118-125	The results of the machine-learner and the extracted events are then used	to generate fine-grained advice for the students .	The results of the machine-learner and the extracted events are then used	to generate fine-grained advice for the students .	106-125	106-125	The results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students .	The results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students .	1<2	none	enablement	enablement
P06-1109	1-14	15-27	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	Unsupervised DOP models assign all possible binary trees to a set of sentences	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	Unsupervised DOP models assign all possible binary trees to a set of sentences	1-14	15-51	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees .	1<2	none	elab-addition	elab-addition
P06-1109	15-27	28-43	Unsupervised DOP models assign all possible binary trees to a set of sentences	and next use ( a large random subset of ) all subtrees from these binary trees	Unsupervised DOP models assign all possible binary trees to a set of sentences	and next use ( a large random subset of ) all subtrees from these binary trees	15-51	15-51	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees .	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees .	1<2	none	joint	joint
P06-1109	28-43	44-51	and next use ( a large random subset of ) all subtrees from these binary trees	to compute the most probable parse trees .	and next use ( a large random subset of ) all subtrees from these binary trees	to compute the most probable parse trees .	15-51	15-51	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees .	Unsupervised DOP models assign all possible binary trees to a set of sentences and next use ( a large random subset of ) all subtrees from these binary trees to compute the most probable parse trees .	1<2	none	enablement	enablement
P06-1109	1-14	52-67	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	1-14	52-75	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent .	1<2	none	manner-means	manner-means
P06-1109	52-67	68-75	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	which is known to be statistically consistent .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	which is known to be statistically consistent .	52-75	52-75	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent .	1<2	none	elab-addition	elab-addition
P06-1109	52-67	76-96	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	We report state-ofthe-art results on English ( WSJ ) , German ( NEGRA ) and Chinese ( CTB ) data .	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator	We report state-ofthe-art results on English ( WSJ ) , German ( NEGRA ) and Chinese ( CTB ) data .	52-75	76-96	We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent .	We report state-ofthe-art results on English ( WSJ ) , German ( NEGRA ) and Chinese ( CTB ) data .	1<2	none	result	result
P06-1109	1-14	97-107	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	To the best of our knowledge this is the first paper	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	To the best of our knowledge this is the first paper	1-14	97-143	We investigate generalizations of the allsubtrees `` DOP '' approach to unsupervised parsing .	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	1<2	none	evaluation	evaluation
P06-1109	97-107	108-121	To the best of our knowledge this is the first paper	which tests a maximum likelihood estimator for DOP on the Wall Street Journal ,	To the best of our knowledge this is the first paper	which tests a maximum likelihood estimator for DOP on the Wall Street Journal ,	97-143	97-143	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	1<2	none	elab-addition	elab-addition
P06-1109	97-107	122-126	To the best of our knowledge this is the first paper	leading to the surprising result	To the best of our knowledge this is the first paper	leading to the surprising result	97-143	97-143	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	1<2	none	result	result
P06-1109	122-126	127-143	leading to the surprising result	that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	leading to the surprising result	that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	97-143	97-143	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal , leading to the surprising result that an unsupervised parsing model beats a widely used supervised model ( a treebank PCFG ) .	1<2	none	elab-addition	elab-addition
P06-1110	1-13	14-22	The present work advances the accuracy and training speed of discriminative parsing .	Our discriminative parsing method has no generative component ,	The present work advances the accuracy and training speed of discriminative parsing .	Our discriminative parsing method has no generative component ,	1-13	14-39	The present work advances the accuracy and training speed of discriminative parsing .	Our discriminative parsing method has no generative component , yet surpasses a generative baseline on constituent parsing , and does so with minimal linguistic cleverness .	1<2	none	elab-aspect	elab-aspect
P06-1110	14-22	23-31	Our discriminative parsing method has no generative component ,	yet surpasses a generative baseline on constituent parsing ,	Our discriminative parsing method has no generative component ,	yet surpasses a generative baseline on constituent parsing ,	14-39	14-39	Our discriminative parsing method has no generative component , yet surpasses a generative baseline on constituent parsing , and does so with minimal linguistic cleverness .	Our discriminative parsing method has no generative component , yet surpasses a generative baseline on constituent parsing , and does so with minimal linguistic cleverness .	1<2	none	contrast	contrast
P06-1110	23-31	32-39	yet surpasses a generative baseline on constituent parsing ,	and does so with minimal linguistic cleverness .	yet surpasses a generative baseline on constituent parsing ,	and does so with minimal linguistic cleverness .	14-39	14-39	Our discriminative parsing method has no generative component , yet surpasses a generative baseline on constituent parsing , and does so with minimal linguistic cleverness .	Our discriminative parsing method has no generative component , yet surpasses a generative baseline on constituent parsing , and does so with minimal linguistic cleverness .	1<2	none	joint	joint
P06-1110	1-13	40-52	The present work advances the accuracy and training speed of discriminative parsing .	Our model can incorporate arbitrary features of the input and parse state ,	The present work advances the accuracy and training speed of discriminative parsing .	Our model can incorporate arbitrary features of the input and parse state ,	1-13	40-65	The present work advances the accuracy and training speed of discriminative parsing .	Our model can incorporate arbitrary features of the input and parse state , and performs feature selection incrementally over an exponential feature space during training .	1<2	none	elab-aspect	elab-aspect
P06-1110	40-52	53-62	Our model can incorporate arbitrary features of the input and parse state ,	and performs feature selection incrementally over an exponential feature space	Our model can incorporate arbitrary features of the input and parse state ,	and performs feature selection incrementally over an exponential feature space	40-65	40-65	Our model can incorporate arbitrary features of the input and parse state , and performs feature selection incrementally over an exponential feature space during training .	Our model can incorporate arbitrary features of the input and parse state , and performs feature selection incrementally over an exponential feature space during training .	1<2	none	joint	joint
P06-1110	53-62	63-65	and performs feature selection incrementally over an exponential feature space	during training .	and performs feature selection incrementally over an exponential feature space	during training .	40-65	40-65	Our model can incorporate arbitrary features of the input and parse state , and performs feature selection incrementally over an exponential feature space during training .	Our model can incorporate arbitrary features of the input and parse state , and performs feature selection incrementally over an exponential feature space during training .	1<2	none	temporal	temporal
P06-1110	1-13	66-72	The present work advances the accuracy and training speed of discriminative parsing .	We demonstrate the flexibility of our approach	The present work advances the accuracy and training speed of discriminative parsing .	We demonstrate the flexibility of our approach	1-13	66-84	The present work advances the accuracy and training speed of discriminative parsing .	We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets .	1<2	none	evaluation	evaluation
P06-1110	66-72	73-84	We demonstrate the flexibility of our approach	by testing it with several parsing strategies and various feature sets .	We demonstrate the flexibility of our approach	by testing it with several parsing strategies and various feature sets .	66-84	66-84	We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets .	We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets .	1<2	none	manner-means	manner-means
P06-1110	66-72	85-95	We demonstrate the flexibility of our approach	Our implementation is freely available at : http : //nlp.cs.nyu.edu/parser/ .	We demonstrate the flexibility of our approach	Our implementation is freely available at : http : //nlp.cs.nyu.edu/parser/ .	66-84	85-95	We demonstrate the flexibility of our approach by testing it with several parsing strategies and various feature sets .	Our implementation is freely available at : http : //nlp.cs.nyu.edu/parser/ .	1<2	none	result	result
P06-1111	1-10	11-16	We investigate prototype-driven learning for primarily unsupervised grammar induction .	Prior knowledge is specified declaratively ,	We investigate prototype-driven learning for primarily unsupervised grammar induction .	Prior knowledge is specified declaratively ,	1-10	11-28	We investigate prototype-driven learning for primarily unsupervised grammar induction .	Prior knowledge is specified declaratively , by providing a few canonical examples of each target phrase type .	1<2	none	elab-addition	elab-addition
P06-1111	11-16	17-28	Prior knowledge is specified declaratively ,	by providing a few canonical examples of each target phrase type .	Prior knowledge is specified declaratively ,	by providing a few canonical examples of each target phrase type .	11-28	11-28	Prior knowledge is specified declaratively , by providing a few canonical examples of each target phrase type .	Prior knowledge is specified declaratively , by providing a few canonical examples of each target phrase type .	1<2	none	manner-means	manner-means
P06-1111	17-28	29-38	by providing a few canonical examples of each target phrase type .	This sparse prototype information is then propagated across a corpus	by providing a few canonical examples of each target phrase type .	This sparse prototype information is then propagated across a corpus	11-28	29-51	Prior knowledge is specified declaratively , by providing a few canonical examples of each target phrase type .	This sparse prototype information is then propagated across a corpus using distributional similarity features , which augment an otherwise standard PCFG model .	1<2	none	elab-addition	elab-addition
P06-1111	29-38	39-43	This sparse prototype information is then propagated across a corpus	using distributional similarity features ,	This sparse prototype information is then propagated across a corpus	using distributional similarity features ,	29-51	29-51	This sparse prototype information is then propagated across a corpus using distributional similarity features , which augment an otherwise standard PCFG model .	This sparse prototype information is then propagated across a corpus using distributional similarity features , which augment an otherwise standard PCFG model .	1<2	none	manner-means	manner-means
P06-1111	39-43	44-51	using distributional similarity features ,	which augment an otherwise standard PCFG model .	using distributional similarity features ,	which augment an otherwise standard PCFG model .	29-51	29-51	This sparse prototype information is then propagated across a corpus using distributional similarity features , which augment an otherwise standard PCFG model .	This sparse prototype information is then propagated across a corpus using distributional similarity features , which augment an otherwise standard PCFG model .	1<2	none	elab-addition	elab-addition
P06-1111	52-53	54-63	We show	that distributional features are effective at distinguishing bracket labels ,	We show	that distributional features are effective at distinguishing bracket labels ,	52-69	52-69	We show that distributional features are effective at distinguishing bracket labels , but not determining bracket locations .	We show that distributional features are effective at distinguishing bracket labels , but not determining bracket locations .	1>2	none	attribution	attribution
P06-1111	1-10	54-63	We investigate prototype-driven learning for primarily unsupervised grammar induction .	that distributional features are effective at distinguishing bracket labels ,	We investigate prototype-driven learning for primarily unsupervised grammar induction .	that distributional features are effective at distinguishing bracket labels ,	1-10	52-69	We investigate prototype-driven learning for primarily unsupervised grammar induction .	We show that distributional features are effective at distinguishing bracket labels , but not determining bracket locations .	1<2	none	elab-aspect	elab-aspect
P06-1111	54-63	64-69	that distributional features are effective at distinguishing bracket labels ,	but not determining bracket locations .	that distributional features are effective at distinguishing bracket labels ,	but not determining bracket locations .	52-69	52-69	We show that distributional features are effective at distinguishing bracket labels , but not determining bracket locations .	We show that distributional features are effective at distinguishing bracket labels , but not determining bracket locations .	1<2	none	contrast	contrast
P06-1111	70-78	79-95	To improve the quality of the induced trees ,	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	To improve the quality of the induced trees ,	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	70-109	70-109	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	1>2	none	enablement	enablement
P06-1111	1-10	79-95	We investigate prototype-driven learning for primarily unsupervised grammar induction .	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	We investigate prototype-driven learning for primarily unsupervised grammar induction .	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	1-10	70-109	We investigate prototype-driven learning for primarily unsupervised grammar induction .	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	1<2	none	elab-aspect	elab-aspect
P06-1111	79-95	96-100	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	which has complementary stengths :	we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) ,	which has complementary stengths :	70-109	70-109	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	1<2	none	elab-addition	elab-addition
P06-1111	96-100	101-103	which has complementary stengths :	it identifies brackets	which has complementary stengths :	it identifies brackets	70-109	70-109	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	1<2	none	elab-addition	elab-addition
P06-1111	101-103	104-109	it identifies brackets	but does not label them .	it identifies brackets	but does not label them .	70-109	70-109	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	To improve the quality of the induced trees , we combine our PCFG induction with the CCM model of Klein and Manning ( 2002 ) , which has complementary stengths : it identifies brackets but does not label them .	1<2	none	contrast	contrast
P06-1111	110-116	117-131	Using only a handful of prototypes ,	we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	Using only a handful of prototypes ,	we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	110-131	110-131	Using only a handful of prototypes , we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	Using only a handful of prototypes , we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	1>2	none	manner-means	manner-means
P06-1111	1-10	117-131	We investigate prototype-driven learning for primarily unsupervised grammar induction .	we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	We investigate prototype-driven learning for primarily unsupervised grammar induction .	we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	1-10	110-131	We investigate prototype-driven learning for primarily unsupervised grammar induction .	Using only a handful of prototypes , we show substantial improvements over naive PCFG induction for English and Chinese grammar induction .	1<2	none	evaluation	evaluation
P06-1112	1-11	12-19	In this paper , we explore correlation of dependency relation paths	to rank candidate answers in answer extraction .	In this paper , we explore correlation of dependency relation paths	to rank candidate answers in answer extraction .	1-19	1-19	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .	1<2	none	enablement	enablement
P06-1112	20-24	25-45	Using the correlation measure ,	we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	Using the correlation measure ,	we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	20-45	20-45	Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	1>2	none	manner-means	manner-means
P06-1112	1-11	25-45	In this paper , we explore correlation of dependency relation paths	we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	In this paper , we explore correlation of dependency relation paths	we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	1-19	20-45	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .	Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .	1<2	none	manner-means	manner-means
P06-1112	46-50	51-57	Different from previous studies ,	we propose an approximate phrase mapping algorithm	Different from previous studies ,	we propose an approximate phrase mapping algorithm	46-67	46-67	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	1>2	none	contrast	contrast
P06-1112	1-11	51-57	In this paper , we explore correlation of dependency relation paths	we propose an approximate phrase mapping algorithm	In this paper , we explore correlation of dependency relation paths	we propose an approximate phrase mapping algorithm	1-19	46-67	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	1<2	none	progression	progression
P06-1112	51-57	58-67	we propose an approximate phrase mapping algorithm	and incorporate the mapping score into the correlation measure .	we propose an approximate phrase mapping algorithm	and incorporate the mapping score into the correlation measure .	46-67	46-67	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	1<2	none	joint	joint
P06-1112	51-57	68-78	we propose an approximate phrase mapping algorithm	The correlations are further incorporated into a Maximum Entropy-based ranking model	we propose an approximate phrase mapping algorithm	The correlations are further incorporated into a Maximum Entropy-based ranking model	46-67	68-85	Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .	The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training .	1<2	none	elab-addition	elab-addition
P06-1112	68-78	79-85	The correlations are further incorporated into a Maximum Entropy-based ranking model	which estimates path weights from training .	The correlations are further incorporated into a Maximum Entropy-based ranking model	which estimates path weights from training .	68-85	68-85	The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training .	The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training .	1<2	none	elab-addition	elab-addition
P06-1112	86-88	89-105	Experimental results show	that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	Experimental results show	that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	86-105	86-105	Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	1>2	none	attribution	attribution
P06-1112	1-11	89-105	In this paper , we explore correlation of dependency relation paths	that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	In this paper , we explore correlation of dependency relation paths	that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	1-19	86-105	In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .	Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .	1<2	none	evaluation	evaluation
P06-1113	1-5	6-12	This paper describes an algorithm	for propagating verb arguments along lexical chains	This paper describes an algorithm	for propagating verb arguments along lexical chains	1-17	1-17	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	1<2	none	enablement	enablement
P06-1113	6-12	13-17	for propagating verb arguments along lexical chains	consisting of WordNet relations .	for propagating verb arguments along lexical chains	consisting of WordNet relations .	1-17	1-17	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	1<2	none	elab-addition	elab-addition
P06-1113	1-5	18-23	This paper describes an algorithm	The algorithm creates verb argument structures	This paper describes an algorithm	The algorithm creates verb argument structures	1-17	18-28	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	The algorithm creates verb argument structures using VerbNet syntactic patterns .	1<2	none	elab-addition	elab-addition
P06-1113	18-23	24-28	The algorithm creates verb argument structures	using VerbNet syntactic patterns .	The algorithm creates verb argument structures	using VerbNet syntactic patterns .	18-28	18-28	The algorithm creates verb argument structures using VerbNet syntactic patterns .	The algorithm creates verb argument structures using VerbNet syntactic patterns .	1<2	none	manner-means	manner-means
P06-1113	29-35	36-51	In order to increase the coverage ,	a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	In order to increase the coverage ,	a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	29-51	29-51	In order to increase the coverage , a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	In order to increase the coverage , a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	1>2	none	enablement	enablement
P06-1113	18-23	36-51	The algorithm creates verb argument structures	a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	The algorithm creates verb argument structures	a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	18-28	29-51	The algorithm creates verb argument structures using VerbNet syntactic patterns .	In order to increase the coverage , a larger set of verb senses were automatically associated with the existing patterns from VerbNet .	1<2	none	elab-addition	elab-addition
P06-1113	18-23	52-61	The algorithm creates verb argument structures	The algorithm is used in an in-house Question Answering system	The algorithm creates verb argument structures	The algorithm is used in an in-house Question Answering system	18-28	52-69	The algorithm creates verb argument structures using VerbNet syntactic patterns .	The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers .	1<2	none	elab-addition	elab-addition
P06-1113	52-61	62-69	The algorithm is used in an in-house Question Answering system	for re-ranking the set of candidate answers .	The algorithm is used in an in-house Question Answering system	for re-ranking the set of candidate answers .	52-69	52-69	The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers .	The algorithm is used in an in-house Question Answering system for re-ranking the set of candidate answers .	1<2	none	elab-addition	elab-addition
P06-1113	70-77	78-88	Tests on factoid questions from TREC 2004 indicate	that the algorithm improved the system performance by 2.4 % .	Tests on factoid questions from TREC 2004 indicate	that the algorithm improved the system performance by 2.4 % .	70-88	70-88	Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4 % .	Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4 % .	1>2	none	attribution	attribution
P06-1113	1-5	78-88	This paper describes an algorithm	that the algorithm improved the system performance by 2.4 % .	This paper describes an algorithm	that the algorithm improved the system performance by 2.4 % .	1-17	70-88	This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations .	Tests on factoid questions from TREC 2004 indicate that the algorithm improved the system performance by 2.4 % .	1<2	none	evaluation	evaluation
P06-1114	1-8	9-29	Work on the semantics of questions has argued	that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	Work on the semantics of questions has argued	that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	1-29	1-29	Work on the semantics of questions has argued that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	Work on the semantics of questions has argued that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	1>2	none	attribution	attribution
P06-1114	9-29	36-38,44-46	that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	how computational systems <*> can be used	that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	how computational systems <*> can be used	1-29	30-61	Work on the semantics of questions has argued that the relation between a question and its answer ( s ) can be cast in terms of logical entailment .	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	1>2	none	bg-goal	bg-goal
P06-1114	30-35	36-38,44-46	In this paper , we demonstrate	how computational systems <*> can be used	In this paper , we demonstrate	how computational systems <*> can be used	30-61	30-61	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	1>2	none	attribution	attribution
P06-1114	36-38,44-46	39-43	how computational systems <*> can be used	designed to recognize textual entailment	how computational systems <*> can be used	designed to recognize textual entailment	30-61	30-61	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	1<2	none	elab-addition	elab-addition
P06-1114	44-46	47-61	can be used	to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	can be used	to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	30-61	30-61	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	1<2	none	enablement	enablement
P06-1114	62-67	87-98	In our experiments , we show	accuracy can be increased by as much as 20 % overall .	In our experiments , we show	accuracy can be increased by as much as 20 % overall .	62-98	62-98	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1>2	none	attribution	attribution
P06-1114	68-75	87-98	that when textual entailment information is used to	accuracy can be increased by as much as 20 % overall .	that when textual entailment information is used to	accuracy can be increased by as much as 20 % overall .	62-98	62-98	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1>2	none	temporal	temporal
P06-1114	68-75	76-77	that when textual entailment information is used to	either filter	that when textual entailment information is used to	either filter	62-98	62-98	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1<2	none	enablement	enablement
P06-1114	76-77	78-80	either filter	or rank answers	either filter	or rank answers	62-98	62-98	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1<2	none	joint	joint
P06-1114	78-80	81-86	or rank answers	returned by a Q/A system ,	or rank answers	returned by a Q/A system ,	62-98	62-98	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1<2	none	elab-addition	elab-addition
P06-1114	36-38,44-46	87-98	how computational systems <*> can be used	accuracy can be increased by as much as 20 % overall .	how computational systems <*> can be used	accuracy can be increased by as much as 20 % overall .	30-61	62-98	In this paper , we demonstrate how computational systems designed to recognize textual entailment can be used to enhance the accuracy of current open-domain automatic question answering ( Q/A ) systems .	In our experiments , we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system , accuracy can be increased by as much as 20 % overall .	1<2	none	evaluation	evaluation
P06-1115	1-5	6-15	We present a new approach	for mapping natural language sentences to their formal meaning representations	We present a new approach	for mapping natural language sentences to their formal meaning representations	1-20	1-20	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	1<2	none	enablement	enablement
P06-1115	6-15	16-20	for mapping natural language sentences to their formal meaning representations	using string kernel-based classifiers .	for mapping natural language sentences to their formal meaning representations	using string kernel-based classifiers .	1-20	1-20	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	1<2	none	elab-addition	elab-addition
P06-1115	1-5	21-34	We present a new approach	Our system learns these classifiers for every production in the formal language grammar .	We present a new approach	Our system learns these classifiers for every production in the formal language grammar .	1-20	21-34	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	Our system learns these classifiers for every production in the formal language grammar .	1<2	none	elab-aspect	elab-aspect
P06-1115	1-5	35-43	We present a new approach	Meaning representations for novel natural language sentences are obtained	We present a new approach	Meaning representations for novel natural language sentences are obtained	1-20	35-55	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers .	1<2	none	elab-aspect	elab-aspect
P06-1115	35-43	44-50	Meaning representations for novel natural language sentences are obtained	by finding the most probable semantic parse	Meaning representations for novel natural language sentences are obtained	by finding the most probable semantic parse	35-55	35-55	Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers .	Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers .	1<2	none	manner-means	manner-means
P06-1115	44-50	51-55	by finding the most probable semantic parse	using these string classifiers .	by finding the most probable semantic parse	using these string classifiers .	35-55	35-55	Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers .	Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers .	1<2	none	elab-addition	elab-addition
P06-1115	56-63	64-72	Our experiments on two realworld data sets show	that this approach compares favorably to other existing systems	Our experiments on two realworld data sets show	that this approach compares favorably to other existing systems	56-79	56-79	Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise .	Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise .	1>2	none	attribution	attribution
P06-1115	1-5	64-72	We present a new approach	that this approach compares favorably to other existing systems	We present a new approach	that this approach compares favorably to other existing systems	1-20	56-79	We present a new approach for mapping natural language sentences to their formal meaning representations using string kernel-based classifiers .	Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise .	1<2	none	evaluation	evaluation
P06-1115	64-72	73-79	that this approach compares favorably to other existing systems	and is particularly robust to noise .	that this approach compares favorably to other existing systems	and is particularly robust to noise .	56-79	56-79	Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise .	Our experiments on two realworld data sets show that this approach compares favorably to other existing systems and is particularly robust to noise .	1<2	none	joint	joint
P06-1116	1-9	10-23	We investigate the unsupervised detection of semi-fixed cue phrases	such as "This paper proposes a novel approach ... " from unseen text ,	We investigate the unsupervised detection of semi-fixed cue phrases	such as "This paper proposes a novel approach ... " from unseen text ,	1-39	1-39	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	1<2	none	elab-example	elab-example
P06-1116	10-23	24-39	such as "This paper proposes a novel approach ... " from unseen text ,	on the basis of only a handful of seed cue phrases with the desired semantics .	such as "This paper proposes a novel approach ... " from unseen text ,	on the basis of only a handful of seed cue phrases with the desired semantics .	1-39	1-39	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	1<2	none	bg-general	bg-general
P06-1116	1-9	40-42,55-71	We investigate the unsupervised detection of semi-fixed cue phrases	The problem , <*> is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	We investigate the unsupervised detection of semi-fixed cue phrases	The problem , <*> is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	1-39	40-71	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	The problem , in contrast to bootstrapping approaches for Question Answering and Information Extraction , is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	1<2	none	elab-addition	elab-addition
P06-1116	40-42,55-71	43-54	The problem , <*> is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	in contrast to bootstrapping approaches for Question Answering and Information Extraction ,	The problem , <*> is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	in contrast to bootstrapping approaches for Question Answering and Information Extraction ,	40-71	40-71	The problem , in contrast to bootstrapping approaches for Question Answering and Information Extraction , is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	The problem , in contrast to bootstrapping approaches for Question Answering and Information Extraction , is that it is hard to find a constraining context for occurrences of semi-fixed cue phrases .	1<2	none	contrast	contrast
P06-1116	1-9	72-86	We investigate the unsupervised detection of semi-fixed cue phrases	Our method uses components of the cue phrase itself , rather than external context ,	We investigate the unsupervised detection of semi-fixed cue phrases	Our method uses components of the cue phrase itself , rather than external context ,	1-39	72-89	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	Our method uses components of the cue phrase itself , rather than external context , to bootstrap .	1<2	none	manner-means	manner-means
P06-1116	72-86	87-89	Our method uses components of the cue phrase itself , rather than external context ,	to bootstrap .	Our method uses components of the cue phrase itself , rather than external context ,	to bootstrap .	72-89	72-89	Our method uses components of the cue phrase itself , rather than external context , to bootstrap .	Our method uses components of the cue phrase itself , rather than external context , to bootstrap .	1<2	none	enablement	enablement
P06-1116	1-9	90-93	We investigate the unsupervised detection of semi-fixed cue phrases	It successfully excludes phrases	We investigate the unsupervised detection of semi-fixed cue phrases	It successfully excludes phrases	1-39	90-107	We investigate the unsupervised detection of semi-fixed cue phrases such as "This paper proposes a novel approach ... " from unseen text , on the basis of only a handful of seed cue phrases with the desired semantics .	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	1<2	none	evaluation	evaluation
P06-1116	90-93	94-96	It successfully excludes phrases	which are different	It successfully excludes phrases	which are different	90-107	90-107	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	1<2	none	elab-addition	elab-addition
P06-1116	94-96	97-101	which are different	from the target semantics ,	which are different	from the target semantics ,	90-107	90-107	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	1<2	none	elab-addition	elab-addition
P06-1116	94-96	102-107	which are different	but which look superficially similar .	which are different	but which look superficially similar .	90-107	90-107	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	1<2	none	contrast	contrast
P06-1116	90-93	108-114	It successfully excludes phrases	The method achieves 88 % accuracy ,	It successfully excludes phrases	The method achieves 88 % accuracy ,	90-107	108-119	It successfully excludes phrases which are different from the target semantics , but which look superficially similar .	The method achieves 88 % accuracy , outperforming standard bootstrapping approaches .	1<2	none	exp-evidence	exp-evidence
P06-1116	108-114	115-119	The method achieves 88 % accuracy ,	outperforming standard bootstrapping approaches .	The method achieves 88 % accuracy ,	outperforming standard bootstrapping approaches .	108-119	108-119	The method achieves 88 % accuracy , outperforming standard bootstrapping approaches .	The method achieves 88 % accuracy , outperforming standard bootstrapping approaches .	1<2	none	elab-addition	elab-addition
P06-1117	1-7	8-13	This article describes a robust semantic parser	that uses a broad knowledge base	This article describes a robust semantic parser	that uses a broad knowledge base	1-26	1-26	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	1<2	none	elab-addition	elab-addition
P06-1117	8-13	14-20	that uses a broad knowledge base	created by interconnecting three major resources :	that uses a broad knowledge base	created by interconnecting three major resources :	1-26	1-26	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	1<2	none	elab-addition	elab-addition
P06-1117	14-20	21-26	created by interconnecting three major resources :	FrameNet , VerbNet and PropBank .	created by interconnecting three major resources :	FrameNet , VerbNet and PropBank .	1-26	1-26	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	1<2	none	elab-enumember	elab-enumember
P06-1117	1-7	27-32	This article describes a robust semantic parser	The FrameNet corpus contains the examples	This article describes a robust semantic parser	The FrameNet corpus contains the examples	1-26	27-51	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	1<2	none	elab-aspect	elab-aspect
P06-1117	27-32	33-36	The FrameNet corpus contains the examples	annotated with semantic roles	The FrameNet corpus contains the examples	annotated with semantic roles	27-51	27-51	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	1<2	none	elab-addition	elab-addition
P06-1117	27-32	37-51	The FrameNet corpus contains the examples	whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	The FrameNet corpus contains the examples	whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	27-51	27-51	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	1<2	none	contrast	contrast
P06-1117	27-32	52-56	The FrameNet corpus contains the examples	We connect VerbNet and FrameNet	The FrameNet corpus contains the examples	We connect VerbNet and FrameNet	27-51	52-68	The FrameNet corpus contains the examples annotated with semantic roles whereas the VerbNet lexicon provides the knowledge about the syntactic behavior of the verbs .	We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes .	1<2	none	elab-addition	elab-addition
P06-1117	52-56	57-68	We connect VerbNet and FrameNet	by mapping the FrameNet frames to the VerbNet Intersective Levin classes .	We connect VerbNet and FrameNet	by mapping the FrameNet frames to the VerbNet Intersective Levin classes .	52-68	52-68	We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes .	We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes .	1<2	none	manner-means	manner-means
P06-1117	1-7	69-72,82-83	This article describes a robust semantic parser	The PropBank corpus , <*> is used	This article describes a robust semantic parser	The PropBank corpus , <*> is used	1-26	69-98	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	1<2	none	elab-aspect	elab-aspect
P06-1117	69-72,82-83	73-81	The PropBank corpus , <*> is used	which is tightly connected to the VerbNet lexicon ,	The PropBank corpus , <*> is used	which is tightly connected to the VerbNet lexicon ,	69-98	69-98	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	1<2	none	elab-addition	elab-addition
P06-1117	82-83	84-88	is used	to increase the verb coverage	is used	to increase the verb coverage	69-98	69-98	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	1<2	none	enablement	enablement
P06-1117	84-88	89-98	to increase the verb coverage	and also to test the effectiveness of our approach .	to increase the verb coverage	and also to test the effectiveness of our approach .	69-98	69-98	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	The PropBank corpus , which is tightly connected to the VerbNet lexicon , is used to increase the verb coverage and also to test the effectiveness of our approach .	1<2	none	joint	joint
P06-1117	99-101	102-117	The results indicate	that our model is an interesting step towards the design of more robust semantic parsers .	The results indicate	that our model is an interesting step towards the design of more robust semantic parsers .	99-117	99-117	The results indicate that our model is an interesting step towards the design of more robust semantic parsers .	The results indicate that our model is an interesting step towards the design of more robust semantic parsers .	1>2	none	attribution	attribution
P06-1117	1-7	102-117	This article describes a robust semantic parser	that our model is an interesting step towards the design of more robust semantic parsers .	This article describes a robust semantic parser	that our model is an interesting step towards the design of more robust semantic parsers .	1-26	99-117	This article describes a robust semantic parser that uses a broad knowledge base created by interconnecting three major resources : FrameNet , VerbNet and PropBank .	The results indicate that our model is an interesting step towards the design of more robust semantic parsers .	1<2	none	evaluation	evaluation
P06-1118	1-20	21-33	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	1-20	21-66	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	1<2	none	elab-addition	elab-addition
P06-1118	21-33	34-39	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	used within the Alpine Convention2 ,	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	used within the Alpine Convention2 ,	21-66	21-66	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	1<2	none	elab-addition	elab-addition
P06-1118	21-33	40-56	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	so that the member states are able to cooperate and communicate efficiently in the four official languages	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development	so that the member states are able to cooperate and communicate efficiently in the four official languages	21-66	21-66	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	1<2	none	result	result
P06-1118	40-56	57-66	so that the member states are able to cooperate and communicate efficiently in the four official languages	( French , German , Italian and Slovene ) .	so that the member states are able to cooperate and communicate efficiently in the four official languages	( French , German , Italian and Slovene ) .	21-66	21-66	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	LexALP's goal is to harmonise the terminology on spatial planning and sustainable development used within the Alpine Convention2 , so that the member states are able to cooperate and communicate efficiently in the four official languages ( French , German , Italian and Slovene ) .	1<2	none	elab-enumember	elab-enumember
P06-1118	1-20	67-75	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	To this purpose , LexALP uses the Jibiki platform	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	To this purpose , LexALP uses the Jibiki platform	1-20	67-100	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	To this purpose , LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages .	1<2	none	elab-addition	elab-addition
P06-1118	67-75	76-88	To this purpose , LexALP uses the Jibiki platform	to build a term bank for the contrastive analysis of the specialised terminology	To this purpose , LexALP uses the Jibiki platform	to build a term bank for the contrastive analysis of the specialised terminology	67-100	67-100	To this purpose , LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages .	To this purpose , LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages .	1<2	none	enablement	enablement
P06-1118	76-88	89-100	to build a term bank for the contrastive analysis of the specialised terminology	used in six different national legal systems and four different languages .	to build a term bank for the contrastive analysis of the specialised terminology	used in six different national legal systems and four different languages .	67-100	67-100	To this purpose , LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages .	To this purpose , LexALP uses the Jibiki platform to build a term bank for the contrastive analysis of the specialised terminology used in six different national legal systems and four different languages .	1<2	none	elab-addition	elab-addition
P06-1118	101-105	106-120	In this paper we present	how a generic platform like Jibiki can cope with a new kind of dictionary .	In this paper we present	how a generic platform like Jibiki can cope with a new kind of dictionary .	101-120	101-120	In this paper we present how a generic platform like Jibiki can cope with a new kind of dictionary .	In this paper we present how a generic platform like Jibiki can cope with a new kind of dictionary .	1>2	none	attribution	attribution
P06-1118	1-20	106-120	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	how a generic platform like Jibiki can cope with a new kind of dictionary .	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	how a generic platform like Jibiki can cope with a new kind of dictionary .	1-20	101-120	This paper presents the particular use of "Jibiki" ( Papillon's web server development platform ) for the LexALP1 project .	In this paper we present how a generic platform like Jibiki can cope with a new kind of dictionary .	1<2	none	progression	progression
P06-1119	1-6	55-63	Thesauri and ontologies provide important value	In this paper , we present an efficient process	Thesauri and ontologies provide important value	In this paper , we present an efficient process	1-19	55-73	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	1>2	none	bg-compare	bg-compare
P06-1119	1-6	7-12	Thesauri and ontologies provide important value	in facilitating access to digital archives	Thesauri and ontologies provide important value	in facilitating access to digital archives	1-19	1-19	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	1<2	none	elab-addition	elab-addition
P06-1119	1-6	13-19	Thesauri and ontologies provide important value	by representing underlying principles of organization .	Thesauri and ontologies provide important value	by representing underlying principles of organization .	1-19	1-19	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	1<2	none	manner-means	manner-means
P06-1119	1-6	20-30	Thesauri and ontologies provide important value	Translation of such resources into multiple languages is an important component	Thesauri and ontologies provide important value	Translation of such resources into multiple languages is an important component	1-19	20-35	Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization .	Translation of such resources into multiple languages is an important component for providing multilingual access .	1<2	none	elab-addition	elab-addition
P06-1119	20-30	31-35	Translation of such resources into multiple languages is an important component	for providing multilingual access .	Translation of such resources into multiple languages is an important component	for providing multilingual access .	20-35	20-35	Translation of such resources into multiple languages is an important component for providing multilingual access .	Translation of such resources into multiple languages is an important component for providing multilingual access .	1<2	none	elab-addition	elab-addition
P06-1119	20-30	36-49	Translation of such resources into multiple languages is an important component	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation	Translation of such resources into multiple languages is an important component	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation	20-35	36-54	Translation of such resources into multiple languages is an important component for providing multilingual access .	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources .	1<2	none	contrast	contrast
P06-1119	36-49	50-54	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation	using general-domain lexical resources .	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation	using general-domain lexical resources .	36-54	36-54	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources .	However , the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources .	1<2	none	elab-addition	elab-addition
P06-1119	55-63	64-67	In this paper , we present an efficient process	for leveraging human translations	In this paper , we present an efficient process	for leveraging human translations	55-73	55-73	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	1<2	none	enablement	enablement
P06-1119	64-67	68-73	for leveraging human translations	when constructing domain-specific lexical resources .	for leveraging human translations	when constructing domain-specific lexical resources .	55-73	55-73	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	1<2	none	temporal	temporal
P06-1119	74-80	104-117	We evaluate the effectiveness of this process	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	We evaluate the effectiveness of this process	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	74-103	104-117	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	1>2	none	result	result
P06-1119	74-80	81-86	We evaluate the effectiveness of this process	by producing a probabilistic phrase dictionary	We evaluate the effectiveness of this process	by producing a probabilistic phrase dictionary	74-103	74-103	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	1<2	none	manner-means	manner-means
P06-1119	81-86	87-93	by producing a probabilistic phrase dictionary	and translating a thesaurus of 56,000 concepts	by producing a probabilistic phrase dictionary	and translating a thesaurus of 56,000 concepts	74-103	74-103	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	1<2	none	joint	joint
P06-1119	87-93	94-103	and translating a thesaurus of 56,000 concepts	used to catalogue a large archive of oral histories .	and translating a thesaurus of 56,000 concepts	used to catalogue a large archive of oral histories .	74-103	74-103	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories .	1<2	none	elab-addition	elab-addition
P06-1119	55-63	104-117	In this paper , we present an efficient process	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	In this paper , we present an efficient process	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	55-73	104-117	In this paper , we present an efficient process for leveraging human translations when constructing domain-specific lexical resources .	Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies .	1<2	none	evaluation	evaluation
P06-1120	1-18	19-23	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	A hybrid system is presented	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	A hybrid system is presented	1-18	19-45	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English , French , Spanish and Italian corpora .	1<2	none	manner-means	manner-means
P06-1120	19-23	24-30	A hybrid system is presented	that combines statistical methods and multilingual parsing	A hybrid system is presented	that combines statistical methods and multilingual parsing	19-45	19-45	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English , French , Spanish and Italian corpora .	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English , French , Spanish and Italian corpora .	1<2	none	elab-addition	elab-addition
P06-1120	24-30	31-45	that combines statistical methods and multilingual parsing	for detecting accurate collocational information from English , French , Spanish and Italian corpora .	that combines statistical methods and multilingual parsing	for detecting accurate collocational information from English , French , Spanish and Italian corpora .	19-45	19-45	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English , French , Spanish and Italian corpora .	A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English , French , Spanish and Italian corpora .	1<2	none	enablement	enablement
P06-1120	1-18	46-53,66-70	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	The advantage of relying on full parsing over <*> is first theoretically motivated ,	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	The advantage of relying on full parsing over <*> is first theoretically motivated ,	1-18	46-79	This paper focuses on the use of advanced techniques of text analysis as support for collocation extraction .	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	1<2	none	elab-addition	elab-addition
P06-1120	46-53,66-70	54-58	The advantage of relying on full parsing over <*> is first theoretically motivated ,	using a traditional window method	The advantage of relying on full parsing over <*> is first theoretically motivated ,	using a traditional window method	46-79	46-79	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	1<2	none	elab-addition	elab-addition
P06-1120	54-58	59-65	using a traditional window method	( which ignores the syntactic information )	using a traditional window method	( which ignores the syntactic information )	46-79	46-79	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	1<2	none	elab-addition	elab-addition
P06-1120	66-70	71-79	is first theoretically motivated ,	then empirically validated by a comparative evaluation experiment .	is first theoretically motivated ,	then empirically validated by a comparative evaluation experiment .	46-79	46-79	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	The advantage of relying on full parsing over using a traditional window method ( which ignores the syntactic information ) is first theoretically motivated , then empirically validated by a comparative evaluation experiment .	1<2	none	progression	progression
P06-1121	1-12	26-33	Statistical MT has made great progress in the last few years ,	Syntactic approaches seek to remedy these problems .	Statistical MT has made great progress in the last few years ,	Syntactic approaches seek to remedy these problems .	1-25	26-33	Statistical MT has made great progress in the last few years , but current translation models are weak on re-ordering and target language fluency .	Syntactic approaches seek to remedy these problems .	1>2	none	elab-addition	elab-addition
P06-1121	1-12	13-25	Statistical MT has made great progress in the last few years ,	but current translation models are weak on re-ordering and target language fluency .	Statistical MT has made great progress in the last few years ,	but current translation models are weak on re-ordering and target language fluency .	1-25	1-25	Statistical MT has made great progress in the last few years , but current translation models are weak on re-ordering and target language fluency .	Statistical MT has made great progress in the last few years , but current translation models are weak on re-ordering and target language fluency .	1<2	none	contrast	contrast
P06-1121	26-33	34-41	Syntactic approaches seek to remedy these problems .	In this paper , we take the framework	Syntactic approaches seek to remedy these problems .	In this paper , we take the framework	26-33	34-107	Syntactic approaches seek to remedy these problems .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1>2	none	bg-goal	bg-goal
P06-1121	34-41	42-60	In this paper , we take the framework	for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs ,	In this paper , we take the framework	for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs ,	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	elab-addition	elab-addition
P06-1121	42-60	61-69	for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs ,	and present two main extensions of their approach :	for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs ,	and present two main extensions of their approach :	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	joint	joint
P06-1121	70-78	86-92	first , instead of merely computing a single derivation	we construct a large number of derivations	first , instead of merely computing a single derivation	we construct a large number of derivations	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1>2	none	contrast	contrast
P06-1121	70-78	79-85	first , instead of merely computing a single derivation	that minimally explains a sentence pair ,	first , instead of merely computing a single derivation	that minimally explains a sentence pair ,	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	elab-addition	elab-addition
P06-1121	61-69	86-92	and present two main extensions of their approach :	we construct a large number of derivations	and present two main extensions of their approach :	we construct a large number of derivations	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	elab-addition	elab-addition
P06-1121	86-92	93-98	we construct a large number of derivations	that include contextually richer rules ,	we construct a large number of derivations	that include contextually richer rules ,	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	elab-addition	elab-addition
P06-1121	93-98	99-107	that include contextually richer rules ,	and account for multiple interpretations of unaligned words .	that include contextually richer rules ,	and account for multiple interpretations of unaligned words .	34-107	34-107	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	1<2	none	joint	joint
P06-1121	61-69	108-117	and present two main extensions of their approach :	Second , we propose probability estimates and a training procedure	and present two main extensions of their approach :	Second , we propose probability estimates and a training procedure	34-107	108-122	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	Second , we propose probability estimates and a training procedure for weighting these rules .	1<2	none	elab-addition	elab-addition
P06-1121	108-117	118-122	Second , we propose probability estimates and a training procedure	for weighting these rules .	Second , we propose probability estimates and a training procedure	for weighting these rules .	108-122	108-122	Second , we propose probability estimates and a training procedure for weighting these rules .	Second , we propose probability estimates and a training procedure for weighting these rules .	1<2	none	elab-addition	elab-addition
P06-1121	123-130	132-134	We contrast different approaches on real examples ,	that our estimates	We contrast different approaches on real examples ,	that our estimates	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1>2	none	result	result
P06-1121	131	132-134	show	that our estimates	show	that our estimates	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1>2	none	attribution	attribution
P06-1121	34-41	132-134	In this paper , we take the framework	that our estimates	In this paper , we take the framework	that our estimates	34-107	123-163	In this paper , we take the framework for acquiring multi-level syntactic translation rules of ( Galley et al. , 2004 ) from aligned tree-string pairs , and present two main extensions of their approach : first , instead of merely computing a single derivation that minimally explains a sentence pair , we construct a large number of derivations that include contextually richer rules , and account for multiple interpretations of unaligned words .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1<2	none	evaluation	evaluation
P06-1121	132-134	135-141	that our estimates	based on multiple derivations favor phrasal re-orderings	that our estimates	based on multiple derivations favor phrasal re-orderings	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1<2	none	bg-general	bg-general
P06-1121	135-141	142-147	based on multiple derivations favor phrasal re-orderings	that are linguistically better motivated ,	based on multiple derivations favor phrasal re-orderings	that are linguistically better motivated ,	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1<2	none	elab-addition	elab-addition
P06-1121	148-149	150-163	and establish	that our larger rules provide a 3.63 BLEU point increase over minimal rules .	and establish	that our larger rules provide a 3.63 BLEU point increase over minimal rules .	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1>2	none	attribution	attribution
P06-1121	132-134	150-163	that our estimates	that our larger rules provide a 3.63 BLEU point increase over minimal rules .	that our estimates	that our larger rules provide a 3.63 BLEU point increase over minimal rules .	123-163	123-163	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	We contrast different approaches on real examples , show that our estimates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated , and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules .	1<2	none	joint	joint
P06-1122	1-2,10-12	19-34	Certain distinctions <*> may be redundant	We quantify redundancy among source types by the similarity of their distributions over target types .	Certain distinctions <*> may be redundant	We quantify redundancy among source types by the similarity of their distributions over target types .	1-18	19-34	Certain distinctions made in the lexicon of one language may be redundant when translating into another language .	We quantify redundancy among source types by the similarity of their distributions over target types .	1>2	none	elab-addition	elab-addition
P06-1122	1-2,10-12	3-9	Certain distinctions <*> may be redundant	made in the lexicon of one language	Certain distinctions <*> may be redundant	made in the lexicon of one language	1-18	1-18	Certain distinctions made in the lexicon of one language may be redundant when translating into another language .	Certain distinctions made in the lexicon of one language may be redundant when translating into another language .	1<2	none	elab-addition	elab-addition
P06-1122	1-2,10-12	13-18	Certain distinctions <*> may be redundant	when translating into another language .	Certain distinctions <*> may be redundant	when translating into another language .	1-18	1-18	Certain distinctions made in the lexicon of one language may be redundant when translating into another language .	Certain distinctions made in the lexicon of one language may be redundant when translating into another language .	1<2	none	temporal	temporal
P06-1122	19-34	35-39	We quantify redundancy among source types by the similarity of their distributions over target types .	We propose a language-independent framework	We quantify redundancy among source types by the similarity of their distributions over target types .	We propose a language-independent framework	19-34	35-52	We quantify redundancy among source types by the similarity of their distributions over target types .	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	1>2	none	bg-goal	bg-goal
P06-1122	35-39	40-43	We propose a language-independent framework	for minimising lexical redundancy	We propose a language-independent framework	for minimising lexical redundancy	35-52	35-52	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	1<2	none	enablement	enablement
P06-1122	40-43	44-52	for minimising lexical redundancy	that can be optimised directly from parallel text .	for minimising lexical redundancy	that can be optimised directly from parallel text .	35-52	35-52	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	1<2	none	elab-addition	elab-addition
P06-1122	35-39	53-75	We propose a language-independent framework	Optimisation of the source lexicon for a given target language is viewed as model selection over a set of cluster-based translation models .	We propose a language-independent framework	Optimisation of the source lexicon for a given target language is viewed as model selection over a set of cluster-based translation models .	35-52	53-75	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	Optimisation of the source lexicon for a given target language is viewed as model selection over a set of cluster-based translation models .	1<2	none	elab-addition	elab-addition
P06-1122	35-39	76-90	We propose a language-independent framework	Redundant distinctions between types may exhibit monolingual regularities , for example , inflexion patterns .	We propose a language-independent framework	Redundant distinctions between types may exhibit monolingual regularities , for example , inflexion patterns .	35-52	76-90	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	Redundant distinctions between types may exhibit monolingual regularities , for example , inflexion patterns .	1<2	none	elab-addition	elab-addition
P06-1122	35-39	91-97	We propose a language-independent framework	We define a prior over model structure	We propose a language-independent framework	We define a prior over model structure	35-52	91-117	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	1<2	none	manner-means	manner-means
P06-1122	91-97	98-102	We define a prior over model structure	using a Markov random field	We define a prior over model structure	using a Markov random field	91-117	91-117	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	1<2	none	elab-addition	elab-addition
P06-1122	91-97	103-110	We define a prior over model structure	and learn features over sets of monolingual types	We define a prior over model structure	and learn features over sets of monolingual types	91-117	91-117	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	1<2	none	joint	joint
P06-1122	103-110	111-117	and learn features over sets of monolingual types	that are predictive of bilingual redundancy .	and learn features over sets of monolingual types	that are predictive of bilingual redundancy .	91-117	91-117	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	1<2	none	elab-addition	elab-addition
P06-1122	91-97	118-124	We define a prior over model structure	The prior makes model selection more robust	We define a prior over model structure	The prior makes model selection more robust	91-117	118-133	We define a prior over model structure using a Markov random field and learn features over sets of monolingual types that are predictive of bilingual redundancy .	The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy .	1<2	none	elab-addition	elab-addition
P06-1122	118-124	125-130	The prior makes model selection more robust	without the need for language-specific assumptions	The prior makes model selection more robust	without the need for language-specific assumptions	118-133	118-133	The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy .	The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy .	1<2	none	elab-addition	elab-addition
P06-1122	125-130	131-133	without the need for language-specific assumptions	regarding redundancy .	without the need for language-specific assumptions	regarding redundancy .	118-133	118-133	The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy .	The prior makes model selection more robust without the need for language-specific assumptions regarding redundancy .	1<2	none	elab-addition	elab-addition
P06-1122	134-142	143-154	Using these models in a phrase-based SMT system ,	we show significant improvements in translation quality for certain language pairs .	Using these models in a phrase-based SMT system ,	we show significant improvements in translation quality for certain language pairs .	134-154	134-154	Using these models in a phrase-based SMT system , we show significant improvements in translation quality for certain language pairs .	Using these models in a phrase-based SMT system , we show significant improvements in translation quality for certain language pairs .	1>2	none	elab-addition	elab-addition
P06-1122	35-39	143-154	We propose a language-independent framework	we show significant improvements in translation quality for certain language pairs .	We propose a language-independent framework	we show significant improvements in translation quality for certain language pairs .	35-52	134-154	We propose a language-independent framework for minimising lexical redundancy that can be optimised directly from parallel text .	Using these models in a phrase-based SMT system , we show significant improvements in translation quality for certain language pairs .	1<2	none	evaluation	evaluation
P06-1123	1-11	12-18	This paper describes a study of the patterns of translational equivalence	exhibited by a variety of bitexts .	This paper describes a study of the patterns of translational equivalence	exhibited by a variety of bitexts .	1-18	1-18	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	1<2	none	elab-addition	elab-addition
P06-1123	19-21	22-32	The study found	that the complexity of these patterns in every bitext was higher	The study found	that the complexity of these patterns in every bitext was higher	19-38	19-38	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature .	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature .	1>2	none	attribution	attribution
P06-1123	1-11	22-32	This paper describes a study of the patterns of translational equivalence	that the complexity of these patterns in every bitext was higher	This paper describes a study of the patterns of translational equivalence	that the complexity of these patterns in every bitext was higher	1-18	19-38	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature .	1<2	none	elab-aspect	elab-aspect
P06-1123	22-32	33-38	that the complexity of these patterns in every bitext was higher	than suggested in the literature .	that the complexity of these patterns in every bitext was higher	than suggested in the literature .	19-38	19-38	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature .	The study found that the complexity of these patterns in every bitext was higher than suggested in the literature .	1<2	none	comparison	comparison
P06-1123	1-11	39-58	This paper describes a study of the patterns of translational equivalence	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models ,	This paper describes a study of the patterns of translational equivalence	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models ,	1-18	39-70	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models , including finitestate phrase-based models , tree-to-string models , and tree-to-tree models .	1<2	none	elab-aspect	elab-aspect
P06-1123	39-58	59-70	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models ,	including finitestate phrase-based models , tree-to-string models , and tree-to-tree models .	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models ,	including finitestate phrase-based models , tree-to-string models , and tree-to-tree models .	39-70	39-70	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models , including finitestate phrase-based models , tree-to-string models , and tree-to-tree models .	These findings shed new light on why " syntactic " constraints have not helped to improve statistical translation models , including finitestate phrase-based models , tree-to-string models , and tree-to-tree models .	1<2	none	elab-example	elab-example
P06-1123	1-11	71-75	This paper describes a study of the patterns of translational equivalence	The paper also presents evidence	This paper describes a study of the patterns of translational equivalence	The paper also presents evidence	1-18	71-101	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order .	1<2	none	elab-aspect	elab-aspect
P06-1123	71-75	76-86	The paper also presents evidence	that inversion transduction grammars cannot generate some translational equivalence relations ,	The paper also presents evidence	that inversion transduction grammars cannot generate some translational equivalence relations ,	71-101	71-101	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order .	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order .	1<2	none	elab-addition	elab-addition
P06-1123	76-86	87-101	that inversion transduction grammars cannot generate some translational equivalence relations ,	even in relatively simple real bitexts in syntactically similar languages with rigid word order .	that inversion transduction grammars cannot generate some translational equivalence relations ,	even in relatively simple real bitexts in syntactically similar languages with rigid word order .	71-101	71-101	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order .	The paper also presents evidence that inversion transduction grammars cannot generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order .	1<2	none	contrast	contrast
P06-1123	1-11	102-111	This paper describes a study of the patterns of translational equivalence	Instructions for replicating our experiments are at http : //nlp.cs.nyu.edu/GenPar/ACL06	This paper describes a study of the patterns of translational equivalence	Instructions for replicating our experiments are at http : //nlp.cs.nyu.edu/GenPar/ACL06	1-18	102-111	This paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts .	Instructions for replicating our experiments are at http : //nlp.cs.nyu.edu/GenPar/ACL06	1<2	none	elab-addition	elab-addition
P06-1124	1-12	13-25	We propose a new hierarchical Bayesian n-gram model of natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions	We propose a new hierarchical Bayesian n-gram model of natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions	1-12	13-40	We propose a new hierarchical Bayesian n-gram model of natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	1<2	none	manner-means	manner-means
P06-1124	13-25	26-28	Our model makes use of a generalization of the commonly used Dirichlet distributions	called Pitman-Yor processes	Our model makes use of a generalization of the commonly used Dirichlet distributions	called Pitman-Yor processes	13-40	13-40	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	1<2	none	elab-addition	elab-addition
P06-1124	26-28	29-32	called Pitman-Yor processes	which produce power-law distributions	called Pitman-Yor processes	which produce power-law distributions	13-40	13-40	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	1<2	none	elab-addition	elab-addition
P06-1124	29-32	33-40	which produce power-law distributions	more closely resembling those in natural languages .	which produce power-law distributions	more closely resembling those in natural languages .	13-40	13-40	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages .	1<2	none	elab-addition	elab-addition
P06-1124	41-42	43-70	We show	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	We show	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	41-70	41-70	We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	1>2	none	attribution	attribution
P06-1124	1-12	43-70	We propose a new hierarchical Bayesian n-gram model of natural languages .	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	We propose a new hierarchical Bayesian n-gram model of natural languages .	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	1-12	41-70	We propose a new hierarchical Bayesian n-gram model of natural languages .	We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	1<2	none	evaluation	evaluation
P06-1124	71-72	73-79	Experiments verify	that our model gives cross entropy results	Experiments verify	that our model gives cross entropy results	71-89	71-89	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	1>2	none	attribution	attribution
P06-1124	43-70	73-79	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	that our model gives cross entropy results	that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	that our model gives cross entropy results	41-70	71-89	We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney , one of the best smoothing methods for n-gram language models .	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	1<2	none	exp-evidence	exp-evidence
P06-1124	73-79	80-83	that our model gives cross entropy results	superior to interpolated Kneser-Ney	that our model gives cross entropy results	superior to interpolated Kneser-Ney	71-89	71-89	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	1<2	none	comparison	comparison
P06-1124	80-83	84-89	superior to interpolated Kneser-Ney	and comparable to modified Kneser-Ney .	superior to interpolated Kneser-Ney	and comparable to modified Kneser-Ney .	71-89	71-89	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney .	1<2	none	joint	joint
P06-1125	1-17	18-24	Chatting is a popular communication media on the Internet via ICQ , chat rooms , etc. .	Chat language is different from natural language	Chatting is a popular communication media on the Internet via ICQ , chat rooms , etc. .	Chat language is different from natural language	1-17	18-39	Chatting is a popular communication media on the Internet via ICQ , chat rooms , etc. .	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	1>2	none	elab-addition	elab-addition
P06-1125	18-24	40-45	Chat language is different from natural language	The dynamic problem is enormously troublesome	Chat language is different from natural language	The dynamic problem is enormously troublesome	18-39	40-60	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	1>2	none	elab-addition	elab-addition
P06-1125	18-24	25-32	Chat language is different from natural language	due to its anomalous and dynamic natures ,	Chat language is different from natural language	due to its anomalous and dynamic natures ,	18-39	18-39	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	1<2	none	exp-reason	exp-reason
P06-1125	25-32	33-39	due to its anomalous and dynamic natures ,	which renders conventional NLP tools inapplicable .	due to its anomalous and dynamic natures ,	which renders conventional NLP tools inapplicable .	18-39	18-39	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	Chat language is different from natural language due to its anomalous and dynamic natures , which renders conventional NLP tools inapplicable .	1<2	none	elab-addition	elab-addition
P06-1125	40-45	61-66	The dynamic problem is enormously troublesome	To address the dynamic problem ,	The dynamic problem is enormously troublesome	To address the dynamic problem ,	40-60	61-92	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	1>2	none	elab-addition	elab-addition
P06-1125	40-45	46-54	The dynamic problem is enormously troublesome	because it makes static chat language corpus outdated quickly	The dynamic problem is enormously troublesome	because it makes static chat language corpus outdated quickly	40-60	40-60	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	1<2	none	exp-reason	exp-reason
P06-1125	46-54	55-60	because it makes static chat language corpus outdated quickly	in representing contemporary chat language .	because it makes static chat language corpus outdated quickly	in representing contemporary chat language .	40-60	40-60	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	The dynamic problem is enormously troublesome because it makes static chat language corpus outdated quickly in representing contemporary chat language .	1<2	none	elab-addition	elab-addition
P06-1125	61-66	67-72	To address the dynamic problem ,	we propose the phonetic mapping models	To address the dynamic problem ,	we propose the phonetic mapping models	61-92	61-92	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	1>2	none	enablement	enablement
P06-1125	67-72	73-92	we propose the phonetic mapping models	to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	we propose the phonetic mapping models	to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	61-92	61-92	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	1<2	none	enablement	enablement
P06-1125	93-97	98-109	Different from character mappings ,	the phonetic mappings can be constructed from available standard Chinese corpus .	Different from character mappings ,	the phonetic mappings can be constructed from available standard Chinese corpus .	93-109	93-109	Different from character mappings , the phonetic mappings can be constructed from available standard Chinese corpus .	Different from character mappings , the phonetic mappings can be constructed from available standard Chinese corpus .	1>2	none	contrast	contrast
P06-1125	67-72	98-109	we propose the phonetic mapping models	the phonetic mappings can be constructed from available standard Chinese corpus .	we propose the phonetic mapping models	the phonetic mappings can be constructed from available standard Chinese corpus .	61-92	93-109	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	Different from character mappings , the phonetic mappings can be constructed from available standard Chinese corpus .	1<2	none	elab-addition	elab-addition
P06-1125	110-120	121-126	To perform the task of dynamic chat language term normalization ,	we extend the source channel model	To perform the task of dynamic chat language term normalization ,	we extend the source channel model	110-133	110-133	To perform the task of dynamic chat language term normalization , we extend the source channel model by incorporating the phonetic mapping models .	To perform the task of dynamic chat language term normalization , we extend the source channel model by incorporating the phonetic mapping models .	1>2	none	enablement	enablement
P06-1125	67-72	121-126	we propose the phonetic mapping models	we extend the source channel model	we propose the phonetic mapping models	we extend the source channel model	61-92	110-133	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	To perform the task of dynamic chat language term normalization , we extend the source channel model by incorporating the phonetic mapping models .	1<2	none	elab-addition	elab-addition
P06-1125	121-126	127-133	we extend the source channel model	by incorporating the phonetic mapping models .	we extend the source channel model	by incorporating the phonetic mapping models .	110-133	110-133	To perform the task of dynamic chat language term normalization , we extend the source channel model by incorporating the phonetic mapping models .	To perform the task of dynamic chat language term normalization , we extend the source channel model by incorporating the phonetic mapping models .	1<2	none	manner-means	manner-means
P06-1125	134-136	137-143	Experimental results show	that this method is effective and stable	Experimental results show	that this method is effective and stable	134-150	134-150	Experimental results show that this method is effective and stable in normalizing dynamic chat language terms .	Experimental results show that this method is effective and stable in normalizing dynamic chat language terms .	1>2	none	attribution	attribution
P06-1125	67-72	137-143	we propose the phonetic mapping models	that this method is effective and stable	we propose the phonetic mapping models	that this method is effective and stable	61-92	134-150	To address the dynamic problem , we propose the phonetic mapping models to present mappings between chat terms and standard words via phonetic transcription , i.e. Chinese Pinyin in our case .	Experimental results show that this method is effective and stable in normalizing dynamic chat language terms .	1<2	none	evaluation	evaluation
P06-1125	137-143	144-150	that this method is effective and stable	in normalizing dynamic chat language terms .	that this method is effective and stable	in normalizing dynamic chat language terms .	134-150	134-150	Experimental results show that this method is effective and stable in normalizing dynamic chat language terms .	Experimental results show that this method is effective and stable in normalizing dynamic chat language terms .	1<2	none	elab-addition	elab-addition
P06-1126	17-24	35-46	To reduce the size of the language model	importance of each bigram is computed in terms of discriminative pruning criterion	To reduce the size of the language model	importance of each bigram is computed in terms of discriminative pruning criterion	17-59	17-59	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	1>2	none	enablement	enablement
P06-1126	17-24	25-34	To reduce the size of the language model	that is used in a Chinese word segmentation system ,	To reduce the size of the language model	that is used in a Chinese word segmentation system ,	17-59	17-59	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	1<2	none	elab-addition	elab-addition
P06-1126	1-16	35-46	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	importance of each bigram is computed in terms of discriminative pruning criterion	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	importance of each bigram is computed in terms of discriminative pruning criterion	1-16	17-59	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	1<2	none	elab-process_step	elab-process_step
P06-1126	35-46	47-53	importance of each bigram is computed in terms of discriminative pruning criterion	that is related to the performance loss	importance of each bigram is computed in terms of discriminative pruning criterion	that is related to the performance loss	17-59	17-59	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	1<2	none	elab-addition	elab-addition
P06-1126	47-53	54-59	that is related to the performance loss	caused by pruning the bigram .	that is related to the performance loss	caused by pruning the bigram .	17-59	17-59	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	To reduce the size of the language model that is used in a Chinese word segmentation system , importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bigram .	1<2	none	elab-addition	elab-addition
P06-1126	1-16	60-66	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	Then we propose a step-by-step growing algorithm	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	Then we propose a step-by-step growing algorithm	1-16	60-75	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	Then we propose a step-by-step growing algorithm to build the language model of desired size .	1<2	none	elab-process_step	elab-process_step
P06-1126	60-66	67-75	Then we propose a step-by-step growing algorithm	to build the language model of desired size .	Then we propose a step-by-step growing algorithm	to build the language model of desired size .	60-75	60-75	Then we propose a step-by-step growing algorithm to build the language model of desired size .	Then we propose a step-by-step growing algorithm to build the language model of desired size .	1<2	none	enablement	enablement
P06-1126	76-78	79-89	Experimental results show	that the discriminative pruning method leads to a much smaller model	Experimental results show	that the discriminative pruning method leads to a much smaller model	76-99	76-99	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	1>2	none	attribution	attribution
P06-1126	1-16	79-89	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	that the discriminative pruning method leads to a much smaller model	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	that the discriminative pruning method leads to a much smaller model	1-16	76-99	This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation .	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	1<2	none	evaluation	evaluation
P06-1126	79-89	90-93	that the discriminative pruning method leads to a much smaller model	compared with the model	that the discriminative pruning method leads to a much smaller model	compared with the model	76-99	76-99	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	1<2	none	comparison	comparison
P06-1126	90-93	94-99	compared with the model	pruned using the state-of-the-art method .	compared with the model	pruned using the state-of-the-art method .	76-99	76-99	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	1<2	none	elab-addition	elab-addition
P06-1126	79-89	100-123	that the discriminative pruning method leads to a much smaller model	At the same Chinese word segmentation F-measure , the number of bigrams in the model can be reduced by up to 90 % .	that the discriminative pruning method leads to a much smaller model	At the same Chinese word segmentation F-measure , the number of bigrams in the model can be reduced by up to 90 % .	76-99	100-123	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	At the same Chinese word segmentation F-measure , the number of bigrams in the model can be reduced by up to 90 % .	1<2	none	exp-evidence	exp-evidence
P06-1126	79-89	124-136	that the discriminative pruning method leads to a much smaller model	Correlation between language model perplexity and word segmentation performance is also discussed .	that the discriminative pruning method leads to a much smaller model	Correlation between language model perplexity and word segmentation performance is also discussed .	76-99	124-136	Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method .	Correlation between language model perplexity and word segmentation performance is also discussed .	1<2	none	joint	joint
P06-1127	1-9	10-18	A web search with double checking model is proposed	to explore the web as a live corpus .	A web search with double checking model is proposed	to explore the web as a live corpus .	1-18	1-18	A web search with double checking model is proposed to explore the web as a live corpus .	A web search with double checking model is proposed to explore the web as a live corpus .	1<2	none	enablement	enablement
P06-1127	1-9	19-21,45-47	A web search with double checking model is proposed	Five association measures <*> are presented .	A web search with double checking model is proposed	Five association measures <*> are presented .	1-18	19-47	A web search with double checking model is proposed to explore the web as a live corpus .	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	1<2	none	elab-addition	elab-addition
P06-1127	19-21,45-47	22-44	Five association measures <*> are presented .	including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) ,	Five association measures <*> are presented .	including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) ,	19-47	19-47	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	1<2	none	elab-enumember	elab-enumember
P06-1127	19-21,45-47	48-64	Five association measures <*> are presented .	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 ,	Five association measures <*> are presented .	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 ,	19-47	48-78	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 , which competes with the performance ( 0.8914 ) of the model using WordNet .	1<2	none	elab-aspect	elab-aspect
P06-1127	48-64	65-75	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 ,	which competes with the performance ( 0.8914 ) of the model	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 ,	which competes with the performance ( 0.8914 ) of the model	48-78	48-78	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 , which competes with the performance ( 0.8914 ) of the model using WordNet .	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 , which competes with the performance ( 0.8914 ) of the model using WordNet .	1<2	none	elab-addition	elab-addition
P06-1127	65-75	76-78	which competes with the performance ( 0.8914 ) of the model	using WordNet .	which competes with the performance ( 0.8914 ) of the model	using WordNet .	48-78	48-78	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 , which competes with the performance ( 0.8914 ) of the model using WordNet .	In the experiments on Rubenstein-Goodenough's benchmark data set , the CODC measure achieves correlation coefficient 0.8492 , which competes with the performance ( 0.8914 ) of the model using WordNet .	1<2	none	elab-addition	elab-addition
P06-1127	79-86,100	101-107	The experiments on link detection of named entities <*> verify	that the double-check frequencies are reliable .	The experiments on link detection of named entities <*> verify	that the double-check frequencies are reliable .	79-107	79-107	The experiments on link detection of named entities using the strategies of direct association , association matrix and scalar association matrix verify that the double-check frequencies are reliable .	The experiments on link detection of named entities using the strategies of direct association , association matrix and scalar association matrix verify that the double-check frequencies are reliable .	1>2	none	attribution	attribution
P06-1127	79-86,100	87-99	The experiments on link detection of named entities <*> verify	using the strategies of direct association , association matrix and scalar association matrix	The experiments on link detection of named entities <*> verify	using the strategies of direct association , association matrix and scalar association matrix	79-107	79-107	The experiments on link detection of named entities using the strategies of direct association , association matrix and scalar association matrix verify that the double-check frequencies are reliable .	The experiments on link detection of named entities using the strategies of direct association , association matrix and scalar association matrix verify that the double-check frequencies are reliable .	1<2	none	elab-addition	elab-addition
P06-1127	19-21,45-47	101-107	Five association measures <*> are presented .	that the double-check frequencies are reliable .	Five association measures <*> are presented .	that the double-check frequencies are reliable .	19-47	79-107	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	The experiments on link detection of named entities using the strategies of direct association , association matrix and scalar association matrix verify that the double-check frequencies are reliable .	1<2	none	elab-aspect	elab-aspect
P06-1127	108-114	115-122	Further study on named entity clustering shows	that the five measures are quite useful .	Further study on named entity clustering shows	that the five measures are quite useful .	108-122	108-122	Further study on named entity clustering shows that the five measures are quite useful .	Further study on named entity clustering shows that the five measures are quite useful .	1>2	none	attribution	attribution
P06-1127	19-21,45-47	115-122	Five association measures <*> are presented .	that the five measures are quite useful .	Five association measures <*> are presented .	that the five measures are quite useful .	19-47	108-122	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	Further study on named entity clustering shows that the five measures are quite useful .	1<2	none	elab-aspect	elab-aspect
P06-1127	115-122	123-136	that the five measures are quite useful .	In particular , CODC measure is very stable on wordword and name-name experiments .	that the five measures are quite useful .	In particular , CODC measure is very stable on wordword and name-name experiments .	108-122	123-136	Further study on named entity clustering shows that the five measures are quite useful .	In particular , CODC measure is very stable on wordword and name-name experiments .	1<2	none	elab-addition	elab-addition
P06-1127	19-21,45-47	137-141,150-156	Five association measures <*> are presented .	The application of CODC measure <*> achieves 9.65 % and 14.22 % increase	Five association measures <*> are presented .	The application of CODC measure <*> achieves 9.65 % and 14.22 % increase	19-47	137-164	Five association measures including variants of Dice , Overlap Ratio , Jaccard , and Cosine , as well as Co-Occurrence Double Check ( CODC ) , are presented .	The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65 % and 14.22 % increase compared to the system without community expansion .	1<2	none	elab-aspect	elab-aspect
P06-1127	137-141,150-156	142-149	The application of CODC measure <*> achieves 9.65 % and 14.22 % increase	to expand community chains for personal name disambiguation	The application of CODC measure <*> achieves 9.65 % and 14.22 % increase	to expand community chains for personal name disambiguation	137-164	137-164	The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65 % and 14.22 % increase compared to the system without community expansion .	The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65 % and 14.22 % increase compared to the system without community expansion .	1<2	none	elab-addition	elab-addition
P06-1127	150-156	157-164	achieves 9.65 % and 14.22 % increase	compared to the system without community expansion .	achieves 9.65 % and 14.22 % increase	compared to the system without community expansion .	137-164	137-164	The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65 % and 14.22 % increase compared to the system without community expansion .	The application of CODC measure to expand community chains for personal name disambiguation achieves 9.65 % and 14.22 % increase compared to the system without community expansion .	1<2	none	comparison	comparison
P06-1127	165-168	169-187	All the experiments illustrate	that the novel model of web search with double checking is feasible for mining associations from the web .	All the experiments illustrate	that the novel model of web search with double checking is feasible for mining associations from the web .	165-187	165-187	All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web .	All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web .	1>2	none	attribution	attribution
P06-1127	1-9	169-187	A web search with double checking model is proposed	that the novel model of web search with double checking is feasible for mining associations from the web .	A web search with double checking model is proposed	that the novel model of web search with double checking is feasible for mining associations from the web .	1-18	165-187	A web search with double checking model is proposed to explore the web as a live corpus .	All the experiments illustrate that the novel model of web search with double checking is feasible for mining associations from the web .	1<2	none	summary	summary
P06-1128	18-21	22-32	Prior to retrieval ,	all sentences are annotated with predicate argument structures and ontological identifiers	Prior to retrieval ,	all sentences are annotated with predicate argument structures and ontological identifiers	18-42	18-42	Prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer .	Prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer .	1>2	none	temporal	temporal
P06-1128	1-17	22-32	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	all sentences are annotated with predicate argument structures and ontological identifiers	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	all sentences are annotated with predicate argument structures and ontological identifiers	1-17	18-42	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	Prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer .	1<2	none	elab-example	elab-example
P06-1128	22-32	33-42	all sentences are annotated with predicate argument structures and ontological identifiers	by applying a deep parser and a term recognizer .	all sentences are annotated with predicate argument structures and ontological identifiers	by applying a deep parser and a term recognizer .	18-42	18-42	Prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer .	Prior to retrieval , all sentences are annotated with predicate argument structures and ontological identifiers by applying a deep parser and a term recognizer .	1<2	none	manner-means	manner-means
P06-1128	43-47	48-60	During the run time ,	user requests are converted into queries of region algebra on these annotations .	During the run time ,	user requests are converted into queries of region algebra on these annotations .	43-60	43-60	During the run time , user requests are converted into queries of region algebra on these annotations .	During the run time , user requests are converted into queries of region algebra on these annotations .	1>2	none	temporal	temporal
P06-1128	1-17	48-60	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	user requests are converted into queries of region algebra on these annotations .	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	user requests are converted into queries of region algebra on these annotations .	1-17	43-60	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	During the run time , user requests are converted into queries of region algebra on these annotations .	1<2	none	elab-example	elab-example
P06-1128	1-17	61-76	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts .	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts .	1-17	61-76	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts .	1<2	none	elab-addition	elab-addition
P06-1128	77-88	97-105	This framework was applied to a text retrieval system for MEDLINE .	that the cost is sufficiently small for real-time applications	This framework was applied to a text retrieval system for MEDLINE .	that the cost is sufficiently small for real-time applications	77-88	89-114	This framework was applied to a text retrieval system for MEDLINE .	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	1>2	none	manner-means	manner-means
P06-1128	89-96	97-105	Experiments on the retrieval of biomedical correlations revealed	that the cost is sufficiently small for real-time applications	Experiments on the retrieval of biomedical correlations revealed	that the cost is sufficiently small for real-time applications	89-114	89-114	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	1>2	none	attribution	attribution
P06-1128	1-17	97-105	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	that the cost is sufficiently small for real-time applications	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	that the cost is sufficiently small for real-time applications	1-17	89-114	This paper introduces a novel framework for the accurate retrieval of relational concepts from huge texts .	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	1<2	none	evaluation	evaluation
P06-1128	97-105	106-114	that the cost is sufficiently small for real-time applications	and that the retrieval precision is significantly improved .	that the cost is sufficiently small for real-time applications	and that the retrieval precision is significantly improved .	89-114	89-114	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	Experiments on the retrieval of biomedical correlations revealed that the cost is sufficiently small for real-time applications and that the retrieval precision is significantly improved .	1<2	none	joint	joint
P06-1129	1-8	15-24	A query speller is crucial to search engine	This paper describes novel methods for use of distributional similarity	A query speller is crucial to search engine	This paper describes novel methods for use of distributional similarity	1-14	15-36	A query speller is crucial to search engine in improving web search relevance .	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	1>2	none	bg-goal	bg-goal
P06-1129	1-8	9-14	A query speller is crucial to search engine	in improving web search relevance .	A query speller is crucial to search engine	in improving web search relevance .	1-14	1-14	A query speller is crucial to search engine in improving web search relevance .	A query speller is crucial to search engine in improving web search relevance .	1<2	none	elab-addition	elab-addition
P06-1129	15-24	25-28	This paper describes novel methods for use of distributional similarity	estimated from query logs	This paper describes novel methods for use of distributional similarity	estimated from query logs	15-36	15-36	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	1<2	none	elab-addition	elab-addition
P06-1129	25-28	29-36	estimated from query logs	in learning improved query spelling correction models .	estimated from query logs	in learning improved query spelling correction models .	15-36	15-36	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	1<2	none	elab-addition	elab-addition
P06-1129	15-24	37-51	This paper describes novel methods for use of distributional similarity	The key to our methods is the property of distributional similarity between two terms :	This paper describes novel methods for use of distributional similarity	The key to our methods is the property of distributional similarity between two terms :	15-36	37-74	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	The key to our methods is the property of distributional similarity between two terms : it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings .	1<2	none	elab-addition	elab-addition
P06-1129	37-51	52-74	The key to our methods is the property of distributional similarity between two terms :	it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings .	The key to our methods is the property of distributional similarity between two terms :	it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings .	37-74	37-74	The key to our methods is the property of distributional similarity between two terms : it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings .	The key to our methods is the property of distributional similarity between two terms : it is high between a frequently occurring misspelling and its correction , and low between two irrelevant terms only with similar spellings .	1<2	none	elab-enumember	elab-enumember
P06-1129	15-24	75-78	This paper describes novel methods for use of distributional similarity	We present two models	This paper describes novel methods for use of distributional similarity	We present two models	15-36	75-88	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	We present two models that are able to take advantage of this property .	1<2	none	elab-addition	elab-addition
P06-1129	75-78	79-88	We present two models	that are able to take advantage of this property .	We present two models	that are able to take advantage of this property .	75-88	75-88	We present two models that are able to take advantage of this property .	We present two models that are able to take advantage of this property .	1<2	none	elab-addition	elab-addition
P06-1129	89-91	92-111	Experimental results demonstrate	that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	Experimental results demonstrate	that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	89-111	89-111	Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	1>2	none	attribution	attribution
P06-1129	15-24	92-111	This paper describes novel methods for use of distributional similarity	that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	This paper describes novel methods for use of distributional similarity	that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	15-36	89-111	This paper describes novel methods for use of distributional similarity estimated from query logs in learning improved query spelling correction models .	Experimental results demonstrate that the distributional similarity based models can significantly outperform their baseline systems in the web query spelling correction task .	1<2	none	evaluation	evaluation
P06-1130	1-10	11-22	We present a novel PCFG-based architecture for robust probabilistic generation	based on wide-coverage LFG approximations ( Cahill et al. , 2004 )	We present a novel PCFG-based architecture for robust probabilistic generation	based on wide-coverage LFG approximations ( Cahill et al. , 2004 )	1-37	1-37	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	1<2	none	bg-general	bg-general
P06-1130	11-22	23-27	based on wide-coverage LFG approximations ( Cahill et al. , 2004 )	automatically extracted from treebanks ,	based on wide-coverage LFG approximations ( Cahill et al. , 2004 )	automatically extracted from treebanks ,	1-37	1-37	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	1<2	none	elab-addition	elab-addition
P06-1130	23-27	28-33	automatically extracted from treebanks ,	maximising the probability of a tree	automatically extracted from treebanks ,	maximising the probability of a tree	1-37	1-37	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	1<2	none	elab-addition	elab-addition
P06-1130	28-33	34-37	maximising the probability of a tree	given an f-structure .	maximising the probability of a tree	given an f-structure .	1-37	1-37	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	1<2	none	condition	condition
P06-1130	1-10	38-41	We present a novel PCFG-based architecture for robust probabilistic generation	We evaluate our approach	We present a novel PCFG-based architecture for robust probabilistic generation	We evaluate our approach	1-37	38-45	We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations ( Cahill et al. , 2004 ) automatically extracted from treebanks , maximising the probability of a tree given an f-structure .	We evaluate our approach using stringbased evaluation .	1<2	none	evaluation	evaluation
P06-1130	38-41	42-45	We evaluate our approach	using stringbased evaluation .	We evaluate our approach	using stringbased evaluation .	38-45	38-45	We evaluate our approach using stringbased evaluation .	We evaluate our approach using stringbased evaluation .	1<2	none	elab-addition	elab-addition
P06-1130	38-41	46-74	We evaluate our approach	We currently achieve coverage of 95.26 % , a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length ≤20 .	We evaluate our approach	We currently achieve coverage of 95.26 % , a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length ≤20 .	38-45	46-74	We evaluate our approach using stringbased evaluation .	We currently achieve coverage of 95.26 % , a BLEU score of 0.7227 and string accuracy of 0.7476 on the Penn-II WSJ Section 23 sentences of length ≤20 .	1<2	none	result	result
P06-1131	1-5	6-11	This paper presents an approach	to incrementally generating locative expressions .	This paper presents an approach	to incrementally generating locative expressions .	1-11	1-11	This paper presents an approach to incrementally generating locative expressions .	This paper presents an approach to incrementally generating locative expressions .	1<2	none	enablement	enablement
P06-1131	1-5	12-26	This paper presents an approach	It addresses the issue of combinatorial explosion inherent in the construction of relational context models	This paper presents an approach	It addresses the issue of combinatorial explosion inherent in the construction of relational context models	1-11	12-74	This paper presents an approach to incrementally generating locative expressions .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	manner-means	manner-means
P06-1131	12-26	27-40	It addresses the issue of combinatorial explosion inherent in the construction of relational context models	by : ( a ) contextually defining the set of objects in the context	It addresses the issue of combinatorial explosion inherent in the construction of relational context models	by : ( a ) contextually defining the set of objects in the context	12-74	12-74	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	manner-means	manner-means
P06-1131	27-40	41-47	by : ( a ) contextually defining the set of objects in the context	that may function as a landmark ,	by : ( a ) contextually defining the set of objects in the context	that may function as a landmark ,	12-74	12-74	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	elab-addition	elab-addition
P06-1131	27-40	48-54	by : ( a ) contextually defining the set of objects in the context	and ( b ) sequencing the order	by : ( a ) contextually defining the set of objects in the context	and ( b ) sequencing the order	12-74	12-74	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	joint	joint
P06-1131	48-54	55-60	and ( b ) sequencing the order	in which spatial relations are considered	and ( b ) sequencing the order	in which spatial relations are considered	12-74	12-74	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	elab-addition	elab-addition
P06-1131	55-60	61-74	in which spatial relations are considered	using a cognitively motivated hierarchy of relations , and visual and discourse salience .	in which spatial relations are considered	using a cognitively motivated hierarchy of relations , and visual and discourse salience .	12-74	12-74	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	It addresses the issue of combinatorial explosion inherent in the construction of relational context models by : ( a ) contextually defining the set of objects in the context that may function as a landmark , and ( b ) sequencing the order in which spatial relations are considered using a cognitively motivated hierarchy of relations , and visual and discourse salience .	1<2	none	manner-means	manner-means
P06-1132	1-4,18-27	47-54	Japanese case markers , <*> often pose challenges to the generation of Japanese text ,	In this paper , we describe the task	Japanese case markers , <*> often pose challenges to the generation of Japanese text ,	In this paper , we describe the task	1-46	47-106	Japanese case markers , which indicate the grammatical relation of the complement NP to the predicate , often pose challenges to the generation of Japanese text , be it done by a foreign language learner , or by a machine translation ( MT ) system .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1>2	none	bg-goal	bg-goal
P06-1132	1-4,18-27	5-17	Japanese case markers , <*> often pose challenges to the generation of Japanese text ,	which indicate the grammatical relation of the complement NP to the predicate ,	Japanese case markers , <*> often pose challenges to the generation of Japanese text ,	which indicate the grammatical relation of the complement NP to the predicate ,	1-46	1-46	Japanese case markers , which indicate the grammatical relation of the complement NP to the predicate , often pose challenges to the generation of Japanese text , be it done by a foreign language learner , or by a machine translation ( MT ) system .	Japanese case markers , which indicate the grammatical relation of the complement NP to the predicate , often pose challenges to the generation of Japanese text , be it done by a foreign language learner , or by a machine translation ( MT ) system .	1<2	none	elab-addition	elab-addition
P06-1132	18-27	28-46	often pose challenges to the generation of Japanese text ,	be it done by a foreign language learner , or by a machine translation ( MT ) system .	often pose challenges to the generation of Japanese text ,	be it done by a foreign language learner , or by a machine translation ( MT ) system .	1-46	1-46	Japanese case markers , which indicate the grammatical relation of the complement NP to the predicate , often pose challenges to the generation of Japanese text , be it done by a foreign language learner , or by a machine translation ( MT ) system .	Japanese case markers , which indicate the grammatical relation of the complement NP to the predicate , often pose challenges to the generation of Japanese text , be it done by a foreign language learner , or by a machine translation ( MT ) system .	1<2	none	elab-addition	elab-addition
P06-1132	47-54	55-59	In this paper , we describe the task	of predicting Japanese case markers	In this paper , we describe the task	of predicting Japanese case markers	47-106	47-106	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1<2	none	elab-addition	elab-addition
P06-1132	55-59	60-64	of predicting Japanese case markers	and propose machine learning methods	of predicting Japanese case markers	and propose machine learning methods	47-106	47-106	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1<2	none	progression	progression
P06-1132	60-64	65-71	and propose machine learning methods	for solving it in two settings :	and propose machine learning methods	for solving it in two settings :	47-106	47-106	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1<2	none	elab-addition	elab-addition
P06-1132	65-71	72-85	for solving it in two settings :	( i ) monolingual , when given information only from the Japanese sentence ;	for solving it in two settings :	( i ) monolingual , when given information only from the Japanese sentence ;	47-106	47-106	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1<2	none	elab-addition	elab-addition
P06-1132	65-71	86-106	for solving it in two settings :	and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	for solving it in two settings :	and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	47-106	47-106	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	1<2	none	elab-addition	elab-addition
P06-1132	65-71	107-120	for solving it in two settings :	We formulate the task after the well-studied task of English semantic role labelling ,	for solving it in two settings :	We formulate the task after the well-studied task of English semantic role labelling ,	47-106	107-132	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	We formulate the task after the well-studied task of English semantic role labelling , and explore features from a syntactic dependency structure of the sentence .	1<2	none	elab-addition	elab-addition
P06-1132	107-120	121-132	We formulate the task after the well-studied task of English semantic role labelling ,	and explore features from a syntactic dependency structure of the sentence .	We formulate the task after the well-studied task of English semantic role labelling ,	and explore features from a syntactic dependency structure of the sentence .	107-132	107-132	We formulate the task after the well-studied task of English semantic role labelling , and explore features from a syntactic dependency structure of the sentence .	We formulate the task after the well-studied task of English semantic role labelling , and explore features from a syntactic dependency structure of the sentence .	1<2	none	joint	joint
P06-1132	65-71	133-145	for solving it in two settings :	For the monolingual task , we evaluated our models on the Kyoto Corpus	for solving it in two settings :	For the monolingual task , we evaluated our models on the Kyoto Corpus	47-106	133-160	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	For the monolingual task , we evaluated our models on the Kyoto Corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase .	1<2	none	elab-addition	elab-addition
P06-1132	133-145	146-151	For the monolingual task , we evaluated our models on the Kyoto Corpus	and achieved over 84 % accuracy	For the monolingual task , we evaluated our models on the Kyoto Corpus	and achieved over 84 % accuracy	133-160	133-160	For the monolingual task , we evaluated our models on the Kyoto Corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase .	For the monolingual task , we evaluated our models on the Kyoto Corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase .	1<2	none	result	result
P06-1132	146-151	152-160	and achieved over 84 % accuracy	in assigning correct case markers for each phrase .	and achieved over 84 % accuracy	in assigning correct case markers for each phrase .	133-160	133-160	For the monolingual task , we evaluated our models on the Kyoto Corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase .	For the monolingual task , we evaluated our models on the Kyoto Corpus and achieved over 84 % accuracy in assigning correct case markers for each phrase .	1<2	none	elab-addition	elab-addition
P06-1132	65-71	161-174	for solving it in two settings :	For the bilingual task , we achieved an accuracy of 92 % per phrase	for solving it in two settings :	For the bilingual task , we achieved an accuracy of 92 % per phrase	47-106	161-183	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	For the bilingual task , we achieved an accuracy of 92 % per phrase using a bilingual dataset from a technical domain .	1<2	none	elab-addition	elab-addition
P06-1132	161-174	175-183	For the bilingual task , we achieved an accuracy of 92 % per phrase	using a bilingual dataset from a technical domain .	For the bilingual task , we achieved an accuracy of 92 % per phrase	using a bilingual dataset from a technical domain .	161-183	161-183	For the bilingual task , we achieved an accuracy of 92 % per phrase using a bilingual dataset from a technical domain .	For the bilingual task , we achieved an accuracy of 92 % per phrase using a bilingual dataset from a technical domain .	1<2	none	elab-addition	elab-addition
P06-1132	184-185	186-191,206-214	We show	that in both settings , features <*> contribute significantly to the prediction of case markers .	We show	that in both settings , features <*> contribute significantly to the prediction of case markers .	184-214	184-214	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	1>2	none	attribution	attribution
P06-1132	47-54	186-191,206-214	In this paper , we describe the task	that in both settings , features <*> contribute significantly to the prediction of case markers .	In this paper , we describe the task	that in both settings , features <*> contribute significantly to the prediction of case markers .	47-106	184-214	In this paper , we describe the task of predicting Japanese case markers and propose machine learning methods for solving it in two settings : ( i ) monolingual , when given information only from the Japanese sentence ; and ( ii ) bilingual , when also given information from a corresponding English source sentence in an MT context .	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	1<2	none	evaluation	evaluation
P06-1132	186-191,206-214	192-196	that in both settings , features <*> contribute significantly to the prediction of case markers .	that exploit dependency information ,	that in both settings , features <*> contribute significantly to the prediction of case markers .	that exploit dependency information ,	184-214	184-214	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	1<2	none	elab-addition	elab-addition
P06-1132	192-196	197-201	that exploit dependency information ,	whether derived from gold-standard annotations	that exploit dependency information ,	whether derived from gold-standard annotations	184-214	184-214	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	1<2	none	elab-addition	elab-addition
P06-1132	197-201	202-205	whether derived from gold-standard annotations	or automatically assigned ,	whether derived from gold-standard annotations	or automatically assigned ,	184-214	184-214	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	We show that in both settings , features that exploit dependency information , whether derived from gold-standard annotations or automatically assigned , contribute significantly to the prediction of case markers .	1<2	none	joint	joint
P06-1133	1-5	6-9	In this paper we investigate	how to automatically determine	In this paper we investigate	how to automatically determine	1-19	1-19	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	1>2	none	attribution	attribution
P06-1133	6-9	10-19	how to automatically determine	if two document collections are written from different perspectives .	how to automatically determine	if two document collections are written from different perspectives .	1-19	1-19	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	1>2	none	attribution	attribution
P06-1133	10-19	20-39	if two document collections are written from different perspectives .	By perspectives we mean a point of view , for example , from the perspective of Democrats or Republicans .	if two document collections are written from different perspectives .	By perspectives we mean a point of view , for example , from the perspective of Democrats or Republicans .	1-19	20-39	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	By perspectives we mean a point of view , for example , from the perspective of Democrats or Republicans .	1<2	none	elab-addition	elab-addition
P06-1133	10-19	40-46	if two document collections are written from different perspectives .	We propose a test of different perspectives	if two document collections are written from different perspectives .	We propose a test of different perspectives	1-19	40-58	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	We propose a test of different perspectives based on distribution divergence between the statistical models of two collections .	1<2	none	manner-means	manner-means
P06-1133	40-46	47-58	We propose a test of different perspectives	based on distribution divergence between the statistical models of two collections .	We propose a test of different perspectives	based on distribution divergence between the statistical models of two collections .	40-58	40-58	We propose a test of different perspectives based on distribution divergence between the statistical models of two collections .	We propose a test of different perspectives based on distribution divergence between the statistical models of two collections .	1<2	none	bg-general	bg-general
P06-1133	59-61	62-78	Experimental results show	that the test can successfully distinguish document collections of different perspectives from other types of collections .	Experimental results show	that the test can successfully distinguish document collections of different perspectives from other types of collections .	59-78	59-78	Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections .	Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections .	1>2	none	attribution	attribution
P06-1133	10-19	62-78	if two document collections are written from different perspectives .	that the test can successfully distinguish document collections of different perspectives from other types of collections .	if two document collections are written from different perspectives .	that the test can successfully distinguish document collections of different perspectives from other types of collections .	1-19	59-78	In this paper we investigate how to automatically determine if two document collections are written from different perspectives .	Experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections .	1<2	none	evaluation	evaluation
P06-1134	1-10	11-16	Subjectivity and meaning are both important properties of language .	This paper explores their interaction ,	Subjectivity and meaning are both important properties of language .	This paper explores their interaction ,	1-10	11-55	Subjectivity and meaning are both important properties of language .	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	1>2	none	elab-addition	elab-addition
P06-1134	11-16	17-25	This paper explores their interaction ,	and brings empirical evidence in support of the hypotheses	This paper explores their interaction ,	and brings empirical evidence in support of the hypotheses	11-55	11-55	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	1<2	none	progression	progression
P06-1134	17-25	26-33	and brings empirical evidence in support of the hypotheses	that ( 1 ) subjectivity is a property	and brings empirical evidence in support of the hypotheses	that ( 1 ) subjectivity is a property	11-55	11-55	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	1<2	none	elab-addition	elab-addition
P06-1134	26-33	34-41	that ( 1 ) subjectivity is a property	that can be associated with word senses ,	that ( 1 ) subjectivity is a property	that can be associated with word senses ,	11-55	11-55	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	1<2	none	elab-addition	elab-addition
P06-1134	17-25	42-55	and brings empirical evidence in support of the hypotheses	and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	and brings empirical evidence in support of the hypotheses	and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	11-55	11-55	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	This paper explores their interaction , and brings empirical evidence in support of the hypotheses that ( 1 ) subjectivity is a property that can be associated with word senses , and ( 2 ) word sense disambiguation can directly benefit from subjectivity annotations .	1<2	none	elab-addition	elab-addition
P06-1135	1-9	10-20	This paper demonstrates a conceptually simple but effective method	of increasing the accuracy of QA systems on factoid-style questions .	This paper demonstrates a conceptually simple but effective method	of increasing the accuracy of QA systems on factoid-style questions .	1-20	1-20	This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions .	This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions .	1<2	none	elab-addition	elab-addition
P06-1135	1-9	21-29	This paper demonstrates a conceptually simple but effective method	We define the notion of an inverted question ,	This paper demonstrates a conceptually simple but effective method	We define the notion of an inverted question ,	1-20	21-58	This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1<2	none	elab-addition	elab-addition
P06-1135	30-31	48-53	and show	incorrect answers get demoted in confidence	and show	incorrect answers get demoted in confidence	21-58	21-58	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1>2	none	attribution	attribution
P06-1135	32-34	35-47	that by requiring	that the answers to the original and inverted questions be mutually consistent ,	that by requiring	that the answers to the original and inverted questions be mutually consistent ,	21-58	21-58	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1>2	none	attribution	attribution
P06-1135	35-47	48-53	that the answers to the original and inverted questions be mutually consistent ,	incorrect answers get demoted in confidence	that the answers to the original and inverted questions be mutually consistent ,	incorrect answers get demoted in confidence	21-58	21-58	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1>2	none	manner-means	manner-means
P06-1135	21-29	48-53	We define the notion of an inverted question ,	incorrect answers get demoted in confidence	We define the notion of an inverted question ,	incorrect answers get demoted in confidence	21-58	21-58	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1<2	none	progression	progression
P06-1135	48-53	54-58	incorrect answers get demoted in confidence	and correct ones promoted .	incorrect answers get demoted in confidence	and correct ones promoted .	21-58	21-58	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	1<2	none	joint	joint
P06-1135	59-62	63-69	Additionally , we show	that lack of validation can be used	Additionally , we show	that lack of validation can be used	59-77	59-77	Additionally , we show that lack of validation can be used to assert no-answer ( nil ) conditions .	Additionally , we show that lack of validation can be used to assert no-answer ( nil ) conditions .	1>2	none	attribution	attribution
P06-1135	21-29	63-69	We define the notion of an inverted question ,	that lack of validation can be used	We define the notion of an inverted question ,	that lack of validation can be used	21-58	59-77	We define the notion of an inverted question , and show that by requiring that the answers to the original and inverted questions be mutually consistent , incorrect answers get demoted in confidence and correct ones promoted .	Additionally , we show that lack of validation can be used to assert no-answer ( nil ) conditions .	1<2	none	progression	progression
P06-1135	63-69	70-77	that lack of validation can be used	to assert no-answer ( nil ) conditions .	that lack of validation can be used	to assert no-answer ( nil ) conditions .	59-77	59-77	Additionally , we show that lack of validation can be used to assert no-answer ( nil ) conditions .	Additionally , we show that lack of validation can be used to assert no-answer ( nil ) conditions .	1<2	none	enablement	enablement
P06-1135	1-9	78-88	This paper demonstrates a conceptually simple but effective method	We demonstrate increases of performance on TREC and other question-sets ,	This paper demonstrates a conceptually simple but effective method	We demonstrate increases of performance on TREC and other question-sets ,	1-20	78-106	This paper demonstrates a conceptually simple but effective method of increasing the accuracy of QA systems on factoid-style questions .	We demonstrate increases of performance on TREC and other question-sets , and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours .	1<2	none	evaluation	evaluation
P06-1135	78-88	89-95	We demonstrate increases of performance on TREC and other question-sets ,	and discuss the kinds of future activities	We demonstrate increases of performance on TREC and other question-sets ,	and discuss the kinds of future activities	78-106	78-106	We demonstrate increases of performance on TREC and other question-sets , and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours .	We demonstrate increases of performance on TREC and other question-sets , and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours .	1<2	none	progression	progression
P06-1135	89-95	96-106	and discuss the kinds of future activities	that can be particularly beneficial to approaches such as ours .	and discuss the kinds of future activities	that can be particularly beneficial to approaches such as ours .	78-106	78-106	We demonstrate increases of performance on TREC and other question-sets , and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours .	We demonstrate increases of performance on TREC and other question-sets , and discuss the kinds of future activities that can be particularly beneficial to approaches such as ours .	1<2	none	elab-addition	elab-addition
P06-1136	1-10,15-30	58-67	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	this paper proposes a novel language model-based answer reranking method	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	this paper proposes a novel language model-based answer reranking method	1-30	53-86	Statistical ranking methods based on centroid vector ( profile ) extracted from external knowledge have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	1>2	none	bg-compare	bg-compare
P06-1136	1-10,15-30	11-14	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	extracted from external knowledge	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	extracted from external knowledge	1-30	1-30	Statistical ranking methods based on centroid vector ( profile ) extracted from external knowledge have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	Statistical ranking methods based on centroid vector ( profile ) extracted from external knowledge have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	1<2	none	elab-addition	elab-addition
P06-1136	1-10,15-30	31-46	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	In these approaches , terms in the centroid vector are treated as a bag of words	Statistical ranking methods based on centroid vector ( profile ) <*> have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	In these approaches , terms in the centroid vector are treated as a bag of words	1-30	31-52	Statistical ranking methods based on centroid vector ( profile ) extracted from external knowledge have become widely adopted in the top definitional QA systems in TREC 2003 and 2004 .	In these approaches , terms in the centroid vector are treated as a bag of words based on the independent assumption .	1<2	none	elab-addition	elab-addition
P06-1136	31-46	47-52	In these approaches , terms in the centroid vector are treated as a bag of words	based on the independent assumption .	In these approaches , terms in the centroid vector are treated as a bag of words	based on the independent assumption .	31-52	31-52	In these approaches , terms in the centroid vector are treated as a bag of words based on the independent assumption .	In these approaches , terms in the centroid vector are treated as a bag of words based on the independent assumption .	1<2	none	bg-general	bg-general
P06-1136	53-57	58-67	To relax this assumption ,	this paper proposes a novel language model-based answer reranking method	To relax this assumption ,	this paper proposes a novel language model-based answer reranking method	53-86	53-86	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	1>2	none	enablement	enablement
P06-1136	58-67	68-74	this paper proposes a novel language model-based answer reranking method	to improve the existing bag-ofwords model approach	this paper proposes a novel language model-based answer reranking method	to improve the existing bag-ofwords model approach	53-86	53-86	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	1<2	none	enablement	enablement
P06-1136	68-74	75-86	to improve the existing bag-ofwords model approach	by considering the dependence of the words in the centroid vector .	to improve the existing bag-ofwords model approach	by considering the dependence of the words in the centroid vector .	53-86	53-86	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	1<2	none	manner-means	manner-means
P06-1136	87-90	107-137	Experiments have been conducted	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	Experiments have been conducted	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	87-97	98-137	Experiments have been conducted to evaluate the different dependence models .	The results on the TREC 2003 test set show that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	1>2	none	manner-means	manner-means
P06-1136	87-90	91-97	Experiments have been conducted	to evaluate the different dependence models .	Experiments have been conducted	to evaluate the different dependence models .	87-97	87-97	Experiments have been conducted to evaluate the different dependence models .	Experiments have been conducted to evaluate the different dependence models .	1<2	none	enablement	enablement
P06-1136	98-106	107-137	The results on the TREC 2003 test set show	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	The results on the TREC 2003 test set show	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	98-137	98-137	The results on the TREC 2003 test set show that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	The results on the TREC 2003 test set show that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	1>2	none	attribution	attribution
P06-1136	58-67	107-137	this paper proposes a novel language model-based answer reranking method	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	this paper proposes a novel language model-based answer reranking method	that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	53-86	98-137	To relax this assumption , this paper proposes a novel language model-based answer reranking method to improve the existing bag-ofwords model approach by considering the dependence of the words in the centroid vector .	The results on the TREC 2003 test set show that the reranking approach with biterm language model , significantly outperforms the one with the bag-ofwords model and unigram language model by 14.9 % and 12.5 % respectively in F-Measure .	1<2	none	evaluation	evaluation
P06-1137	1-9	48-55	Unification grammars are widely accepted as an expressive means	We present two natural constraints on unification grammars	Unification grammars are widely accepted as an expressive means	We present two natural constraints on unification grammars	1-17	48-60	Unification grammars are widely accepted as an expressive means for describing the structure of natural languages .	We present two natural constraints on unification grammars which limit their expressivity .	1>2	none	bg-goal	bg-goal
P06-1137	1-9	10-17	Unification grammars are widely accepted as an expressive means	for describing the structure of natural languages .	Unification grammars are widely accepted as an expressive means	for describing the structure of natural languages .	1-17	1-17	Unification grammars are widely accepted as an expressive means for describing the structure of natural languages .	Unification grammars are widely accepted as an expressive means for describing the structure of natural languages .	1<2	none	enablement	enablement
P06-1137	1-9	18-29	Unification grammars are widely accepted as an expressive means	In general , the recognition problem is undecidable for unification grammars .	Unification grammars are widely accepted as an expressive means	In general , the recognition problem is undecidable for unification grammars .	1-17	18-29	Unification grammars are widely accepted as an expressive means for describing the structure of natural languages .	In general , the recognition problem is undecidable for unification grammars .	1<2	none	elab-addition	elab-addition
P06-1137	30-41	42-47	Even with restricted variants of the formalism , offline parsable grammars ,	the problem is computationally hard .	Even with restricted variants of the formalism , offline parsable grammars ,	the problem is computationally hard .	30-47	30-47	Even with restricted variants of the formalism , offline parsable grammars , the problem is computationally hard .	Even with restricted variants of the formalism , offline parsable grammars , the problem is computationally hard .	1>2	none	contrast	contrast
P06-1137	18-29	42-47	In general , the recognition problem is undecidable for unification grammars .	the problem is computationally hard .	In general , the recognition problem is undecidable for unification grammars .	the problem is computationally hard .	18-29	30-47	In general , the recognition problem is undecidable for unification grammars .	Even with restricted variants of the formalism , offline parsable grammars , the problem is computationally hard .	1<2	none	exp-reason	exp-reason
P06-1137	48-55	56-60	We present two natural constraints on unification grammars	which limit their expressivity .	We present two natural constraints on unification grammars	which limit their expressivity .	48-60	48-60	We present two natural constraints on unification grammars which limit their expressivity .	We present two natural constraints on unification grammars which limit their expressivity .	1<2	none	elab-addition	elab-addition
P06-1137	61-63	64-75	We first show	that non-reentrant unification grammars generate exactly the class of contextfree languages .	We first show	that non-reentrant unification grammars generate exactly the class of contextfree languages .	61-75	61-75	We first show that non-reentrant unification grammars generate exactly the class of contextfree languages .	We first show that non-reentrant unification grammars generate exactly the class of contextfree languages .	1>2	none	attribution	attribution
P06-1137	48-55	64-75	We present two natural constraints on unification grammars	that non-reentrant unification grammars generate exactly the class of contextfree languages .	We present two natural constraints on unification grammars	that non-reentrant unification grammars generate exactly the class of contextfree languages .	48-60	61-75	We present two natural constraints on unification grammars which limit their expressivity .	We first show that non-reentrant unification grammars generate exactly the class of contextfree languages .	1<2	none	elab-addition	elab-addition
P06-1137	48-55	76-80	We present two natural constraints on unification grammars	We then relax the constraint	We present two natural constraints on unification grammars	We then relax the constraint	48-60	76-94	We present two natural constraints on unification grammars which limit their expressivity .	We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	1<2	none	elab-addition	elab-addition
P06-1137	81-82	83-94	and show	that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	and show	that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	76-94	76-94	We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	1>2	none	attribution	attribution
P06-1137	76-80	83-94	We then relax the constraint	that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	We then relax the constraint	that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	76-94	76-94	We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	We then relax the constraint and show that one-reentrant unification grammars generate exactly the class of tree-adjoining languages .	1<2	none	result	result
P06-1137	48-55	95-117	We present two natural constraints on unification grammars	We thus relate the commonly used and linguistically motivated formalism of unification grammars to more restricted , computationally tractable classes of languages .	We present two natural constraints on unification grammars	We thus relate the commonly used and linguistically motivated formalism of unification grammars to more restricted , computationally tractable classes of languages .	48-60	95-117	We present two natural constraints on unification grammars which limit their expressivity .	We thus relate the commonly used and linguistically motivated formalism of unification grammars to more restricted , computationally tractable classes of languages .	1<2	none	progression	progression
P06-1138	1-12	13-22	This paper describes a minimal topology driven parsing algorithm for topological grammars	that synchronizes a rewriting grammar and a dependency grammar ,	This paper describes a minimal topology driven parsing algorithm for topological grammars	that synchronizes a rewriting grammar and a dependency grammar ,	1-29	1-29	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	1<2	none	elab-addition	elab-addition
P06-1138	13-22	23-29	that synchronizes a rewriting grammar and a dependency grammar ,	obtaining two linguistically motivated syntactic structures .	that synchronizes a rewriting grammar and a dependency grammar ,	obtaining two linguistically motivated syntactic structures .	1-29	1-29	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	1<2	none	elab-addition	elab-addition
P06-1138	1-12	30-40	This paper describes a minimal topology driven parsing algorithm for topological grammars	The use of non-local slash and visitor features can be restricted	This paper describes a minimal topology driven parsing algorithm for topological grammars	The use of non-local slash and visitor features can be restricted	1-29	30-50	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	The use of non-local slash and visitor features can be restricted to obtain a CKY type analysis in polynomial time .	1<2	none	elab-aspect	elab-aspect
P06-1138	30-40	41-50	The use of non-local slash and visitor features can be restricted	to obtain a CKY type analysis in polynomial time .	The use of non-local slash and visitor features can be restricted	to obtain a CKY type analysis in polynomial time .	30-50	30-50	The use of non-local slash and visitor features can be restricted to obtain a CKY type analysis in polynomial time .	The use of non-local slash and visitor features can be restricted to obtain a CKY type analysis in polynomial time .	1<2	none	enablement	enablement
P06-1138	1-12	51-58	This paper describes a minimal topology driven parsing algorithm for topological grammars	German long distance phenomena illustrate the algorithm ,	This paper describes a minimal topology driven parsing algorithm for topological grammars	German long distance phenomena illustrate the algorithm ,	1-29	51-80	This paper describes a minimal topology driven parsing algorithm for topological grammars that synchronizes a rewriting grammar and a dependency grammar , obtaining two linguistically motivated syntactic structures .	German long distance phenomena illustrate the algorithm , bringing to the fore the procedural needs of the analyses of syntax-topology mismatches in constraint based approaches like for example HPSG .	1<2	none	elab-aspect	elab-aspect
P06-1138	51-58	59-80	German long distance phenomena illustrate the algorithm ,	bringing to the fore the procedural needs of the analyses of syntax-topology mismatches in constraint based approaches like for example HPSG .	German long distance phenomena illustrate the algorithm ,	bringing to the fore the procedural needs of the analyses of syntax-topology mismatches in constraint based approaches like for example HPSG .	51-80	51-80	German long distance phenomena illustrate the algorithm , bringing to the fore the procedural needs of the analyses of syntax-topology mismatches in constraint based approaches like for example HPSG .	German long distance phenomena illustrate the algorithm , bringing to the fore the procedural needs of the analyses of syntax-topology mismatches in constraint based approaches like for example HPSG .	1<2	none	elab-addition	elab-addition
P06-1139	1-7	8-23	We propose WIDL-expressions as a flexible formalism	that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	We propose WIDL-expressions as a flexible formalism	that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	1-23	1-23	We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	1<2	none	elab-addition	elab-addition
P06-1139	1-7	24-35	We propose WIDL-expressions as a flexible formalism	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations ,	We propose WIDL-expressions as a flexible formalism	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations ,	1-23	24-49	We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations , and have optimal algorithms for realization via interpolation with language model probability distributions .	1<2	none	elab-addition	elab-addition
P06-1139	24-35	36-49	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations ,	and have optimal algorithms for realization via interpolation with language model probability distributions .	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations ,	and have optimal algorithms for realization via interpolation with language model probability distributions .	24-49	24-49	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations , and have optimal algorithms for realization via interpolation with language model probability distributions .	WIDL-expressions represent compactly probability distributions over finite sets of candidate realizations , and have optimal algorithms for realization via interpolation with language model probability distributions .	1<2	none	joint	joint
P06-1139	1-7	50-64	We propose WIDL-expressions as a flexible formalism	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks :	We propose WIDL-expressions as a flexible formalism	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks :	1-23	50-70	We propose WIDL-expressions as a flexible formalism that facilitates the integration of a generic sentence realization system within end-to-end language processing applications .	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks : automatic translation and headline generation .	1<2	none	evaluation	evaluation
P06-1139	50-64	65-70	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks :	automatic translation and headline generation .	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks :	automatic translation and headline generation .	50-70	50-70	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks : automatic translation and headline generation .	We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks : automatic translation and headline generation .	1<2	none	elab-enumember	elab-enumember
P06-1140	1-5	6-20	This paper presents a method	for adapting a language generator to the strengths and weaknesses of a synthetic voice ,	This paper presents a method	for adapting a language generator to the strengths and weaknesses of a synthetic voice ,	1-34	1-34	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	1<2	none	elab-addition	elab-addition
P06-1140	6-20	21-34	for adapting a language generator to the strengths and weaknesses of a synthetic voice ,	thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	for adapting a language generator to the strengths and weaknesses of a synthetic voice ,	thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	1-34	1-34	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	1<2	none	enablement	enablement
P06-1140	1-5	35-40	This paper presents a method	The method trains a discriminative reranker	This paper presents a method	The method trains a discriminative reranker	1-34	35-52	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	1<2	none	manner-means	manner-means
P06-1140	35-40	41-43	The method trains a discriminative reranker	to select paraphrases	The method trains a discriminative reranker	to select paraphrases	35-52	35-52	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	1<2	none	enablement	enablement
P06-1140	41-43	44-49	to select paraphrases	that are predicted to sound natural	to select paraphrases	that are predicted to sound natural	35-52	35-52	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	1<2	none	elab-addition	elab-addition
P06-1140	44-49	50-52	that are predicted to sound natural	when synthesized .	that are predicted to sound natural	when synthesized .	35-52	35-52	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	1<2	none	temporal	temporal
P06-1140	35-40	53-65	The method trains a discriminative reranker	The ranker is trained on realizer and synthesizer features in supervised fashion ,	The method trains a discriminative reranker	The ranker is trained on realizer and synthesizer features in supervised fashion ,	35-52	53-84	The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized .	The ranker is trained on realizer and synthesizer features in supervised fashion , using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability .	1<2	none	elab-addition	elab-addition
P06-1140	53-65	66-84	The ranker is trained on realizer and synthesizer features in supervised fashion ,	using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability .	The ranker is trained on realizer and synthesizer features in supervised fashion ,	using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability .	53-84	53-84	The ranker is trained on realizer and synthesizer features in supervised fashion , using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability .	The ranker is trained on realizer and synthesizer features in supervised fashion , using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator's capability .	1<2	none	manner-means	manner-means
P06-1140	85-90	91-103	Results from a cross-validation study indicate	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	Results from a cross-validation study indicate	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	85-119	85-119	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	1>2	none	attribution	attribution
P06-1140	1-5	91-103	This paper presents a method	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	This paper presents a method	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	1-34	85-119	This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system .	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	1<2	none	evaluation	evaluation
P06-1140	91-103	104-111	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	ameliorating the problem of highly variable synthesis quality	that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average ,	ameliorating the problem of highly variable synthesis quality	85-119	85-119	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	1<2	none	enablement	enablement
P06-1140	104-111	112-119	ameliorating the problem of highly variable synthesis quality	typically encountered with today's unit selection synthesizers .	ameliorating the problem of highly variable synthesis quality	typically encountered with today's unit selection synthesizers .	85-119	85-119	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with today's unit selection synthesizers .	1<2	none	elab-addition	elab-addition
P06-1141	1-3	4-8,20-23	This paper shows	that a simple two-stage approach <*> can outperform existing approaches	This paper shows	that a simple two-stage approach <*> can outperform existing approaches	1-35	1-35	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	1>2	none	attribution	attribution
P06-1141	4-8,20-23	9-19	that a simple two-stage approach <*> can outperform existing approaches	to handle non-local dependencies in Named Entity Recognition ( NER )	that a simple two-stage approach <*> can outperform existing approaches	to handle non-local dependencies in Named Entity Recognition ( NER )	1-35	1-35	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	1<2	none	elab-addition	elab-addition
P06-1141	20-23	24-28	can outperform existing approaches	that handle non-local dependencies ,	can outperform existing approaches	that handle non-local dependencies ,	1-35	1-35	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	1<2	none	elab-addition	elab-addition
P06-1141	20-23	29-35	can outperform existing approaches	while being much more computationally efficient .	can outperform existing approaches	while being much more computationally efficient .	1-35	1-35	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	1<2	none	temporal	temporal
P06-1141	4-8,20-23	36-45	that a simple two-stage approach <*> can outperform existing approaches	NER systems typically use sequence models for tractable inference ,	that a simple two-stage approach <*> can outperform existing approaches	NER systems typically use sequence models for tractable inference ,	1-35	36-60	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	NER systems typically use sequence models for tractable inference , but this makes them unable to capture the long distance structure present in text .	1<2	none	bg-compare	bg-compare
P06-1141	36-45	46-60	NER systems typically use sequence models for tractable inference ,	but this makes them unable to capture the long distance structure present in text .	NER systems typically use sequence models for tractable inference ,	but this makes them unable to capture the long distance structure present in text .	36-60	36-60	NER systems typically use sequence models for tractable inference , but this makes them unable to capture the long distance structure present in text .	NER systems typically use sequence models for tractable inference , but this makes them unable to capture the long distance structure present in text .	1<2	none	contrast	contrast
P06-1141	4-8,20-23	61-72	that a simple two-stage approach <*> can outperform existing approaches	We use a Conditional Random Field ( CRF ) based NER system	that a simple two-stage approach <*> can outperform existing approaches	We use a Conditional Random Field ( CRF ) based NER system	1-35	61-99	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	elab-addition	elab-addition
P06-1141	61-72	73-75	We use a Conditional Random Field ( CRF ) based NER system	using local features	We use a Conditional Random Field ( CRF ) based NER system	using local features	61-99	61-99	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	elab-addition	elab-addition
P06-1141	61-72	76-78	We use a Conditional Random Field ( CRF ) based NER system	to make predictions	We use a Conditional Random Field ( CRF ) based NER system	to make predictions	61-99	61-99	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	enablement	enablement
P06-1141	61-72	79-83	We use a Conditional Random Field ( CRF ) based NER system	and then train another CRF	We use a Conditional Random Field ( CRF ) based NER system	and then train another CRF	61-99	61-99	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	joint	joint
P06-1141	79-83	84-90	and then train another CRF	which uses both local information and features	and then train another CRF	which uses both local information and features	61-99	61-99	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	elab-addition	elab-addition
P06-1141	84-90	91-99	which uses both local information and features	extracted from the output of the first CRF .	which uses both local information and features	extracted from the output of the first CRF .	61-99	61-99	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	We use a Conditional Random Field ( CRF ) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF .	1<2	none	elab-addition	elab-addition
P06-1141	100-101	110-127	Using features	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	Using features	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1>2	none	manner-means	manner-means
P06-1141	100-101	102-109	Using features	capturing non-local dependencies from the same document ,	Using features	capturing non-local dependencies from the same document ,	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	elab-addition	elab-addition
P06-1141	4-8,20-23	110-127	that a simple two-stage approach <*> can outperform existing approaches	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	that a simple two-stage approach <*> can outperform existing approaches	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	1-35	100-150	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	evaluation	evaluation
P06-1141	110-127	128-131	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	using local-information alone ,	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	using local-information alone ,	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	elab-addition	elab-addition
P06-1141	110-127	132-140	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	when compared to the 9.3 % relative error reduction	our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems	when compared to the 9.3 % relative error reduction	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	comparison	comparison
P06-1141	132-140	141-145	when compared to the 9.3 % relative error reduction	offered by the best systems	when compared to the 9.3 % relative error reduction	offered by the best systems	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	elab-addition	elab-addition
P06-1141	141-145	146-150	offered by the best systems	that exploit non-local information .	offered by the best systems	that exploit non-local information .	100-150	100-150	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	Using features capturing non-local dependencies from the same document , our approach yields a 12.6 % relative error reduction on the F1 score , over state-of-theart NER systems using local-information alone , when compared to the 9.3 % relative error reduction offered by the best systems that exploit non-local information .	1<2	none	elab-addition	elab-addition
P06-1141	4-8,20-23	151-168	that a simple two-stage approach <*> can outperform existing approaches	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus ,	that a simple two-stage approach <*> can outperform existing approaches	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus ,	1-35	151-184	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over NER systems using local-information alone .	1<2	none	evaluation	evaluation
P06-1141	151-168	169-180	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus ,	and this gives us a 13.3 % error reduction over NER systems	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus ,	and this gives us a 13.3 % error reduction over NER systems	151-184	151-184	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over NER systems using local-information alone .	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over NER systems using local-information alone .	1<2	none	exp-evidence	exp-evidence
P06-1141	169-180	181-184	and this gives us a 13.3 % error reduction over NER systems	using local-information alone .	and this gives us a 13.3 % error reduction over NER systems	using local-information alone .	151-184	151-184	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over NER systems using local-information alone .	Our approach also makes it easy to incorporate non-local information from other documents in the test corpus , and this gives us a 13.3 % error reduction over NER systems using local-information alone .	1<2	none	elab-addition	elab-addition
P06-1141	4-8,20-23	185-201	that a simple two-stage approach <*> can outperform existing approaches	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	that a simple two-stage approach <*> can outperform existing approaches	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	1-35	185-222	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition ( NER ) can outperform existing approaches that handle non-local dependencies , while being much more computationally efficient .	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	1<2	none	evaluation	evaluation
P06-1141	185-201	202-212	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	which is much less than that of other more complicated approaches	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	which is much less than that of other more complicated approaches	185-222	185-222	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	1<2	none	elab-addition	elab-addition
P06-1141	202-212	213-217	which is much less than that of other more complicated approaches	that directly model the dependencies	which is much less than that of other more complicated approaches	that directly model the dependencies	185-222	185-222	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	1<2	none	elab-addition	elab-addition
P06-1141	185-201	218-222	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	and do approximate inference .	Additionally , our running time for inference is just the inference time of two sequential CRFs ,	and do approximate inference .	185-222	185-222	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	Additionally , our running time for inference is just the inference time of two sequential CRFs , which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference .	1<2	none	joint	joint
P06-1142	1-14	15-23	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	that supports the automatic construction of transliteration lexicons .	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	that supports the automatic construction of transliteration lexicons .	1-23	1-23	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM ) that supports the automatic construction of transliteration lexicons .	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM ) that supports the automatic construction of transliteration lexicons .	1<2	none	elab-addition	elab-addition
P06-1142	1-14	24-35	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	The learning algorithm starts with minimum prior knowledge about machine transliteration ,	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	The learning algorithm starts with minimum prior knowledge about machine transliteration ,	1-23	24-43	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM ) that supports the automatic construction of transliteration lexicons .	The learning algorithm starts with minimum prior knowledge about machine transliteration , and acquires knowledge iteratively from the Web .	1<2	none	elab-addition	elab-addition
P06-1142	24-35	36-43	The learning algorithm starts with minimum prior knowledge about machine transliteration ,	and acquires knowledge iteratively from the Web .	The learning algorithm starts with minimum prior knowledge about machine transliteration ,	and acquires knowledge iteratively from the Web .	24-43	24-43	The learning algorithm starts with minimum prior knowledge about machine transliteration , and acquires knowledge iteratively from the Web .	The learning algorithm starts with minimum prior knowledge about machine transliteration , and acquires knowledge iteratively from the Web .	1<2	none	joint	joint
P06-1142	1-14	44-53	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	We study the active learning and the unsupervised learning strategies	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	We study the active learning and the unsupervised learning strategies	1-23	44-63	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM ) that supports the automatic construction of transliteration lexicons .	We study the active learning and the unsupervised learning strategies that minimize human supervision in terms of data labeling .	1<2	none	manner-means	manner-means
P06-1142	44-53	54-63	We study the active learning and the unsupervised learning strategies	that minimize human supervision in terms of data labeling .	We study the active learning and the unsupervised learning strategies	that minimize human supervision in terms of data labeling .	44-63	44-63	We study the active learning and the unsupervised learning strategies that minimize human supervision in terms of data labeling .	We study the active learning and the unsupervised learning strategies that minimize human supervision in terms of data labeling .	1<2	none	elab-addition	elab-addition
P06-1142	44-53	64-69	We study the active learning and the unsupervised learning strategies	The learning process refines the PSM	We study the active learning and the unsupervised learning strategies	The learning process refines the PSM	44-63	64-79	We study the active learning and the unsupervised learning strategies that minimize human supervision in terms of data labeling .	The learning process refines the PSM and constructs a transliteration lexicon at the same time .	1<2	none	result	result
P06-1142	64-69	70-79	The learning process refines the PSM	and constructs a transliteration lexicon at the same time .	The learning process refines the PSM	and constructs a transliteration lexicon at the same time .	64-79	64-79	The learning process refines the PSM and constructs a transliteration lexicon at the same time .	The learning process refines the PSM and constructs a transliteration lexicon at the same time .	1<2	none	joint	joint
P06-1142	1-14	80-95	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments ,	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM )	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments ,	1-23	80-109	This paper presents an adaptive learning framework for Phonetic Similarity Modeling ( PSM ) that supports the automatic construction of transliteration lexicons .	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments , which show that the proposed framework is reliably effective on two independent databases .	1<2	none	evaluation	evaluation
P06-1142	96-97	98-109	which show	that the proposed framework is reliably effective on two independent databases .	which show	that the proposed framework is reliably effective on two independent databases .	80-109	80-109	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments , which show that the proposed framework is reliably effective on two independent databases .	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments , which show that the proposed framework is reliably effective on two independent databases .	1>2	none	attribution	attribution
P06-1142	80-95	98-109	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments ,	that the proposed framework is reliably effective on two independent databases .	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments ,	that the proposed framework is reliably effective on two independent databases .	80-109	80-109	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments , which show that the proposed framework is reliably effective on two independent databases .	We evaluate the proposed PSM and its learning algorithm through a series of systematic experiments , which show that the proposed framework is reliably effective on two independent databases .	1<2	none	elab-addition	elab-addition
P06-1143	1-7,12-19	37-49	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	1-19	37-95	Machine Transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	1>2	none	elab-addition	elab-addition
P06-1143	1-7,12-19	8-11	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	written in a script	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	written in a script	1-19	1-19	Machine Transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language .	Machine Transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language .	1<2	none	elab-addition	elab-addition
P06-1143	1-7,12-19	20-36	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	It is useful for machine translation , cross-lingual information retrieval , multilingual text and speech processing .	Machine Transliteration is to transcribe a word <*> with approximate phonetic equivalence in another language .	It is useful for machine translation , cross-lingual information retrieval , multilingual text and speech processing .	1-19	20-36	Machine Transliteration is to transcribe a word written in a script with approximate phonetic equivalence in another language .	It is useful for machine translation , cross-lingual information retrieval , multilingual text and speech processing .	1<2	none	elab-addition	elab-addition
P06-1143	37-49	50-59,66-95	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	and is a process of converting a word from Shahmukhi <*> to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	and is a process of converting a word from Shahmukhi <*> to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	37-95	37-95	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	1<2	none	progression	progression
P06-1143	50-59,66-95	60-65	and is a process of converting a word from Shahmukhi <*> to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	( based on Arabic script )	and is a process of converting a word from Shahmukhi <*> to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	( based on Arabic script )	37-95	37-95	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	1<2	none	bg-general	bg-general
P06-1143	37-49	96-118	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	37-95	96-118	Punjabi Machine Transliteration ( PMT ) is a special case of machine transliteration and is a process of converting a word from Shahmukhi ( based on Arabic script ) to Gurmukhi ( derivation of Landa , Shardha and Takri , old scripts of Indian subcontinent ) , two scripts of Punjabi , irrespective of the type of word .	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	1<2	none	manner-means	manner-means
P06-1143	96-118	119-125	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	The PMT system can transliterate every word	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	The PMT system can transliterate every word	96-118	119-129	The Punjabi Machine Transliteration System uses transliteration rules ( character mappings and dependency rules ) for transliteration of Shahmukhi words into Gurmukhi .	The PMT system can transliterate every word written in Shahmukhi .	1<2	none	elab-addition	elab-addition
P06-1143	119-125	126-129	The PMT system can transliterate every word	written in Shahmukhi .	The PMT system can transliterate every word	written in Shahmukhi .	119-129	119-129	The PMT system can transliterate every word written in Shahmukhi .	The PMT system can transliterate every word written in Shahmukhi .	1<2	none	elab-addition	elab-addition
P06-1144	1-13	14-19	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The algorithm is of heuristic nature	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The algorithm is of heuristic nature	1-13	14-41	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora .	1<2	none	elab-addition	elab-addition
P06-1144	14-19	20-25	The algorithm is of heuristic nature	and it uses as unique evidence	The algorithm is of heuristic nature	and it uses as unique evidence	14-41	14-41	The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora .	The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora .	1<2	none	joint	joint
P06-1144	20-25	26-41	and it uses as unique evidence	for clustering the identification of cognate named entities between both sides of the comparable corpora .	and it uses as unique evidence	for clustering the identification of cognate named entities between both sides of the comparable corpora .	14-41	14-41	The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora .	The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora .	1<2	none	enablement	enablement
P06-1144	1-13	42-61	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	1-13	42-61	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	1<2	none	elab-addition	elab-addition
P06-1144	42-61	62-68	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	However , it depends on the possibility	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	However , it depends on the possibility	42-61	62-81	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	However , it depends on the possibility of identifying cognate named entities between the languages used in the corpus .	1<2	none	contrast	contrast
P06-1144	62-68	69-76	However , it depends on the possibility	of identifying cognate named entities between the languages	However , it depends on the possibility	of identifying cognate named entities between the languages	62-81	62-81	However , it depends on the possibility of identifying cognate named entities between the languages used in the corpus .	However , it depends on the possibility of identifying cognate named entities between the languages used in the corpus .	1<2	none	elab-addition	elab-addition
P06-1144	69-76	77-81	of identifying cognate named entities between the languages	used in the corpus .	of identifying cognate named entities between the languages	used in the corpus .	62-81	62-81	However , it depends on the possibility of identifying cognate named entities between the languages used in the corpus .	However , it depends on the possibility of identifying cognate named entities between the languages used in the corpus .	1<2	none	elab-addition	elab-addition
P06-1144	42-61	82-102	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	An additional advantage of the approach is that it does not need any information about the right number of clusters ;	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	An additional advantage of the approach is that it does not need any information about the right number of clusters ;	42-61	82-107	One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources .	An additional advantage of the approach is that it does not need any information about the right number of clusters ; the algorithm calculates it .	1<2	none	progression	progression
P06-1144	82-102	103-107	An additional advantage of the approach is that it does not need any information about the right number of clusters ;	the algorithm calculates it .	An additional advantage of the approach is that it does not need any information about the right number of clusters ;	the algorithm calculates it .	82-107	82-107	An additional advantage of the approach is that it does not need any information about the right number of clusters ; the algorithm calculates it .	An additional advantage of the approach is that it does not need any information about the right number of clusters ; the algorithm calculates it .	1<2	none	joint	joint
P06-1144	108-118	142-147	We have tested this approach with a comparable corpus of news	The obtained results are encouraging .	We have tested this approach with a comparable corpus of news	The obtained results are encouraging .	108-124	142-147	We have tested this approach with a comparable corpus of news written in English and Spanish .	The obtained results are encouraging .	1>2	none	manner-means	manner-means
P06-1144	108-118	119-124	We have tested this approach with a comparable corpus of news	written in English and Spanish .	We have tested this approach with a comparable corpus of news	written in English and Spanish .	108-124	108-124	We have tested this approach with a comparable corpus of news written in English and Spanish .	We have tested this approach with a comparable corpus of news written in English and Spanish .	1<2	none	elab-addition	elab-addition
P06-1144	125-135	142-147	In addition , we have compared the results with a system	The obtained results are encouraging .	In addition , we have compared the results with a system	The obtained results are encouraging .	125-141	142-147	In addition , we have compared the results with a system which translates selected document features .	The obtained results are encouraging .	1>2	none	manner-means	manner-means
P06-1144	125-135	136-141	In addition , we have compared the results with a system	which translates selected document features .	In addition , we have compared the results with a system	which translates selected document features .	125-141	125-141	In addition , we have compared the results with a system which translates selected document features .	In addition , we have compared the results with a system which translates selected document features .	1<2	none	elab-addition	elab-addition
P06-1144	1-13	142-147	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The obtained results are encouraging .	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The obtained results are encouraging .	1-13	142-147	This paper presents an approach for Multilingual Document Clustering in comparable corpora .	The obtained results are encouraging .	1<2	none	evaluation	evaluation
P06-1145	1-5	6-8,12-13	This study aims at identifying	when an event <*> occurs .	This study aims at identifying	when an event <*> occurs .	1-13	1-13	This study aims at identifying when an event written in text occurs .	This study aims at identifying when an event written in text occurs .	1>2	none	attribution	attribution
P06-1145	6-8,12-13	9-11	when an event <*> occurs .	written in text	when an event <*> occurs .	written in text	1-13	1-13	This study aims at identifying when an event written in text occurs .	This study aims at identifying when an event written in text occurs .	1<2	none	elab-addition	elab-addition
P06-1145	6-8,12-13	14-27	when an event <*> occurs .	In particular , we classify a sentence for an event into four time-slots ;	when an event <*> occurs .	In particular , we classify a sentence for an event into four time-slots ;	1-13	14-36	This study aims at identifying when an event written in text occurs .	In particular , we classify a sentence for an event into four time-slots ; morning , daytime , evening , and night .	1<2	none	elab-addition	elab-addition
P06-1145	14-27	28-36	In particular , we classify a sentence for an event into four time-slots ;	morning , daytime , evening , and night .	In particular , we classify a sentence for an event into four time-slots ;	morning , daytime , evening , and night .	14-36	14-36	In particular , we classify a sentence for an event into four time-slots ; morning , daytime , evening , and night .	In particular , we classify a sentence for an event into four time-slots ; morning , daytime , evening , and night .	1<2	none	elab-enumember	elab-enumember
P06-1145	37-41	42-45	To realize our goal ,	we focus on expressions	To realize our goal ,	we focus on expressions	37-53	37-53	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	1>2	none	enablement	enablement
P06-1145	6-8,12-13	42-45	when an event <*> occurs .	we focus on expressions	when an event <*> occurs .	we focus on expressions	1-13	37-53	This study aims at identifying when an event written in text occurs .	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	1<2	none	elab-addition	elab-addition
P06-1145	42-45	46-53	we focus on expressions	associated with time-slot ( time-associated words ) .	we focus on expressions	associated with time-slot ( time-associated words ) .	37-53	37-53	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	1<2	none	elab-addition	elab-addition
P06-1145	54-64	72-83	However , listing up all the time-associated words is impractical ,	We therefore use a semi-supervised learning method , the Naive Bayes classifier	However , listing up all the time-associated words is impractical ,	We therefore use a semi-supervised learning method , the Naive Bayes classifier	54-71	72-103	However , listing up all the time-associated words is impractical , because there are numerous time-associated expressions .	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	1>2	none	cause	cause
P06-1145	54-64	65-71	However , listing up all the time-associated words is impractical ,	because there are numerous time-associated expressions .	However , listing up all the time-associated words is impractical ,	because there are numerous time-associated expressions .	54-71	54-71	However , listing up all the time-associated words is impractical , because there are numerous time-associated expressions .	However , listing up all the time-associated words is impractical , because there are numerous time-associated expressions .	1<2	none	exp-reason	exp-reason
P06-1145	42-45	72-83	we focus on expressions	We therefore use a semi-supervised learning method , the Naive Bayes classifier	we focus on expressions	We therefore use a semi-supervised learning method , the Naive Bayes classifier	37-53	72-103	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	1<2	none	manner-means	manner-means
P06-1145	72-83	84-91	We therefore use a semi-supervised learning method , the Naive Bayes classifier	backed up with the Expectation Maximization algorithm ,	We therefore use a semi-supervised learning method , the Naive Bayes classifier	backed up with the Expectation Maximization algorithm ,	72-103	72-103	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	1<2	none	elab-addition	elab-addition
P06-1145	72-83	92-98	We therefore use a semi-supervised learning method , the Naive Bayes classifier	in order to iteratively extract time-associated words	We therefore use a semi-supervised learning method , the Naive Bayes classifier	in order to iteratively extract time-associated words	72-103	72-103	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	1<2	none	enablement	enablement
P06-1145	92-98	99-103	in order to iteratively extract time-associated words	while improving the classifier .	in order to iteratively extract time-associated words	while improving the classifier .	72-103	72-103	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	We therefore use a semi-supervised learning method , the Naive Bayes classifier backed up with the Expectation Maximization algorithm , in order to iteratively extract time-associated words while improving the classifier .	1<2	none	temporal	temporal
P06-1145	42-45	104-111	we focus on expressions	We also propose to use Support Vector Machines	we focus on expressions	We also propose to use Support Vector Machines	37-53	104-123	To realize our goal , we focus on expressions associated with time-slot ( time-associated words ) .	We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period .	1<2	none	manner-means	manner-means
P06-1145	104-111	112-116	We also propose to use Support Vector Machines	to filter out noisy instances	We also propose to use Support Vector Machines	to filter out noisy instances	104-123	104-123	We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period .	We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period .	1<2	none	enablement	enablement
P06-1145	112-116	117-123	to filter out noisy instances	that indicates no specific time period .	to filter out noisy instances	that indicates no specific time period .	104-123	104-123	We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period .	We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period .	1<2	none	elab-addition	elab-addition
P06-1145	6-8,12-13	124-136	when an event <*> occurs .	As a result of experiments , the proposed method achieved 0.864 of accuracy	when an event <*> occurs .	As a result of experiments , the proposed method achieved 0.864 of accuracy	1-13	124-141	This study aims at identifying when an event written in text occurs .	As a result of experiments , the proposed method achieved 0.864 of accuracy and outperformed other methods .	1<2	none	evaluation	evaluation
P06-1145	124-136	137-141	As a result of experiments , the proposed method achieved 0.864 of accuracy	and outperformed other methods .	As a result of experiments , the proposed method achieved 0.864 of accuracy	and outperformed other methods .	124-141	124-141	As a result of experiments , the proposed method achieved 0.864 of accuracy and outperformed other methods .	As a result of experiments , the proposed method achieved 0.864 of accuracy and outperformed other methods .	1<2	none	progression	progression
P06-1146	1-5	6-19	Given a parallel corpus ,	semantic projection attempts to transfer semantic role annotations from one language to another ,	Given a parallel corpus ,	semantic projection attempts to transfer semantic role annotations from one language to another ,	1-25	1-25	Given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments .	Given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments .	1>2	none	condition	condition
P06-1146	6-19	26-34	semantic projection attempts to transfer semantic role annotations from one language to another ,	In this paper , we present an improved method	semantic projection attempts to transfer semantic role annotations from one language to another ,	In this paper , we present an improved method	1-25	26-48	Given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments .	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	1>2	none	bg-goal	bg-goal
P06-1146	6-19	20-25	semantic projection attempts to transfer semantic role annotations from one language to another ,	typically by exploiting word alignments .	semantic projection attempts to transfer semantic role annotations from one language to another ,	typically by exploiting word alignments .	1-25	1-25	Given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments .	Given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments .	1<2	none	elab-addition	elab-addition
P06-1146	26-34	35-41	In this paper , we present an improved method	for obtaining constituent alignments between parallel sentences	In this paper , we present an improved method	for obtaining constituent alignments between parallel sentences	26-48	26-48	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	1<2	none	elab-addition	elab-addition
P06-1146	35-41	42-48	for obtaining constituent alignments between parallel sentences	to guide the role projection task .	for obtaining constituent alignments between parallel sentences	to guide the role projection task .	26-48	26-48	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	1<2	none	enablement	enablement
P06-1146	26-34	49-53	In this paper , we present an improved method	Our extensions are twofold :	In this paper , we present an improved method	Our extensions are twofold :	26-48	49-97	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	1<2	none	elab-addition	elab-addition
P06-1146	49-53	54-70	Our extensions are twofold :	( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph ,	Our extensions are twofold :	( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph ,	49-97	49-97	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	1<2	none	elab-addition	elab-addition
P06-1146	54-70	71-81	( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph ,	which allows us to find a globally optimal solution efficiently ;	( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph ,	which allows us to find a globally optimal solution efficiently ;	49-97	49-97	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	1<2	none	elab-addition	elab-addition
P06-1146	49-53	82-92	Our extensions are twofold :	( b ) we propose tree pruning as a promising strategy	Our extensions are twofold :	( b ) we propose tree pruning as a promising strategy	49-97	49-97	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	1<2	none	elab-addition	elab-addition
P06-1146	82-92	93-97	( b ) we propose tree pruning as a promising strategy	for reducing alignment noise .	( b ) we propose tree pruning as a promising strategy	for reducing alignment noise .	49-97	49-97	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	Our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise .	1<2	none	elab-addition	elab-addition
P06-1146	26-34	98-110	In this paper , we present an improved method	Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models .	In this paper , we present an improved method	Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models .	26-48	98-110	In this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task .	Experimental results on an English-German parallel corpus demonstrate improvements over state-of-the-art models .	1<2	none	evaluation	evaluation
P06-1147	1-6	7-13	In this paper , we discuss	how to utilize the co-occurrence of answers	In this paper , we discuss	how to utilize the co-occurrence of answers	1-35	1-35	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	1>2	none	attribution	attribution
P06-1147	7-13	14-20	how to utilize the co-occurrence of answers	in building an automatic question answering system	how to utilize the co-occurrence of answers	in building an automatic question answering system	1-35	1-35	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	1<2	none	elab-addition	elab-addition
P06-1147	14-20	21-35	in building an automatic question answering system	that answers a series of questions on a specific topic in a batch mode .	in building an automatic question answering system	that answers a series of questions on a specific topic in a batch mode .	1-35	1-35	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	1<2	none	elab-addition	elab-addition
P06-1147	36-37	38-61	Experiments show	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	Experiments show	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	36-61	36-61	Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	1>2	none	attribution	attribution
P06-1147	7-13	38-61	how to utilize the co-occurrence of answers	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	how to utilize the co-occurrence of answers	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	1-35	36-61	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	1<2	none	evaluation	evaluation
P06-1147	38-61	62-73	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	This feature sometimes can't be easily utilized in an automatic QA system	that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	This feature sometimes can't be easily utilized in an automatic QA system	36-61	62-78	Experiments show that the answers to the many of the questions in the series usually have a high degree of co-occurrence in relevant document passages .	This feature sometimes can't be easily utilized in an automatic QA system which processes questions independently .	1<2	none	elab-addition	elab-addition
P06-1147	62-73	74-78	This feature sometimes can't be easily utilized in an automatic QA system	which processes questions independently .	This feature sometimes can't be easily utilized in an automatic QA system	which processes questions independently .	62-78	62-78	This feature sometimes can't be easily utilized in an automatic QA system which processes questions independently .	This feature sometimes can't be easily utilized in an automatic QA system which processes questions independently .	1<2	none	elab-addition	elab-addition
P06-1147	62-73	79-87	This feature sometimes can't be easily utilized in an automatic QA system	However it can be utilized in a QA system	This feature sometimes can't be easily utilized in an automatic QA system	However it can be utilized in a QA system	62-78	79-95	This feature sometimes can't be easily utilized in an automatic QA system which processes questions independently .	However it can be utilized in a QA system that processes questions in a batch mode .	1<2	none	contrast	contrast
P06-1147	79-87	88-95	However it can be utilized in a QA system	that processes questions in a batch mode .	However it can be utilized in a QA system	that processes questions in a batch mode .	79-95	79-95	However it can be utilized in a QA system that processes questions in a batch mode .	However it can be utilized in a QA system that processes questions in a batch mode .	1<2	none	elab-addition	elab-addition
P06-1147	7-13	96-105	how to utilize the co-occurrence of answers	We have used our pervious TREC QA system as baseline	how to utilize the co-occurrence of answers	We have used our pervious TREC QA system as baseline	1-35	96-123	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system .	1<2	none	manner-means	manner-means
P06-1147	96-105	106-116	We have used our pervious TREC QA system as baseline	and augmented it with new answer clustering and co-occurrence maximization components	We have used our pervious TREC QA system as baseline	and augmented it with new answer clustering and co-occurrence maximization components	96-123	96-123	We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system .	We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system .	1<2	none	progression	progression
P06-1147	106-116	117-123	and augmented it with new answer clustering and co-occurrence maximization components	to build the batch QA system .	and augmented it with new answer clustering and co-occurrence maximization components	to build the batch QA system .	96-123	96-123	We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system .	We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system .	1<2	none	enablement	enablement
P06-1147	124-127	128-131,137-147	The experiment results show	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	The experiment results show	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	124-147	124-147	The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system .	The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system .	1>2	none	attribution	attribution
P06-1147	7-13	128-131,137-147	how to utilize the co-occurrence of answers	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	how to utilize the co-occurrence of answers	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	1-35	124-147	In this paper , we discuss how to utilize the co-occurrence of answers in building an automatic question answering system that answers a series of questions on a specific topic in a batch mode .	The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system .	1<2	none	evaluation	evaluation
P06-1147	128-131,137-147	132-136	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	running under the batch mode	that the QA system <*> get significant performance improvement over our baseline TREC QA system .	running under the batch mode	124-147	124-147	The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system .	The experiment results show that the QA system running under the batch mode get significant performance improvement over our baseline TREC QA system .	1<2	none	elab-addition	elab-addition
P14-1000_anno1	1-3	19-26	Recent studies show	some students need ten times more learning content	Recent studies show	some students need ten times more learning content	1-33	1-33	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	1>2	none	attribution	attribution
P14-1000_anno1	4-18	19-26	that to achieve mastery of a topic by 95 % of the student population ,	some students need ten times more learning content	that to achieve mastery of a topic by 95 % of the student population ,	some students need ten times more learning content	1-33	1-33	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	1>2	none	enablement	enablement
P14-1000_anno1	19-26	42-49	some students need ten times more learning content	but the need for a highly differentiated content	some students need ten times more learning content	but the need for a highly differentiated content	1-33	34-59	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	1>2	none	bg-general	bg-general
P14-1000_anno1	19-26	27-33	some students need ten times more learning content	than is available in current curricula .	some students need ten times more learning content	than is available in current curricula .	1-33	1-33	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	Recent studies show that to achieve mastery of a topic by 95 % of the student population , some students need ten times more learning content than is available in current curricula .	1<2	none	comparison	comparison
P14-1000_anno1	34-41	42-49	At issue is not just increased volume ,	but the need for a highly differentiated content	At issue is not just increased volume ,	but the need for a highly differentiated content	34-59	34-59	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	1>2	none	progression	progression
P14-1000_anno1	42-49	65-70	but the need for a highly differentiated content	we have developed a generative platform	but the need for a highly differentiated content	we have developed a generative platform	34-59	60-82	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	1>2	none	bg-goal	bg-goal
P14-1000_anno1	42-49	50-59	but the need for a highly differentiated content	specialized to promote optimal learning for each unique learner .	but the need for a highly differentiated content	specialized to promote optimal learning for each unique learner .	34-59	34-59	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	At issue is not just increased volume , but the need for a highly differentiated content specialized to promote optimal learning for each unique learner .	1<2	none	elab-addition	elab-addition
P14-1000_anno1	60-64	65-70	To address this synthesis problem	we have developed a generative platform	To address this synthesis problem	we have developed a generative platform	60-82	60-82	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	1>2	none	enablement	enablement
P14-1000_anno1	65-70	71-75	we have developed a generative platform	capable of dynamically varying content	we have developed a generative platform	capable of dynamically varying content	60-82	60-82	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	1<2	none	elab-addition	elab-addition
P14-1000_anno1	65-70	76-82	we have developed a generative platform	based on the individual student needs .	we have developed a generative platform	based on the individual student needs .	60-82	60-82	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	1<2	none	bg-general	bg-general
P14-1000_anno1	65-70	83-104	we have developed a generative platform	This approach recently achieved 93 % mastery of a key algebra concept even for primary school students in three state-wide challenges .	we have developed a generative platform	This approach recently achieved 93 % mastery of a key algebra concept even for primary school students in three state-wide challenges .	60-82	83-104	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	This approach recently achieved 93 % mastery of a key algebra concept even for primary school students in three state-wide challenges .	1<2	none	evaluation	evaluation
P14-1000_anno1	65-70	105-116	we have developed a generative platform	In this talk I will describe our work on extending the platform	we have developed a generative platform	In this talk I will describe our work on extending the platform	60-82	105-157	To address this synthesis problem we have developed a generative platform capable of dynamically varying content based on the individual student needs .	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	1<2	none	progression	progression
P14-1000_anno1	105-116	117-130	In this talk I will describe our work on extending the platform	to enable students to solve all word problems in high-school within their preferred context	In this talk I will describe our work on extending the platform	to enable students to solve all word problems in high-school within their preferred context	105-157	105-157	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	1<2	none	enablement	enablement
P14-1000_anno1	117-130	131-140	to enable students to solve all word problems in high-school within their preferred context	( e.g. sci-fi , medieval , Harry Potter ) ,	to enable students to solve all word problems in high-school within their preferred context	( e.g. sci-fi , medieval , Harry Potter ) ,	105-157	105-157	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	1<2	none	elab-example	elab-example
P14-1000_anno1	117-130	141-157	to enable students to solve all word problems in high-school within their preferred context	as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	to enable students to solve all word problems in high-school within their preferred context	as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	105-157	105-157	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	In this talk I will describe our work on extending the platform to enable students to solve all word problems in high-school within their preferred context ( e.g. sci-fi , medieval , Harry Potter ) , as well as to automatically generate adaptive learning progressions for reading comprehension curricula in middle school .	1<2	none	joint	joint
P14-1001_anno1	1-9	10-18	We present a series of algorithms with theoretical guarantees	for learning accurate ensembles of several structured prediction rules	We present a series of algorithms with theoretical guarantees	for learning accurate ensembles of several structured prediction rules	1-26	1-26	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	1<2	none	enablement	enablement
P14-1001_anno1	10-18	19-26	for learning accurate ensembles of several structured prediction rules	for which no prior knowledge is as-sumed .	for learning accurate ensembles of several structured prediction rules	for which no prior knowledge is as-sumed .	1-26	1-26	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	1<2	none	elab-addition	elab-addition
P14-1001_anno1	1-9	27-35	We present a series of algorithms with theoretical guarantees	This includes a number of randomized and deterministic algorithms	We present a series of algorithms with theoretical guarantees	This includes a number of randomized and deterministic algorithms	1-26	27-63	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	This includes a number of randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones , and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	1<2	none	elab-enumember	elab-enumember
P14-1001_anno1	27-35	36-45	This includes a number of randomized and deterministic algorithms	devised by converting on-line learning algorithms to batch ones ,	This includes a number of randomized and deterministic algorithms	devised by converting on-line learning algorithms to batch ones ,	27-63	27-63	This includes a number of randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones , and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	This includes a number of randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones , and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	1<2	none	elab-addition	elab-addition
P14-1001_anno1	27-35	46-63	This includes a number of randomized and deterministic algorithms	and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	This includes a number of randomized and deterministic algorithms	and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	27-63	27-63	This includes a number of randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones , and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	This includes a number of randomized and deterministic algorithms devised by converting on-line learning algorithms to batch ones , and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels .	1<2	none	joint	joint
P14-1001_anno1	1-9	64-75	We present a series of algorithms with theoretical guarantees	We also report the results of extensive experiments with these algorithms .	We present a series of algorithms with theoretical guarantees	We also report the results of extensive experiments with these algorithms .	1-26	64-75	We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is as-sumed .	We also report the results of extensive experiments with these algorithms .	1<2	none	evaluation	evaluation
P14-1002_anno1	1-7	26-36	Text-level discourse parsing is notoriously difficult ,	In this paper , we present a representation learning approach ,	Text-level discourse parsing is notoriously difficult ,	In this paper , we present a representation learning approach ,	1-25	26-52	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	1>2	none	bg-goal	bg-goal
P14-1002_anno1	1-7	8-16	Text-level discourse parsing is notoriously difficult ,	as distinctions between discourse relations require subtle semantic judgments	Text-level discourse parsing is notoriously difficult ,	as distinctions between discourse relations require subtle semantic judgments	1-25	1-25	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	1<2	none	exp-reason	exp-reason
P14-1002_anno1	8-16	17-21	as distinctions between discourse relations require subtle semantic judgments	that are not easily captured	as distinctions between discourse relations require subtle semantic judgments	that are not easily captured	1-25	1-25	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	1<2	none	elab-addition	elab-addition
P14-1002_anno1	17-21	22-25	that are not easily captured	using standard features .	that are not easily captured	using standard features .	1-25	1-25	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	Text-level discourse parsing is notoriously difficult , as distinctions between discourse relations require subtle semantic judgments that are not easily captured using standard features .	1<2	none	manner-means	manner-means
P14-1002_anno1	26-36	37-46	In this paper , we present a representation learning approach ,	in which we transform surface features into a latent space	In this paper , we present a representation learning approach ,	in which we transform surface features into a latent space	26-52	26-52	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	1<2	none	elab-addition	elab-addition
P14-1002_anno1	37-46	47-52	in which we transform surface features into a latent space	that facilitates RST discourse parsing .	in which we transform surface features into a latent space	that facilitates RST discourse parsing .	26-52	26-52	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	1<2	none	elab-addition	elab-addition
P14-1002_anno1	53-65	66-72	By combining the machinery of large-margin transition-based structured prediction with representation learning ,	our method jointly learns to parse discourse	By combining the machinery of large-margin transition-based structured prediction with representation learning ,	our method jointly learns to parse discourse	53-85	53-85	By combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features .	By combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features .	1>2	none	manner-means	manner-means
P14-1002_anno1	26-36	66-72	In this paper , we present a representation learning approach ,	our method jointly learns to parse discourse	In this paper , we present a representation learning approach ,	our method jointly learns to parse discourse	26-52	53-85	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	By combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features .	1<2	none	elab-addition	elab-addition
P14-1002_anno1	66-72	73-85	our method jointly learns to parse discourse	while at the same time learning a discourse-driven projection of surface features .	our method jointly learns to parse discourse	while at the same time learning a discourse-driven projection of surface features .	53-85	53-85	By combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features .	By combining the machinery of large-margin transition-based structured prediction with representation learning , our method jointly learns to parse discourse while at the same time learning a discourse-driven projection of surface features .	1<2	none	joint	joint
P14-1002_anno1	26-36	86-97	In this paper , we present a representation learning approach ,	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art	In this paper , we present a representation learning approach ,	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art	26-52	86-107	In this paper , we present a representation learning approach , in which we transform surface features into a latent space that facilitates RST discourse parsing .	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank .	1<2	none	evaluation	evaluation
P14-1002_anno1	86-97	98-107	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art	in predicting relations and nuclearity on the RST Treebank .	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art	in predicting relations and nuclearity on the RST Treebank .	86-107	86-107	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank .	The resulting shift-reduce discourse parser obtains substantial improvements over the previous state-of-the-art in predicting relations and nuclearity on the RST Treebank .	1<2	none	elab-addition	elab-addition
P14-1003_anno1	1-12	23-35	Previous researches on Text-level discourse parsing mainly made use of constituency structure	In this paper , we present the limitations of constituency based discourse parsing	Previous researches on Text-level discourse parsing mainly made use of constituency structure	In this paper , we present the limitations of constituency based discourse parsing	1-22	23-55	Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree .	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	1>2	none	bg-compare	bg-compare
P14-1003_anno1	1-12	13-22	Previous researches on Text-level discourse parsing mainly made use of constituency structure	to parse the whole document into one discourse tree .	Previous researches on Text-level discourse parsing mainly made use of constituency structure	to parse the whole document into one discourse tree .	1-22	1-22	Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree .	Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree .	1<2	none	enablement	enablement
P14-1003_anno1	23-35	36-42	In this paper , we present the limitations of constituency based discourse parsing	and first propose to use dependency structure	In this paper , we present the limitations of constituency based discourse parsing	and first propose to use dependency structure	23-55	23-55	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	1<2	none	progression	progression
P14-1003_anno1	36-42	43-55	and first propose to use dependency structure	to directly represent the relations between elementary discourse units ( EDUs ) .	and first propose to use dependency structure	to directly represent the relations between elementary discourse units ( EDUs ) .	23-55	23-55	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	1<2	none	enablement	enablement
P14-1003_anno1	23-35	56-61,74-75	In this paper , we present the limitations of constituency based discourse parsing	The state-of-the-art dependency parsing techniques , <*> are adopted	In this paper , we present the limitations of constituency based discourse parsing	The state-of-the-art dependency parsing techniques , <*> are adopted	23-55	56-93	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	1<2	none	elab-addition	elab-addition
P14-1003_anno1	56-61,74-75	62-73	The state-of-the-art dependency parsing techniques , <*> are adopted	the Eisner algorithm and maximum spanning tree ( MST ) algorithm ,	The state-of-the-art dependency parsing techniques , <*> are adopted	the Eisner algorithm and maximum spanning tree ( MST ) algorithm ,	56-93	56-93	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	1<2	none	elab-enumember	elab-enumember
P14-1003_anno1	74-75	76-82	are adopted	to parse an optimal discourse dependency tree	are adopted	to parse an optimal discourse dependency tree	56-93	56-93	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	1<2	none	enablement	enablement
P14-1003_anno1	56-61,74-75	83-93	The state-of-the-art dependency parsing techniques , <*> are adopted	based on the arc-factored model and the large-margin learning techniques .	The state-of-the-art dependency parsing techniques , <*> are adopted	based on the arc-factored model and the large-margin learning techniques .	56-93	56-93	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	The state-of-the-art dependency parsing techniques , the Eisner algorithm and maximum spanning tree ( MST ) algorithm , are adopted to parse an optimal discourse dependency tree based on the arc-factored model and the large-margin learning techniques .	1<2	none	manner-means	manner-means
P14-1003_anno1	94-95	96-109	Experiments show	that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	Experiments show	that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	94-109	94-109	Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	1>2	none	attribution	attribution
P14-1003_anno1	23-35	96-109	In this paper , we present the limitations of constituency based discourse parsing	that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	In this paper , we present the limitations of constituency based discourse parsing	that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	23-55	94-109	In this paper , we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units ( EDUs ) .	Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing .	1<2	none	evaluation	evaluation
P14-1004_anno1	1-16	32-37	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue ,	We propose three new unsupervised models	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue ,	We propose three new unsupervised models	1-31	32-45	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems .	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	1>2	none	bg-goal	bg-goal
P14-1004_anno1	1-16	17-21	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue ,	since it provides a basis	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue ,	since it provides a basis	1-31	1-31	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems .	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems .	1<2	none	exp-reason	exp-reason
P14-1004_anno1	17-21	22-31	since it provides a basis	for analysing , evaluating , and building conversational systems .	since it provides a basis	for analysing , evaluating , and building conversational systems .	1-31	1-31	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems .	A key challenge for computational conversation models is to discover latent structure in task-oriented dialogue , since it provides a basis for analysing , evaluating , and building conversational systems .	1<2	none	elab-addition	elab-addition
P14-1004_anno1	32-37	38-45	We propose three new unsupervised models	to discover latent structures in task-oriented dialogues .	We propose three new unsupervised models	to discover latent structures in task-oriented dialogues .	32-45	32-45	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	1<2	none	enablement	enablement
P14-1004_anno1	32-37	46-56	We propose three new unsupervised models	Our methods synthesize hidden Markov models ( for underlying state )	We propose three new unsupervised models	Our methods synthesize hidden Markov models ( for underlying state )	32-45	46-67	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	Our methods synthesize hidden Markov models ( for underlying state ) and topic models ( to connect words to states ) .	1<2	none	elab-aspect	elab-aspect
P14-1004_anno1	46-56	57-59	Our methods synthesize hidden Markov models ( for underlying state )	and topic models	Our methods synthesize hidden Markov models ( for underlying state )	and topic models	46-67	46-67	Our methods synthesize hidden Markov models ( for underlying state ) and topic models ( to connect words to states ) .	Our methods synthesize hidden Markov models ( for underlying state ) and topic models ( to connect words to states ) .	1<2	none	joint	joint
P14-1004_anno1	57-59	60-67	and topic models	( to connect words to states ) .	and topic models	( to connect words to states ) .	46-67	46-67	Our methods synthesize hidden Markov models ( for underlying state ) and topic models ( to connect words to states ) .	Our methods synthesize hidden Markov models ( for underlying state ) and topic models ( to connect words to states ) .	1<2	none	enablement	enablement
P14-1004_anno1	32-37	68-77	We propose three new unsupervised models	We apply them to two real , non-trivial datasets :	We propose three new unsupervised models	We apply them to two real , non-trivial datasets :	32-45	68-96	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	We apply them to two real , non-trivial datasets : human-computer spoken dialogues in bus query service , and human-human text-based chats from a live technical support service .	1<2	none	elab-aspect	elab-aspect
P14-1004_anno1	68-77	78-96	We apply them to two real , non-trivial datasets :	human-computer spoken dialogues in bus query service , and human-human text-based chats from a live technical support service .	We apply them to two real , non-trivial datasets :	human-computer spoken dialogues in bus query service , and human-human text-based chats from a live technical support service .	68-96	68-96	We apply them to two real , non-trivial datasets : human-computer spoken dialogues in bus query service , and human-human text-based chats from a live technical support service .	We apply them to two real , non-trivial datasets : human-computer spoken dialogues in bus query service , and human-human text-based chats from a live technical support service .	1<2	none	elab-enumember	elab-enumember
P14-1004_anno1	97-98	99-108	We show	that our models extract meaningful state representations and dialogue structures	We show	that our models extract meaningful state representations and dialogue structures	97-113	97-113	We show that our models extract meaningful state representations and dialogue structures consistent with human annotations .	We show that our models extract meaningful state representations and dialogue structures consistent with human annotations .	1>2	none	attribution	attribution
P14-1004_anno1	32-37	99-108	We propose three new unsupervised models	that our models extract meaningful state representations and dialogue structures	We propose three new unsupervised models	that our models extract meaningful state representations and dialogue structures	32-45	97-113	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	We show that our models extract meaningful state representations and dialogue structures consistent with human annotations .	1<2	none	evaluation	evaluation
P14-1004_anno1	99-108	109-113	that our models extract meaningful state representations and dialogue structures	consistent with human annotations .	that our models extract meaningful state representations and dialogue structures	consistent with human annotations .	97-113	97-113	We show that our models extract meaningful state representations and dialogue structures consistent with human annotations .	We show that our models extract meaningful state representations and dialogue structures consistent with human annotations .	1<2	none	elab-addition	elab-addition
P14-1004_anno1	114-117	118-132	Quantitatively , we show	our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	Quantitatively , we show	our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	114-132	114-132	Quantitatively , we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	Quantitatively , we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	1>2	none	attribution	attribution
P14-1004_anno1	32-37	118-132	We propose three new unsupervised models	our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	We propose three new unsupervised models	our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	32-45	114-132	We propose three new unsupervised models to discover latent structures in task-oriented dialogues .	Quantitatively , we show our models achieve superior performance on held-out log likelihood evaluation and an ordering task .	1<2	none	evaluation	evaluation
P14-1005_anno1	1-12	52-63	We investigate different ways of learning structured perceptron models for coreference resolution	By modifying LaSO to delay updates until the end of each instance	We investigate different ways of learning structured perceptron models for coreference resolution	By modifying LaSO to delay updates until the end of each instance	1-20	52-71	We investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search .	By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline .	1>2	none	elab-aspect	elab-aspect
P14-1005_anno1	1-12	13-20	We investigate different ways of learning structured perceptron models for coreference resolution	when using non-local features and beam search .	We investigate different ways of learning structured perceptron models for coreference resolution	when using non-local features and beam search .	1-20	1-20	We investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search .	We investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search .	1<2	none	condition	condition
P14-1005_anno1	21-24	25-45	Our experimental results indicate	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	Our experimental results indicate	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	21-51	21-51	Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline that only uses local features .	Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline that only uses local features .	1>2	none	attribution	attribution
P14-1005_anno1	25-45	52-63	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	By modifying LaSO to delay updates until the end of each instance	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	By modifying LaSO to delay updates until the end of each instance	21-51	52-71	Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline that only uses local features .	By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline .	1>2	none	evaluation	evaluation
P14-1005_anno1	25-45	46-51	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	that only uses local features .	that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline	that only uses local features .	21-51	21-51	Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline that only uses local features .	Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization ( LaSO ) perform worse than a greedy baseline that only uses local features .	1<2	none	elab-addition	elab-addition
P14-1005_anno1	52-63	64-71	By modifying LaSO to delay updates until the end of each instance	we obtain significant improvements over the baseline .	By modifying LaSO to delay updates until the end of each instance	we obtain significant improvements over the baseline .	52-71	52-71	By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline .	By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline .	1<2	none	evaluation	evaluation
P14-1005_anno1	52-63	72-92	By modifying LaSO to delay updates until the end of each instance	Our model obtains the best results to date on recent shared task data for Arabic , Chinese , and English .	By modifying LaSO to delay updates until the end of each instance	Our model obtains the best results to date on recent shared task data for Arabic , Chinese , and English .	52-71	72-92	By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline .	Our model obtains the best results to date on recent shared task data for Arabic , Chinese , and English .	1<2	none	evaluation	evaluation
P14-1006_anno1	1-5	6-10	We present a novel technique	for learning semantic representations ,	We present a novel technique	for learning semantic representations ,	1-22	1-22	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	1<2	none	enablement	enablement
P14-1006_anno1	1-5	11-22	We present a novel technique	which extends the distributional hypothesis to multilingual data and joint-space embeddings .	We present a novel technique	which extends the distributional hypothesis to multilingual data and joint-space embeddings .	1-22	1-22	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	1<2	none	elab-addition	elab-addition
P14-1006_anno1	1-5	23-27	We present a novel technique	Our models leverage parallel data	We present a novel technique	Our models leverage parallel data	1-22	23-49	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	1<2	none	elab-aspect	elab-aspect
P14-1006_anno1	23-27	28-39	Our models leverage parallel data	and learn to strongly align the embeddings of semantically equivalent sentences ,	Our models leverage parallel data	and learn to strongly align the embeddings of semantically equivalent sentences ,	23-49	23-49	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	1<2	none	joint	joint
P14-1006_anno1	28-39	40-49	and learn to strongly align the embeddings of semantically equivalent sentences ,	while maintaining sufficient distance between those of dissimilar sentences .	and learn to strongly align the embeddings of semantically equivalent sentences ,	while maintaining sufficient distance between those of dissimilar sentences .	23-49	23-49	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	1<2	none	temporal	temporal
P14-1006_anno1	23-27	50-61	Our models leverage parallel data	The models do not rely on word alignments or any syntactic information	Our models leverage parallel data	The models do not rely on word alignments or any syntactic information	23-49	50-72	Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .	The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages .	1<2	none	elab-addition	elab-addition
P14-1006_anno1	50-61	62-72	The models do not rely on word alignments or any syntactic information	and are successfully applied to a number of diverse languages .	The models do not rely on word alignments or any syntactic information	and are successfully applied to a number of diverse languages .	50-72	50-72	The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages .	The models do not rely on word alignments or any syntactic information and are successfully applied to a number of diverse languages .	1<2	none	joint	joint
P14-1006_anno1	1-5	73-87	We present a novel technique	We extend our approach to learn semantic representations at the document level , too .	We present a novel technique	We extend our approach to learn semantic representations at the document level , too .	1-22	73-87	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	We extend our approach to learn semantic representations at the document level , too .	1<2	none	elab-aspect	elab-aspect
P14-1006_anno1	73-87	88-98	We extend our approach to learn semantic representations at the document level , too .	We evaluate these models on two cross-lingual document classification tasks ,	We extend our approach to learn semantic representations at the document level , too .	We evaluate these models on two cross-lingual document classification tasks ,	73-87	88-106	We extend our approach to learn semantic representations at the document level , too .	We evaluate these models on two cross-lingual document classification tasks , outperforming the prior state of the art .	1<2	none	evaluation	evaluation
P14-1006_anno1	88-98	99-106	We evaluate these models on two cross-lingual document classification tasks ,	outperforming the prior state of the art .	We evaluate these models on two cross-lingual document classification tasks ,	outperforming the prior state of the art .	88-106	88-106	We evaluate these models on two cross-lingual document classification tasks , outperforming the prior state of the art .	We evaluate these models on two cross-lingual document classification tasks , outperforming the prior state of the art .	1<2	none	cause	cause
P14-1006_anno1	107-115	116-117	Through qualitative analysis and the study of pivoting effects	we demonstrate	Through qualitative analysis and the study of pivoting effects	we demonstrate	107-134	107-134	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	1>2	none	manner-means	manner-means
P14-1006_anno1	116-117	118-123	we demonstrate	that our representations are semantically plausible	we demonstrate	that our representations are semantically plausible	107-134	107-134	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	1>2	none	attribution	attribution
P14-1006_anno1	1-5	118-123	We present a novel technique	that our representations are semantically plausible	We present a novel technique	that our representations are semantically plausible	1-22	107-134	We present a novel technique for learning semantic representations , which extends the distributional hypothesis to multilingual data and joint-space embeddings .	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	1<2	none	evaluation	evaluation
P14-1006_anno1	118-123	124-130	that our representations are semantically plausible	and can capture semantic relationships across languages	that our representations are semantically plausible	and can capture semantic relationships across languages	107-134	107-134	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	1<2	none	joint	joint
P14-1006_anno1	124-130	131-134	and can capture semantic relationships across languages	without parallel data .	and can capture semantic relationships across languages	without parallel data .	107-134	107-134	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic relationships across languages without parallel data .	1<2	none	elab-addition	elab-addition
P14-1007_anno1	1-16	33-44	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference :	our approach works over explicit and formal representations of propositional semantics ,	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference :	our approach works over explicit and formal representations of propositional semantics ,	1-22	23-63	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference : the automated analysis of negation .	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	1>2	none	bg-goal	bg-goal
P14-1007_anno1	1-16	17-22	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference :	the automated analysis of negation .	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference :	the automated analysis of negation .	1-22	1-22	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference : the automated analysis of negation .	In this work , we revisit Shared Task 1 from the 2012 * SEM Conference : the automated analysis of negation .	1<2	none	elab-addition	elab-addition
P14-1007_anno1	23-32	33-44	Unlike the vast majority of participating systems in 2012 ,	our approach works over explicit and formal representations of propositional semantics ,	Unlike the vast majority of participating systems in 2012 ,	our approach works over explicit and formal representations of propositional semantics ,	23-63	23-63	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	1>2	none	contrast	contrast
P14-1007_anno1	33-44	45-51	our approach works over explicit and formal representations of propositional semantics ,	i.e. derives the notion of negation scope	our approach works over explicit and formal representations of propositional semantics ,	i.e. derives the notion of negation scope	23-63	23-63	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	1<2	none	elab-example	elab-example
P14-1007_anno1	45-51	52-63	i.e. derives the notion of negation scope	assumed in this task from the structure of logical-form meaning representations .	i.e. derives the notion of negation scope	assumed in this task from the structure of logical-form meaning representations .	23-63	23-63	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	1<2	none	elab-addition	elab-addition
P14-1007_anno1	33-44	64-88	our approach works over explicit and formal representations of propositional semantics ,	We relate the task-specific interpretation of ( negation ) scope to the concept of ( quantifier and operator ) scope in mainstream underspecified semantics .	our approach works over explicit and formal representations of propositional semantics ,	We relate the task-specific interpretation of ( negation ) scope to the concept of ( quantifier and operator ) scope in mainstream underspecified semantics .	23-63	64-88	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	We relate the task-specific interpretation of ( negation ) scope to the concept of ( quantifier and operator ) scope in mainstream underspecified semantics .	1<2	none	elab-aspect	elab-aspect
P14-1007_anno1	33-44	89-105	our approach works over explicit and formal representations of propositional semantics ,	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	our approach works over explicit and formal representations of propositional semantics ,	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	23-63	89-134	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	1<2	none	elab-aspect	elab-aspect
P14-1007_anno1	89-105	106-113	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	made for the 2012 * SEM task ,	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	made for the 2012 * SEM task ,	89-134	89-134	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	1<2	none	elab-addition	elab-addition
P14-1007_anno1	114-115	116-134	and demonstrate	how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	and demonstrate	how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	89-134	89-134	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	1>2	none	attribution	attribution
P14-1007_anno1	89-105	116-134	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions	how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	89-134	89-134	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	With reference to an explicit encoding of semantic predicate-argument structure , we can operationalize the annotation decisions made for the 2012 * SEM task , and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system .	1<2	none	joint	joint
P14-1007_anno1	33-44	135-154	our approach works over explicit and formal representations of propositional semantics ,	In a system combination setting , our approach improves over the best published results on this task to date .	our approach works over explicit and formal representations of propositional semantics ,	In a system combination setting , our approach improves over the best published results on this task to date .	23-63	135-154	Unlike the vast majority of participating systems in 2012 , our approach works over explicit and formal representations of propositional semantics , i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations .	In a system combination setting , our approach improves over the best published results on this task to date .	1<2	none	evaluation	evaluation
P14-1008_anno1	1-21	22-34	Dependency-based Compositional Semantics (DCS ) is a framework of natural language semantics with easy-to-process structures as well as strict semantics .	In this paper , we equip the DCS framework with logical inference ,	Dependency-based Compositional Semantics (DCS ) is a framework of natural language semantics with easy-to-process structures as well as strict semantics .	In this paper , we equip the DCS framework with logical inference ,	1-21	22-51	Dependency-based Compositional Semantics (DCS ) is a framework of natural language semantics with easy-to-process structures as well as strict semantics .	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	1>2	none	elab-definition	elab-definition
P14-1008_anno1	22-34	35-51	In this paper , we equip the DCS framework with logical inference ,	by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	In this paper , we equip the DCS framework with logical inference ,	by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	22-51	22-51	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	1<2	none	manner-means	manner-means
P14-1008_anno1	22-34	52-56	In this paper , we equip the DCS framework with logical inference ,	An inference engine is built	In this paper , we equip the DCS framework with logical inference ,	An inference engine is built	22-51	52-63	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	An inference engine is built to achieve inference on abstract denotations .	1<2	none	elab-aspect	elab-aspect
P14-1008_anno1	52-56	57-63	An inference engine is built	to achieve inference on abstract denotations .	An inference engine is built	to achieve inference on abstract denotations .	52-63	52-63	An inference engine is built to achieve inference on abstract denotations .	An inference engine is built to achieve inference on abstract denotations .	1<2	none	enablement	enablement
P14-1008_anno1	22-34	64-69	In this paper , we equip the DCS framework with logical inference ,	Furthermore , we propose a way	In this paper , we equip the DCS framework with logical inference ,	Furthermore , we propose a way	22-51	64-88	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	Furthermore , we propose a way to generate on-the-fly knowledge in logical inference , by combining our framework with the idea of tree transformation .	1<2	none	elab-aspect	elab-aspect
P14-1008_anno1	64-69	70-77	Furthermore , we propose a way	to generate on-the-fly knowledge in logical inference ,	Furthermore , we propose a way	to generate on-the-fly knowledge in logical inference ,	64-88	64-88	Furthermore , we propose a way to generate on-the-fly knowledge in logical inference , by combining our framework with the idea of tree transformation .	Furthermore , we propose a way to generate on-the-fly knowledge in logical inference , by combining our framework with the idea of tree transformation .	1<2	none	enablement	enablement
P14-1008_anno1	70-77	78-88	to generate on-the-fly knowledge in logical inference ,	by combining our framework with the idea of tree transformation .	to generate on-the-fly knowledge in logical inference ,	by combining our framework with the idea of tree transformation .	64-88	64-88	Furthermore , we propose a way to generate on-the-fly knowledge in logical inference , by combining our framework with the idea of tree transformation .	Furthermore , we propose a way to generate on-the-fly knowledge in logical inference , by combining our framework with the idea of tree transformation .	1<2	none	manner-means	manner-means
P14-1008_anno1	22-34	89-99	In this paper , we equip the DCS framework with logical inference ,	Experiments on FraCaS and PASCAL RTE datasets show promising results .	In this paper , we equip the DCS framework with logical inference ,	Experiments on FraCaS and PASCAL RTE datasets show promising results .	22-51	89-99	In this paper , we equip the DCS framework with logical inference , by defining abstract denotations as an abstraction of the computing process of denotations in original DCS .	Experiments on FraCaS and PASCAL RTE datasets show promising results .	1<2	none	evaluation	evaluation
P14-1009_anno1	1-3	36-41	Distributional semantic methods	We present here a new model	Distributional semantic methods	We present here a new model	1-35	36-73	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	We present here a new model that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	1>2	none	bg-compare	bg-compare
P14-1009_anno1	1-3	4-10	Distributional semantic methods	to approximate word meaning with context vectors	Distributional semantic methods	to approximate word meaning with context vectors	1-35	1-35	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	1<2	none	enablement	enablement
P14-1009_anno1	1-3	11-16	Distributional semantic methods	have been very successful empirically ,	Distributional semantic methods	have been very successful empirically ,	1-35	1-35	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	1<2	none	evaluation	evaluation
P14-1009_anno1	1-3	17-35	Distributional semantic methods	and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	Distributional semantic methods	and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	1-35	1-35	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically , and the last years have seen a surge of interest in their compositional extension to phrases and sentences .	1<2	none	joint	joint
P14-1009_anno1	36-41	42-73	We present here a new model	that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	We present here a new model	that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	36-73	36-73	We present here a new model that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	We present here a new model that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	1<2	none	elab-addition	elab-addition
P14-1009_anno1	36-41	74-82	We present here a new model	However , our approach avoids a number of issues	We present here a new model	However , our approach avoids a number of issues	36-73	74-98	We present here a new model that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	However , our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged , real-life sentences .	1<2	none	elab-aspect	elab-aspect
P14-1009_anno1	74-82	83-98	However , our approach avoids a number of issues	that have prevented the application of the earlier linguistically-motivated models to full-fledged , real-life sentences .	However , our approach avoids a number of issues	that have prevented the application of the earlier linguistically-motivated models to full-fledged , real-life sentences .	74-98	74-98	However , our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged , real-life sentences .	However , our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged , real-life sentences .	1<2	none	elab-addition	elab-addition
P14-1009_anno1	36-41	99-109	We present here a new model	We test the model on a variety of empirical tasks ,	We present here a new model	We test the model on a variety of empirical tasks ,	36-73	99-120	We present here a new model that , like those of Coecke et al ( 2010 ) and Baroni and Zamparelli ( 2010 ) , closely mimics the standard Montagovian semantic treatment of composition in distributional terms .	We test the model on a variety of empirical tasks , showing that it consistently outperforms a set of competitive rivals .	1<2	none	evaluation	evaluation
P14-1009_anno1	99-109	110	We test the model on a variety of empirical tasks ,	showing	We test the model on a variety of empirical tasks ,	showing	99-120	99-120	We test the model on a variety of empirical tasks , showing that it consistently outperforms a set of competitive rivals .	We test the model on a variety of empirical tasks , showing that it consistently outperforms a set of competitive rivals .	1<2	none	elab-addition	elab-addition
P14-1009_anno1	110	111-120	showing	that it consistently outperforms a set of competitive rivals .	showing	that it consistently outperforms a set of competitive rivals .	99-120	99-120	We test the model on a variety of empirical tasks , showing that it consistently outperforms a set of competitive rivals .	We test the model on a variety of empirical tasks , showing that it consistently outperforms a set of competitive rivals .	1<2	none	attribution	attribution
P14-1010_anno1	1-15	49-57	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT )	In this paper , we expand our translation options	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT )	In this paper , we expand our translation options	1-20	49-64	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT ) involving morphologically complex languages .	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	1>2	none	bg-goal	bg-goal
P14-1010_anno1	1-15	16-20	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT )	involving morphologically complex languages .	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT )	involving morphologically complex languages .	1-20	1-20	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT ) involving morphologically complex languages .	Morphological segmentation is an effective sparsity reduction strategy for statistical machine translation ( SMT ) involving morphologically complex languages .	1<2	none	elab-addition	elab-addition
P14-1010_anno1	21-27	28-32	When translating into a segmented language ,	an extra step is required	When translating into a segmented language ,	an extra step is required	21-48	21-48	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	1>2	none	condition	condition
P14-1010_anno1	28-32	38-48	an extra step is required	previous studies have desegmented the 1-best output from the decoder .	an extra step is required	previous studies have desegmented the 1-best output from the decoder .	21-48	21-48	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	1>2	none	bg-general	bg-general
P14-1010_anno1	28-32	33-37	an extra step is required	to desegment the output ;	an extra step is required	to desegment the output ;	21-48	21-48	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	1<2	none	enablement	enablement
P14-1010_anno1	38-48	49-57	previous studies have desegmented the 1-best output from the decoder .	In this paper , we expand our translation options	previous studies have desegmented the 1-best output from the decoder .	In this paper , we expand our translation options	21-48	49-64	When translating into a segmented language , an extra step is required to desegment the output ; previous studies have desegmented the 1-best output from the decoder .	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	1>2	none	bg-compare	bg-compare
P14-1010_anno1	49-57	58-64	In this paper , we expand our translation options	by desegmenting n-best lists or lattices .	In this paper , we expand our translation options	by desegmenting n-best lists or lattices .	49-64	49-64	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	1<2	none	manner-means	manner-means
P14-1010_anno1	49-57	65-89	In this paper , we expand our translation options	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs ,	In this paper , we expand our translation options	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs ,	49-64	65-112	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( LM ) .	1<2	none	elab-aspect	elab-aspect
P14-1010_anno1	65-89	90-95,102-112	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs ,	which allows for inclusion of features <*> as well as an unsegmented language model ( LM ) .	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs ,	which allows for inclusion of features <*> as well as an unsegmented language model ( LM ) .	65-112	65-112	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( LM ) .	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( LM ) .	1<2	none	elab-addition	elab-addition
P14-1010_anno1	90-95,102-112	96-101	which allows for inclusion of features <*> as well as an unsegmented language model ( LM ) .	related to the desegmentation process ,	which allows for inclusion of features <*> as well as an unsegmented language model ( LM ) .	related to the desegmentation process ,	65-112	65-112	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( LM ) .	Our novel lattice desegmentation algorithm effectively combines both segmented and desegmented views of the target language for a large subspace of possible translation outputs , which allows for inclusion of features related to the desegmentation process , as well as an unsegmented language model ( LM ) .	1<2	none	elab-addition	elab-addition
P14-1010_anno1	49-57	113-125	In this paper , we expand our translation options	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation ,	In this paper , we expand our translation options	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation ,	49-64	113-138	In this paper , we expand our translation options by desegmenting n-best lists or lattices .	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation , showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .	1<2	none	evaluation	evaluation
P14-1010_anno1	113-125	126-138	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation ,	showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation ,	showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .	113-138	113-138	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation , showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .	We investigate this technique in the context of English-to-Arabic and English-to-Finnish translation , showing significant improvements in translation quality over desegmentation of 1-best decoder outputs .	1<2	none	cause	cause
P14-1011_anno1	1-8	9-13	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	to learn semantic phrase embeddings	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	to learn semantic phrase embeddings	1-31	1-31	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	1<2	none	enablement	enablement
P14-1011_anno1	9-13	14-21	to learn semantic phrase embeddings	( compact vector representations for phrases ) ,	to learn semantic phrase embeddings	( compact vector representations for phrases ) ,	1-31	1-31	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	1<2	none	elab-definition	elab-definition
P14-1011_anno1	9-13	22-31	to learn semantic phrase embeddings	which can distinguish the phrases with different semantic meanings .	to learn semantic phrase embeddings	which can distinguish the phrases with different semantic meanings .	1-31	1-31	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	1<2	none	elab-addition	elab-addition
P14-1011_anno1	1-8	32-38	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	The BRAE is trained in a way	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	The BRAE is trained in a way	1-31	32-56	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously .	1<2	none	elab-process_step	elab-process_step
P14-1011_anno1	32-38	39-46	The BRAE is trained in a way	that minimizes the semantic distance of translation equivalents	The BRAE is trained in a way	that minimizes the semantic distance of translation equivalents	32-56	32-56	The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously .	The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously .	1<2	none	elab-addition	elab-addition
P14-1011_anno1	39-46	47-56	that minimizes the semantic distance of translation equivalents	and maximizes the semantic distance of non-translation pairs simultaneously .	that minimizes the semantic distance of translation equivalents	and maximizes the semantic distance of non-translation pairs simultaneously .	32-56	32-56	The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously .	The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of non-translation pairs simultaneously .	1<2	none	joint	joint
P14-1011_anno1	57-59	60-71	After training ,	the model learns how to embed each phrase semantically in two languages	After training ,	the model learns how to embed each phrase semantically in two languages	57-87	57-87	After training , the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other .	After training , the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other .	1>2	none	temporal	temporal
P14-1011_anno1	1-8	60-71	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	the model learns how to embed each phrase semantically in two languages	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	the model learns how to embed each phrase semantically in two languages	1-31	57-87	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	After training , the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other .	1<2	none	elab-process_step	elab-process_step
P14-1011_anno1	60-71	72-87	the model learns how to embed each phrase semantically in two languages	and also learns how to transform semantic embedding space in one language to the other .	the model learns how to embed each phrase semantically in two languages	and also learns how to transform semantic embedding space in one language to the other .	57-87	57-87	After training , the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other .	After training , the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other .	1<2	none	joint	joint
P14-1011_anno1	1-8	88-97	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	We evaluate our proposed method on two end-to-end SMT tasks	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE )	We evaluate our proposed method on two end-to-end SMT tasks	1-31	88-123	We propose Bilingually-constrained Recursive Auto-encoders ( BRAE ) to learn semantic phrase embeddings ( compact vector representations for phrases ) , which can distinguish the phrases with different semantic meanings .	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	1<2	none	evaluation	evaluation
P14-1011_anno1	88-97	98-108	We evaluate our proposed method on two end-to-end SMT tasks	( phrase table pruning and decoding with phrasal semantic similarities )	We evaluate our proposed method on two end-to-end SMT tasks	( phrase table pruning and decoding with phrasal semantic similarities )	88-123	88-123	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	1<2	none	elab-enumember	elab-enumember
P14-1011_anno1	88-97	109-123	We evaluate our proposed method on two end-to-end SMT tasks	which need to measure semantic similarity between a source phrase and its translation candidates .	We evaluate our proposed method on two end-to-end SMT tasks	which need to measure semantic similarity between a source phrase and its translation candidates .	88-123	88-123	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	1<2	none	elab-addition	elab-addition
P14-1011_anno1	124-126	127-137	Extensive experiments show	that the BRAE is remarkably effective in these two tasks .	Extensive experiments show	that the BRAE is remarkably effective in these two tasks .	124-137	124-137	Extensive experiments show that the BRAE is remarkably effective in these two tasks .	Extensive experiments show that the BRAE is remarkably effective in these two tasks .	1>2	none	attribution	attribution
P14-1011_anno1	88-97	127-137	We evaluate our proposed method on two end-to-end SMT tasks	that the BRAE is remarkably effective in these two tasks .	We evaluate our proposed method on two end-to-end SMT tasks	that the BRAE is remarkably effective in these two tasks .	88-123	124-137	We evaluate our proposed method on two end-to-end SMT tasks ( phrase table pruning and decoding with phrasal semantic similarities ) which need to measure semantic similarity between a source phrase and its translation candidates .	Extensive experiments show that the BRAE is remarkably effective in these two tasks .	1<2	none	cause	cause
P14-1012_anno1	1-9	19-25	In this paper , instead of designing new features	we learn some new and effective features	In this paper , instead of designing new features	we learn some new and effective features	1-38	1-38	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	1>2	none	contrast	contrast
P14-1012_anno1	1-9	10-18	In this paper , instead of designing new features	based on intuition , linguistic knowledge and domain ,	In this paper , instead of designing new features	based on intuition , linguistic knowledge and domain ,	1-38	1-38	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	1<2	none	bg-general	bg-general
P14-1012_anno1	19-25	26-38	we learn some new and effective features	using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	we learn some new and effective features	using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	1-38	1-38	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	1<2	none	manner-means	manner-means
P14-1012_anno1	39-48	67-73	Using the unsupervised pre-trained deep belief net ( DBN )	we learn new semi-supervised DAE features ,	Using the unsupervised pre-trained deep belief net ( DBN )	we learn new semi-supervised DAE features ,	39-85	39-85	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	1>2	none	manner-means	manner-means
P14-1012_anno1	39-48	49-52	Using the unsupervised pre-trained deep belief net ( DBN )	to initialize DAE's parameters	Using the unsupervised pre-trained deep belief net ( DBN )	to initialize DAE's parameters	39-85	39-85	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	1<2	none	enablement	enablement
P14-1012_anno1	53-66	67-73	and using the input original phrase features as a teacher for semi-supervised fine-tuning ,	we learn new semi-supervised DAE features ,	and using the input original phrase features as a teacher for semi-supervised fine-tuning ,	we learn new semi-supervised DAE features ,	39-85	39-85	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	1>2	none	manner-means	manner-means
P14-1012_anno1	19-25	67-73	we learn some new and effective features	we learn new semi-supervised DAE features ,	we learn some new and effective features	we learn new semi-supervised DAE features ,	1-38	39-85	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	1<2	none	elab-aspect	elab-aspect
P14-1012_anno1	67-73	74-85	we learn new semi-supervised DAE features ,	which are more effective and stable than the unsupervised DBN features .	we learn new semi-supervised DAE features ,	which are more effective and stable than the unsupervised DBN features .	39-85	39-85	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	Using the unsupervised pre-trained deep belief net ( DBN ) to initialize DAE's parameters and using the input original phrase features as a teacher for semi-supervised fine-tuning , we learn new semi-supervised DAE features , which are more effective and stable than the unsupervised DBN features .	1<2	none	elab-addition	elab-addition
P14-1012_anno1	86-94	95-110	Moreover , to learn high dimensional feature representation ,	we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	Moreover , to learn high dimensional feature representation ,	we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	86-110	86-110	Moreover , to learn high dimensional feature representation , we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	Moreover , to learn high dimensional feature representation , we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	1>2	none	enablement	enablement
P14-1012_anno1	19-25	95-110	we learn some new and effective features	we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	we learn some new and effective features	we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	1-38	86-110	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	Moreover , to learn high dimensional feature representation , we introduce a natural horizontal composition of more DAEs for large hidden layers feature learning .	1<2	none	elab-aspect	elab-aspect
P14-1012_anno1	19-25	111-147	we learn some new and effective features	On two Chinese-English tasks , our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 ( IWSLT ) and 0.82/1.52 ( NIST ) BLEU points over the unsupervised DBN features and the baseline features , respectively .	we learn some new and effective features	On two Chinese-English tasks , our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 ( IWSLT ) and 0.82/1.52 ( NIST ) BLEU points over the unsupervised DBN features and the baseline features , respectively .	1-38	111-147	In this paper , instead of designing new features based on intuition , linguistic knowledge and domain , we learn some new and effective features using the deep autoencoder ( DAE ) paradigm for phrase-based translation model .	On two Chinese-English tasks , our semi-supervised DAE features obtain statistically significant improvements of 1.34/2.45 ( IWSLT ) and 0.82/1.52 ( NIST ) BLEU points over the unsupervised DBN features and the baseline features , respectively .	1<2	none	evaluation	evaluation
P14-1013_anno1	1-10	16-27	Statistical Machine Translation ( SMT ) usually utilizes contextual information	However , it is often limited to contexts within sentence boundaries ,	Statistical Machine Translation ( SMT ) usually utilizes contextual information	However , it is often limited to contexts within sentence boundaries ,	1-15	16-35	Statistical Machine Translation ( SMT ) usually utilizes contextual information to disambiguate translation candidates .	However , it is often limited to contexts within sentence boundaries , hence broader topical information cannot be leveraged .	1>2	none	bg-general	bg-general
P14-1013_anno1	1-10	11-15	Statistical Machine Translation ( SMT ) usually utilizes contextual information	to disambiguate translation candidates .	Statistical Machine Translation ( SMT ) usually utilizes contextual information	to disambiguate translation candidates .	1-15	1-15	Statistical Machine Translation ( SMT ) usually utilizes contextual information to disambiguate translation candidates .	Statistical Machine Translation ( SMT ) usually utilizes contextual information to disambiguate translation candidates .	1<2	none	enablement	enablement
P14-1013_anno1	16-27	36-44	However , it is often limited to contexts within sentence boundaries ,	In this paper , we propose a novel approach	However , it is often limited to contexts within sentence boundaries ,	In this paper , we propose a novel approach	16-35	36-69	However , it is often limited to contexts within sentence boundaries , hence broader topical information cannot be leveraged .	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	1>2	none	bg-compare	bg-compare
P14-1013_anno1	16-27	28-35	However , it is often limited to contexts within sentence boundaries ,	hence broader topical information cannot be leveraged .	However , it is often limited to contexts within sentence boundaries ,	hence broader topical information cannot be leveraged .	16-35	16-35	However , it is often limited to contexts within sentence boundaries , hence broader topical information cannot be leveraged .	However , it is often limited to contexts within sentence boundaries , hence broader topical information cannot be leveraged .	1<2	none	cause	cause
P14-1013_anno1	36-44	45-51	In this paper , we propose a novel approach	to learning topic representation for parallel data	In this paper , we propose a novel approach	to learning topic representation for parallel data	36-69	36-69	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	1<2	none	elab-addition	elab-addition
P14-1013_anno1	45-51	52-57	to learning topic representation for parallel data	using a neural network architecture ,	to learning topic representation for parallel data	using a neural network architecture ,	36-69	36-69	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	1<2	none	manner-means	manner-means
P14-1013_anno1	52-57	58-63	using a neural network architecture ,	where abundant topical contexts are embedded	using a neural network architecture ,	where abundant topical contexts are embedded	36-69	36-69	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	1<2	none	elab-addition	elab-addition
P14-1013_anno1	58-63	64-69	where abundant topical contexts are embedded	via topic relevant monolingual data .	where abundant topical contexts are embedded	via topic relevant monolingual data .	36-69	36-69	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	1<2	none	elab-addition	elab-addition
P14-1013_anno1	70-79	80-84	By associating each translation rule with the topic representation ,	topic relevant rules are selected	By associating each translation rule with the topic representation ,	topic relevant rules are selected	70-97	70-97	By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding .	By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding .	1>2	none	manner-means	manner-means
P14-1013_anno1	36-44	80-84	In this paper , we propose a novel approach	topic relevant rules are selected	In this paper , we propose a novel approach	topic relevant rules are selected	36-69	70-97	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding .	1<2	none	elab-aspect	elab-aspect
P14-1013_anno1	80-84	85-97	topic relevant rules are selected	according to the distributional similarity with the source text during SMT decoding .	topic relevant rules are selected	according to the distributional similarity with the source text during SMT decoding .	70-97	70-97	By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding .	By associating each translation rule with the topic representation , topic relevant rules are selected according to the distributional similarity with the source text during SMT decoding .	1<2	none	elab-addition	elab-addition
P14-1013_anno1	98-100	101-113	Experimental results show	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	Experimental results show	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	98-119	98-119	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline .	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline .	1>2	none	attribution	attribution
P14-1013_anno1	36-44	101-113	In this paper , we propose a novel approach	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	In this paper , we propose a novel approach	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	36-69	98-119	In this paper , we propose a novel approach to learning topic representation for parallel data using a neural network architecture , where abundant topical contexts are embedded via topic relevant monolingual data .	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline .	1<2	none	evaluation	evaluation
P14-1013_anno1	101-113	114-119	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	compared to a state-of-the-art baseline .	that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task	compared to a state-of-the-art baseline .	98-119	98-119	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline .	Experimental results show that our method significantly improves translation accuracy in the NIST Chinese-to-English translation task compared to a state-of-the-art baseline .	1<2	none	comparison	comparison
P14-1014_anno1	1-12	13-17	In this paper , we address the problem of web-domain POS tagging	using a two-phase approach .	In this paper , we address the problem of web-domain POS tagging	using a two-phase approach .	1-17	1-17	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	1<2	none	manner-means	manner-means
P14-1014_anno1	1-12	18-22	In this paper , we address the problem of web-domain POS tagging	The first phase learns representations	In this paper , we address the problem of web-domain POS tagging	The first phase learns representations	1-17	18-29	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	The first phase learns representations that capture regularities underlying web text .	1<2	none	elab-process_step	elab-process_step
P14-1014_anno1	18-22	23-25	The first phase learns representations	that capture regularities	The first phase learns representations	that capture regularities	18-29	18-29	The first phase learns representations that capture regularities underlying web text .	The first phase learns representations that capture regularities underlying web text .	1<2	none	elab-addition	elab-addition
P14-1014_anno1	23-25	26-29	that capture regularities	underlying web text .	that capture regularities	underlying web text .	18-29	18-29	The first phase learns representations that capture regularities underlying web text .	The first phase learns representations that capture regularities underlying web text .	1<2	none	elab-addition	elab-addition
P14-1014_anno1	1-12	30-39	In this paper , we address the problem of web-domain POS tagging	The representation is integrated as features into a neural network	In this paper , we address the problem of web-domain POS tagging	The representation is integrated as features into a neural network	1-17	30-50	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger .	1<2	none	elab-process_step	elab-process_step
P14-1014_anno1	30-39	40-50	The representation is integrated as features into a neural network	that serves as a scorer for an easy-first POS tagger .	The representation is integrated as features into a neural network	that serves as a scorer for an easy-first POS tagger .	30-50	30-50	The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger .	The representation is integrated as features into a neural network that serves as a scorer for an easy-first POS tagger .	1<2	none	elab-addition	elab-addition
P14-1014_anno1	1-12	51-57	In this paper , we address the problem of web-domain POS tagging	Parameters of the neural network are trained	In this paper , we address the problem of web-domain POS tagging	Parameters of the neural network are trained	1-17	51-65	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	Parameters of the neural network are trained using guided learning in the second phase .	1<2	none	elab-process_step	elab-process_step
P14-1014_anno1	51-57	58-65	Parameters of the neural network are trained	using guided learning in the second phase .	Parameters of the neural network are trained	using guided learning in the second phase .	51-65	51-65	Parameters of the neural network are trained using guided learning in the second phase .	Parameters of the neural network are trained using guided learning in the second phase .	1<2	none	manner-means	manner-means
P14-1014_anno1	66-73	74-83	Experiment on the SANCL 2012 shared task show	that our approach achieves 93.15 % average tagging accuracy ,	Experiment on the SANCL 2012 shared task show	that our approach achieves 93.15 % average tagging accuracy ,	66-105	66-105	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	1>2	none	attribution	attribution
P14-1014_anno1	1-12	74-83	In this paper , we address the problem of web-domain POS tagging	that our approach achieves 93.15 % average tagging accuracy ,	In this paper , we address the problem of web-domain POS tagging	that our approach achieves 93.15 % average tagging accuracy ,	1-17	66-105	In this paper , we address the problem of web-domain POS tagging using a two-phase approach .	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	1<2	none	evaluation	evaluation
P14-1014_anno1	74-83	84-88	that our approach achieves 93.15 % average tagging accuracy ,	which is the best accuracy	that our approach achieves 93.15 % average tagging accuracy ,	which is the best accuracy	66-105	66-105	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	1<2	none	elab-addition	elab-addition
P14-1014_anno1	84-88	89-99	which is the best accuracy	reported so far on this data set , higher than those	which is the best accuracy	reported so far on this data set , higher than those	66-105	66-105	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	1<2	none	elab-addition	elab-addition
P14-1014_anno1	89-99	100-105	reported so far on this data set , higher than those	given by ensembled syntactic parsers .	reported so far on this data set , higher than those	given by ensembled syntactic parsers .	66-105	66-105	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	Experiment on the SANCL 2012 shared task show that our approach achieves 93.15 % average tagging accuracy , which is the best accuracy reported so far on this data set , higher than those given by ensembled syntactic parsers .	1<2	none	elab-addition	elab-addition
P14-1015_anno1	1-10	16-30	Discussion forums have evolved into a dependable source of knowledge	However , only a minority of the posts in discussion forums are solution posts .	Discussion forums have evolved into a dependable source of knowledge	However , only a minority of the posts in discussion forums are solution posts .	1-15	16-30	Discussion forums have evolved into a dependable source of knowledge to solve common problems .	However , only a minority of the posts in discussion forums are solution posts .	1>2	none	contrast	contrast
P14-1015_anno1	1-10	11-15	Discussion forums have evolved into a dependable source of knowledge	to solve common problems .	Discussion forums have evolved into a dependable source of knowledge	to solve common problems .	1-15	1-15	Discussion forums have evolved into a dependable source of knowledge to solve common problems .	Discussion forums have evolved into a dependable source of knowledge to solve common problems .	1<2	none	enablement	enablement
P14-1015_anno1	16-30	31-45	However , only a minority of the posts in discussion forums are solution posts .	Identifying solution posts from discussion forums , hence , is an important research problem .	However , only a minority of the posts in discussion forums are solution posts .	Identifying solution posts from discussion forums , hence , is an important research problem .	16-30	31-45	However , only a minority of the posts in discussion forums are solution posts .	Identifying solution posts from discussion forums , hence , is an important research problem .	1>2	none	result	result
P14-1015_anno1	31-45	46-58	Identifying solution posts from discussion forums , hence , is an important research problem .	In this paper , we present a technique for unsupervised solution post identification	Identifying solution posts from discussion forums , hence , is an important research problem .	In this paper , we present a technique for unsupervised solution post identification	31-45	46-75	Identifying solution posts from discussion forums , hence , is an important research problem .	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	1>2	none	bg-goal	bg-goal
P14-1015_anno1	46-58	59-75	In this paper , we present a technique for unsupervised solution post identification	leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	In this paper , we present a technique for unsupervised solution post identification	leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	46-75	46-75	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	1<2	none	manner-means	manner-means
P14-1015_anno1	46-58	76-82	In this paper , we present a technique for unsupervised solution post identification	We use translation models and language models	In this paper , we present a technique for unsupervised solution post identification	We use translation models and language models	46-75	76-92	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	We use translation models and language models to exploit lexical correlations and solution post character respectively .	1<2	none	elab-aspect	elab-aspect
P14-1015_anno1	76-82	83-92	We use translation models and language models	to exploit lexical correlations and solution post character respectively .	We use translation models and language models	to exploit lexical correlations and solution post character respectively .	76-92	76-92	We use translation models and language models to exploit lexical correlations and solution post character respectively .	We use translation models and language models to exploit lexical correlations and solution post character respectively .	1<2	none	enablement	enablement
P14-1015_anno1	46-58	93-96	In this paper , we present a technique for unsupervised solution post identification	Our technique is designed	In this paper , we present a technique for unsupervised solution post identification	Our technique is designed	46-75	93-118	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	Our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums .	1<2	none	elab-aspect	elab-aspect
P14-1015_anno1	93-96	97-107	Our technique is designed	to not rely much on structural features such as post metadata	Our technique is designed	to not rely much on structural features such as post metadata	93-118	93-118	Our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums .	Our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums .	1<2	none	enablement	enablement
P14-1015_anno1	97-107	108-118	to not rely much on structural features such as post metadata	since such features are often not uniformly available across forums .	to not rely much on structural features such as post metadata	since such features are often not uniformly available across forums .	93-118	93-118	Our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums .	Our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums .	1<2	none	exp-reason	exp-reason
P14-1015_anno1	46-58	119-124,129-135	In this paper , we present a technique for unsupervised solution post identification	Our clustering-based iterative solution identification approach <*> performs favorably in an empirical evaluation ,	In this paper , we present a technique for unsupervised solution post identification	Our clustering-based iterative solution identification approach <*> performs favorably in an empirical evaluation ,	46-75	119-150	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	Our clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin .	1<2	none	evaluation	evaluation
P14-1015_anno1	119-124,129-135	125-128	Our clustering-based iterative solution identification approach <*> performs favorably in an empirical evaluation ,	based on the EM-formulation	Our clustering-based iterative solution identification approach <*> performs favorably in an empirical evaluation ,	based on the EM-formulation	119-150	119-150	Our clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin .	Our clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin .	1<2	none	manner-means	manner-means
P14-1015_anno1	129-135	136-150	performs favorably in an empirical evaluation ,	beating the only unsupervised solution identification technique from literature by a very large margin .	performs favorably in an empirical evaluation ,	beating the only unsupervised solution identification technique from literature by a very large margin .	119-150	119-150	Our clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin .	Our clustering-based iterative solution identification approach based on the EM-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin .	1<2	none	exp-evidence	exp-evidence
P14-1015_anno1	151-153	154-161	We also show	that our unsupervised technique is competitive against methods	We also show	that our unsupervised technique is competitive against methods	151-171	151-171	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	1>2	none	attribution	attribution
P14-1015_anno1	46-58	154-161	In this paper , we present a technique for unsupervised solution post identification	that our unsupervised technique is competitive against methods	In this paper , we present a technique for unsupervised solution post identification	that our unsupervised technique is competitive against methods	46-75	151-171	In this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions .	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	1<2	none	evaluation	evaluation
P14-1015_anno1	154-161	162-165	that our unsupervised technique is competitive against methods	that require supervision ,	that our unsupervised technique is competitive against methods	that require supervision ,	151-171	151-171	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	1<2	none	elab-addition	elab-addition
P14-1015_anno1	154-161	166-171	that our unsupervised technique is competitive against methods	outperforming one such technique comfortably .	that our unsupervised technique is competitive against methods	outperforming one such technique comfortably .	151-171	151-171	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	We also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .	1<2	none	exp-evidence	exp-evidence
P14-1016_anno1	1-12	13-21	While user attribute extraction on social media has received considerable attention ,	existing approaches , mostly supervised , encounter great difficulty	While user attribute extraction on social media has received considerable attention ,	existing approaches , mostly supervised , encounter great difficulty	1-40	1-40	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	1>2	none	contrast	contrast
P14-1016_anno1	13-21	41-56	existing approaches , mostly supervised , encounter great difficulty	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	existing approaches , mostly supervised , encounter great difficulty	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	1-40	41-56	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	1>2	none	bg-compare	bg-compare
P14-1016_anno1	13-21	22-26	existing approaches , mostly supervised , encounter great difficulty	in obtaining gold standard data	existing approaches , mostly supervised , encounter great difficulty	in obtaining gold standard data	1-40	1-40	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	1<2	none	elab-addition	elab-addition
P14-1016_anno1	13-21	27-34	existing approaches , mostly supervised , encounter great difficulty	and are therefore limited to predicting unary predicates	existing approaches , mostly supervised , encounter great difficulty	and are therefore limited to predicting unary predicates	1-40	1-40	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	1<2	none	cause	cause
P14-1016_anno1	27-34	35-40	and are therefore limited to predicting unary predicates	( e.g. , gender ) .	and are therefore limited to predicting unary predicates	( e.g. , gender ) .	1-40	1-40	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	While user attribute extraction on social media has received considerable attention , existing approaches , mostly supervised , encounter great difficulty in obtaining gold standard data and are therefore limited to predicting unary predicates ( e.g. , gender ) .	1<2	none	elab-example	elab-example
P14-1016_anno1	41-56	57-85	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	Users' profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text .	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	Users' profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text .	41-56	57-85	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	Users' profiles from social media websites such as Facebook or Google Plus are used as a distant source of supervision for extraction of their attributes from user-generated text .	1<2	none	elab-aspect	elab-aspect
P14-1016_anno1	86-91	92-99	In addition to traditional linguistic features	used in distant supervision for information extraction ,	In addition to traditional linguistic features	used in distant supervision for information extraction ,	86-116	86-116	In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information , a unique opportunity offered by social media .	In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information , a unique opportunity offered by social media .	1<2	none	elab-addition	elab-addition
P14-1016_anno1	41-56	86-91,100-111	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	<*> In addition to traditional linguistic features <*> our approach also takes into account network information , a unique opportunity	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	In addition to traditional linguistic features <*> our approach also takes into account network information , a unique opportunity	41-56	86-116	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information , a unique opportunity offered by social media .	1<2	none	elab-aspect	elab-aspect
P14-1016_anno1	86-91,100-111	112-116	<*> In addition to traditional linguistic features <*> our approach also takes into account network information , a unique opportunity	offered by social media .	In addition to traditional linguistic features <*> our approach also takes into account network information , a unique opportunity	offered by social media .	86-116	86-116	In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information , a unique opportunity offered by social media .	In addition to traditional linguistic features used in distant supervision for information extraction , our approach also takes into account network information , a unique opportunity offered by social media .	1<2	none	elab-addition	elab-addition
P14-1016_anno1	41-56	117-125	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	We test our algorithm on three attribute domains :	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	We test our algorithm on three attribute domains :	41-56	117-149	In this paper , we present a weaklysupervised approach to user profile extraction from Twitter .	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	1<2	none	evaluation	evaluation
P14-1016_anno1	117-125	126-131	We test our algorithm on three attribute domains :	spouse , education and job ;	We test our algorithm on three attribute domains :	spouse , education and job ;	117-149	117-149	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	1<2	none	elab-enumember	elab-enumember
P14-1016_anno1	132-134	135-145	experimental results demonstrate	our approach is able to make accurate predictions for users' attributes	experimental results demonstrate	our approach is able to make accurate predictions for users' attributes	117-149	117-149	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	1>2	none	attribution	attribution
P14-1016_anno1	117-125	135-145	We test our algorithm on three attribute domains :	our approach is able to make accurate predictions for users' attributes	We test our algorithm on three attribute domains :	our approach is able to make accurate predictions for users' attributes	117-149	117-149	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	1<2	none	cause	cause
P14-1016_anno1	135-145	146-149	our approach is able to make accurate predictions for users' attributes	based on their tweets.	our approach is able to make accurate predictions for users' attributes	based on their tweets.	117-149	117-149	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	We test our algorithm on three attribute domains : spouse , education and job ; experimental results demonstrate our approach is able to make accurate predictions for users' attributes based on their tweets.	1<2	none	bg-general	bg-general
P14-1017_anno1	1-3	15-24	Consider a person	He/she can spend hours trying to craft the message .	Consider a person	He/she can spend hours trying to craft the message .	1-14	15-24	Consider a person trying to spread an important message on a social network .	He/she can spend hours trying to craft the message .	1>2	none	bg-general	bg-general
P14-1017_anno1	1-3	4-14	Consider a person	trying to spread an important message on a social network .	Consider a person	trying to spread an important message on a social network .	1-14	1-14	Consider a person trying to spread an important message on a social network .	Consider a person trying to spread an important message on a social network .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	15-24	25-29	He/she can spend hours trying to craft the message .	Does it actually matter ?	He/she can spend hours trying to craft the message .	Does it actually matter ?	15-24	25-29	He/she can spend hours trying to craft the message .	Does it actually matter ?	1>2	none	result	result
P14-1017_anno1	25-29	45-54	Does it actually matter ?	the effect of wording per se has rarely been studied	Does it actually matter ?	the effect of wording per se has rarely been studied	25-29	30-69	Does it actually matter ?	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	1>2	none	bg-general	bg-general
P14-1017_anno1	30-36	45-54	While there has been extensive prior work	the effect of wording per se has rarely been studied	While there has been extensive prior work	the effect of wording per se has rarely been studied	30-69	30-69	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	1>2	none	contrast	contrast
P14-1017_anno1	30-36	37-44	While there has been extensive prior work	looking into predicting popularity of social-media content ,	While there has been extensive prior work	looking into predicting popularity of social-media content ,	30-69	30-69	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	45-54	113-118	the effect of wording per se has rarely been studied	which version attracts more retweets ?	the effect of wording per se has rarely been studied	which version attracts more retweets ?	30-69	106-118	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	Given such pairs , we ask : which version attracts more retweets ?	1>2	none	bg-general	bg-general
P14-1017_anno1	45-54	55-69	the effect of wording per se has rarely been studied	since it is often confounded with the popularity of the author and the topic .	the effect of wording per se has rarely been studied	since it is often confounded with the popularity of the author and the topic .	30-69	30-69	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	While there has been extensive prior work looking into predicting popularity of social-media content , the effect of wording per se has rarely been studied since it is often confounded with the popularity of the author and the topic .	1<2	none	exp-reason	exp-reason
P14-1017_anno1	70-76	77-83	To control for these confounding factors ,	we take advantage of the surprising fact	To control for these confounding factors ,	we take advantage of the surprising fact	70-105	70-105	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	1>2	none	enablement	enablement
P14-1017_anno1	77-83	113-118	we take advantage of the surprising fact	which version attracts more retweets ?	we take advantage of the surprising fact	which version attracts more retweets ?	70-105	106-118	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	Given such pairs , we ask : which version attracts more retweets ?	1>2	none	bg-general	bg-general
P14-1017_anno1	77-83	84-90	we take advantage of the surprising fact	that there are many pairs of tweets	we take advantage of the surprising fact	that there are many pairs of tweets	70-105	70-105	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	84-90	91-94	that there are many pairs of tweets	containing the same url	that there are many pairs of tweets	containing the same url	70-105	70-105	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	91-94	95-100	containing the same url	and written by the same user	containing the same url	and written by the same user	70-105	70-105	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	1<2	none	joint	joint
P14-1017_anno1	91-94	101-105	containing the same url	but employing different wording .	containing the same url	but employing different wording .	70-105	70-105	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	To control for these confounding factors , we take advantage of the surprising fact that there are many pairs of tweets containing the same url and written by the same user but employing different wording .	1<2	none	contrast	contrast
P14-1017_anno1	106-109	110-112	Given such pairs ,	we ask :	Given such pairs ,	we ask :	106-118	106-118	Given such pairs , we ask : which version attracts more retweets ?	Given such pairs , we ask : which version attracts more retweets ?	1>2	none	condition	condition
P14-1017_anno1	110-112	113-118	we ask :	which version attracts more retweets ?	we ask :	which version attracts more retweets ?	106-118	106-118	Given such pairs , we ask : which version attracts more retweets ?	Given such pairs , we ask : which version attracts more retweets ?	1>2	none	attribution	attribution
P14-1017_anno1	113-118	150-153,156-168	which version attracts more retweets ?	and the computational methods <*> can do better than both an average human and a strong competing method	which version attracts more retweets ?	and the computational methods <*> can do better than both an average human and a strong competing method	106-118	133-173	Given such pairs , we ask : which version attracts more retweets ?	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	1>2	none	bg-goal	bg-goal
P14-1017_anno1	113-118	119-127	which version attracts more retweets ?	This turns out to be a more difficult task	which version attracts more retweets ?	This turns out to be a more difficult task	106-118	119-132	Given such pairs , we ask : which version attracts more retweets ?	This turns out to be a more difficult task than predicting popular topics .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	119-127	128-132	This turns out to be a more difficult task	than predicting popular topics .	This turns out to be a more difficult task	than predicting popular topics .	119-132	119-132	This turns out to be a more difficult task than predicting popular topics .	This turns out to be a more difficult task than predicting popular topics .	1<2	none	comparison	comparison
P14-1017_anno1	133-142	150-153,156-168	Still , humans can answer this question better than chance	and the computational methods <*> can do better than both an average human and a strong competing method	Still , humans can answer this question better than chance	and the computational methods <*> can do better than both an average human and a strong competing method	133-173	133-173	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	1>2	none	comparison	comparison
P14-1017_anno1	133-142	143-149	Still , humans can answer this question better than chance	( but far from perfectly ) ,	Still , humans can answer this question better than chance	( but far from perfectly ) ,	133-173	133-173	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	1<2	none	contrast	contrast
P14-1017_anno1	150-153,156-168	154-155	and the computational methods <*> can do better than both an average human and a strong competing method	we develop	and the computational methods <*> can do better than both an average human and a strong competing method	we develop	133-173	133-173	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	1<2	none	elab-addition	elab-addition
P14-1017_anno1	156-168	169-173	can do better than both an average human and a strong competing method	trained on non-controlled data .	can do better than both an average human and a strong competing method	trained on non-controlled data .	133-173	133-173	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	Still , humans can answer this question better than chance ( but far from perfectly ) , and the computational methods we develop can do better than both an average human and a strong competing method trained on non-controlled data .	1<2	none	elab-addition	elab-addition
P14-1018_anno1	1-16	32-46	Existing models for social media personal analytics assume access to thousands of messages per user ,	we : ( i ) leverage content from the local neighborhood of a user ;	Existing models for social media personal analytics assume access to thousands of messages per user ,	we : ( i ) leverage content from the local neighborhood of a user ;	1-27	28-89	Existing models for social media personal analytics assume access to thousands of messages per user , even though most users author content only sporadically over time .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1>2	none	bg-compare	bg-compare
P14-1018_anno1	1-16	17-27	Existing models for social media personal analytics assume access to thousands of messages per user ,	even though most users author content only sporadically over time .	Existing models for social media personal analytics assume access to thousands of messages per user ,	even though most users author content only sporadically over time .	1-27	1-27	Existing models for social media personal analytics assume access to thousands of messages per user , even though most users author content only sporadically over time .	Existing models for social media personal analytics assume access to thousands of messages per user , even though most users author content only sporadically over time .	1<2	none	contrast	contrast
P14-1018_anno1	28-31	32-46	Given this sparsity ,	we : ( i ) leverage content from the local neighborhood of a user ;	Given this sparsity ,	we : ( i ) leverage content from the local neighborhood of a user ;	28-89	28-89	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1>2	none	bg-general	bg-general
P14-1018_anno1	32-46	47-68	we : ( i ) leverage content from the local neighborhood of a user ;	( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ;	we : ( i ) leverage content from the local neighborhood of a user ;	( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ;	28-89	28-89	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1<2	none	progression	progression
P14-1018_anno1	47-68	69-79	( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ;	and ( iii ) estimate the amount of time and tweets	( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ;	and ( iii ) estimate the amount of time and tweets	28-89	28-89	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1<2	none	progression	progression
P14-1018_anno1	69-79	80-84	and ( iii ) estimate the amount of time and tweets	required for a dynamic model	and ( iii ) estimate the amount of time and tweets	required for a dynamic model	28-89	28-89	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1<2	none	elab-addition	elab-addition
P14-1018_anno1	80-84	85-89	required for a dynamic model	to predict user preferences .	required for a dynamic model	to predict user preferences .	28-89	28-89	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	1<2	none	enablement	enablement
P14-1018_anno1	90-91	103-117	We show	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	We show	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	90-117	90-117	We show that even when limited or no self-authored data is available , language from friend , retweet and user mention communications provide sufficient evidence for prediction .	We show that even when limited or no self-authored data is available , language from friend , retweet and user mention communications provide sufficient evidence for prediction .	1>2	none	attribution	attribution
P14-1018_anno1	92-102	103-117	that even when limited or no self-authored data is available ,	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	that even when limited or no self-authored data is available ,	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	90-117	90-117	We show that even when limited or no self-authored data is available , language from friend , retweet and user mention communications provide sufficient evidence for prediction .	We show that even when limited or no self-authored data is available , language from friend , retweet and user mention communications provide sufficient evidence for prediction .	1>2	none	condition	condition
P14-1018_anno1	32-46	103-117	we : ( i ) leverage content from the local neighborhood of a user ;	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	we : ( i ) leverage content from the local neighborhood of a user ;	language from friend , retweet and user mention communications provide sufficient evidence for prediction .	28-89	90-117	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	We show that even when limited or no self-authored data is available , language from friend , retweet and user mention communications provide sufficient evidence for prediction .	1<2	none	evaluation	evaluation
P14-1018_anno1	118-122	129-136	When updating models over time	that political preference can be often be predicted	When updating models over time	that political preference can be often be predicted	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1>2	none	temporal	temporal
P14-1018_anno1	118-122	123-126	When updating models over time	based on Twitter ,	When updating models over time	based on Twitter ,	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	bg-general	bg-general
P14-1018_anno1	127-128	129-136	we find	that political preference can be often be predicted	we find	that political preference can be often be predicted	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1>2	none	attribution	attribution
P14-1018_anno1	32-46	129-136	we : ( i ) leverage content from the local neighborhood of a user ;	that political preference can be often be predicted	we : ( i ) leverage content from the local neighborhood of a user ;	that political preference can be often be predicted	28-89	118-165	Given this sparsity , we : ( i ) leverage content from the local neighborhood of a user ; ( ii ) evaluate batch models as a function of size and the amount of messages in various types of neighborhoods ; and ( iii ) estimate the amount of time and tweets required for a dynamic model to predict user preferences .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	evaluation	evaluation
P14-1018_anno1	129-136	137-141	that political preference can be often be predicted	using roughly 100 tweets ,	that political preference can be often be predicted	using roughly 100 tweets ,	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	manner-means	manner-means
P14-1018_anno1	137-141	142-149	using roughly 100 tweets ,	depending on the context of user selection ,	using roughly 100 tweets ,	depending on the context of user selection ,	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	elab-addition	elab-addition
P14-1018_anno1	137-141	150-158	using roughly 100 tweets ,	where this could mean hours , or weeks ,	using roughly 100 tweets ,	where this could mean hours , or weeks ,	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	elab-addition	elab-addition
P14-1018_anno1	150-158	159-165	where this could mean hours , or weeks ,	based on the author's tweeting frequency .	where this could mean hours , or weeks ,	based on the author's tweeting frequency .	118-165	118-165	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	When updating models over time based on Twitter , we find that political preference can be often be predicted using roughly 100 tweets , depending on the context of user selection , where this could mean hours , or weeks , based on the author's tweeting frequency .	1<2	none	bg-general	bg-general
P14-1019_anno1	1-16	28-41	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems	that highly expressive scoring functions can be used with substantially simpler inference procedures .	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems	that highly expressive scoring functions can be used with substantially simpler inference procedures .	1-22	23-41	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions .	In contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures .	1>2	none	contrast	contrast
P14-1019_anno1	1-16	17-22	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems	associated with rich scoring functions .	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems	associated with rich scoring functions .	1-22	1-22	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions .	Much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions .	1<2	none	elab-addition	elab-addition
P14-1019_anno1	23-27	28-41	In contrast , we demonstrate	that highly expressive scoring functions can be used with substantially simpler inference procedures .	In contrast , we demonstrate	that highly expressive scoring functions can be used with substantially simpler inference procedures .	23-41	23-41	In contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures .	In contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures .	1>2	none	attribution	attribution
P14-1019_anno1	28-41	42-48	that highly expressive scoring functions can be used with substantially simpler inference procedures .	Specifically , we introduce a sampling-based parser	that highly expressive scoring functions can be used with substantially simpler inference procedures .	Specifically , we introduce a sampling-based parser	23-41	42-56	In contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures .	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	1>2	none	bg-general	bg-general
P14-1019_anno1	42-48	49-56	Specifically , we introduce a sampling-based parser	that can easily handle arbitrary global features .	Specifically , we introduce a sampling-based parser	that can easily handle arbitrary global features .	42-56	42-56	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	1<2	none	elab-addition	elab-addition
P14-1019_anno1	57-60	61-73	Inspired by SampleRank ,	we learn to take guided stochastic steps towards a high scoring parse .	Inspired by SampleRank ,	we learn to take guided stochastic steps towards a high scoring parse .	57-73	57-73	Inspired by SampleRank , we learn to take guided stochastic steps towards a high scoring parse .	Inspired by SampleRank , we learn to take guided stochastic steps towards a high scoring parse .	1>2	none	bg-general	bg-general
P14-1019_anno1	42-48	61-73	Specifically , we introduce a sampling-based parser	we learn to take guided stochastic steps towards a high scoring parse .	Specifically , we introduce a sampling-based parser	we learn to take guided stochastic steps towards a high scoring parse .	42-56	57-73	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	Inspired by SampleRank , we learn to take guided stochastic steps towards a high scoring parse .	1<2	none	elab-aspect	elab-aspect
P14-1019_anno1	42-48	74-77	Specifically , we introduce a sampling-based parser	We introduce two samplers	Specifically , we introduce a sampling-based parser	We introduce two samplers	42-56	74-91	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	We introduce two samplers for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk .	1<2	none	elab-aspect	elab-aspect
P14-1019_anno1	74-77	78-91	We introduce two samplers	for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk .	We introduce two samplers	for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk .	74-91	74-91	We introduce two samplers for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk .	We introduce two samplers for traversing the space of trees , Gibbs and Metropolis-Hastings with Random Walk .	1<2	none	elab-addition	elab-addition
P14-1019_anno1	92-96	97-106	The model outperforms state-of-the-art results	when evaluated on 14 languages of non-projective CoNLL datasets .	The model outperforms state-of-the-art results	when evaluated on 14 languages of non-projective CoNLL datasets .	92-106	92-106	The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets .	The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets .	1>2	none	cause	cause
P14-1019_anno1	42-48	97-106	Specifically , we introduce a sampling-based parser	when evaluated on 14 languages of non-projective CoNLL datasets .	Specifically , we introduce a sampling-based parser	when evaluated on 14 languages of non-projective CoNLL datasets .	42-56	92-106	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	The model outperforms state-of-the-art results when evaluated on 14 languages of non-projective CoNLL datasets .	1<2	none	evaluation	evaluation
P14-1019_anno1	42-48	107-124	Specifically , we introduce a sampling-based parser	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	Specifically , we introduce a sampling-based parser	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	42-56	107-124	Specifically , we introduce a sampling-based parser that can easily handle arbitrary global features .	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	1<2	none	elab-aspect	elab-aspect
P14-1019_anno1	107-124	125-137	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	The resulting method outperforms the best reported results on the CATiB dataset ,	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	The resulting method outperforms the best reported results on the CATiB dataset ,	107-124	125-144	Our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and POS correction .	The resulting method outperforms the best reported results on the CATiB dataset , approaching performance of parsing with gold tags.	1<2	none	evaluation	evaluation
P14-1019_anno1	125-137	138-144	The resulting method outperforms the best reported results on the CATiB dataset ,	approaching performance of parsing with gold tags.	The resulting method outperforms the best reported results on the CATiB dataset ,	approaching performance of parsing with gold tags.	125-144	125-144	The resulting method outperforms the best reported results on the CATiB dataset , approaching performance of parsing with gold tags.	The resulting method outperforms the best reported results on the CATiB dataset , approaching performance of parsing with gold tags.	1<2	none	elab-addition	elab-addition
P14-1020_anno1	1-8	9-21	Due to their origin in computer graphics ,	graphics processing units ( GPUs ) are highly optimized for dense problems ,	Due to their origin in computer graphics ,	graphics processing units ( GPUs ) are highly optimized for dense problems ,	1-34	1-34	Due to their origin in computer graphics , graphics processing units ( GPUs ) are highly optimized for dense problems , where the exact same operation is applied repeatedly to all data points .	Due to their origin in computer graphics , graphics processing units ( GPUs ) are highly optimized for dense problems , where the exact same operation is applied repeatedly to all data points .	1>2	none	exp-reason	exp-reason
P14-1020_anno1	9-21	35-49	graphics processing units ( GPUs ) are highly optimized for dense problems ,	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	graphics processing units ( GPUs ) are highly optimized for dense problems ,	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	1-34	35-54	Due to their origin in computer graphics , graphics processing units ( GPUs ) are highly optimized for dense problems , where the exact same operation is applied repeatedly to all data points .	Natural language processing algorithms , on the other hand , are traditionally constructed in ways that exploit structural sparsity .	1>2	none	contrast	contrast
P14-1020_anno1	9-21	22-34	graphics processing units ( GPUs ) are highly optimized for dense problems ,	where the exact same operation is applied repeatedly to all data points .	graphics processing units ( GPUs ) are highly optimized for dense problems ,	where the exact same operation is applied repeatedly to all data points .	1-34	1-34	Due to their origin in computer graphics , graphics processing units ( GPUs ) are highly optimized for dense problems , where the exact same operation is applied repeatedly to all data points .	Due to their origin in computer graphics , graphics processing units ( GPUs ) are highly optimized for dense problems , where the exact same operation is applied repeatedly to all data points .	1<2	none	elab-addition	elab-addition
P14-1020_anno1	35-49	55-68	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	35-54	55-102	Natural language processing algorithms , on the other hand , are traditionally constructed in ways that exploit structural sparsity .	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	1>2	none	bg-general	bg-general
P14-1020_anno1	35-49	50-54	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	that exploit structural sparsity .	Natural language processing algorithms , on the other hand , are traditionally constructed in ways	that exploit structural sparsity .	35-54	35-54	Natural language processing algorithms , on the other hand , are traditionally constructed in ways that exploit structural sparsity .	Natural language processing algorithms , on the other hand , are traditionally constructed in ways that exploit structural sparsity .	1<2	none	elab-addition	elab-addition
P14-1020_anno1	55-68	103-112	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	In this work , we reintroduce sparsity to GPU parsing	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	In this work , we reintroduce sparsity to GPU parsing	55-102	103-125	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	In this work , we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	1>2	none	bg-compare	bg-compare
P14-1020_anno1	55-68	69-79	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	that sacrifices traditional sparsity in exchange for raw computational power ,	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing	that sacrifices traditional sparsity in exchange for raw computational power ,	55-102	55-102	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	1<2	none	elab-addition	elab-addition
P14-1020_anno1	69-79	80-82	that sacrifices traditional sparsity in exchange for raw computational power ,	obtaining a system	that sacrifices traditional sparsity in exchange for raw computational power ,	obtaining a system	55-102	55-102	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	1<2	none	cause	cause
P14-1020_anno1	80-82	83-102	obtaining a system	that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	obtaining a system	that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	55-102	55-102	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	Recently , Canny et al ( 2013 ) presented an approach to GPU parsing that sacrifices traditional sparsity in exchange for raw computational power , obtaining a system that can compute Viterbi parses for a high-quality grammar at about 164 sentences per second on a mid-range GPU .	1<2	none	elab-addition	elab-addition
P14-1020_anno1	103-112	113-125	In this work , we reintroduce sparsity to GPU parsing	by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	In this work , we reintroduce sparsity to GPU parsing	by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	103-125	103-125	In this work , we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	In this work , we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	1<2	none	manner-means	manner-means
P14-1020_anno1	103-112	126-148	In this work , we reintroduce sparsity to GPU parsing	The resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware .	In this work , we reintroduce sparsity to GPU parsing	The resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware .	103-125	126-148	In this work , we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	The resulting system is capable of computing over 404 Viterbi parses per second more than a 2x speedup on the same hardware .	1<2	none	evaluation	evaluation
P14-1020_anno1	103-112	149-164	In this work , we reintroduce sparsity to GPU parsing	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference ,	In this work , we reintroduce sparsity to GPU parsing	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference ,	103-125	149-191	In this work , we reintroduce sparsity to GPU parsing by adapting a coarse-to-fine pruning approach to the constraints of a GPU .	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference , improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup .	1<2	none	evaluation	evaluation
P14-1020_anno1	149-164	165-184	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference ,	improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference ,	improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second	149-191	149-191	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference , improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup .	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference , improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup .	1<2	none	elab-addition	elab-addition
P14-1020_anno1	165-184	185-191	improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second	using pruning nearly a 6x speedup .	improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second	using pruning nearly a 6x speedup .	149-191	149-191	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference , improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup .	Moreover , our approach allows us to efficiently implement less GPU-friendly minimum Bayes risk inference , improving throughput for this more accurate algorithm from only 32 sentences per second unpruned to over 190 sentences per second using pruning nearly a 6x speedup .	1<2	none	manner-means	manner-means
P14-1021_anno1	1-13	14-23	This paper presents the first dependency model for a shift-reduce CCG parser .	Modelling dependencies is desirable for a number of reasons ,	This paper presents the first dependency model for a shift-reduce CCG parser .	Modelling dependencies is desirable for a number of reasons ,	1-13	14-52	This paper presents the first dependency model for a shift-reduce CCG parser .	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	1<2	none	bg-goal	bg-goal
P14-1021_anno1	14-23	24-33	Modelling dependencies is desirable for a number of reasons ,	including handling the " spurious " ambiguity of CCG ;	Modelling dependencies is desirable for a number of reasons ,	including handling the " spurious " ambiguity of CCG ;	14-52	14-52	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	1<2	none	elab-enumember	elab-enumember
P14-1021_anno1	24-33	34-41	including handling the " spurious " ambiguity of CCG ;	fitting well with the theory of CCG ;	including handling the " spurious " ambiguity of CCG ;	fitting well with the theory of CCG ;	14-52	14-52	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	1<2	none	joint	joint
P14-1021_anno1	34-41	42-45	fitting well with the theory of CCG ;	and optimizing for structures	fitting well with the theory of CCG ;	and optimizing for structures	14-52	14-52	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	1<2	none	joint	joint
P14-1021_anno1	42-45	46-52	and optimizing for structures	which are evaluated at test time .	and optimizing for structures	which are evaluated at test time .	14-52	14-52	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	Modelling dependencies is desirable for a number of reasons , including handling the " spurious " ambiguity of CCG ; fitting well with the theory of CCG ; and optimizing for structures which are evaluated at test time .	1<2	none	elab-addition	elab-addition
P14-1021_anno1	1-13	53-58	This paper presents the first dependency model for a shift-reduce CCG parser .	We develop a novel training technique	This paper presents the first dependency model for a shift-reduce CCG parser .	We develop a novel training technique	1-13	53-70	This paper presents the first dependency model for a shift-reduce CCG parser .	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	1<2	none	elab-aspect	elab-aspect
P14-1021_anno1	53-58	59-63	We develop a novel training technique	using a dependency oracle ,	We develop a novel training technique	using a dependency oracle ,	53-70	53-70	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	1<2	none	manner-means	manner-means
P14-1021_anno1	59-63	64-70	using a dependency oracle ,	in which all derivations are hidden .	using a dependency oracle ,	in which all derivations are hidden .	53-70	53-70	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	1<2	none	elab-addition	elab-addition
P14-1021_anno1	59-63	71-76	using a dependency oracle ,	A challenge arises from the fact	using a dependency oracle ,	A challenge arises from the fact	53-70	71-103	We develop a novel training technique using a dependency oracle , in which all derivations are hidden .	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	1<2	none	elab-addition	elab-addition
P14-1021_anno1	71-76	77-89	A challenge arises from the fact	that the oracle needs to keep track of exponentially many gold-standard derivations ,	A challenge arises from the fact	that the oracle needs to keep track of exponentially many gold-standard derivations ,	71-103	71-103	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	1<2	none	elab-addition	elab-addition
P14-1021_anno1	71-76	90-92	A challenge arises from the fact	which is solved	A challenge arises from the fact	which is solved	71-103	71-103	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	1<2	none	elab-addition	elab-addition
P14-1021_anno1	90-92	93-103	which is solved	by integrating a packed parse forest with the beam-search decoder .	which is solved	by integrating a packed parse forest with the beam-search decoder .	71-103	71-103	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	A challenge arises from the fact that the oracle needs to keep track of exponentially many gold-standard derivations , which is solved by integrating a packed parse forest with the beam-search decoder .	1<2	none	manner-means	manner-means
P14-1021_anno1	104-107	108-125	Standard CCGBank tests show	the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	Standard CCGBank tests show	the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	104-125	104-125	Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	1>2	none	attribution	attribution
P14-1021_anno1	1-13	108-125	This paper presents the first dependency model for a shift-reduce CCG parser .	the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	This paper presents the first dependency model for a shift-reduce CCG parser .	the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	1-13	104-125	This paper presents the first dependency model for a shift-reduce CCG parser .	Standard CCGBank tests show the model achieves up to 1.05 labeled F-score improvements over three existing , competitive CCG parsing models .	1<2	none	evaluation	evaluation
P14-1022_anno1	1-4	5-14	We present a parser	that relies primarily on extracting information directly from surface spans	We present a parser	that relies primarily on extracting information directly from surface spans	1-24	1-24	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	1<2	none	elab-addition	elab-addition
P14-1022_anno1	5-14	15-19	that relies primarily on extracting information directly from surface spans	rather than on propagating information	that relies primarily on extracting information directly from surface spans	rather than on propagating information	1-24	1-24	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	1<2	none	contrast	contrast
P14-1022_anno1	15-19	20-24	rather than on propagating information	through enriched grammar structure .	rather than on propagating information	through enriched grammar structure .	1-24	1-24	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	1<2	none	elab-addition	elab-addition
P14-1022_anno1	25-33	42-57	For example , instead of creating separate grammar symbols	our parser might instead capture the same information from the first word of the NP .	For example , instead of creating separate grammar symbols	our parser might instead capture the same information from the first word of the NP .	25-57	25-57	For example , instead of creating separate grammar symbols to mark the definiteness of an NP , our parser might instead capture the same information from the first word of the NP .	For example , instead of creating separate grammar symbols to mark the definiteness of an NP , our parser might instead capture the same information from the first word of the NP .	1>2	none	contrast	contrast
P14-1022_anno1	25-33	34-41	For example , instead of creating separate grammar symbols	to mark the definiteness of an NP ,	For example , instead of creating separate grammar symbols	to mark the definiteness of an NP ,	25-57	25-57	For example , instead of creating separate grammar symbols to mark the definiteness of an NP , our parser might instead capture the same information from the first word of the NP .	For example , instead of creating separate grammar symbols to mark the definiteness of an NP , our parser might instead capture the same information from the first word of the NP .	1<2	none	enablement	enablement
P14-1022_anno1	1-4	42-57	We present a parser	our parser might instead capture the same information from the first word of the NP .	We present a parser	our parser might instead capture the same information from the first word of the NP .	1-24	25-57	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	For example , instead of creating separate grammar symbols to mark the definiteness of an NP , our parser might instead capture the same information from the first word of the NP .	1<2	none	elab-example	elab-example
P14-1022_anno1	1-4	58-77	We present a parser	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	We present a parser	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	1-24	58-102	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	1<2	none	elab-aspect	elab-aspect
P14-1022_anno1	78-87	88-102	because so many deep syntactic cues have surface reflexes ,	our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	because so many deep syntactic cues have surface reflexes ,	our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	58-102	58-102	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	1>2	none	exp-reason	exp-reason
P14-1022_anno1	58-77	88-102	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	58-102	58-102	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	1<2	none	elab-addition	elab-addition
P14-1022_anno1	58-77	103-126	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser :	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	58-102	103-126	Moving context out of the grammar and onto surface features can greatly simplify the structural component of the parser : because so many deep syntactic cues have surface reflexes , our system can still parse accurately with context-free backbones as minimal as X-bar grammars .	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	1<2	none	elab-addition	elab-addition
P14-1022_anno1	103-126	127-164	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	On the SPMRL 2013 multilingual constituency parsing shared task ( Seddah et al. , 2013 ) , our system outperforms the top single parser system of Björkelund et al. ( 2013 ) on a range of languages .	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	On the SPMRL 2013 multilingual constituency parsing shared task ( Seddah et al. , 2013 ) , our system outperforms the top single parser system of Björkelund et al. ( 2013 ) on a range of languages .	103-126	127-164	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	On the SPMRL 2013 multilingual constituency parsing shared task ( Seddah et al. , 2013 ) , our system outperforms the top single parser system of Björkelund et al. ( 2013 ) on a range of languages .	1<2	none	evaluation	evaluation
P14-1022_anno1	165-173	174-192	In addition, despite being designed for syntactic analysis ,	our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	In addition, despite being designed for syntactic analysis ,	our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	165-192	165-192	In addition, despite being designed for syntactic analysis , our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	In addition, despite being designed for syntactic analysis , our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	1>2	none	contrast	contrast
P14-1022_anno1	103-126	174-192	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	103-126	165-192	Keeping the structural backbone simple and moving features to the surface also allows easy adaptation to new languages and even to new tasks .	In addition, despite being designed for syntactic analysis , our system also achieves state-of-the-art numbers on the structural sentiment task of Socher et al. ( 2013 ) .	1<2	none	evaluation	evaluation
P14-1022_anno1	193-196	197-213	Finally , we show	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	Finally , we show	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	193-217	193-217	Finally , we show that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured via surface features .	Finally , we show that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured via surface features .	1>2	none	attribution	attribution
P14-1022_anno1	1-4	197-213	We present a parser	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	We present a parser	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	1-24	193-217	We present a parser that relies primarily on extracting information directly from surface spans rather than on propagating information through enriched grammar structure .	Finally , we show that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured via surface features .	1<2	none	elab-aspect	elab-aspect
P14-1022_anno1	197-213	214-217	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	via surface features .	that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured	via surface features .	193-217	193-217	Finally , we show that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured via surface features .	Finally , we show that , in both syntactic parsing and sentiment analysis , many broad linguistic trends can be captured via surface features .	1<2	none	elab-addition	elab-addition
P14-1023_anno1	1-2,14-23	31-50	Context-predicting models <*> are the new kids on the distributional semantics block .	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	Context-predicting models <*> are the new kids on the distributional semantics block .	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	1-23	24-50	Context-predicting models ( more commonly known as embeddings or neural language models ) are the new kids on the distributional semantics block .	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	1>2	none	bg-general	bg-general
P14-1023_anno1	1-2,14-23	3-13	Context-predicting models <*> are the new kids on the distributional semantics block .	( more commonly known as embeddings or neural language models )	Context-predicting models <*> are the new kids on the distributional semantics block .	( more commonly known as embeddings or neural language models )	1-23	1-23	Context-predicting models ( more commonly known as embeddings or neural language models ) are the new kids on the distributional semantics block .	Context-predicting models ( more commonly known as embeddings or neural language models ) are the new kids on the distributional semantics block .	1<2	none	elab-definition	elab-definition
P14-1023_anno1	24-26	31-50	Despite the buzz	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	Despite the buzz	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	24-50	24-50	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	1>2	none	contrast	contrast
P14-1023_anno1	24-26	27-30	Despite the buzz	surrounding these models ,	Despite the buzz	surrounding these models ,	24-50	24-50	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	1<2	none	elab-addition	elab-addition
P14-1023_anno1	31-50	51-75	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	24-50	51-75	Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches .	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	1>2	none	bg-general	bg-general
P14-1023_anno1	76-84	85-91	The results , to our own surprise , show	that the buzz is fully justified ,	The results , to our own surprise , show	that the buzz is fully justified ,	76-106	76-106	The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	1>2	none	attribution	attribution
P14-1023_anno1	51-75	85-91	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	that the buzz is fully justified ,	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	that the buzz is fully justified ,	51-75	76-106	In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings .	The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	1<2	none	elab-aspect	elab-aspect
P14-1023_anno1	85-91	92-106	that the buzz is fully justified ,	as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	that the buzz is fully justified ,	as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	76-106	76-106	The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts .	1<2	none	exp-reason	exp-reason
P14-1024_anno1	1-2	3-18	We show	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	We show	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	1-31	1-31	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	1>2	none	attribution	attribution
P14-1024_anno1	3-18	32-35	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	Our model is constructed	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	Our model is constructed	1-31	32-52	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	1>2	none	bg-general	bg-general
P14-1024_anno1	3-18	19-25	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	using lexical semantic features of the words	that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically	using lexical semantic features of the words	1-31	1-31	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	1<2	none	manner-means	manner-means
P14-1024_anno1	19-25	26-31	using lexical semantic features of the words	that participate in the construction .	using lexical semantic features of the words	that participate in the construction .	1-31	1-31	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	We show that it is possible to reliably discriminate whether a syntactic construction is meant literally or metaphorically using lexical semantic features of the words that participate in the construction .	1<2	none	elab-addition	elab-addition
P14-1024_anno1	32-35	36-39	Our model is constructed	using English resources ,	Our model is constructed	using English resources ,	32-52	32-52	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	1<2	none	manner-means	manner-means
P14-1024_anno1	32-35	40-52	Our model is constructed	and we obtain state-of-the-art performance relative to previous work in this language .	Our model is constructed	and we obtain state-of-the-art performance relative to previous work in this language .	32-52	32-52	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	1<2	none	evaluation	evaluation
P14-1024_anno1	53-64	67-76	Using a model transfer approach by pivoting through a bilingual dictionary ,	our model can identify metaphoric expressions in other languages .	Using a model transfer approach by pivoting through a bilingual dictionary ,	our model can identify metaphoric expressions in other languages .	53-76	53-76	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	1>2	none	manner-means	manner-means
P14-1024_anno1	65-66	67-76	we show	our model can identify metaphoric expressions in other languages .	we show	our model can identify metaphoric expressions in other languages .	53-76	53-76	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	1>2	none	attribution	attribution
P14-1024_anno1	32-35	67-76	Our model is constructed	our model can identify metaphoric expressions in other languages .	Our model is constructed	our model can identify metaphoric expressions in other languages .	32-52	53-76	Our model is constructed using English resources , and we obtain state-of-the-art performance relative to previous work in this language .	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	1<2	none	evaluation	evaluation
P14-1024_anno1	67-76	77-92	our model can identify metaphoric expressions in other languages .	We provide results on three new test sets in Spanish , Farsi , and Russian .	our model can identify metaphoric expressions in other languages .	We provide results on three new test sets in Spanish , Farsi , and Russian .	53-76	77-92	Using a model transfer approach by pivoting through a bilingual dictionary , we show our model can identify metaphoric expressions in other languages .	We provide results on three new test sets in Spanish , Farsi , and Russian .	1<2	none	elab-addition	elab-addition
P14-1024_anno1	77-92	93-97	We provide results on three new test sets in Spanish , Farsi , and Russian .	The results support the hypothesis	We provide results on three new test sets in Spanish , Farsi , and Russian .	The results support the hypothesis	77-92	93-109	We provide results on three new test sets in Spanish , Farsi , and Russian .	The results support the hypothesis that metaphors are conceptual , rather than lexical , in nature .	1<2	none	elab-addition	elab-addition
P14-1024_anno1	93-97	98-109	The results support the hypothesis	that metaphors are conceptual , rather than lexical , in nature .	The results support the hypothesis	that metaphors are conceptual , rather than lexical , in nature .	93-109	93-109	The results support the hypothesis that metaphors are conceptual , rather than lexical , in nature .	The results support the hypothesis that metaphors are conceptual , rather than lexical , in nature .	1<2	none	elab-addition	elab-addition
P14-1025_anno1	1-15	25-38	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	1-24	25-48	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data .	Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions .	1>2	none	bg-general	bg-general
P14-1025_anno1	1-15	16-24	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD	due to their non-reliance on expensive annotated data .	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD	due to their non-reliance on expensive annotated data .	1-24	1-24	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data .	Unsupervised word sense disambiguation ( WSD ) methods are an attractive approach to all-words WSD due to their non-reliance on expensive annotated data .	1<2	none	exp-reason	exp-reason
P14-1025_anno1	25-38	49-62	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	25-48	49-95	Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions .	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	1>2	none	bg-general	bg-general
P14-1025_anno1	25-38	39-48	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	due to the skewed nature of word sense distributions .	Unsupervised estimates of sense frequency have been shown to be very useful for WSD	due to the skewed nature of word sense distributions .	25-48	25-48	Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions .	Unsupervised estimates of sense frequency have been shown to be very useful for WSD due to the skewed nature of word sense distributions .	1<2	none	exp-reason	exp-reason
P14-1025_anno1	49-62	63-73	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	which is highly portable to different corpora and sense inventories ,	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	which is highly portable to different corpora and sense inventories ,	49-95	49-95	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	1<2	none	elab-addition	elab-addition
P14-1025_anno1	63-73	74-82	which is highly portable to different corpora and sense inventories ,	in being applicable to any part of speech ,	which is highly portable to different corpora and sense inventories ,	in being applicable to any part of speech ,	49-95	49-95	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	1<2	none	joint	joint
P14-1025_anno1	74-82	83-95	in being applicable to any part of speech ,	and not requiring a hierarchical sense inventory , parsing or parallel text .	in being applicable to any part of speech ,	and not requiring a hierarchical sense inventory , parsing or parallel text .	49-95	49-95	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	1<2	none	joint	joint
P14-1025_anno1	49-62	96-119	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation ,	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks	49-95	96-144	This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text .	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	1<2	none	evaluation	evaluation
P14-1025_anno1	96-119	120-122	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks	of detecting senses	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks	of detecting senses	96-144	96-144	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	1<2	none	elab-addition	elab-addition
P14-1025_anno1	120-122	123-129	of detecting senses	which aren't attested in the corpus ,	of detecting senses	which aren't attested in the corpus ,	96-144	96-144	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	1<2	none	elab-addition	elab-addition
P14-1025_anno1	120-122	130-136	of detecting senses	and identifying novel senses in the corpus	of detecting senses	and identifying novel senses in the corpus	96-144	96-144	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	1<2	none	joint	joint
P14-1025_anno1	130-136	137-144	and identifying novel senses in the corpus	which aren't captured in the sense inventory .	and identifying novel senses in the corpus	which aren't captured in the sense inventory .	96-144	96-144	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which aren't attested in the corpus , and identifying novel senses in the corpus which aren't captured in the sense inventory .	1<2	none	elab-addition	elab-addition
P14-1026_anno1	1-4	5-13	We present an approach	for automatically learning to solve algebra word problems .	We present an approach	for automatically learning to solve algebra word problems .	1-13	1-13	We present an approach for automatically learning to solve algebra word problems .	We present an approach for automatically learning to solve algebra word problems .	1<2	none	elab-addition	elab-addition
P14-1026_anno1	1-4	14-19	We present an approach	Our algorithm reasons across sentence boundaries	We present an approach	Our algorithm reasons across sentence boundaries	1-13	14-47	We present an approach for automatically learning to solve algebra word problems .	Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	1<2	none	elab-aspect	elab-aspect
P14-1026_anno1	14-19	20-29	Our algorithm reasons across sentence boundaries	to construct and solve a system of linear equations ,	Our algorithm reasons across sentence boundaries	to construct and solve a system of linear equations ,	14-47	14-47	Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	1<2	none	enablement	enablement
P14-1026_anno1	14-19	30-47	Our algorithm reasons across sentence boundaries	while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	Our algorithm reasons across sentence boundaries	while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	14-47	14-47	Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations , while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text .	1<2	none	temporal	temporal
P14-1026_anno1	1-4	48-54	We present an approach	The learning algorithm uses varied supervision ,	We present an approach	The learning algorithm uses varied supervision ,	1-13	48-64	We present an approach for automatically learning to solve algebra word problems .	The learning algorithm uses varied supervision , including either full equations or just the final answers .	1<2	none	elab-aspect	elab-aspect
P14-1026_anno1	48-54	55-64	The learning algorithm uses varied supervision ,	including either full equations or just the final answers .	The learning algorithm uses varied supervision ,	including either full equations or just the final answers .	48-64	48-64	The learning algorithm uses varied supervision , including either full equations or just the final answers .	The learning algorithm uses varied supervision , including either full equations or just the final answers .	1<2	none	elab-enumember	elab-enumember
P14-1026_anno1	1-4	65-77	We present an approach	We evaluate performance on a newly gathered corpus of algebra word problems ,	We present an approach	We evaluate performance on a newly gathered corpus of algebra word problems ,	1-13	65-94	We present an approach for automatically learning to solve algebra word problems .	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	1<2	none	evaluation	evaluation
P14-1026_anno1	78	79-94	demonstrating	that the system can correctly answer almost 70 % of the questions in the dataset .	demonstrating	that the system can correctly answer almost 70 % of the questions in the dataset .	65-94	65-94	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	1>2	none	attribution	attribution
P14-1026_anno1	65-77	79-94	We evaluate performance on a newly gathered corpus of algebra word problems ,	that the system can correctly answer almost 70 % of the questions in the dataset .	We evaluate performance on a newly gathered corpus of algebra word problems ,	that the system can correctly answer almost 70 % of the questions in the dataset .	65-94	65-94	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	1<2	none	cause	cause
P14-1026_anno1	79-94	95-109	that the system can correctly answer almost 70 % of the questions in the dataset .	This is , to our knowledge , the first learning result for this task .	that the system can correctly answer almost 70 % of the questions in the dataset .	This is , to our knowledge , the first learning result for this task .	65-94	95-109	We evaluate performance on a newly gathered corpus of algebra word problems , demonstrating that the system can correctly answer almost 70 % of the questions in the dataset .	This is , to our knowledge , the first learning result for this task .	1<2	none	elab-addition	elab-addition
P14-1027_anno1	1-5	18-31	Inspired by experimental psychological findings	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	Inspired by experimental psychological findings	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	1-55	1-55	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	1>2	none	bg-general	bg-general
P14-1027_anno1	6	7-17	suggesting	that function words play a special role in word learning ,	suggesting	that function words play a special role in word learning ,	1-55	1-55	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	1>2	none	attribution	attribution
P14-1027_anno1	1-5	7-17	Inspired by experimental psychological findings	that function words play a special role in word learning ,	Inspired by experimental psychological findings	that function words play a special role in word learning ,	1-55	1-55	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	1<2	none	elab-addition	elab-addition
P14-1027_anno1	18-31	32-55	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	1-55	1-55	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	1<2	none	enablement	enablement
P14-1027_anno1	18-31	56-79	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	1-55	56-103	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	1<2	none	evaluation	evaluation
P14-1027_anno1	56-79	80-84	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	compared to a model identical	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	compared to a model identical	56-103	56-103	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	1<2	none	comparison	comparison
P14-1027_anno1	80-84	85-93	compared to a model identical	except that it does not special-case "function words" ,	compared to a model identical	except that it does not special-case "function words" ,	56-103	56-103	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	1<2	none	elab-addition	elab-addition
P14-1027_anno1	56-79	94-103	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	setting a new state-of-the-art of 92.4 % token f-score .	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score	setting a new state-of-the-art of 92.4 % token f-score .	56-103	56-103	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	This modification improves unsupervised word segmentation on the standard Bernstein-Ratner ( 1987 ) corpus of child-directed English by more than 4 % token f-score compared to a model identical except that it does not special-case "function words" , setting a new state-of-the-art of 92.4 % token f-score .	1<2	none	cause	cause
P14-1027_anno1	104-108	109-117	Our function word model assumes	that function words appear at the left periphery ,	Our function word model assumes	that function words appear at the left periphery ,	104-134	104-134	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	1>2	none	attribution	attribution
P14-1027_anno1	18-31	109-117	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	that function words appear at the left periphery ,	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	that function words appear at the left periphery ,	1-55	104-134	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	1<2	none	elab-aspect	elab-aspect
P14-1027_anno1	118-128	129-134	and while this is true of languages such as English ,	it is not true universally .	and while this is true of languages such as English ,	it is not true universally .	104-134	104-134	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	1>2	none	contrast	contrast
P14-1027_anno1	109-117	129-134	that function words appear at the left periphery ,	it is not true universally .	that function words appear at the left periphery ,	it is not true universally .	104-134	104-134	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	Our function word model assumes that function words appear at the left periphery , and while this is true of languages such as English , it is not true universally .	1<2	none	elab-addition	elab-addition
P14-1027_anno1	135-136	137-144	We show	that a learner can use Bayesian model selection	We show	that a learner can use Bayesian model selection	135-170	135-170	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	1>2	none	attribution	attribution
P14-1027_anno1	18-31	137-144	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	that a learner can use Bayesian model selection	we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model	that a learner can use Bayesian model selection	1-55	135-170	Inspired by experimental psychological findings suggesting that function words play a special role in word learning , we make a simple modification to an Adaptor Grammar based Bayesian word segmentation model to allow it to learn sequences of monosyllabic "function words" at the beginnings and endings of collocations of ( possibly multi-syllabic ) words .	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	1<2	none	elab-aspect	elab-aspect
P14-1027_anno1	137-144	145-155	that a learner can use Bayesian model selection	to determine the location of function words in their language ,	that a learner can use Bayesian model selection	to determine the location of function words in their language ,	135-170	135-170	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	1<2	none	enablement	enablement
P14-1027_anno1	137-144	156-170	that a learner can use Bayesian model selection	even though the input to the model only consists of unsegmented sequences of phones .	that a learner can use Bayesian model selection	even though the input to the model only consists of unsegmented sequences of phones .	135-170	135-170	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	1<2	none	elab-addition	elab-addition
P14-1027_anno1	137-144	171-177	that a learner can use Bayesian model selection	Thus our computational models support the hypothesis	that a learner can use Bayesian model selection	Thus our computational models support the hypothesis	135-170	171-188	We show that a learner can use Bayesian model selection to determine the location of function words in their language , even though the input to the model only consists of unsegmented sequences of phones .	Thus our computational models support the hypothesis that function words play a special role in word learning .	1<2	none	cause	cause
P14-1027_anno1	171-177	178-188	Thus our computational models support the hypothesis	that function words play a special role in word learning .	Thus our computational models support the hypothesis	that function words play a special role in word learning .	171-188	171-188	Thus our computational models support the hypothesis that function words play a special role in word learning .	Thus our computational models support the hypothesis that function words play a special role in word learning .	1<2	none	elab-addition	elab-addition
P14-1028_anno1	1-18	28-42	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability	In this paper , we propose a novel neural network model for Chinese word segmentation	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability	In this paper , we propose a novel neural network model for Chinese word segmentation	1-27	28-51	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering .	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	1>2	none	bg-general	bg-general
P14-1028_anno1	1-18	19-27	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability	to alleviate the burden of manual feature engineering .	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability	to alleviate the burden of manual feature engineering .	1-27	1-27	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering .	Recently , neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering .	1<2	none	enablement	enablement
P14-1028_anno1	28-42	43-51	In this paper , we propose a novel neural network model for Chinese word segmentation	called Max-Margin Tensor Neural Network ( MMTNN ) .	In this paper , we propose a novel neural network model for Chinese word segmentation	called Max-Margin Tensor Neural Network ( MMTNN ) .	28-51	28-51	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	1<2	none	elab-addition	elab-addition
P14-1028_anno1	52-59	60-63	By exploiting tag embeddings and tensor-based transformation ,	MMTNN has the ability	By exploiting tag embeddings and tensor-based transformation ,	MMTNN has the ability	52-73	52-73	By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters .	By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters .	1>2	none	manner-means	manner-means
P14-1028_anno1	28-42	60-63	In this paper , we propose a novel neural network model for Chinese word segmentation	MMTNN has the ability	In this paper , we propose a novel neural network model for Chinese word segmentation	MMTNN has the ability	28-51	52-73	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters .	1<2	none	elab-aspect	elab-aspect
P14-1028_anno1	60-63	64-73	MMTNN has the ability	to model complicated interactions between tags and context characters .	MMTNN has the ability	to model complicated interactions between tags and context characters .	52-73	52-73	By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters .	By exploiting tag embeddings and tensor-based transformation , MMTNN has the ability to model complicated interactions between tags and context characters .	1<2	none	elab-addition	elab-addition
P14-1028_anno1	28-42	74-82	In this paper , we propose a novel neural network model for Chinese word segmentation	Furthermore , a new tensor factorization approach is proposed	In this paper , we propose a novel neural network model for Chinese word segmentation	Furthermore , a new tensor factorization approach is proposed	28-51	74-91	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting .	1<2	none	elab-aspect	elab-aspect
P14-1028_anno1	74-82	83-87	Furthermore , a new tensor factorization approach is proposed	to speed up the model	Furthermore , a new tensor factorization approach is proposed	to speed up the model	74-91	74-91	Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting .	Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting .	1<2	none	enablement	enablement
P14-1028_anno1	83-87	88-91	to speed up the model	and avoid overfitting .	to speed up the model	and avoid overfitting .	74-91	74-91	Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting .	Furthermore , a new tensor factorization approach is proposed to speed up the model and avoid overfitting .	1<2	none	joint	joint
P14-1028_anno1	92-97	98-108	Experiments on the benchmark dataset show	that our model achieves better performances than previous neural network models	Experiments on the benchmark dataset show	that our model achieves better performances than previous neural network models	92-122	92-122	Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering .	Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering .	1>2	none	attribution	attribution
P14-1028_anno1	28-42	98-108	In this paper , we propose a novel neural network model for Chinese word segmentation	that our model achieves better performances than previous neural network models	In this paper , we propose a novel neural network model for Chinese word segmentation	that our model achieves better performances than previous neural network models	28-51	92-122	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering .	1<2	none	evaluation	evaluation
P14-1028_anno1	98-108	109-122	that our model achieves better performances than previous neural network models	and that our model can achieve a competitive performance with minimal feature engineering .	that our model achieves better performances than previous neural network models	and that our model can achieve a competitive performance with minimal feature engineering .	92-122	92-122	Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering .	Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering .	1<2	none	joint	joint
P14-1028_anno1	123-131	132-144	Despite Chinese word segmentation being a specific case ,	MMTNN can be easily generalized and applied to other sequence labeling tasks .	Despite Chinese word segmentation being a specific case ,	MMTNN can be easily generalized and applied to other sequence labeling tasks .	123-144	123-144	Despite Chinese word segmentation being a specific case , MMTNN can be easily generalized and applied to other sequence labeling tasks .	Despite Chinese word segmentation being a specific case , MMTNN can be easily generalized and applied to other sequence labeling tasks .	1>2	none	contrast	contrast
P14-1028_anno1	28-42	132-144	In this paper , we propose a novel neural network model for Chinese word segmentation	MMTNN can be easily generalized and applied to other sequence labeling tasks .	In this paper , we propose a novel neural network model for Chinese word segmentation	MMTNN can be easily generalized and applied to other sequence labeling tasks .	28-51	123-144	In this paper , we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network ( MMTNN ) .	Despite Chinese word segmentation being a specific case , MMTNN can be easily generalized and applied to other sequence labeling tasks .	1<2	none	elab-aspect	elab-aspect
P14-1029_anno1	1-13	21-44	Negation words , such as no and not , play a fundamental role	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Negation words , such as no and not , play a fundamental role	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	1-20	21-44	Negation words , such as no and not , play a fundamental role in modifying sentiment of textual expressions .	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	1>2	none	bg-general	bg-general
P14-1029_anno1	1-13	14-20	Negation words , such as no and not , play a fundamental role	in modifying sentiment of textual expressions .	Negation words , such as no and not , play a fundamental role	in modifying sentiment of textual expressions .	1-20	1-20	Negation words , such as no and not , play a fundamental role in modifying sentiment of textual expressions .	Negation words , such as no and not , play a fundamental role in modifying sentiment of textual expressions .	1<2	none	elab-addition	elab-addition
P14-1029_anno1	21-44	45-47,55-61	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Commonly used heuristics <*> rely simply on the sentiment of argument	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Commonly used heuristics <*> rely simply on the sentiment of argument	21-44	45-73	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument ( and not on the negator or the argument itself ) .	1<2	none	bg-compare	bg-compare
P14-1029_anno1	45-47,55-61	48-54	Commonly used heuristics <*> rely simply on the sentiment of argument	to estimate the sentiment of negated expressions	Commonly used heuristics <*> rely simply on the sentiment of argument	to estimate the sentiment of negated expressions	45-73	45-73	Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument ( and not on the negator or the argument itself ) .	Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument ( and not on the negator or the argument itself ) .	1<2	none	elab-addition	elab-addition
P14-1029_anno1	55-61	62-73	rely simply on the sentiment of argument	( and not on the negator or the argument itself ) .	rely simply on the sentiment of argument	( and not on the negator or the argument itself ) .	45-73	45-73	Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument ( and not on the negator or the argument itself ) .	Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument ( and not on the negator or the argument itself ) .	1<2	none	joint	joint
P14-1029_anno1	21-44	74-78	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We use a sentiment treebank	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We use a sentiment treebank	21-44	74-90	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment .	1<2	none	elab-process_step	elab-process_step
P14-1029_anno1	79-80	81-90	to show	that these existing heuristics are poor estimators of sentiment .	to show	that these existing heuristics are poor estimators of sentiment .	74-90	74-90	We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment .	We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment .	1>2	none	attribution	attribution
P14-1029_anno1	74-78	81-90	We use a sentiment treebank	that these existing heuristics are poor estimators of sentiment .	We use a sentiment treebank	that these existing heuristics are poor estimators of sentiment .	74-90	74-90	We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment .	We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment .	1<2	none	cause	cause
P14-1029_anno1	21-44	91-101	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We then modify these heuristics to be dependent on the negators	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We then modify these heuristics to be dependent on the negators	21-44	91-108	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	We then modify these heuristics to be dependent on the negators and show that this improves prediction .	1<2	none	elab-process_step	elab-process_step
P14-1029_anno1	102-103	104-108	and show	that this improves prediction .	and show	that this improves prediction .	91-108	91-108	We then modify these heuristics to be dependent on the negators and show that this improves prediction .	We then modify these heuristics to be dependent on the negators and show that this improves prediction .	1>2	none	attribution	attribution
P14-1029_anno1	91-101	104-108	We then modify these heuristics to be dependent on the negators	that this improves prediction .	We then modify these heuristics to be dependent on the negators	that this improves prediction .	91-108	91-108	We then modify these heuristics to be dependent on the negators and show that this improves prediction .	We then modify these heuristics to be dependent on the negators and show that this improves prediction .	1<2	none	cause	cause
P14-1029_anno1	21-44	109-124	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	21-44	109-134	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 ) that relies on both the negator and the argument .	1<2	none	evaluation	evaluation
P14-1029_anno1	109-124	125-134	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	that relies on both the negator and the argument .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	that relies on both the negator and the argument .	109-134	109-134	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 ) that relies on both the negator and the argument .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 ) that relies on both the negator and the argument .	1<2	none	elab-addition	elab-addition
P14-1029_anno1	109-124	135-151	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	This model learns the syntax and semantics of the negator's argument with a recursive neural network .	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	This model learns the syntax and semantics of the negator's argument with a recursive neural network .	109-134	135-151	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 ) that relies on both the negator and the argument .	This model learns the syntax and semantics of the negator's argument with a recursive neural network .	1<2	none	elab-addition	elab-addition
P14-1029_anno1	152-153	154-160	We show	that this approach performs better than those	We show	that this approach performs better than those	152-163	152-163	We show that this approach performs better than those mentioned above .	We show that this approach performs better than those mentioned above .	1>2	none	attribution	attribution
P14-1029_anno1	109-124	154-160	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	that this approach performs better than those	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 )	that this approach performs better than those	109-134	152-163	Next , we evaluate a recently proposed composition model ( Socher et al. , 2013 ) that relies on both the negator and the argument .	We show that this approach performs better than those mentioned above .	1<2	none	cause	cause
P14-1029_anno1	154-160	161-163	that this approach performs better than those	mentioned above .	that this approach performs better than those	mentioned above .	152-163	152-163	We show that this approach performs better than those mentioned above .	We show that this approach performs better than those mentioned above .	1<2	none	elab-addition	elab-addition
P14-1029_anno1	21-44	164-175	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	In addition , we explicitly incorporate the prior sentiment of the argument	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	In addition , we explicitly incorporate the prior sentiment of the argument	21-44	164-186	We will refer to a negation word as the negator and the text span within the scope of the negator as the argument .	In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .	1<2	none	elab-process_step	elab-process_step
P14-1029_anno1	176-177	178-186	and observe	that this information can help reduce fitting errors .	and observe	that this information can help reduce fitting errors .	164-186	164-186	In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .	In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .	1>2	none	attribution	attribution
P14-1029_anno1	164-175	178-186	In addition , we explicitly incorporate the prior sentiment of the argument	that this information can help reduce fitting errors .	In addition , we explicitly incorporate the prior sentiment of the argument	that this information can help reduce fitting errors .	164-186	164-186	In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .	In addition , we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors .	1<2	none	cause	cause
P14-1030_anno1	1-17	18-23	Extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining .	This paper proposes a novel approach	Extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining .	This paper proposes a novel approach	1-17	18-31	Extracting opinion targets and opinion words from online reviews are two fundamental tasks in opinion mining .	This paper proposes a novel approach to collectively extract them with graph co-ranking .	1>2	none	bg-goal	bg-goal
P14-1030_anno1	18-23	24-31	This paper proposes a novel approach	to collectively extract them with graph co-ranking .	This paper proposes a novel approach	to collectively extract them with graph co-ranking .	18-31	18-31	This paper proposes a novel approach to collectively extract them with graph co-ranking .	This paper proposes a novel approach to collectively extract them with graph co-ranking .	1<2	none	enablement	enablement
P14-1030_anno1	32-37	46-51	First , compared to previous methods	our method constructs a heterogeneous graph	First , compared to previous methods	our method constructs a heterogeneous graph	32-65	32-65	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	1>2	none	comparison	comparison
P14-1030_anno1	32-37	38-45	First , compared to previous methods	which solely employed opinion relations among words ,	First , compared to previous methods	which solely employed opinion relations among words ,	32-65	32-65	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	1<2	none	elab-addition	elab-addition
P14-1030_anno1	18-23	46-51	This paper proposes a novel approach	our method constructs a heterogeneous graph	This paper proposes a novel approach	our method constructs a heterogeneous graph	18-31	32-65	This paper proposes a novel approach to collectively extract them with graph co-ranking .	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	1<2	none	elab-process_step	elab-process_step
P14-1030_anno1	46-51	52-58	our method constructs a heterogeneous graph	to model two types of relations ,	our method constructs a heterogeneous graph	to model two types of relations ,	32-65	32-65	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	1<2	none	enablement	enablement
P14-1030_anno1	52-58	59-65	to model two types of relations ,	including semantic relations and opinion relations .	to model two types of relations ,	including semantic relations and opinion relations .	32-65	32-65	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	First , compared to previous methods which solely employed opinion relations among words , our method constructs a heterogeneous graph to model two types of relations , including semantic relations and opinion relations .	1<2	none	elab-enumember	elab-enumember
P14-1030_anno1	18-23	66-72	This paper proposes a novel approach	Next , a co-ranking algorithm is proposed	This paper proposes a novel approach	Next , a co-ranking algorithm is proposed	18-31	66-93	This paper proposes a novel approach to collectively extract them with graph co-ranking .	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	1<2	none	elab-process_step	elab-process_step
P14-1030_anno1	66-72	73-80	Next , a co-ranking algorithm is proposed	to estimate the confidence of each candidate ,	Next , a co-ranking algorithm is proposed	to estimate the confidence of each candidate ,	66-93	66-93	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	1<2	none	enablement	enablement
P14-1030_anno1	66-72	81-93	Next , a co-ranking algorithm is proposed	and the candidates with higher confidence will be extracted as opinion targets/words .	Next , a co-ranking algorithm is proposed	and the candidates with higher confidence will be extracted as opinion targets/words .	66-93	66-93	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	1<2	none	progression	progression
P14-1030_anno1	66-72	94-107	Next , a co-ranking algorithm is proposed	In this way , different relations make cooperative effects on candidate's confidence estimation .	Next , a co-ranking algorithm is proposed	In this way , different relations make cooperative effects on candidate's confidence estimation .	66-93	94-107	Next , a co-ranking algorithm is proposed to estimate the confidence of each candidate , and the candidates with higher confidence will be extracted as opinion targets/words .	In this way , different relations make cooperative effects on candidate's confidence estimation .	1<2	none	summary	summary
P14-1030_anno1	18-23	108-120	This paper proposes a novel approach	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	This paper proposes a novel approach	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	18-31	108-120	This paper proposes a novel approach to collectively extract them with graph co-ranking .	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	1<2	none	elab-process_step	elab-process_step
P14-1030_anno1	108-120	121-128	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	In this way , our co-ranking is personalized	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	In this way , our co-ranking is personalized	108-120	121-140	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	In this way , our co-ranking is personalized and each candidate's confidence is only determined by its preferred collocations .	1<2	none	summary	summary
P14-1030_anno1	121-128	129-140	In this way , our co-ranking is personalized	and each candidate's confidence is only determined by its preferred collocations .	In this way , our co-ranking is personalized	and each candidate's confidence is only determined by its preferred collocations .	121-140	121-140	In this way , our co-ranking is personalized and each candidate's confidence is only determined by its preferred collocations .	In this way , our co-ranking is personalized and each candidate's confidence is only determined by its preferred collocations .	1<2	none	joint	joint
P14-1030_anno1	108-120	141-148	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	It helps to improve the extraction precision .	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	It helps to improve the extraction precision .	108-120	141-148	Moreover , word preference is captured and incorporated into our co-ranking algorithm .	It helps to improve the extraction precision .	1<2	none	elab-addition	elab-addition
P14-1030_anno1	149-161	162-171	The experimental results on three data sets with different sizes and languages show	that our approach achieves better performance than state-of-the-art methods .	The experimental results on three data sets with different sizes and languages show	that our approach achieves better performance than state-of-the-art methods .	149-171	149-171	The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods .	The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods .	1>2	none	attribution	attribution
P14-1030_anno1	18-23	162-171	This paper proposes a novel approach	that our approach achieves better performance than state-of-the-art methods .	This paper proposes a novel approach	that our approach achieves better performance than state-of-the-art methods .	18-31	149-171	This paper proposes a novel approach to collectively extract them with graph co-ranking .	The experimental results on three data sets with different sizes and languages show that our approach achieves better performance than state-of-the-art methods .	1<2	none	evaluation	evaluation
P14-1031_anno1	1-7	8-17	This paper proposes a novel context-aware method	for analyzing sentiment at the level of individual sentences .	This paper proposes a novel context-aware method	for analyzing sentiment at the level of individual sentences .	1-17	1-17	This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences .	This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences .	1<2	none	elab-addition	elab-addition
P14-1031_anno1	18-34	50-59	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences	In contrast , our approach allows structured modeling of sentiment	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences	In contrast , our approach allows structured modeling of sentiment	18-49	50-70	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation .	In contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information .	1>2	none	contrast	contrast
P14-1031_anno1	18-34	35-42	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences	and often fail to capture non-local contextual cues	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences	and often fail to capture non-local contextual cues	18-49	18-49	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation .	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation .	1<2	none	joint	joint
P14-1031_anno1	35-42	43-49	and often fail to capture non-local contextual cues	that are important for sentiment interpretation .	and often fail to capture non-local contextual cues	that are important for sentiment interpretation .	18-49	18-49	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation .	Most existing machine learning approaches suffer from limitations in the modeling of complex linguistic structures across sentences and often fail to capture non-local contextual cues that are important for sentiment interpretation .	1<2	none	elab-addition	elab-addition
P14-1031_anno1	1-7	50-59	This paper proposes a novel context-aware method	In contrast , our approach allows structured modeling of sentiment	This paper proposes a novel context-aware method	In contrast , our approach allows structured modeling of sentiment	1-17	50-70	This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences .	In contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information .	1<2	none	elab-aspect	elab-aspect
P14-1031_anno1	50-59	60-70	In contrast , our approach allows structured modeling of sentiment	while taking into account both local and global contextual information .	In contrast , our approach allows structured modeling of sentiment	while taking into account both local and global contextual information .	50-70	50-70	In contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information .	In contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information .	1<2	none	temporal	temporal
P14-1031_anno1	50-59	71-82	In contrast , our approach allows structured modeling of sentiment	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	In contrast , our approach allows structured modeling of sentiment	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	50-70	71-97	In contrast , our approach allows structured modeling of sentiment while taking into account both local and global contextual information .	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	1<2	none	elab-addition	elab-addition
P14-1031_anno1	71-82	83-93	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	and integrate them into the learning of conditional random field models	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	and integrate them into the learning of conditional random field models	71-97	71-97	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	1<2	none	progression	progression
P14-1031_anno1	83-93	94-97	and integrate them into the learning of conditional random field models	via posterior regularization .	and integrate them into the learning of conditional random field models	via posterior regularization .	71-97	71-97	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	1<2	none	manner-means	manner-means
P14-1031_anno1	71-82	98-107	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	The context-aware constraints provide additional power to the CRF model	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints	The context-aware constraints provide additional power to the CRF model	71-97	98-118	Specifically , we encode intuitive lexical and discourse knowledge as expressive constraints and integrate them into the learning of conditional random field models via posterior regularization .	The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited .	1<2	none	elab-addition	elab-addition
P14-1031_anno1	98-107	108-112	The context-aware constraints provide additional power to the CRF model	and can guide semi-supervised learning	The context-aware constraints provide additional power to the CRF model	and can guide semi-supervised learning	98-118	98-118	The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited .	The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited .	1<2	none	joint	joint
P14-1031_anno1	108-112	113-118	and can guide semi-supervised learning	when labeled data is limited .	and can guide semi-supervised learning	when labeled data is limited .	98-118	98-118	The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited .	The context-aware constraints provide additional power to the CRF model and can guide semi-supervised learning when labeled data is limited .	1<2	none	condition	condition
P14-1031_anno1	119-125	126-140	Experiments on standard product review datasets show	that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	Experiments on standard product review datasets show	that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	119-140	119-140	Experiments on standard product review datasets show that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	Experiments on standard product review datasets show that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	1>2	none	attribution	attribution
P14-1031_anno1	1-7	126-140	This paper proposes a novel context-aware method	that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	This paper proposes a novel context-aware method	that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	1-17	119-140	This paper proposes a novel context-aware method for analyzing sentiment at the level of individual sentences .	Experiments on standard product review datasets show that our method outperforms the state-of-the-art methods in both the supervised and semi-supervised settings .	1<2	none	evaluation	evaluation
P14-1032_anno1	1-12	41-49	Product feature mining is a key subtask in fine-grained opinion mining .	This paper proposes a novel product feature mining method	Product feature mining is a key subtask in fine-grained opinion mining .	This paper proposes a novel product feature mining method	1-12	41-57	Product feature mining is a key subtask in fine-grained opinion mining .	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	1>2	none	bg-goal	bg-goal
P14-1032_anno1	13-22	41-49	Previous works often use syntax constituents in this task .	This paper proposes a novel product feature mining method	Previous works often use syntax constituents in this task .	This paper proposes a novel product feature mining method	13-22	41-57	Previous works often use syntax constituents in this task .	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	1>2	none	bg-compare	bg-compare
P14-1032_anno1	13-22	23-33	Previous works often use syntax constituents in this task .	However , syntax-based methods can only use discrete contextual information ,	Previous works often use syntax constituents in this task .	However , syntax-based methods can only use discrete contextual information ,	13-22	23-40	Previous works often use syntax constituents in this task .	However , syntax-based methods can only use discrete contextual information , which may suffer from data sparsity .	1<2	none	contrast	contrast
P14-1032_anno1	23-33	34-40	However , syntax-based methods can only use discrete contextual information ,	which may suffer from data sparsity .	However , syntax-based methods can only use discrete contextual information ,	which may suffer from data sparsity .	23-40	23-40	However , syntax-based methods can only use discrete contextual information , which may suffer from data sparsity .	However , syntax-based methods can only use discrete contextual information , which may suffer from data sparsity .	1<2	none	elab-addition	elab-addition
P14-1032_anno1	41-49	50-57	This paper proposes a novel product feature mining method	which leverages lexical and contextual semantic clues .	This paper proposes a novel product feature mining method	which leverages lexical and contextual semantic clues .	41-57	41-57	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	1<2	none	elab-addition	elab-addition
P14-1032_anno1	50-57	58-72	which leverages lexical and contextual semantic clues .	Lexical semantic clue verifies whether a candidate term is related to the target product ,	which leverages lexical and contextual semantic clues .	Lexical semantic clue verifies whether a candidate term is related to the target product ,	41-57	58-103	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	1<2	none	elab-addition	elab-addition
P14-1032_anno1	58-72	73-82	Lexical semantic clue verifies whether a candidate term is related to the target product ,	and contextual semantic clue serves as a soft pattern miner	Lexical semantic clue verifies whether a candidate term is related to the target product ,	and contextual semantic clue serves as a soft pattern miner	58-103	58-103	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	1<2	none	joint	joint
P14-1032_anno1	73-82	83-86	and contextual semantic clue serves as a soft pattern miner	to find candidates ,	and contextual semantic clue serves as a soft pattern miner	to find candidates ,	58-103	58-103	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	1<2	none	enablement	enablement
P14-1032_anno1	73-82	87-94	and contextual semantic clue serves as a soft pattern miner	which exploits semantics of each word in context	and contextual semantic clue serves as a soft pattern miner	which exploits semantics of each word in context	58-103	58-103	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	1<2	none	elab-definition	elab-definition
P14-1032_anno1	87-94	95-103	which exploits semantics of each word in context	so as to alleviate the data sparsity problem .	which exploits semantics of each word in context	so as to alleviate the data sparsity problem .	58-103	58-103	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	Lexical semantic clue verifies whether a candidate term is related to the target product , and contextual semantic clue serves as a soft pattern miner to find candidates , which exploits semantics of each word in context so as to alleviate the data sparsity problem .	1<2	none	enablement	enablement
P14-1032_anno1	41-49	104-109	This paper proposes a novel product feature mining method	We build a semantic similarity graph	This paper proposes a novel product feature mining method	We build a semantic similarity graph	41-57	104-127	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	1<2	none	elab-process_step	elab-process_step
P14-1032_anno1	104-109	110-115	We build a semantic similarity graph	to encode lexical semantic clue ,	We build a semantic similarity graph	to encode lexical semantic clue ,	104-127	104-127	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	1<2	none	enablement	enablement
P14-1032_anno1	41-49	116-121	This paper proposes a novel product feature mining method	and employ a convolutional neural model	This paper proposes a novel product feature mining method	and employ a convolutional neural model	41-57	104-127	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	1<2	none	elab-process_step	elab-process_step
P14-1032_anno1	116-121	122-127	and employ a convolutional neural model	to capture contextual semantic clue .	and employ a convolutional neural model	to capture contextual semantic clue .	104-127	104-127	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	We build a semantic similarity graph to encode lexical semantic clue , and employ a convolutional neural model to capture contextual semantic clue .	1<2	none	enablement	enablement
P14-1032_anno1	41-49	128-138	This paper proposes a novel product feature mining method	Then Label Propagation is applied to combine both semantic clues .	This paper proposes a novel product feature mining method	Then Label Propagation is applied to combine both semantic clues .	41-57	128-138	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	Then Label Propagation is applied to combine both semantic clues .	1<2	none	elab-process_step	elab-process_step
P14-1032_anno1	139-141	142-151	Experimental results show	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	Experimental results show	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	139-168	139-168	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	1>2	none	attribution	attribution
P14-1032_anno1	41-49	142-151	This paper proposes a novel product feature mining method	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	This paper proposes a novel product feature mining method	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	41-57	139-168	This paper proposes a novel product feature mining method which leverages lexical and contextual semantic clues .	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	1<2	none	evaluation	evaluation
P14-1032_anno1	142-151	152-160	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	which not only mines product features more accurately ,	that our semantics-based method significantly outperforms conventional syntax-based approaches ,	which not only mines product features more accurately ,	139-168	139-168	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	1<2	none	elab-addition	elab-addition
P14-1032_anno1	152-160	161-168	which not only mines product features more accurately ,	but also extracts more infrequent product features .	which not only mines product features more accurately ,	but also extracts more infrequent product features .	139-168	139-168	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	Experimental results show that our semantics-based method significantly outperforms conventional syntax-based approaches , which not only mines product features more accurately , but also extracts more infrequent product features .	1<2	none	joint	joint
P14-1033_anno1	1-10	11-20	Aspect extraction is an important task in sentiment analysis .	Topic modeling is a popular method for the task .	Aspect extraction is an important task in sentiment analysis .	Topic modeling is a popular method for the task .	1-10	11-20	Aspect extraction is an important task in sentiment analysis .	Topic modeling is a popular method for the task .	1>2	none	bg-goal	bg-goal
P14-1033_anno1	11-20	21-30	Topic modeling is a popular method for the task .	However , unsupervised topic models often generate incoherent aspects .	Topic modeling is a popular method for the task .	However , unsupervised topic models often generate incoherent aspects .	11-20	21-30	Topic modeling is a popular method for the task .	However , unsupervised topic models often generate incoherent aspects .	1>2	none	contrast	contrast
P14-1033_anno1	21-30	36-41	However , unsupervised topic models often generate incoherent aspects .	several knowledge-based models have been proposed	However , unsupervised topic models often generate incoherent aspects .	several knowledge-based models have been proposed	21-30	31-53	However , unsupervised topic models often generate incoherent aspects .	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	1>2	none	bg-compare	bg-compare
P14-1033_anno1	31-35	36-41	To address the issue ,	several knowledge-based models have been proposed	To address the issue ,	several knowledge-based models have been proposed	31-53	31-53	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	1>2	none	enablement	enablement
P14-1033_anno1	36-41	66-97	several knowledge-based models have been proposed	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	several knowledge-based models have been proposed	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	31-53	54-97	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	1>2	none	bg-compare	bg-compare
P14-1033_anno1	36-41	42-45	several knowledge-based models have been proposed	to incorporate prior knowledge	several knowledge-based models have been proposed	to incorporate prior knowledge	31-53	31-53	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	1<2	none	enablement	enablement
P14-1033_anno1	42-45	46-49	to incorporate prior knowledge	provided by the user	to incorporate prior knowledge	provided by the user	31-53	31-53	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	1<2	none	elab-addition	elab-addition
P14-1033_anno1	42-45	50-53	to incorporate prior knowledge	to guide modeling .	to incorporate prior knowledge	to guide modeling .	31-53	31-53	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	To address the issue , several knowledge-based models have been proposed to incorporate prior knowledge provided by the user to guide modeling .	1<2	none	enablement	enablement
P14-1033_anno1	64-65	66-97	and show	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	and show	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	54-97	54-97	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	1>2	none	attribution	attribution
P14-1033_anno1	54-63	66-97	In this paper , we take a major step forward	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	In this paper , we take a major step forward	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	54-97	54-97	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	1<2	none	elab-addition	elab-addition
P14-1033_anno1	66-97	98-107	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	Such knowledge can then be used by a topic model	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	Such knowledge can then be used by a topic model	54-97	98-113	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	Such knowledge can then be used by a topic model to discover more coherent aspects .	1<2	none	elab-addition	elab-addition
P14-1033_anno1	98-107	108-113	Such knowledge can then be used by a topic model	to discover more coherent aspects .	Such knowledge can then be used by a topic model	to discover more coherent aspects .	98-113	98-113	Such knowledge can then be used by a topic model to discover more coherent aspects .	Such knowledge can then be used by a topic model to discover more coherent aspects .	1<2	none	enablement	enablement
P14-1033_anno1	114-118	146-150	There are two key challenges	A novel approach is proposed	There are two key challenges	A novel approach is proposed	114-145	146-155	There are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	A novel approach is proposed to solve these problems .	1>2	none	bg-goal	bg-goal
P14-1033_anno1	114-118	119-131	There are two key challenges	: ( 1 ) learning quality knowledge from reviews of diverse domains ,	There are two key challenges	: ( 1 ) learning quality knowledge from reviews of diverse domains ,	114-145	114-145	There are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	There are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	1<2	none	elab-enumember	elab-enumember
P14-1033_anno1	119-131	132-145	: ( 1 ) learning quality knowledge from reviews of diverse domains ,	and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	: ( 1 ) learning quality knowledge from reviews of diverse domains ,	and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	114-145	114-145	There are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	There are two key challenges : ( 1 ) learning quality knowledge from reviews of diverse domains , and ( 2 ) making the model fault-tolerant to handle possibly wrong knowledge .	1<2	none	joint	joint
P14-1033_anno1	66-97	146-150	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	A novel approach is proposed	that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	A novel approach is proposed	54-97	146-155	In this paper , we take a major step forward and show that in the big data era , without any user input , it is possible to learn prior knowledge automatically from a large amount of review data available on the Web .	A novel approach is proposed to solve these problems .	1<2	none	elab-aspect	elab-aspect
P14-1033_anno1	146-150	151-155	A novel approach is proposed	to solve these problems .	A novel approach is proposed	to solve these problems .	146-155	146-155	A novel approach is proposed to solve these problems .	A novel approach is proposed to solve these problems .	1<2	none	enablement	enablement
P14-1033_anno1	156-157	158-162	Experimental results	using reviews from 36 domains	Experimental results	using reviews from 36 domains	156-174	156-174	Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .	Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .	1<2	none	manner-means	manner-means
P14-1033_anno1	156-157,163	164-174	<*> Experimental results <*> show	that the proposed approach achieves significant improvements over state-of-the-art baselines .	Experimental results <*> show	that the proposed approach achieves significant improvements over state-of-the-art baselines .	156-174	156-174	Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .	Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .	1>2	none	attribution	attribution
P14-1033_anno1	146-150	164-174	A novel approach is proposed	that the proposed approach achieves significant improvements over state-of-the-art baselines .	A novel approach is proposed	that the proposed approach achieves significant improvements over state-of-the-art baselines .	146-155	156-174	A novel approach is proposed to solve these problems .	Experimental results using reviews from 36 domains show that the proposed approach achieves significant improvements over state-of-the-art baselines .	1<2	none	evaluation	evaluation
P14-1034_anno1	1-14	15-23	Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization .	However , these new methods lack the rich priors	Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization .	However , these new methods lack the rich priors	1-14	15-28	Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization .	However , these new methods lack the rich priors associated with probabilistic models .	1>2	none	contrast	contrast
P14-1034_anno1	15-23	40-45	However , these new methods lack the rich priors	and develop new , regularized algorithms	However , these new methods lack the rich priors	and develop new , regularized algorithms	15-28	29-63	However , these new methods lack the rich priors associated with probabilistic models .	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	1>2	none	bg-compare	bg-compare
P14-1034_anno1	15-23	24-28	However , these new methods lack the rich priors	associated with probabilistic models .	However , these new methods lack the rich priors	associated with probabilistic models .	15-28	15-28	However , these new methods lack the rich priors associated with probabilistic models .	However , these new methods lack the rich priors associated with probabilistic models .	1<2	none	elab-addition	elab-addition
P14-1034_anno1	29-39	40-45	We examine Arora et al.'s anchor words algorithm for topic modeling	and develop new , regularized algorithms	We examine Arora et al.'s anchor words algorithm for topic modeling	and develop new , regularized algorithms	29-63	29-63	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	1>2	none	joint	joint
P14-1034_anno1	40-45	46-54	and develop new , regularized algorithms	that not only mathematically resemble Gaussian and Dirichlet priors	and develop new , regularized algorithms	that not only mathematically resemble Gaussian and Dirichlet priors	29-63	29-63	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	1<2	none	elab-aspect	elab-aspect
P14-1034_anno1	40-45	55-63	and develop new , regularized algorithms	but also improve the interpretability of topic models .	and develop new , regularized algorithms	but also improve the interpretability of topic models .	29-63	29-63	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	1<2	none	elab-aspect	elab-aspect
P14-1034_anno1	40-45	64-74	and develop new , regularized algorithms	Our new regularization approaches make these efficient algorithms more flexible ;	and develop new , regularized algorithms	Our new regularization approaches make these efficient algorithms more flexible ;	29-63	64-87	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	Our new regularization approaches make these efficient algorithms more flexible ; we also show that these methods can be combined with informed priors .	1<2	none	elab-aspect	elab-aspect
P14-1034_anno1	75-77	78-87	we also show	that these methods can be combined with informed priors .	we also show	that these methods can be combined with informed priors .	64-87	64-87	Our new regularization approaches make these efficient algorithms more flexible ; we also show that these methods can be combined with informed priors .	Our new regularization approaches make these efficient algorithms more flexible ; we also show that these methods can be combined with informed priors .	1>2	none	attribution	attribution
P14-1034_anno1	40-45	78-87	and develop new , regularized algorithms	that these methods can be combined with informed priors .	and develop new , regularized algorithms	that these methods can be combined with informed priors .	29-63	64-87	We examine Arora et al.'s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models .	Our new regularization approaches make these efficient algorithms more flexible ; we also show that these methods can be combined with informed priors .	1<2	none	elab-aspect	elab-aspect
P14-1035_anno1	1-4	44-47	We consider the problem	we introduce a model	We consider the problem	we introduce a model	1-23	24-65	We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899 .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1>2	none	bg-goal	bg-goal
P14-1035_anno1	1-4	5-17	We consider the problem	of automatically inferring latent character types in a collection of 15,099 English novels	We consider the problem	of automatically inferring latent character types in a collection of 15,099 English novels	1-23	1-23	We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899 .	We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899 .	1<2	none	elab-addition	elab-addition
P14-1035_anno1	5-17	18-23	of automatically inferring latent character types in a collection of 15,099 English novels	published between 1700 and 1899 .	of automatically inferring latent character types in a collection of 15,099 English novels	published between 1700 and 1899 .	1-23	1-23	We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899 .	We consider the problem of automatically inferring latent character types in a collection of 15,099 English novels published between 1700 and 1899 .	1<2	none	elab-addition	elab-addition
P14-1035_anno1	24-26	44-47	Unlike prior work	we introduce a model	Unlike prior work	we introduce a model	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1>2	none	bg-compare	bg-compare
P14-1035_anno1	24-26	27-38	Unlike prior work	in which character types are assumed responsible for probabilistically generating all text	Unlike prior work	in which character types are assumed responsible for probabilistically generating all text	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1<2	none	elab-addition	elab-addition
P14-1035_anno1	27-38	39-43	in which character types are assumed responsible for probabilistically generating all text	associated with a character ,	in which character types are assumed responsible for probabilistically generating all text	associated with a character ,	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1<2	none	elab-addition	elab-addition
P14-1035_anno1	44-47	48-51	we introduce a model	that employs multiple effects	we introduce a model	that employs multiple effects	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1<2	none	elab-addition	elab-addition
P14-1035_anno1	48-51	52-59	that employs multiple effects	to account for the influence of extra-linguistic information	that employs multiple effects	to account for the influence of extra-linguistic information	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1<2	none	enablement	enablement
P14-1035_anno1	52-59	60-65	to account for the influence of extra-linguistic information	( such as author ) .	to account for the influence of extra-linguistic information	( such as author ) .	24-65	24-65	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	1<2	none	elab-example	elab-example
P14-1035_anno1	66-72	73-88	In an empirical evaluation , we find	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	In an empirical evaluation , we find	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	66-95	66-95	In an empirical evaluation , we find that this method leads to improved agreement with the preregistered judgments of a literary scholar , complementing the results of alternative models .	In an empirical evaluation , we find that this method leads to improved agreement with the preregistered judgments of a literary scholar , complementing the results of alternative models .	1>2	none	attribution	attribution
P14-1035_anno1	44-47	73-88	we introduce a model	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	we introduce a model	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	24-65	66-95	Unlike prior work in which character types are assumed responsible for probabilistically generating all text associated with a character , we introduce a model that employs multiple effects to account for the influence of extra-linguistic information ( such as author ) .	In an empirical evaluation , we find that this method leads to improved agreement with the preregistered judgments of a literary scholar , complementing the results of alternative models .	1<2	none	evaluation	evaluation
P14-1035_anno1	73-88	89-95	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	complementing the results of alternative models .	that this method leads to improved agreement with the preregistered judgments of a literary scholar ,	complementing the results of alternative models .	66-95	66-95	In an empirical evaluation , we find that this method leads to improved agreement with the preregistered judgments of a literary scholar , complementing the results of alternative models .	In an empirical evaluation , we find that this method leads to improved agreement with the preregistered judgments of a literary scholar , complementing the results of alternative models .	1<2	none	elab-addition	elab-addition
P14-1036_anno1	1-13	91-98	Wikification for tweets aims to automatically identify each concept mention in a tweet	we propose a novel semi-supervised graph regularization model	Wikification for tweets aims to automatically identify each concept mention in a tweet	we propose a novel semi-supervised graph regularization model	1-30	86-113	Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base ( e.g. , Wikipedia ) .	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	1>2	none	bg-goal	bg-goal
P14-1036_anno1	1-13	14-24	Wikification for tweets aims to automatically identify each concept mention in a tweet	and link it to a concept referent in a knowledge base	Wikification for tweets aims to automatically identify each concept mention in a tweet	and link it to a concept referent in a knowledge base	1-30	1-30	Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base ( e.g. , Wikipedia ) .	Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base ( e.g. , Wikipedia ) .	1<2	none	progression	progression
P14-1036_anno1	14-24	25-30	and link it to a concept referent in a knowledge base	( e.g. , Wikipedia ) .	and link it to a concept referent in a knowledge base	( e.g. , Wikipedia ) .	1-30	1-30	Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base ( e.g. , Wikipedia ) .	Wikification for tweets aims to automatically identify each concept mention in a tweet and link it to a concept referent in a knowledge base ( e.g. , Wikipedia ) .	1<2	none	elab-example	elab-example
P14-1036_anno1	31-38	39-42,51-57	Due to the shortness of a tweet ,	a collective inference model <*> is more appropriate than a noncollecitve approach	Due to the shortness of a tweet ,	a collective inference model <*> is more appropriate than a noncollecitve approach	31-65	31-65	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	1>2	none	exp-reason	exp-reason
P14-1036_anno1	39-42,51-57	91-98	a collective inference model <*> is more appropriate than a noncollecitve approach	we propose a novel semi-supervised graph regularization model	a collective inference model <*> is more appropriate than a noncollecitve approach	we propose a novel semi-supervised graph regularization model	31-65	86-113	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	1>2	none	bg-general	bg-general
P14-1036_anno1	39-42,51-57	43-50	a collective inference model <*> is more appropriate than a noncollecitve approach	incorporating global evidence from multiple mentions and concepts	a collective inference model <*> is more appropriate than a noncollecitve approach	incorporating global evidence from multiple mentions and concepts	31-65	31-65	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	1<2	none	elab-addition	elab-addition
P14-1036_anno1	51-57	58-65	is more appropriate than a noncollecitve approach	which links each mention at a time .	is more appropriate than a noncollecitve approach	which links each mention at a time .	31-65	31-65	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	Due to the shortness of a tweet , a collective inference model incorporating global evidence from multiple mentions and concepts is more appropriate than a noncollecitve approach which links each mention at a time .	1<2	none	elab-addition	elab-addition
P14-1036_anno1	66-85	91-98	In addition , it is challenging to generate sufficient high quality labeled data for supervised models with low cost .	we propose a novel semi-supervised graph regularization model	In addition , it is challenging to generate sufficient high quality labeled data for supervised models with low cost .	we propose a novel semi-supervised graph regularization model	66-85	86-113	In addition , it is challenging to generate sufficient high quality labeled data for supervised models with low cost .	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	1>2	none	bg-general	bg-general
P14-1036_anno1	86-90	91-98	To tackle these challenges ,	we propose a novel semi-supervised graph regularization model	To tackle these challenges ,	we propose a novel semi-supervised graph regularization model	86-113	86-113	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	1>2	none	enablement	enablement
P14-1036_anno1	91-98	99-113	we propose a novel semi-supervised graph regularization model	to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	we propose a novel semi-supervised graph regularization model	to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	86-113	86-113	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	1<2	none	enablement	enablement
P14-1036_anno1	114-123	124-133	In order to identify semantically-related mentions for collective inference ,	we detect meta path-based semantic relations through social networks .	In order to identify semantically-related mentions for collective inference ,	we detect meta path-based semantic relations through social networks .	114-133	114-133	In order to identify semantically-related mentions for collective inference , we detect meta path-based semantic relations through social networks .	In order to identify semantically-related mentions for collective inference , we detect meta path-based semantic relations through social networks .	1>2	none	enablement	enablement
P14-1036_anno1	91-98	124-133	we propose a novel semi-supervised graph regularization model	we detect meta path-based semantic relations through social networks .	we propose a novel semi-supervised graph regularization model	we detect meta path-based semantic relations through social networks .	86-113	114-133	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	In order to identify semantically-related mentions for collective inference , we detect meta path-based semantic relations through social networks .	1<2	none	elab-aspect	elab-aspect
P14-1036_anno1	134-139	147-170	Compared to the state-of-the-art supervised model	our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	Compared to the state-of-the-art supervised model	our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	134-170	134-170	Compared to the state-of-the-art supervised model trained from 100 % labeled data , our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	Compared to the state-of-the-art supervised model trained from 100 % labeled data , our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	1>2	none	comparison	comparison
P14-1036_anno1	134-139	140-146	Compared to the state-of-the-art supervised model	trained from 100 % labeled data ,	Compared to the state-of-the-art supervised model	trained from 100 % labeled data ,	134-170	134-170	Compared to the state-of-the-art supervised model trained from 100 % labeled data , our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	Compared to the state-of-the-art supervised model trained from 100 % labeled data , our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	1<2	none	elab-addition	elab-addition
P14-1036_anno1	91-98	147-170	we propose a novel semi-supervised graph regularization model	our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	we propose a novel semi-supervised graph regularization model	our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	86-113	134-170	To tackle these challenges , we propose a novel semi-supervised graph regularization model to incorporate both local and global evidence from multiple tweets through three fine-grained relations .	Compared to the state-of-the-art supervised model trained from 100 % labeled data , our proposed approach achieves comparable performance with 31 % labeled data and obtains 5 % absolute F1 gain with 50 % labeled data .	1<2	none	evaluation	evaluation
P14-1037_anno1	1-16	17-31	In order to extract entities of a fine-grained category from semi-structured data in web pages ,	existing information extraction systems rely on seed examples or redundancy across multiple web pages .	In order to extract entities of a fine-grained category from semi-structured data in web pages ,	existing information extraction systems rely on seed examples or redundancy across multiple web pages .	1-31	1-31	In order to extract entities of a fine-grained category from semi-structured data in web pages , existing information extraction systems rely on seed examples or redundancy across multiple web pages .	In order to extract entities of a fine-grained category from semi-structured data in web pages , existing information extraction systems rely on seed examples or redundancy across multiple web pages .	1>2	none	enablement	enablement
P14-1037_anno1	17-31	65-75	existing information extraction systems rely on seed examples or redundancy across multiple web pages .	Our approach defines a log-linear model over latent extraction predicates ,	existing information extraction systems rely on seed examples or redundancy across multiple web pages .	Our approach defines a log-linear model over latent extraction predicates ,	1-31	65-85	In order to extract entities of a fine-grained category from semi-structured data in web pages , existing information extraction systems rely on seed examples or redundancy across multiple web pages .	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	1>2	none	bg-compare	bg-compare
P14-1037_anno1	32-45	65-75	In this paper , we consider a new zero-shot learning task of extracting entities	Our approach defines a log-linear model over latent extraction predicates ,	In this paper , we consider a new zero-shot learning task of extracting entities	Our approach defines a log-linear model over latent extraction predicates ,	32-64	65-85	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	1>2	none	bg-goal	bg-goal
P14-1037_anno1	32-45	46-51	In this paper , we consider a new zero-shot learning task of extracting entities	specified by a natural language query	In this paper , we consider a new zero-shot learning task of extracting entities	specified by a natural language query	32-64	32-64	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	1<2	none	elab-addition	elab-addition
P14-1037_anno1	46-51	52-57	specified by a natural language query	( in place of seeds )	specified by a natural language query	( in place of seeds )	32-64	32-64	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	1<2	none	elab-addition	elab-addition
P14-1037_anno1	46-51	58-64	specified by a natural language query	given only a single web page .	specified by a natural language query	given only a single web page .	32-64	32-64	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	In this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page .	1<2	none	elab-addition	elab-addition
P14-1037_anno1	65-75	76-85	Our approach defines a log-linear model over latent extraction predicates ,	which select lists of entities from the web page .	Our approach defines a log-linear model over latent extraction predicates ,	which select lists of entities from the web page .	65-85	65-85	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	1<2	none	elab-addition	elab-addition
P14-1037_anno1	86-99	100-102	The main challenge is to define features on widely varying candidate entity lists .	We tackle this	The main challenge is to define features on widely varying candidate entity lists .	We tackle this	86-99	100-114	The main challenge is to define features on widely varying candidate entity lists .	We tackle this by abstracting list elements and using aggregate statistics to define features .	1>2	none	result	result
P14-1037_anno1	65-75	100-102	Our approach defines a log-linear model over latent extraction predicates ,	We tackle this	Our approach defines a log-linear model over latent extraction predicates ,	We tackle this	65-85	100-114	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	We tackle this by abstracting list elements and using aggregate statistics to define features .	1<2	none	elab-aspect	elab-aspect
P14-1037_anno1	100-102	103-106	We tackle this	by abstracting list elements	We tackle this	by abstracting list elements	100-114	100-114	We tackle this by abstracting list elements and using aggregate statistics to define features .	We tackle this by abstracting list elements and using aggregate statistics to define features .	1<2	none	manner-means	manner-means
P14-1037_anno1	103-106	107-114	by abstracting list elements	and using aggregate statistics to define features .	by abstracting list elements	and using aggregate statistics to define features .	100-114	100-114	We tackle this by abstracting list elements and using aggregate statistics to define features .	We tackle this by abstracting list elements and using aggregate statistics to define features .	1<2	none	joint	joint
P14-1037_anno1	65-75	115-128	Our approach defines a log-linear model over latent extraction predicates ,	Finally , we created a new dataset of diverse queries and web pages ,	Our approach defines a log-linear model over latent extraction predicates ,	Finally , we created a new dataset of diverse queries and web pages ,	65-85	115-142	Our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page .	Finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .	1<2	none	evaluation	evaluation
P14-1037_anno1	129-130	131-142	and show	that our system achieves significantly better accuracy than a natural baseline .	and show	that our system achieves significantly better accuracy than a natural baseline .	115-142	115-142	Finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .	Finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .	1>2	none	attribution	attribution
P14-1037_anno1	115-128	131-142	Finally , we created a new dataset of diverse queries and web pages ,	that our system achieves significantly better accuracy than a natural baseline .	Finally , we created a new dataset of diverse queries and web pages ,	that our system achieves significantly better accuracy than a natural baseline .	115-142	115-142	Finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .	Finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .	1<2	none	cause	cause
P14-1038_anno1	1-6	7-13	We present an incremental joint framework	to simultaneously extract entity mentions and relations	We present an incremental joint framework	to simultaneously extract entity mentions and relations	1-20	1-20	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	1<2	none	enablement	enablement
P14-1038_anno1	7-13	14-20	to simultaneously extract entity mentions and relations	using structured perceptron with efficient beam-search .	to simultaneously extract entity mentions and relations	using structured perceptron with efficient beam-search .	1-20	1-20	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	1<2	none	manner-means	manner-means
P14-1038_anno1	1-6	21-23,31-36	We present an incremental joint framework	A segment-based decoder <*> is adopted to the new framework	We present an incremental joint framework	A segment-based decoder <*> is adopted to the new framework	1-20	21-43	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging .	1<2	none	elab-aspect	elab-aspect
P14-1038_anno1	21-23,31-36	24-30	A segment-based decoder <*> is adopted to the new framework	based on the idea of semi-Markov chain	A segment-based decoder <*> is adopted to the new framework	based on the idea of semi-Markov chain	21-43	21-43	A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging .	A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging .	1<2	none	manner-means	manner-means
P14-1038_anno1	21-23,31-36	37-43	A segment-based decoder <*> is adopted to the new framework	as opposed to traditional token-based tagging .	A segment-based decoder <*> is adopted to the new framework	as opposed to traditional token-based tagging .	21-43	21-43	A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging .	A segment-based decoder based on the idea of semi-Markov chain is adopted to the new framework as opposed to traditional token-based tagging .	1<2	none	bg-compare	bg-compare
P14-1038_anno1	44-53	54-66	In addition , by virtue of the inexact search ,	we developed a number of new and effective global features as soft constraints	In addition , by virtue of the inexact search ,	we developed a number of new and effective global features as soft constraints	44-76	44-76	In addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations .	In addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations .	1>2	none	manner-means	manner-means
P14-1038_anno1	1-6	54-66	We present an incremental joint framework	we developed a number of new and effective global features as soft constraints	We present an incremental joint framework	we developed a number of new and effective global features as soft constraints	1-20	44-76	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	In addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations .	1<2	none	elab-aspect	elab-aspect
P14-1038_anno1	54-66	67-76	we developed a number of new and effective global features as soft constraints	to capture the inter-dependency among entity mentions and relations .	we developed a number of new and effective global features as soft constraints	to capture the inter-dependency among entity mentions and relations .	44-76	44-76	In addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations .	In addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the inter-dependency among entity mentions and relations .	1<2	none	enablement	enablement
P14-1038_anno1	77-86	87-97	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate	that our joint model significantly outperforms a strong pipelined baseline ,	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate	that our joint model significantly outperforms a strong pipelined baseline ,	77-107	77-107	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .	1>2	none	attribution	attribution
P14-1038_anno1	1-6	87-97	We present an incremental joint framework	that our joint model significantly outperforms a strong pipelined baseline ,	We present an incremental joint framework	that our joint model significantly outperforms a strong pipelined baseline ,	1-20	77-107	We present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search .	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .	1<2	none	evaluation	evaluation
P14-1038_anno1	87-97	98-107	that our joint model significantly outperforms a strong pipelined baseline ,	which attains better performance than the best-reported end-to-end system .	that our joint model significantly outperforms a strong pipelined baseline ,	which attains better performance than the best-reported end-to-end system .	77-107	77-107	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .	Experiments on Automatic Content Extraction ( ACE ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	1-12	13-18	We investigate whether parsers can be used for self-monitoring in surface realization	in order to avoid egregious errors	We investigate whether parsers can be used for self-monitoring in surface realization	in order to avoid egregious errors	1-38	1-38	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	1<2	none	manner-means	manner-means
P14-1039_anno1	13-18	19-22	in order to avoid egregious errors	involving "vicious" ambiguities ,	in order to avoid egregious errors	involving "vicious" ambiguities ,	1-38	1-38	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	19-22	23-24	involving "vicious" ambiguities ,	namely those	involving "vicious" ambiguities ,	namely those	1-38	1-38	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	1<2	none	elab-definition	elab-definition
P14-1039_anno1	23-24	25-38	namely those	where the intended interpretation fails to be considerably more likely than alternative ones .	namely those	where the intended interpretation fails to be considerably more likely than alternative ones .	1-38	1-38	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	39-49	50-51	Using parse accuracy in a simple reranking strategy for selfmonitoring ,	we find	Using parse accuracy in a simple reranking strategy for selfmonitoring ,	we find	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1>2	none	manner-means	manner-means
P14-1039_anno1	50-51	62-66	we find	BLEU scores cannot be improved	we find	BLEU scores cannot be improved	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1>2	none	attribution	attribution
P14-1039_anno1	52-61	62-66	that with a state-of-the-art averaged perceptron realization ranking model ,	BLEU scores cannot be improved	that with a state-of-the-art averaged perceptron realization ranking model ,	BLEU scores cannot be improved	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1>2	none	manner-means	manner-means
P14-1039_anno1	1-12	62-66	We investigate whether parsers can be used for self-monitoring in surface realization	BLEU scores cannot be improved	We investigate whether parsers can be used for self-monitoring in surface realization	BLEU scores cannot be improved	1-38	39-92	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1<2	none	elab-aspect	elab-aspect
P14-1039_anno1	62-66	67-73	BLEU scores cannot be improved	with any of the well-known Treebank parsers	BLEU scores cannot be improved	with any of the well-known Treebank parsers	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1<2	none	manner-means	manner-means
P14-1039_anno1	67-73	74-76	with any of the well-known Treebank parsers	we tested ,	with any of the well-known Treebank parsers	we tested ,	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	62-66	77-83	BLEU scores cannot be improved	since these parsers too often make errors	BLEU scores cannot be improved	since these parsers too often make errors	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1<2	none	exp-reason	exp-reason
P14-1039_anno1	77-83	84-92	since these parsers too often make errors	that human readers would be unlikely to make .	since these parsers too often make errors	that human readers would be unlikely to make .	39-92	39-92	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	Using parse accuracy in a simple reranking strategy for selfmonitoring , we find that with a state-of-the-art averaged perceptron realization ranking model , BLEU scores cannot be improved with any of the well-known Treebank parsers we tested , since these parsers too often make errors that human readers would be unlikely to make .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	93-99	128-137	However , by using an SVM ranker	that significant increases in BLEU scores can be achieved .	However , by using an SVM ranker	that significant increases in BLEU scores can be achieved .	93-137	93-137	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	1>2	none	manner-means	manner-means
P14-1039_anno1	93-99	100-112	However , by using an SVM ranker	to combine the realizer's model score together with features from multiple parsers ,	However , by using an SVM ranker	to combine the realizer's model score together with features from multiple parsers ,	93-137	93-137	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	1<2	none	enablement	enablement
P14-1039_anno1	100-112	113-125	to combine the realizer's model score together with features from multiple parsers ,	including ones designed to make the ranker more robust to parsing mistakes ,	to combine the realizer's model score together with features from multiple parsers ,	including ones designed to make the ranker more robust to parsing mistakes ,	93-137	93-137	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	1<2	none	elab-addition	elab-addition
P14-1039_anno1	126-127	128-137	we show	that significant increases in BLEU scores can be achieved .	we show	that significant increases in BLEU scores can be achieved .	93-137	93-137	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	1>2	none	attribution	attribution
P14-1039_anno1	1-12	128-137	We investigate whether parsers can be used for self-monitoring in surface realization	that significant increases in BLEU scores can be achieved .	We investigate whether parsers can be used for self-monitoring in surface realization	that significant increases in BLEU scores can be achieved .	1-38	93-137	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	However , by using an SVM ranker to combine the realizer's model score together with features from multiple parsers , including ones designed to make the ranker more robust to parsing mistakes , we show that significant increases in BLEU scores can be achieved .	1<2	none	elab-aspect	elab-aspect
P14-1039_anno1	138-145	146-147	Moreover , via a targeted manual analysis ,	we demonstrate	Moreover , via a targeted manual analysis ,	we demonstrate	138-172	138-172	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	1>2	none	manner-means	manner-means
P14-1039_anno1	146-147	148-158	we demonstrate	that the SVM reranker frequently manages to avoid vicious ambiguities ,	we demonstrate	that the SVM reranker frequently manages to avoid vicious ambiguities ,	138-172	138-172	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	1>2	none	attribution	attribution
P14-1039_anno1	1-12	148-158	We investigate whether parsers can be used for self-monitoring in surface realization	that the SVM reranker frequently manages to avoid vicious ambiguities ,	We investigate whether parsers can be used for self-monitoring in surface realization	that the SVM reranker frequently manages to avoid vicious ambiguities ,	1-38	138-172	We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving "vicious" ambiguities , namely those where the intended interpretation fails to be considerably more likely than alternative ones .	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	1<2	none	elab-aspect	elab-aspect
P14-1039_anno1	148-158	159-172	that the SVM reranker frequently manages to avoid vicious ambiguities ,	while its ranking errors tend to affect fluency much more often than adequacy .	that the SVM reranker frequently manages to avoid vicious ambiguities ,	while its ranking errors tend to affect fluency much more often than adequacy .	138-172	138-172	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	Moreover , via a targeted manual analysis , we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities , while its ranking errors tend to affect fluency much more often than adequacy .	1<2	none	elab-addition	elab-addition
P14-1040_anno1	1-16	17-43	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	1-16	17-55	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ; and that it takes into account both syntactic and semantic information .	1<2	none	elab-aspect	elab-aspect
P14-1040_anno1	17-43	44-55	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	and that it takes into account both syntactic and semantic information .	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	and that it takes into account both syntactic and semantic information .	17-55	17-55	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ; and that it takes into account both syntactic and semantic information .	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ; and that it takes into account both syntactic and semantic information .	1<2	none	cause	cause
P14-1040_anno1	17-43	56-64	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	The resulting extracted TAG includes a unification based semantics	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ;	The resulting extracted TAG includes a unification based semantics	17-55	56-80	A key feature of this approach is that grammar induction is driven by the extended domain of locality principle of TAG ( Tree Adjoining Grammar ) ; and that it takes into account both syntactic and semantic information .	The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data .	1<2	none	elab-addition	elab-addition
P14-1040_anno1	56-64	65-73	The resulting extracted TAG includes a unification based semantics	and can be used by an existing surface realiser	The resulting extracted TAG includes a unification based semantics	and can be used by an existing surface realiser	56-80	56-80	The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data .	The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data .	1<2	none	joint	joint
P14-1040_anno1	65-73	74-80	and can be used by an existing surface realiser	to generate sentences from KB data .	and can be used by an existing surface realiser	to generate sentences from KB data .	56-80	56-80	The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data .	The resulting extracted TAG includes a unification based semantics and can be used by an existing surface realiser to generate sentences from KB data .	1<2	none	enablement	enablement
P14-1040_anno1	81-87	88-95	Experimental evaluation on the KBGen data shows	that our model outperforms a data-driven generate-and-rank approach	Experimental evaluation on the KBGen data shows	that our model outperforms a data-driven generate-and-rank approach	81-112	81-112	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	1>2	none	attribution	attribution
P14-1040_anno1	1-16	88-95	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	that our model outperforms a data-driven generate-and-rank approach	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	that our model outperforms a data-driven generate-and-rank approach	1-16	81-112	We present a simple , data-driven approach to generation from knowledge bases ( KB ) .	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	1<2	none	evaluation	evaluation
P14-1040_anno1	88-95	96-103	that our model outperforms a data-driven generate-and-rank approach	based on an automatically induced probabilistic grammar ;	that our model outperforms a data-driven generate-and-rank approach	based on an automatically induced probabilistic grammar ;	81-112	81-112	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	1<2	none	elab-addition	elab-addition
P14-1040_anno1	88-95	104-112	that our model outperforms a data-driven generate-and-rank approach	and is comparable with a handcrafted symbolic approach .	that our model outperforms a data-driven generate-and-rank approach	and is comparable with a handcrafted symbolic approach .	81-112	81-112	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	Experimental evaluation on the KBGen data shows that our model outperforms a data-driven generate-and-rank approach based on an automatically induced probabilistic grammar ; and is comparable with a handcrafted symbolic approach .	1<2	none	joint	joint
P14-1041_anno1	1-8	9-16	We present a hybrid approach to sentence simplification	which combines deep semantics and monolingual machine translation	We present a hybrid approach to sentence simplification	which combines deep semantics and monolingual machine translation	1-24	1-24	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	1<2	none	elab-addition	elab-addition
P14-1041_anno1	1-8	17-24	We present a hybrid approach to sentence simplification	to derive simple sentences from complex ones .	We present a hybrid approach to sentence simplification	to derive simple sentences from complex ones .	1-24	1-24	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	1<2	none	enablement	enablement
P14-1041_anno1	1-8	25-35	We present a hybrid approach to sentence simplification	The approach differs from previous work in two main ways .	We present a hybrid approach to sentence simplification	The approach differs from previous work in two main ways .	1-24	25-35	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	The approach differs from previous work in two main ways .	1<2	none	comparison	comparison
P14-1041_anno1	25-35	36-41	The approach differs from previous work in two main ways .	First , it is semantic based	The approach differs from previous work in two main ways .	First , it is semantic based	25-35	36-62	The approach differs from previous work in two main ways .	First , it is semantic based in that it takes as input a deep semantic representation rather than e.g. , a sentence or a parse tree .	1<2	none	elab-aspect	elab-aspect
P14-1041_anno1	36-41	42-62	First , it is semantic based	in that it takes as input a deep semantic representation rather than e.g. , a sentence or a parse tree .	First , it is semantic based	in that it takes as input a deep semantic representation rather than e.g. , a sentence or a parse tree .	36-62	36-62	First , it is semantic based in that it takes as input a deep semantic representation rather than e.g. , a sentence or a parse tree .	First , it is semantic based in that it takes as input a deep semantic representation rather than e.g. , a sentence or a parse tree .	1<2	none	exp-reason	exp-reason
P14-1041_anno1	25-35	63-84	The approach differs from previous work in two main ways .	Second , it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering .	The approach differs from previous work in two main ways .	Second , it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering .	25-35	63-84	The approach differs from previous work in two main ways .	Second , it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering .	1<2	none	elab-aspect	elab-aspect
P14-1041_anno1	85-94	95-100	When compared against current state of the art methods ,	our model yields significantly simpler output	When compared against current state of the art methods ,	our model yields significantly simpler output	85-108	85-108	When compared against current state of the art methods , our model yields significantly simpler output that is both grammatical and meaning preserving .	When compared against current state of the art methods , our model yields significantly simpler output that is both grammatical and meaning preserving .	1>2	none	temporal	temporal
P14-1041_anno1	1-8	95-100	We present a hybrid approach to sentence simplification	our model yields significantly simpler output	We present a hybrid approach to sentence simplification	our model yields significantly simpler output	1-24	85-108	We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones .	When compared against current state of the art methods , our model yields significantly simpler output that is both grammatical and meaning preserving .	1<2	none	evaluation	evaluation
P14-1041_anno1	95-100	101-108	our model yields significantly simpler output	that is both grammatical and meaning preserving .	our model yields significantly simpler output	that is both grammatical and meaning preserving .	85-108	85-108	When compared against current state of the art methods , our model yields significantly simpler output that is both grammatical and meaning preserving .	When compared against current state of the art methods , our model yields significantly simpler output that is both grammatical and meaning preserving .	1<2	none	elab-addition	elab-addition
P14-1042_anno1	1-23	82-87	This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation ( GR ) analysis of Chinese texts .	We present a novel transition system	This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation ( GR ) analysis of Chinese texts .	We present a novel transition system	1-23	82-96	This paper is concerned with building linguistic resources and statistical parsers for deep grammatical relation ( GR ) analysis of Chinese texts .	We present a novel transition system which suits GR graphs better than existing systems .	1>2	none	bg-goal	bg-goal
P14-1042_anno1	24-30	66-71	A set of linguistic rules is defined	Based on the converted corpus ,	A set of linguistic rules is defined	Based on the converted corpus ,	24-51	66-81	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	Based on the converted corpus , we study transition-based , data-driven models for GR parsing .	1>2	none	bg-general	bg-general
P14-1042_anno1	24-30	31-36	A set of linguistic rules is defined	to explore implicit phrase structural information	A set of linguistic rules is defined	to explore implicit phrase structural information	24-51	24-51	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	1<2	none	enablement	enablement
P14-1042_anno1	24-30	37-42	A set of linguistic rules is defined	and thus build high-quality GR annotations	A set of linguistic rules is defined	and thus build high-quality GR annotations	24-51	24-51	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	1<2	none	cause	cause
P14-1042_anno1	37-42	43-51	and thus build high-quality GR annotations	that are represented as general directed dependency graphs .	and thus build high-quality GR annotations	that are represented as general directed dependency graphs .	24-51	24-51	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	1<2	none	elab-addition	elab-addition
P14-1042_anno1	24-30	52-65	A set of linguistic rules is defined	The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation .	A set of linguistic rules is defined	The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation .	24-51	52-65	A set of linguistic rules is defined to explore implicit phrase structural information and thus build high-quality GR annotations that are represented as general directed dependency graphs .	The reliability of this linguistically-motivated GR extraction procedure is highlighted by manual evaluation .	1<2	none	elab-addition	elab-addition
P14-1042_anno1	66-71	72-81	Based on the converted corpus ,	we study transition-based , data-driven models for GR parsing .	Based on the converted corpus ,	we study transition-based , data-driven models for GR parsing .	66-81	66-81	Based on the converted corpus , we study transition-based , data-driven models for GR parsing .	Based on the converted corpus , we study transition-based , data-driven models for GR parsing .	1>2	none	bg-general	bg-general
P14-1042_anno1	72-81	82-87	we study transition-based , data-driven models for GR parsing .	We present a novel transition system	we study transition-based , data-driven models for GR parsing .	We present a novel transition system	66-81	82-96	Based on the converted corpus , we study transition-based , data-driven models for GR parsing .	We present a novel transition system which suits GR graphs better than existing systems .	1>2	none	result	result
P14-1042_anno1	82-87	88-96	We present a novel transition system	which suits GR graphs better than existing systems .	We present a novel transition system	which suits GR graphs better than existing systems .	82-96	82-96	We present a novel transition system which suits GR graphs better than existing systems .	We present a novel transition system which suits GR graphs better than existing systems .	1<2	none	elab-addition	elab-addition
P14-1042_anno1	82-87	97-107	We present a novel transition system	The key idea is to introduce a new type of transition	We present a novel transition system	The key idea is to introduce a new type of transition	82-96	97-117	We present a novel transition system which suits GR graphs better than existing systems .	The key idea is to introduce a new type of transition that reorders top k elements in the memory module .	1<2	none	elab-aspect	elab-aspect
P14-1042_anno1	97-107	108-117	The key idea is to introduce a new type of transition	that reorders top k elements in the memory module .	The key idea is to introduce a new type of transition	that reorders top k elements in the memory module .	97-117	97-117	The key idea is to introduce a new type of transition that reorders top k elements in the memory module .	The key idea is to introduce a new type of transition that reorders top k elements in the memory module .	1<2	none	elab-addition	elab-addition
P14-1042_anno1	118-119	120-132	Evaluation gauges	how successful GR parsing for Chinese can be by applying data-driven models .	Evaluation gauges	how successful GR parsing for Chinese can be by applying data-driven models .	118-132	118-132	Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models .	Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models .	1>2	none	attribution	attribution
P14-1042_anno1	82-87	120-132	We present a novel transition system	how successful GR parsing for Chinese can be by applying data-driven models .	We present a novel transition system	how successful GR parsing for Chinese can be by applying data-driven models .	82-96	118-132	We present a novel transition system which suits GR graphs better than existing systems .	Evaluation gauges how successful GR parsing for Chinese can be by applying data-driven models .	1<2	none	evaluation	evaluation
P14-1043_anno1	1-17	18-24	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	referred to as ambiguity-aware ensemble training .	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	referred to as ambiguity-aware ensemble training .	1-24	1-24	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	1<2	none	elab-addition	elab-addition
P14-1043_anno1	25-35	36-47	Instead of only using 1-best parse trees in previous work ,	our core idea is to utilize parse forest ( ambiguous labelings )	Instead of only using 1-best parse trees in previous work ,	our core idea is to utilize parse forest ( ambiguous labelings )	25-61	25-61	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	1>2	none	bg-compare	bg-compare
P14-1043_anno1	1-17	36-47	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	our core idea is to utilize parse forest ( ambiguous labelings )	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	our core idea is to utilize parse forest ( ambiguous labelings )	1-24	25-61	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	1<2	none	elab-aspect	elab-aspect
P14-1043_anno1	36-47	48-53	our core idea is to utilize parse forest ( ambiguous labelings )	to combine multiple 1-best parse trees	our core idea is to utilize parse forest ( ambiguous labelings )	to combine multiple 1-best parse trees	25-61	25-61	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	1<2	none	enablement	enablement
P14-1043_anno1	48-53	54-61	to combine multiple 1-best parse trees	generated from diverse parsers on unlabeled data .	to combine multiple 1-best parse trees	generated from diverse parsers on unlabeled data .	25-61	25-61	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	Instead of only using 1-best parse trees in previous work , our core idea is to utilize parse forest ( ambiguous labelings ) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data .	1<2	none	elab-addition	elab-addition
P14-1043_anno1	62-71	72-90	With a conditional random field based probabilistic dependency parser ,	our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	With a conditional random field based probabilistic dependency parser ,	our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	62-90	62-90	With a conditional random field based probabilistic dependency parser , our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	With a conditional random field based probabilistic dependency parser , our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	1>2	none	manner-means	manner-means
P14-1043_anno1	1-17	72-90	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	1-24	62-90	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	With a conditional random field based probabilistic dependency parser , our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings .	1<2	none	elab-aspect	elab-aspect
P14-1043_anno1	1-17	91-97	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	This framework offers two promising advantages .	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	This framework offers two promising advantages .	1-24	91-97	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	This framework offers two promising advantages .	1<2	none	elab-aspect	elab-aspect
P14-1043_anno1	91-97	98-111	This framework offers two promising advantages .	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	This framework offers two promising advantages .	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	91-97	98-111	This framework offers two promising advantages .	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	1<2	none	elab-aspect	elab-aspect
P14-1043_anno1	112-114	115-123	During training ,	the parser is aware of these ambiguous structures ,	During training ,	the parser is aware of these ambiguous structures ,	112-143	112-143	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	1>2	none	temporal	temporal
P14-1043_anno1	98-111	115-123	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	the parser is aware of these ambiguous structures ,	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	the parser is aware of these ambiguous structures ,	98-111	112-143	1 ) ambiguity encoded in parse forests compromises noise in 1-best parse trees .	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	1<2	none	elab-addition	elab-addition
P14-1043_anno1	115-123	124-127	the parser is aware of these ambiguous structures ,	and has the flexibility	the parser is aware of these ambiguous structures ,	and has the flexibility	112-143	112-143	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	1<2	none	joint	joint
P14-1043_anno1	124-127	128-136	and has the flexibility	to distribute probability mass to its preferred parse trees	and has the flexibility	to distribute probability mass to its preferred parse trees	112-143	112-143	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	1<2	none	elab-addition	elab-addition
P14-1043_anno1	124-127	137-143	and has the flexibility	as long as the likelihood improves .	and has the flexibility	as long as the likelihood improves .	112-143	112-143	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	During training , the parser is aware of these ambiguous structures , and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves .	1<2	none	condition	condition
P14-1043_anno1	91-97	144-148,153-159	This framework offers two promising advantages .	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	This framework offers two promising advantages .	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	91-97	144-167	This framework offers two promising advantages .	2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser .	1<2	none	elab-aspect	elab-aspect
P14-1043_anno1	144-148,153-159	149-152	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	produced by different parsers	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	produced by different parsers	144-167	144-167	2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser .	2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser .	1<2	none	elab-addition	elab-addition
P14-1043_anno1	144-148,153-159	160-167	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	offering complementary strength to our single-view parser .	2 ) diverse syntactic structures <*> can be naturally compiled into forest ,	offering complementary strength to our single-view parser .	144-167	144-167	2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser .	2 ) diverse syntactic structures produced by different parsers can be naturally compiled into forest , offering complementary strength to our single-view parser .	1<2	none	cause	cause
P14-1043_anno1	168-173	174-197	Experimental results on benchmark data show	that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	Experimental results on benchmark data show	that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	168-197	168-197	Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	1>2	none	attribution	attribution
P14-1043_anno1	1-17	174-197	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level ,	that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	1-24	168-197	This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level , referred to as ambiguity-aware ensemble training .	Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods , such as self-training , co-training and tri-training .	1<2	none	evaluation	evaluation
P14-1044_anno1	1-15	54-59	Lexical resource alignment has been an active field of research over the last decade .	Here we present a unified approach	Lexical resource alignment has been an active field of research over the last decade .	Here we present a unified approach	1-15	54-79	Lexical resource alignment has been an active field of research over the last decade .	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	1>2	none	bg-goal	bg-goal
P14-1044_anno1	16-19,24-34	54-59	However , prior methods <*> have been either specific to a particular pair of resources ,	Here we present a unified approach	However , prior methods <*> have been either specific to a particular pair of resources ,	Here we present a unified approach	16-53	54-79	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	1>2	none	bg-compare	bg-compare
P14-1044_anno1	16-19,24-34	20-23	However , prior methods <*> have been either specific to a particular pair of resources ,	for aligning lexical resources	However , prior methods <*> have been either specific to a particular pair of resources ,	for aligning lexical resources	16-53	16-53	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	1<2	none	elab-addition	elab-addition
P14-1044_anno1	24-34	35-49	have been either specific to a particular pair of resources ,	or heavily dependent on the availability of hand-crafted alignment data for the pair of resources	have been either specific to a particular pair of resources ,	or heavily dependent on the availability of hand-crafted alignment data for the pair of resources	16-53	16-53	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	1<2	none	joint	joint
P14-1044_anno1	35-49	50-53	or heavily dependent on the availability of hand-crafted alignment data for the pair of resources	to be aligned .	or heavily dependent on the availability of hand-crafted alignment data for the pair of resources	to be aligned .	16-53	16-53	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	However , prior methods for aligning lexical resources have been either specific to a particular pair of resources , or heavily dependent on the availability of hand-crafted alignment data for the pair of resources to be aligned .	1<2	none	elab-addition	elab-addition
P14-1044_anno1	54-59	60-71	Here we present a unified approach	that can be applied to an arbitrary pair of lexical resources ,	Here we present a unified approach	that can be applied to an arbitrary pair of lexical resources ,	54-79	54-79	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	1<2	none	elab-addition	elab-addition
P14-1044_anno1	60-71	72-79	that can be applied to an arbitrary pair of lexical resources ,	including machine-readable dictionaries with no network structure .	that can be applied to an arbitrary pair of lexical resources ,	including machine-readable dictionaries with no network structure .	54-79	54-79	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	1<2	none	elab-example	elab-example
P14-1044_anno1	54-59	80-85	Here we present a unified approach	Our approach leverages a similarity measure	Here we present a unified approach	Our approach leverages a similarity measure	54-79	80-117	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	1<2	none	elab-aspect	elab-aspect
P14-1044_anno1	80-85	86-96	Our approach leverages a similarity measure	that enables the structural comparison of senses across lexical resources ,	Our approach leverages a similarity measure	that enables the structural comparison of senses across lexical resources ,	80-117	80-117	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	1<2	none	elab-addition	elab-addition
P14-1044_anno1	54-59	97-102	Here we present a unified approach	achieving state-of-the-art performance on the task	Here we present a unified approach	achieving state-of-the-art performance on the task	54-79	80-117	Here we present a unified approach that can be applied to an arbitrary pair of lexical resources , including machine-readable dictionaries with no network structure .	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	1<2	none	evaluation	evaluation
P14-1044_anno1	97-102	103-111	achieving state-of-the-art performance on the task	of aligning WordNet to three different collaborative resources :	achieving state-of-the-art performance on the task	of aligning WordNet to three different collaborative resources :	80-117	80-117	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	1<2	none	elab-addition	elab-addition
P14-1044_anno1	103-111	112-117	of aligning WordNet to three different collaborative resources :	Wikipedia , Wiktionary and OmegaWiki .	of aligning WordNet to three different collaborative resources :	Wikipedia , Wiktionary and OmegaWiki .	80-117	80-117	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	Our approach leverages a similarity measure that enables the structural comparison of senses across lexical resources , achieving state-of-the-art performance on the task of aligning WordNet to three different collaborative resources : Wikipedia , Wiktionary and OmegaWiki .	1<2	none	elab-enumember	elab-enumember
P14-1045_anno1	1-17	18-29	Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP .	The resulting relations are often noisy or difficult to interpret in general.	Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP .	The resulting relations are often noisy or difficult to interpret in general.	1-17	18-29	Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP .	The resulting relations are often noisy or difficult to interpret in general.	1>2	none	result	result
P14-1045_anno1	18-29	30-35	The resulting relations are often noisy or difficult to interpret in general.	This paper focuses on the issues	The resulting relations are often noisy or difficult to interpret in general.	This paper focuses on the issues	18-29	30-64	The resulting relations are often noisy or difficult to interpret in general.	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1>2	none	bg-general	bg-general
P14-1045_anno1	30-35	36-40	This paper focuses on the issues	of evaluating a distributional resource	This paper focuses on the issues	of evaluating a distributional resource	30-64	30-64	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	36-40	41-44	of evaluating a distributional resource	and filtering the relations	of evaluating a distributional resource	and filtering the relations	30-64	30-64	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1<2	none	joint	joint
P14-1045_anno1	41-44	45-47	and filtering the relations	it contains ,	and filtering the relations	it contains ,	30-64	30-64	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	48-55	56-64	but instead of considering it in abstract ,	we focus on pairs of words in context .	but instead of considering it in abstract ,	we focus on pairs of words in context .	30-64	30-64	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1>2	none	contrast	contrast
P14-1045_anno1	30-35	56-64	This paper focuses on the issues	we focus on pairs of words in context .	This paper focuses on the issues	we focus on pairs of words in context .	30-64	30-64	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	56-64	65-90	we focus on pairs of words in context .	In a discourse , we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant .	we focus on pairs of words in context .	In a discourse , we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant .	30-64	65-90	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	In a discourse , we are interested in knowing if the semantic link between two items is a by-product of textual coherence or is irrelevant .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	30-35	91-100	This paper focuses on the issues	We first set up a human annotation of semantic links	This paper focuses on the issues	We first set up a human annotation of semantic links	30-64	91-134	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	1<2	none	elab-aspect	elab-aspect
P14-1045_anno1	91-100	101-105	We first set up a human annotation of semantic links	with or without contextual information	We first set up a human annotation of semantic links	with or without contextual information	91-134	91-134	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	91-100	106-113	We first set up a human annotation of semantic links	to show the importance of the textual context	We first set up a human annotation of semantic links	to show the importance of the textual context	91-134	91-134	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	1<2	none	enablement	enablement
P14-1045_anno1	106-113	114-121	to show the importance of the textual context	in evaluating the relevance of semantic similarity ,	to show the importance of the textual context	in evaluating the relevance of semantic similarity ,	91-134	91-134	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	106-113	122-134	to show the importance of the textual context	and to assess the prevalence of actual semantic relations between word tokens .	to show the importance of the textual context	and to assess the prevalence of actual semantic relations between word tokens .	91-134	91-134	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	We first set up a human annotation of semantic links with or without contextual information to show the importance of the textual context in evaluating the relevance of semantic similarity , and to assess the prevalence of actual semantic relations between word tokens .	1<2	none	joint	joint
P14-1045_anno1	30-35	135-139	This paper focuses on the issues	We then built an experiment	This paper focuses on the issues	We then built an experiment	30-64	135-161	This paper focuses on the issues of evaluating a distributional resource and filtering the relations it contains , but instead of considering it in abstract , we focus on pairs of words in context .	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	1<2	none	evaluation	evaluation
P14-1045_anno1	135-139	140-145	We then built an experiment	to automatically predict this relevance ,	We then built an experiment	to automatically predict this relevance ,	135-161	135-161	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	1<2	none	enablement	enablement
P14-1045_anno1	135-139	146-152	We then built an experiment	evaluated on the reliable reference data set	We then built an experiment	evaluated on the reliable reference data set	135-161	135-161	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	146-152	153-161	evaluated on the reliable reference data set	which was the outcome of the first annotation .	evaluated on the reliable reference data set	which was the outcome of the first annotation .	135-161	135-161	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	1<2	none	elab-addition	elab-addition
P14-1045_anno1	162-163	164-170	We show	that in-document information greatly improve the prediction	We show	that in-document information greatly improve the prediction	162-177	162-177	We show that in-document information greatly improve the prediction made by the similarity level alone .	We show that in-document information greatly improve the prediction made by the similarity level alone .	1>2	none	attribution	attribution
P14-1045_anno1	135-139	164-170	We then built an experiment	that in-document information greatly improve the prediction	We then built an experiment	that in-document information greatly improve the prediction	135-161	162-177	We then built an experiment to automatically predict this relevance , evaluated on the reliable reference data set which was the outcome of the first annotation .	We show that in-document information greatly improve the prediction made by the similarity level alone .	1<2	none	cause	cause
P14-1045_anno1	164-170	171-177	that in-document information greatly improve the prediction	made by the similarity level alone .	that in-document information greatly improve the prediction	made by the similarity level alone .	162-177	162-177	We show that in-document information greatly improve the prediction made by the similarity level alone .	We show that in-document information greatly improve the prediction made by the similarity level alone .	1<2	none	elab-addition	elab-addition
P14-1046_anno1	1-17	18-21	Vector space models ( VSMs ) represent word meanings as points in a high dimensional space .	VSMs are typically created	Vector space models ( VSMs ) represent word meanings as points in a high dimensional space .	VSMs are typically created	1-17	18-37	Vector space models ( VSMs ) represent word meanings as points in a high dimensional space .	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	1>2	none	bg-general	bg-general
P14-1046_anno1	18-21	38-45	VSMs are typically created	We present a new algorithm ( JNNSE )	VSMs are typically created	We present a new algorithm ( JNNSE )	18-37	38-68	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1>2	none	bg-compare	bg-compare
P14-1046_anno1	18-21	22-27	VSMs are typically created	using a large text corpora ,	VSMs are typically created	using a large text corpora ,	18-37	18-37	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	1<2	none	manner-means	manner-means
P14-1046_anno1	18-21	28-32	VSMs are typically created	and so represent word semantics	VSMs are typically created	and so represent word semantics	18-37	18-37	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	1<2	none	cause	cause
P14-1046_anno1	28-32	33-37	and so represent word semantics	as observed in text .	and so represent word semantics	as observed in text .	18-37	18-37	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	VSMs are typically created using a large text corpora , and so represent word semantics as observed in text .	1<2	none	elab-addition	elab-addition
P14-1046_anno1	38-45	46-52	We present a new algorithm ( JNNSE )	that can incorporate a measure of semantics	We present a new algorithm ( JNNSE )	that can incorporate a measure of semantics	38-68	38-68	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1<2	none	manner-means	manner-means
P14-1046_anno1	46-52	53-55	that can incorporate a measure of semantics	not previously used	that can incorporate a measure of semantics	not previously used	38-68	38-68	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1<2	none	elab-addition	elab-addition
P14-1046_anno1	46-52	56-59	that can incorporate a measure of semantics	to create VSMs :	that can incorporate a measure of semantics	to create VSMs :	38-68	38-68	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1<2	none	enablement	enablement
P14-1046_anno1	46-52	60-63	that can incorporate a measure of semantics	brain activation data recorded	that can incorporate a measure of semantics	brain activation data recorded	38-68	38-68	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1<2	none	elab-addition	elab-addition
P14-1046_anno1	60-63	64-68	brain activation data recorded	while people read words .	brain activation data recorded	while people read words .	38-68	38-68	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	1<2	none	temporal	temporal
P14-1046_anno1	46-52	69-85	that can incorporate a measure of semantics	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data	that can incorporate a measure of semantics	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data	38-68	69-94	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics .	1<2	none	cause	cause
P14-1046_anno1	69-85	86-94	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data	to give a more complete representation of semantics .	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data	to give a more complete representation of semantics .	69-94	69-94	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics .	The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics .	1<2	none	enablement	enablement
P14-1046_anno1	95-96	97-110	Evaluations show	that the model 1 ) matches a behavioral measure of semantics more closely ,	Evaluations show	that the model 1 ) matches a behavioral measure of semantics more closely ,	95-138	95-138	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	1>2	none	attribution	attribution
P14-1046_anno1	38-45	97-110	We present a new algorithm ( JNNSE )	that the model 1 ) matches a behavioral measure of semantics more closely ,	We present a new algorithm ( JNNSE )	that the model 1 ) matches a behavioral measure of semantics more closely ,	38-68	95-138	We present a new algorithm ( JNNSE ) that can incorporate a measure of semantics not previously used to create VSMs : brain activation data recorded while people read words .	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	1<2	none	evaluation	evaluation
P14-1046_anno1	97-110	111-122	that the model 1 ) matches a behavioral measure of semantics more closely ,	2 ) can be used to predict corpus data for unseen words	that the model 1 ) matches a behavioral measure of semantics more closely ,	2 ) can be used to predict corpus data for unseen words	95-138	95-138	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	1<2	none	joint	joint
P14-1046_anno1	111-122	123-128	2 ) can be used to predict corpus data for unseen words	and 3 ) has predictive power	2 ) can be used to predict corpus data for unseen words	and 3 ) has predictive power	95-138	95-138	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	1<2	none	joint	joint
P14-1046_anno1	123-128	129-138	and 3 ) has predictive power	that generalizes across brain imaging technologies and across subjects .	and 3 ) has predictive power	that generalizes across brain imaging technologies and across subjects .	95-138	95-138	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	1<2	none	elab-addition	elab-addition
P14-1046_anno1	139-140	141-153	We believe	that the model is thus a more faithful representation of mental vocabularies .	We believe	that the model is thus a more faithful representation of mental vocabularies .	139-153	139-153	We believe that the model is thus a more faithful representation of mental vocabularies .	We believe that the model is thus a more faithful representation of mental vocabularies .	1>2	none	attribution	attribution
P14-1046_anno1	97-110	141-153	that the model 1 ) matches a behavioral measure of semantics more closely ,	that the model is thus a more faithful representation of mental vocabularies .	that the model 1 ) matches a behavioral measure of semantics more closely ,	that the model is thus a more faithful representation of mental vocabularies .	95-138	139-153	Evaluations show that the model 1 ) matches a behavioral measure of semantics more closely , 2 ) can be used to predict corpus data for unseen words and 3 ) has predictive power that generalizes across brain imaging technologies and across subjects .	We believe that the model is thus a more faithful representation of mental vocabularies .	1<2	none	cause	cause
P14-1047_anno1	1-10	11-21	We use single-agent and multi-agent Reinforcement Learning ( RL )	for learning dialogue policies in a resource allocation negotiation scenario .	We use single-agent and multi-agent Reinforcement Learning ( RL )	for learning dialogue policies in a resource allocation negotiation scenario .	1-21	1-21	We use single-agent and multi-agent Reinforcement Learning ( RL ) for learning dialogue policies in a resource allocation negotiation scenario .	We use single-agent and multi-agent Reinforcement Learning ( RL ) for learning dialogue policies in a resource allocation negotiation scenario .	1<2	none	enablement	enablement
P14-1047_anno1	1-10	22-25	We use single-agent and multi-agent Reinforcement Learning ( RL )	Two agents learn concurrently	We use single-agent and multi-agent Reinforcement Learning ( RL )	Two agents learn concurrently	1-21	22-48	We use single-agent and multi-agent Reinforcement Learning ( RL ) for learning dialogue policies in a resource allocation negotiation scenario .	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	1<2	none	elab-aspect	elab-aspect
P14-1047_anno1	22-25	26-30	Two agents learn concurrently	by interacting with each other	Two agents learn concurrently	by interacting with each other	22-48	22-48	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	1<2	none	manner-means	manner-means
P14-1047_anno1	22-25	31-39,43-44	Two agents learn concurrently	without any need for simulated users ( SUs ) <*> or corpora	Two agents learn concurrently	without any need for simulated users ( SUs ) <*> or corpora	22-48	22-48	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	1<2	none	elab-addition	elab-addition
P14-1047_anno1	31-39,43-44	40-42	without any need for simulated users ( SUs ) <*> or corpora	to train against	without any need for simulated users ( SUs ) <*> or corpora	to train against	22-48	22-48	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	1<2	none	elab-addition	elab-addition
P14-1047_anno1	43-44	45-48	or corpora	to learn from .	or corpora	to learn from .	22-48	22-48	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	Two agents learn concurrently by interacting with each other without any need for simulated users ( SUs ) to train against or corpora to learn from .	1<2	none	elab-addition	elab-addition
P14-1047_anno1	1-10	49-73	We use single-agent and multi-agent Reinforcement Learning ( RL )	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	We use single-agent and multi-agent Reinforcement Learning ( RL )	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	1-21	49-98	We use single-agent and multi-agent Reinforcement Learning ( RL ) for learning dialogue policies in a resource allocation negotiation scenario .	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	1<2	none	evaluation	evaluation
P14-1047_anno1	49-73	74-98	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	49-98	49-98	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	1<2	none	elab-addition	elab-addition
P14-1047_anno1	99-101	108-117	Our results show	whereas PHC and PHC-WoLF always converge and perform similarly .	Our results show	whereas PHC and PHC-WoLF always converge and perform similarly .	99-117	99-117	Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly .	Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly .	1>2	none	attribution	attribution
P14-1047_anno1	102-107	108-117	that generally Q-learning fails to converge	whereas PHC and PHC-WoLF always converge and perform similarly .	that generally Q-learning fails to converge	whereas PHC and PHC-WoLF always converge and perform similarly .	99-117	99-117	Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly .	Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly .	1>2	none	contrast	contrast
P14-1047_anno1	49-73	108-117	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	whereas PHC and PHC-WoLF always converge and perform similarly .	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	whereas PHC and PHC-WoLF always converge and perform similarly .	49-98	99-117	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly .	1<2	none	cause	cause
P14-1047_anno1	118-120	121-132	We also show	that very high gradually decreasing exploration rates are required for convergence .	We also show	that very high gradually decreasing exploration rates are required for convergence .	118-132	118-132	We also show that very high gradually decreasing exploration rates are required for convergence .	We also show that very high gradually decreasing exploration rates are required for convergence .	1>2	none	attribution	attribution
P14-1047_anno1	49-73	121-132	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	that very high gradually decreasing exploration rates are required for convergence .	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	that very high gradually decreasing exploration rates are required for convergence .	49-98	118-132	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	We also show that very high gradually decreasing exploration rates are required for convergence .	1<2	none	cause	cause
P14-1047_anno1	133-134	135-144	We conclude	that multi-agent RL of dialogue policies is a promising alternative	We conclude	that multi-agent RL of dialogue policies is a promising alternative	133-156	133-156	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	1>2	none	attribution	attribution
P14-1047_anno1	49-73	135-144	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	that multi-agent RL of dialogue policies is a promising alternative	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms ,	that multi-agent RL of dialogue policies is a promising alternative	49-98	133-156	In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate .	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	1<2	none	summary	summary
P14-1047_anno1	135-144	145-150	that multi-agent RL of dialogue policies is a promising alternative	to using single-agent RL and SUs	that multi-agent RL of dialogue policies is a promising alternative	to using single-agent RL and SUs	133-156	133-156	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	1<2	none	elab-addition	elab-addition
P14-1047_anno1	145-150	151-156	to using single-agent RL and SUs	or learning directly from corpora .	to using single-agent RL and SUs	or learning directly from corpora .	133-156	133-156	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	We conclude that multi-agent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora .	1<2	none	joint	joint
P14-1048_anno1	1-7	49-58	Text-level discourse parsing remains a challenge .	In this work , we develop a much faster model	Text-level discourse parsing remains a challenge .	In this work , we develop a much faster model	1-7	49-69	Text-level discourse parsing remains a challenge .	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	1>2	none	bg-goal	bg-goal
P14-1048_anno1	8-19	29-40	The current state-of-the-art overall accuracy in relation assignment is 55.73 % ,	However , their model has a high order of time complexity ,	The current state-of-the-art overall accuracy in relation assignment is 55.73 % ,	However , their model has a high order of time complexity ,	8-28	29-48	The current state-of-the-art overall accuracy in relation assignment is 55.73 % , achieved by Joty et al. ( 2013 ) .	However , their model has a high order of time complexity , and thus cannot be applied in practice .	1>2	none	contrast	contrast
P14-1048_anno1	8-19	20-28	The current state-of-the-art overall accuracy in relation assignment is 55.73 % ,	achieved by Joty et al. ( 2013 ) .	The current state-of-the-art overall accuracy in relation assignment is 55.73 % ,	achieved by Joty et al. ( 2013 ) .	8-28	8-28	The current state-of-the-art overall accuracy in relation assignment is 55.73 % , achieved by Joty et al. ( 2013 ) .	The current state-of-the-art overall accuracy in relation assignment is 55.73 % , achieved by Joty et al. ( 2013 ) .	1<2	none	elab-addition	elab-addition
P14-1048_anno1	29-40	49-58	However , their model has a high order of time complexity ,	In this work , we develop a much faster model	However , their model has a high order of time complexity ,	In this work , we develop a much faster model	29-48	49-69	However , their model has a high order of time complexity , and thus cannot be applied in practice .	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	1>2	none	bg-compare	bg-compare
P14-1048_anno1	29-40	41-48	However , their model has a high order of time complexity ,	and thus cannot be applied in practice .	However , their model has a high order of time complexity ,	and thus cannot be applied in practice .	29-48	29-48	However , their model has a high order of time complexity , and thus cannot be applied in practice .	However , their model has a high order of time complexity , and thus cannot be applied in practice .	1<2	none	cause	cause
P14-1048_anno1	49-58	59-69	In this work , we develop a much faster model	whose time complexity is linear in the number of sentences .	In this work , we develop a much faster model	whose time complexity is linear in the number of sentences .	49-69	49-69	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	1<2	none	elab-addition	elab-addition
P14-1048_anno1	49-58	70-77	In this work , we develop a much faster model	Our model adopts a greedy bottom-up approach ,	In this work , we develop a much faster model	Our model adopts a greedy bottom-up approach ,	49-69	70-88	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers .	1<2	none	elab-aspect	elab-aspect
P14-1048_anno1	70-77	78-81	Our model adopts a greedy bottom-up approach ,	with two linear-chain CRFs	Our model adopts a greedy bottom-up approach ,	with two linear-chain CRFs	70-88	70-88	Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers .	Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers .	1<2	none	manner-means	manner-means
P14-1048_anno1	78-81	82-88	with two linear-chain CRFs	applied in cascade as local classifiers .	with two linear-chain CRFs	applied in cascade as local classifiers .	70-88	70-88	Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers .	Our model adopts a greedy bottom-up approach , with two linear-chain CRFs applied in cascade as local classifiers .	1<2	none	elab-addition	elab-addition
P14-1048_anno1	89-96	97-109	To enhance the accuracy of the pipeline ,	we add additional constraints in the Viterbi decoding of the first CRF .	To enhance the accuracy of the pipeline ,	we add additional constraints in the Viterbi decoding of the first CRF .	89-109	89-109	To enhance the accuracy of the pipeline , we add additional constraints in the Viterbi decoding of the first CRF .	To enhance the accuracy of the pipeline , we add additional constraints in the Viterbi decoding of the first CRF .	1>2	none	enablement	enablement
P14-1048_anno1	49-58	97-109	In this work , we develop a much faster model	we add additional constraints in the Viterbi decoding of the first CRF .	In this work , we develop a much faster model	we add additional constraints in the Viterbi decoding of the first CRF .	49-69	89-109	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	To enhance the accuracy of the pipeline , we add additional constraints in the Viterbi decoding of the first CRF .	1<2	none	elab-aspect	elab-aspect
P14-1048_anno1	49-58	110-125	In this work , we develop a much faster model	In addition to efficiency , our parser also significantly outperforms the state of the art .	In this work , we develop a much faster model	In addition to efficiency , our parser also significantly outperforms the state of the art .	49-69	110-125	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	In addition to efficiency , our parser also significantly outperforms the state of the art .	1<2	none	evaluation	evaluation
P14-1048_anno1	49-58	126-133,148-153	In this work , we develop a much faster model	Moreover , our novel approach of post-editing , <*> can further improve the accuracy .	In this work , we develop a much faster model	Moreover , our novel approach of post-editing , <*> can further improve the accuracy .	49-69	126-153	In this work , we develop a much faster model whose time complexity is linear in the number of sentences .	Moreover , our novel approach of post-editing , which modifies a fully-built tree by considering information from constituents on upper levels , can further improve the accuracy .	1<2	none	elab-aspect	elab-aspect
P14-1048_anno1	126-133,148-153	134-147	Moreover , our novel approach of post-editing , <*> can further improve the accuracy .	which modifies a fully-built tree by considering information from constituents on upper levels ,	Moreover , our novel approach of post-editing , <*> can further improve the accuracy .	which modifies a fully-built tree by considering information from constituents on upper levels ,	126-153	126-153	Moreover , our novel approach of post-editing , which modifies a fully-built tree by considering information from constituents on upper levels , can further improve the accuracy .	Moreover , our novel approach of post-editing , which modifies a fully-built tree by considering information from constituents on upper levels , can further improve the accuracy .	1<2	none	elab-addition	elab-addition
P14-1049_anno1	1-8	46-54	Negative expressions are common in natural language text	In this paper , we propose a graph model	Negative expressions are common in natural language text	In this paper , we propose a graph model	1-17	46-68	Negative expressions are common in natural language text and play a critical role in information extraction .	In this paper , we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	1>2	none	bg-goal	bg-goal
P14-1049_anno1	1-8	9-17	Negative expressions are common in natural language text	and play a critical role in information extraction .	Negative expressions are common in natural language text	and play a critical role in information extraction .	1-17	1-17	Negative expressions are common in natural language text and play a critical role in information extraction .	Negative expressions are common in natural language text and play a critical role in information extraction .	1<2	none	joint	joint
P14-1049_anno1	18-29	46-54	However , the performances of current systems are far from satisfaction ,	In this paper , we propose a graph model	However , the performances of current systems are far from satisfaction ,	In this paper , we propose a graph model	18-45	46-68	However , the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information .	In this paper , we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	1>2	none	bg-compare	bg-compare
P14-1049_anno1	18-29	30-40	However , the performances of current systems are far from satisfaction ,	largely due to its focus on intra-sentence information and its failure	However , the performances of current systems are far from satisfaction ,	largely due to its focus on intra-sentence information and its failure	18-45	18-45	However , the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information .	However , the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information .	1<2	none	exp-reason	exp-reason
P14-1049_anno1	30-40	41-45	largely due to its focus on intra-sentence information and its failure	to consider inter-sentence information .	largely due to its focus on intra-sentence information and its failure	to consider inter-sentence information .	18-45	18-45	However , the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information .	However , the performances of current systems are far from satisfaction , largely due to its focus on intra-sentence information and its failure to consider inter-sentence information .	1<2	none	elab-addition	elab-addition
P14-1049_anno1	46-54	55-68	In this paper , we propose a graph model	to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	In this paper , we propose a graph model	to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	46-68	46-68	In this paper , we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	In this paper , we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	1<2	none	enablement	enablement
P14-1049_anno1	46-54	69-88	In this paper , we propose a graph model	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification	In this paper , we propose a graph model	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification	46-68	69-102	In this paper , we propose a graph model to enrich intra-sentence features with inter-sentence features from both lexical and topic perspectives .	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information .	1<2	none	evaluation	evaluation
P14-1049_anno1	69-88	89-96	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification	and justifies the effectiveness of our graph model	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification	and justifies the effectiveness of our graph model	69-102	69-102	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information .	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information .	1<2	none	joint	joint
P14-1049_anno1	89-96	97-102	and justifies the effectiveness of our graph model	in capturing such global information .	and justifies the effectiveness of our graph model	in capturing such global information .	69-102	69-102	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information .	Evaluation on the * SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information .	1<2	none	elab-addition	elab-addition
P14-1050_anno1	1-13	28-40	Automatic extraction of new words is an indispensable precursor to many NLP tasks	This paper aims at extracting new sentiment words from large-scale user-generated content .	Automatic extraction of new words is an indispensable precursor to many NLP tasks	This paper aims at extracting new sentiment words from large-scale user-generated content .	1-27	28-40	Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation , named entity extraction , and sentiment analysis .	This paper aims at extracting new sentiment words from large-scale user-generated content .	1>2	none	bg-general	bg-general
P14-1050_anno1	1-13	14-27	Automatic extraction of new words is an indispensable precursor to many NLP tasks	such as Chinese word segmentation , named entity extraction , and sentiment analysis .	Automatic extraction of new words is an indispensable precursor to many NLP tasks	such as Chinese word segmentation , named entity extraction , and sentiment analysis .	1-27	1-27	Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation , named entity extraction , and sentiment analysis .	Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation , named entity extraction , and sentiment analysis .	1<2	none	elab-example	elab-example
P14-1050_anno1	28-40	41-53	This paper aims at extracting new sentiment words from large-scale user-generated content .	We propose a fully unsupervised , purely data-driven framework for this purpose .	This paper aims at extracting new sentiment words from large-scale user-generated content .	We propose a fully unsupervised , purely data-driven framework for this purpose .	28-40	41-53	This paper aims at extracting new sentiment words from large-scale user-generated content .	We propose a fully unsupervised , purely data-driven framework for this purpose .	1>2	none	bg-goal	bg-goal
P14-1050_anno1	41-53	54-58	We propose a fully unsupervised , purely data-driven framework for this purpose .	We design statistical measures respectively	We propose a fully unsupervised , purely data-driven framework for this purpose .	We design statistical measures respectively	41-53	54-79	We propose a fully unsupervised , purely data-driven framework for this purpose .	We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .	1<2	none	elab-aspect	elab-aspect
P14-1050_anno1	54-58	59-66	We design statistical measures respectively	to quantify the utility of a lexical pattern	We design statistical measures respectively	to quantify the utility of a lexical pattern	54-79	54-79	We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .	We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .	1<2	none	enablement	enablement
P14-1050_anno1	59-66	67-79	to quantify the utility of a lexical pattern	and to measure the possibility of a word being a new word .	to quantify the utility of a lexical pattern	and to measure the possibility of a word being a new word .	54-79	54-79	We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .	We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .	1<2	none	joint	joint
P14-1050_anno1	41-53	80-93	We propose a fully unsupervised , purely data-driven framework for this purpose .	The method is almost free of linguistic resources ( except POS tags ) ,	We propose a fully unsupervised , purely data-driven framework for this purpose .	The method is almost free of linguistic resources ( except POS tags ) ,	41-53	80-100	We propose a fully unsupervised , purely data-driven framework for this purpose .	The method is almost free of linguistic resources ( except POS tags ) , and requires no elaborated linguistic rules .	1<2	none	elab-aspect	elab-aspect
P14-1050_anno1	80-93	94-100	The method is almost free of linguistic resources ( except POS tags ) ,	and requires no elaborated linguistic rules .	The method is almost free of linguistic resources ( except POS tags ) ,	and requires no elaborated linguistic rules .	80-100	80-100	The method is almost free of linguistic resources ( except POS tags ) , and requires no elaborated linguistic rules .	The method is almost free of linguistic resources ( except POS tags ) , and requires no elaborated linguistic rules .	1<2	none	joint	joint
P14-1050_anno1	101-103	104-112	We also demonstrate	how new sentiment word will benefit sentiment analysis .	We also demonstrate	how new sentiment word will benefit sentiment analysis .	101-112	101-112	We also demonstrate how new sentiment word will benefit sentiment analysis .	We also demonstrate how new sentiment word will benefit sentiment analysis .	1>2	none	attribution	attribution
P14-1050_anno1	28-40	104-112	This paper aims at extracting new sentiment words from large-scale user-generated content .	how new sentiment word will benefit sentiment analysis .	This paper aims at extracting new sentiment words from large-scale user-generated content .	how new sentiment word will benefit sentiment analysis .	28-40	101-112	This paper aims at extracting new sentiment words from large-scale user-generated content .	We also demonstrate how new sentiment word will benefit sentiment analysis .	1<2	none	cause	cause
P14-1050_anno1	41-53	113-122	We propose a fully unsupervised , purely data-driven framework for this purpose .	Experiment results demonstrate the effectiveness of the proposed method .	We propose a fully unsupervised , purely data-driven framework for this purpose .	Experiment results demonstrate the effectiveness of the proposed method .	41-53	113-122	We propose a fully unsupervised , purely data-driven framework for this purpose .	Experiment results demonstrate the effectiveness of the proposed method .	1<2	none	evaluation	evaluation
P14-1051_anno1	1-2,7-16	29-35	The sentiment <*> provides interesting and valuable information for social media services .	it is challenging to build a framework	The sentiment <*> provides interesting and valuable information for social media services .	it is challenging to build a framework	1-16	17-41	The sentiment captured in opinionated text provides interesting and valuable information for social media services .	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	1>2	none	bg-general	bg-general
P14-1051_anno1	1-2,7-16	3-6	The sentiment <*> provides interesting and valuable information for social media services .	captured in opinionated text	The sentiment <*> provides interesting and valuable information for social media services .	captured in opinionated text	1-16	1-16	The sentiment captured in opinionated text provides interesting and valuable information for social media services .	The sentiment captured in opinionated text provides interesting and valuable information for social media services .	1<2	none	elab-addition	elab-addition
P14-1051_anno1	17-28	29-35	However , due to the complexity and diversity of linguistic representations ,	it is challenging to build a framework	However , due to the complexity and diversity of linguistic representations ,	it is challenging to build a framework	17-41	17-41	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	1>2	none	exp-reason	exp-reason
P14-1051_anno1	29-35	42-46	it is challenging to build a framework	We propose a semi-supervised framework	it is challenging to build a framework	We propose a semi-supervised framework	17-41	42-60	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	1>2	none	bg-goal	bg-goal
P14-1051_anno1	29-35	36-41	it is challenging to build a framework	that accurately extracts such sentiment .	it is challenging to build a framework	that accurately extracts such sentiment .	17-41	17-41	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	However , due to the complexity and diversity of linguistic representations , it is challenging to build a framework that accurately extracts such sentiment .	1<2	none	elab-addition	elab-addition
P14-1051_anno1	42-46	47-52	We propose a semi-supervised framework	for generating a domain-specific sentiment lexicon	We propose a semi-supervised framework	for generating a domain-specific sentiment lexicon	42-60	42-60	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	1<2	none	elab-addition	elab-addition
P14-1051_anno1	47-52	53-60	for generating a domain-specific sentiment lexicon	and inferring sentiments at the segment level .	for generating a domain-specific sentiment lexicon	and inferring sentiments at the segment level .	42-60	42-60	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	1<2	none	joint	joint
P14-1051_anno1	42-46	61-68	We propose a semi-supervised framework	Our framework can greatly reduce the human effort	We propose a semi-supervised framework	Our framework can greatly reduce the human effort	42-60	61-78	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	Our framework can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality .	1<2	none	elab-aspect	elab-aspect
P14-1051_anno1	61-68	69-78	Our framework can greatly reduce the human effort	for building a domain-specific sentiment lexicon with high quality .	Our framework can greatly reduce the human effort	for building a domain-specific sentiment lexicon with high quality .	61-78	61-78	Our framework can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality .	Our framework can greatly reduce the human effort for building a domain-specific sentiment lexicon with high quality .	1<2	none	elab-addition	elab-addition
P14-1051_anno1	79-92	93-98	Specifically , in our evaluation , working with just 20 manually labeled reviews ,	it generates a domain-specific sentiment lexicon	Specifically , in our evaluation , working with just 20 manually labeled reviews ,	it generates a domain-specific sentiment lexicon	79-108	79-108	Specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3 % .	Specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3 % .	1>2	none	elab-addition	elab-addition
P14-1051_anno1	42-46	93-98	We propose a semi-supervised framework	it generates a domain-specific sentiment lexicon	We propose a semi-supervised framework	it generates a domain-specific sentiment lexicon	42-60	79-108	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	Specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3 % .	1<2	none	evaluation	evaluation
P14-1051_anno1	93-98	99-108	it generates a domain-specific sentiment lexicon	that yields weighted average F-Measure gains of 3 % .	it generates a domain-specific sentiment lexicon	that yields weighted average F-Measure gains of 3 % .	79-108	79-108	Specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3 % .	Specifically , in our evaluation , working with just 20 manually labeled reviews , it generates a domain-specific sentiment lexicon that yields weighted average F-Measure gains of 3 % .	1<2	none	elab-addition	elab-addition
P14-1051_anno1	42-46	109-122	We propose a semi-supervised framework	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach	We propose a semi-supervised framework	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach	42-60	109-128	We propose a semi-supervised framework for generating a domain-specific sentiment lexicon and inferring sentiments at the segment level .	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach based on elementary discourse units .	1<2	none	evaluation	evaluation
P14-1051_anno1	109-122	123-128	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach	based on elementary discourse units .	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach	based on elementary discourse units .	109-128	109-128	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach based on elementary discourse units .	Our sentiment classification model achieves approximately 1 % greater accuracy than a state-of-the-art approach based on elementary discourse units .	1<2	none	elab-addition	elab-addition
P14-1052_anno1	1-9	10-21	We study the problem of generating an English sentence	given an underlying probabilistic grammar , a world and a communicative goal.	We study the problem of generating an English sentence	given an underlying probabilistic grammar , a world and a communicative goal.	1-21	1-21	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	1<2	none	condition	condition
P14-1052_anno1	1-9	22-37	We study the problem of generating an English sentence	We model the generation problem as a Markov decision process with a suitably defined reward function	We study the problem of generating an English sentence	We model the generation problem as a Markov decision process with a suitably defined reward function	1-21	22-42	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	We model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal.	1<2	none	elab-process_step	elab-process_step
P14-1052_anno1	22-37	38-42	We model the generation problem as a Markov decision process with a suitably defined reward function	that reflects the communicative goal.	We model the generation problem as a Markov decision process with a suitably defined reward function	that reflects the communicative goal.	22-42	22-42	We model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal.	We model the generation problem as a Markov decision process with a suitably defined reward function that reflects the communicative goal.	1<2	none	elab-addition	elab-addition
P14-1052_anno1	1-9	43-51	We study the problem of generating an English sentence	We then use probabilistic planning to solve the MDP	We study the problem of generating an English sentence	We then use probabilistic planning to solve the MDP	1-21	43-65	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	We then use probabilistic planning to solve the MDP and generate a sentence that , with high probability , accomplishes the communicative goal.	1<2	none	elab-process_step	elab-process_step
P14-1052_anno1	43-51	52-55	We then use probabilistic planning to solve the MDP	and generate a sentence	We then use probabilistic planning to solve the MDP	and generate a sentence	43-65	43-65	We then use probabilistic planning to solve the MDP and generate a sentence that , with high probability , accomplishes the communicative goal.	We then use probabilistic planning to solve the MDP and generate a sentence that , with high probability , accomplishes the communicative goal.	1<2	none	joint	joint
P14-1052_anno1	52-55	56-65	and generate a sentence	that , with high probability , accomplishes the communicative goal.	and generate a sentence	that , with high probability , accomplishes the communicative goal.	43-65	43-65	We then use probabilistic planning to solve the MDP and generate a sentence that , with high probability , accomplishes the communicative goal.	We then use probabilistic planning to solve the MDP and generate a sentence that , with high probability , accomplishes the communicative goal.	1<2	none	elab-addition	elab-addition
P14-1052_anno1	66-68	69-78	We show empirically	that our approach can generate complex sentences with a speed	We show empirically	that our approach can generate complex sentences with a speed	66-89	66-89	We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art .	We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art .	1>2	none	attribution	attribution
P14-1052_anno1	1-9	69-78	We study the problem of generating an English sentence	that our approach can generate complex sentences with a speed	We study the problem of generating an English sentence	that our approach can generate complex sentences with a speed	1-21	66-89	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art .	1<2	none	evaluation	evaluation
P14-1052_anno1	69-78	79-89	that our approach can generate complex sentences with a speed	that generally matches or surpasses the state of the art .	that our approach can generate complex sentences with a speed	that generally matches or surpasses the state of the art .	66-89	66-89	We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art .	We show empirically that our approach can generate complex sentences with a speed that generally matches or surpasses the state of the art .	1<2	none	elab-addition	elab-addition
P14-1052_anno1	90-93	94-98	Further , we show	that our approach is anytime	Further , we show	that our approach is anytime	90-109	90-109	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	1>2	none	attribution	attribution
P14-1052_anno1	1-9	94-98	We study the problem of generating an English sentence	that our approach is anytime	We study the problem of generating an English sentence	that our approach is anytime	1-21	90-109	We study the problem of generating an English sentence given an underlying probabilistic grammar , a world and a communicative goal.	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	1<2	none	elab-aspect	elab-aspect
P14-1052_anno1	94-98	99-105	that our approach is anytime	and can handle complex communicative goals ,	that our approach is anytime	and can handle complex communicative goals ,	90-109	90-109	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	1<2	none	joint	joint
P14-1052_anno1	99-105	106-109	and can handle complex communicative goals ,	including negated goals .	and can handle complex communicative goals ,	including negated goals .	90-109	90-109	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	Further , we show that our approach is anytime and can handle complex communicative goals , including negated goals .	1<2	none	elab-example	elab-example
P14-1053_anno1	1-11	26-30	A vast majority of L1 vocabulary acquisition occurs through incidental learning	We propose a probabilistic approach	A vast majority of L1 vocabulary acquisition occurs through incidental learning	We propose a probabilistic approach	1-25	26-48	A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading ( Nation , 2001 ; Schmitt et al. , 2001 ) .	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	1>2	none	bg-general	bg-general
P14-1053_anno1	1-11	12-25	A vast majority of L1 vocabulary acquisition occurs through incidental learning	during reading ( Nation , 2001 ; Schmitt et al. , 2001 ) .	A vast majority of L1 vocabulary acquisition occurs through incidental learning	during reading ( Nation , 2001 ; Schmitt et al. , 2001 ) .	1-25	1-25	A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading ( Nation , 2001 ; Schmitt et al. , 2001 ) .	A vast majority of L1 vocabulary acquisition occurs through incidental learning during reading ( Nation , 2001 ; Schmitt et al. , 2001 ) .	1<2	none	temporal	temporal
P14-1053_anno1	26-30	31-38	We propose a probabilistic approach	to generating code-mixed text as an L2 technique	We propose a probabilistic approach	to generating code-mixed text as an L2 technique	26-48	26-48	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	1<2	none	elab-addition	elab-addition
P14-1053_anno1	31-38	39-45	to generating code-mixed text as an L2 technique	for increasing retention in adult lexical learning	to generating code-mixed text as an L2 technique	for increasing retention in adult lexical learning	26-48	26-48	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	1<2	none	enablement	enablement
P14-1053_anno1	39-45	46-48	for increasing retention in adult lexical learning	through reading .	for increasing retention in adult lexical learning	through reading .	26-48	26-48	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	1<2	none	manner-means	manner-means
P14-1053_anno1	26-30	49-50	We propose a probabilistic approach	Our model	We propose a probabilistic approach	Our model	26-48	49-84	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	1<2	none	elab-aspect	elab-aspect
P14-1053_anno1	49-50	51-62	Our model	that takes as input a bilingual dictionary and an English text ,	Our model	that takes as input a bilingual dictionary and an English text ,	49-84	49-84	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	1<2	none	elab-addition	elab-addition
P14-1053_anno1	51-62	63-67	that takes as input a bilingual dictionary and an English text ,	and generates a code-switched text	that takes as input a bilingual dictionary and an English text ,	and generates a code-switched text	49-84	49-84	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	1<2	none	joint	joint
P14-1053_anno1	63-67	68-75	and generates a code-switched text	that optimizes a defined " learnability " metric	and generates a code-switched text	that optimizes a defined " learnability " metric	49-84	49-84	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	1<2	none	elab-addition	elab-addition
P14-1053_anno1	68-75	76-84	that optimizes a defined " learnability " metric	by constructing a factor graph over lexical mentions .	that optimizes a defined " learnability " metric	by constructing a factor graph over lexical mentions .	49-84	49-84	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	Our model that takes as input a bilingual dictionary and an English text , and generates a code-switched text that optimizes a defined " learnability " metric by constructing a factor graph over lexical mentions .	1<2	none	manner-means	manner-means
P14-1053_anno1	85-90	91-96	Using an artificial language vocabulary ,	we evaluate a set of algorithms	Using an artificial language vocabulary ,	we evaluate a set of algorithms	85-117	85-117	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	1>2	none	manner-means	manner-means
P14-1053_anno1	26-30	91-96	We propose a probabilistic approach	we evaluate a set of algorithms	We propose a probabilistic approach	we evaluate a set of algorithms	26-48	85-117	We propose a probabilistic approach to generating code-mixed text as an L2 technique for increasing retention in adult lexical learning through reading .	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	1<2	none	evaluation	evaluation
P14-1053_anno1	91-96	97-101	we evaluate a set of algorithms	for generating code-switched text automatically	we evaluate a set of algorithms	for generating code-switched text automatically	85-117	85-117	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	1<2	none	elab-addition	elab-addition
P14-1053_anno1	97-101	102-108	for generating code-switched text automatically	by presenting it to Mechanical Turk subjects	for generating code-switched text automatically	by presenting it to Mechanical Turk subjects	85-117	85-117	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	1<2	none	manner-means	manner-means
P14-1053_anno1	102-108	109-117	by presenting it to Mechanical Turk subjects	and measuring recall in a sentence completion task .	by presenting it to Mechanical Turk subjects	and measuring recall in a sentence completion task .	85-117	85-117	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	Using an artificial language vocabulary , we evaluate a set of algorithms for generating code-switched text automatically by presenting it to Mechanical Turk subjects and measuring recall in a sentence completion task .	1<2	none	joint	joint
P14-1054_anno1	1-6	13-25	Chinese is an ancient hieroglyphic .	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	Chinese is an ancient hieroglyphic .	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	1-6	13-25	Chinese is an ancient hieroglyphic .	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	1>2	none	bg-general	bg-general
P14-1054_anno1	1-6	7-12	Chinese is an ancient hieroglyphic .	It is inattentive to structure .	Chinese is an ancient hieroglyphic .	It is inattentive to structure .	1-6	7-12	Chinese is an ancient hieroglyphic .	It is inattentive to structure .	1<2	none	elab-addition	elab-addition
P14-1054_anno1	13-25	26-44	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	13-25	26-44	Therefore , segmenting and parsing Chinese are more difficult and less accurate .	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	1>2	none	bg-goal	bg-goal
P14-1054_anno1	26-44	45-58	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	The Omni-word feature uses every potential word in a sentence as lexicon feature ,	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	The Omni-word feature uses every potential word in a sentence as lexicon feature ,	26-44	45-65	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	The Omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation .	1<2	none	elab-aspect	elab-aspect
P14-1054_anno1	45-58	59-60	The Omni-word feature uses every potential word in a sentence as lexicon feature ,	reducing errors	The Omni-word feature uses every potential word in a sentence as lexicon feature ,	reducing errors	45-65	45-65	The Omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation .	The Omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation .	1<2	none	cause	cause
P14-1054_anno1	59-60	61-65	reducing errors	caused by word segmentation .	reducing errors	caused by word segmentation .	45-65	45-65	The Omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation .	The Omni-word feature uses every potential word in a sentence as lexicon feature , reducing errors caused by word segmentation .	1<2	none	elab-addition	elab-addition
P14-1054_anno1	66-77	80-85	In order to utilize the structure information of a relation instance ,	how soft constraint can be used	In order to utilize the structure information of a relation instance ,	how soft constraint can be used	66-91	66-91	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	1>2	none	enablement	enablement
P14-1054_anno1	78-79	80-85	we discuss	how soft constraint can be used	we discuss	how soft constraint can be used	66-91	66-91	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	1>2	none	attribution	attribution
P14-1054_anno1	26-44	80-85	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	how soft constraint can be used	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	how soft constraint can be used	26-44	66-91	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	1<2	none	elab-aspect	elab-aspect
P14-1054_anno1	80-85	86-91	how soft constraint can be used	to capture the local dependency .	how soft constraint can be used	to capture the local dependency .	66-91	66-91	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	In order to utilize the structure information of a relation instance , we discuss how soft constraint can be used to capture the local dependency .	1<2	none	enablement	enablement
P14-1054_anno1	26-44	92-104	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	Both Omni-word feature and soft constraint make a better use of sentence information	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	Both Omni-word feature and soft constraint make a better use of sentence information	26-44	92-116	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	Both Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing .	1<2	none	elab-aspect	elab-aspect
P14-1054_anno1	92-104	105-108	Both Omni-word feature and soft constraint make a better use of sentence information	and minimize the influences	Both Omni-word feature and soft constraint make a better use of sentence information	and minimize the influences	92-116	92-116	Both Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing .	Both Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing .	1<2	none	joint	joint
P14-1054_anno1	105-108	109-116	and minimize the influences	caused by Chinese word segmentation and parsing .	and minimize the influences	caused by Chinese word segmentation and parsing .	92-116	92-116	Both Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing .	Both Omni-word feature and soft constraint make a better use of sentence information and minimize the influences caused by Chinese word segmentation and parsing .	1<2	none	elab-addition	elab-addition
P14-1054_anno1	26-44	117-128	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	We test these methods on the ACE 2005 RDC Chinese corpus .	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	We test these methods on the ACE 2005 RDC Chinese corpus .	26-44	117-128	In this paper , we propose an Omni-word feature and a soft constraint method for Chinese relation extraction .	We test these methods on the ACE 2005 RDC Chinese corpus .	1<2	none	evaluation	evaluation
P14-1054_anno1	117-128	129-139	We test these methods on the ACE 2005 RDC Chinese corpus .	The results show a significant improvement in Chinese relation extraction ,	We test these methods on the ACE 2005 RDC Chinese corpus .	The results show a significant improvement in Chinese relation extraction ,	117-128	129-159	We test these methods on the ACE 2005 RDC Chinese corpus .	The results show a significant improvement in Chinese relation extraction , outperforming other methods in F-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .	1<2	none	cause	cause
P14-1054_anno1	129-139	140-159	The results show a significant improvement in Chinese relation extraction ,	outperforming other methods in F-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .	The results show a significant improvement in Chinese relation extraction ,	outperforming other methods in F-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .	129-159	129-159	The results show a significant improvement in Chinese relation extraction , outperforming other methods in F-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .	The results show a significant improvement in Chinese relation extraction , outperforming other methods in F-score by 10 % in 6 relation types and 15 % in 18 relation subtypes .	1<2	none	exp-evidence	exp-evidence
P14-1055_anno1	1-9	18-32	Active learning ( AL ) has been proven effective	However , previous studies on AL are limited to applications in a single language .	Active learning ( AL ) has been proven effective	However , previous studies on AL are limited to applications in a single language .	1-17	18-32	Active learning ( AL ) has been proven effective to reduce human annotation efforts in NLP .	However , previous studies on AL are limited to applications in a single language .	1>2	none	contrast	contrast
P14-1055_anno1	1-9	10-17	Active learning ( AL ) has been proven effective	to reduce human annotation efforts in NLP .	Active learning ( AL ) has been proven effective	to reduce human annotation efforts in NLP .	1-17	1-17	Active learning ( AL ) has been proven effective to reduce human annotation efforts in NLP .	Active learning ( AL ) has been proven effective to reduce human annotation efforts in NLP .	1<2	none	enablement	enablement
P14-1055_anno1	18-32	33-44	However , previous studies on AL are limited to applications in a single language .	This paper proposes a bilingual active learning paradigm for relation classification ,	However , previous studies on AL are limited to applications in a single language .	This paper proposes a bilingual active learning paradigm for relation classification ,	18-32	33-70	However , previous studies on AL are limited to applications in a single language .	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	1>2	none	bg-compare	bg-compare
P14-1055_anno1	33-44	45-62	This paper proposes a bilingual active learning paradigm for relation classification ,	where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages	This paper proposes a bilingual active learning paradigm for relation classification ,	where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages	33-70	33-70	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	1<2	none	elab-addition	elab-addition
P14-1055_anno1	45-62	63-70	where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages	and then manually labeled by an oracle .	where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages	and then manually labeled by an oracle .	33-70	33-70	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	1<2	none	joint	joint
P14-1055_anno1	71-77	78-92	Instead of using a parallel corpus ,	labeled and unlabeled instances in one language are translated into ones in the other language	Instead of using a parallel corpus ,	labeled and unlabeled instances in one language are translated into ones in the other language	71-112	71-112	Instead of using a parallel corpus , labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	Instead of using a parallel corpus , labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	1>2	none	contrast	contrast
P14-1055_anno1	33-44	78-92	This paper proposes a bilingual active learning paradigm for relation classification ,	labeled and unlabeled instances in one language are translated into ones in the other language	This paper proposes a bilingual active learning paradigm for relation classification ,	labeled and unlabeled instances in one language are translated into ones in the other language	33-70	71-112	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	Instead of using a parallel corpus , labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	1<2	none	elab-process_step	elab-process_step
P14-1055_anno1	33-44	93-112	This paper proposes a bilingual active learning paradigm for relation classification ,	and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	This paper proposes a bilingual active learning paradigm for relation classification ,	and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	33-70	71-112	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	Instead of using a parallel corpus , labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora .	1<2	none	elab-process_step	elab-process_step
P14-1055_anno1	113-124	125-137	Experimental results on the ACE RDC 2005 Chinese and English corpora show	that bilingual active learning for relation classification significantly outperforms monolingual active learning .	Experimental results on the ACE RDC 2005 Chinese and English corpora show	that bilingual active learning for relation classification significantly outperforms monolingual active learning .	113-137	113-137	Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning .	Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning .	1>2	none	attribution	attribution
P14-1055_anno1	33-44	125-137	This paper proposes a bilingual active learning paradigm for relation classification ,	that bilingual active learning for relation classification significantly outperforms monolingual active learning .	This paper proposes a bilingual active learning paradigm for relation classification ,	that bilingual active learning for relation classification significantly outperforms monolingual active learning .	33-70	113-137	This paper proposes a bilingual active learning paradigm for relation classification , where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle .	Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning .	1<2	none	evaluation	evaluation
P14-1056_anno1	1-17	82-85	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task	We extend dual decomposition	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task	We extend dual decomposition	1-26	82-93	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task because the output typically obeys various global constraints .	We extend dual decomposition to perform prediction subject to soft constraints .	1>2	none	bg-goal	bg-goal
P14-1056_anno1	1-17	18-26	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task	because the output typically obeys various global constraints .	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task	because the output typically obeys various global constraints .	1-26	1-26	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task because the output typically obeys various global constraints .	Accurately segmenting a citation string into fields for authors , titles , etc. is a challenging task because the output typically obeys various global constraints .	1<2	none	exp-reason	exp-reason
P14-1056_anno1	27-30	31-35,50-55	Previous work has shown	that modeling soft constraints , <*> can substantially improve segmentation performance .	Previous work has shown	that modeling soft constraints , <*> can substantially improve segmentation performance .	27-55	27-55	Previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance .	Previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance .	1>2	none	attribution	attribution
P14-1056_anno1	31-35,50-55	82-85	that modeling soft constraints , <*> can substantially improve segmentation performance .	We extend dual decomposition	that modeling soft constraints , <*> can substantially improve segmentation performance .	We extend dual decomposition	27-55	82-93	Previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance .	We extend dual decomposition to perform prediction subject to soft constraints .	1>2	none	bg-general	bg-general
P14-1056_anno1	31-35,50-55	36-49	that modeling soft constraints , <*> can substantially improve segmentation performance .	where the model is encouraged , but not require to obey the constraints ,	that modeling soft constraints , <*> can substantially improve segmentation performance .	where the model is encouraged , but not require to obey the constraints ,	27-55	27-55	Previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance .	Previous work has shown that modeling soft constraints , where the model is encouraged , but not require to obey the constraints , can substantially improve segmentation performance .	1<2	none	elab-addition	elab-addition
P14-1056_anno1	56-65	66-74	On the other hand , for imposing hard constraints ,	dual decomposition is a popular technique for efficient prediction	On the other hand , for imposing hard constraints ,	dual decomposition is a popular technique for efficient prediction	56-81	56-81	On the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference .	On the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference .	1>2	none	enablement	enablement
P14-1056_anno1	66-74	82-85	dual decomposition is a popular technique for efficient prediction	We extend dual decomposition	dual decomposition is a popular technique for efficient prediction	We extend dual decomposition	56-81	82-93	On the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference .	We extend dual decomposition to perform prediction subject to soft constraints .	1>2	none	bg-general	bg-general
P14-1056_anno1	66-74	75-81	dual decomposition is a popular technique for efficient prediction	given existing algorithms for unconstrained inference .	dual decomposition is a popular technique for efficient prediction	given existing algorithms for unconstrained inference .	56-81	56-81	On the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference .	On the other hand , for imposing hard constraints , dual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference .	1<2	none	bg-general	bg-general
P14-1056_anno1	82-85	86-93	We extend dual decomposition	to perform prediction subject to soft constraints .	We extend dual decomposition	to perform prediction subject to soft constraints .	82-93	82-93	We extend dual decomposition to perform prediction subject to soft constraints .	We extend dual decomposition to perform prediction subject to soft constraints .	1<2	none	enablement	enablement
P14-1056_anno1	94-101	106-115	Moreover , with a technique for performing inference	it is easy to automatically generate large families of constraints	Moreover , with a technique for performing inference	it is easy to automatically generate large families of constraints	94-128	94-128	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	1>2	none	manner-means	manner-means
P14-1056_anno1	94-101	102-105	Moreover , with a technique for performing inference	given soft constraints ,	Moreover , with a technique for performing inference	given soft constraints ,	94-128	94-128	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	1<2	none	bg-general	bg-general
P14-1056_anno1	82-85	106-115	We extend dual decomposition	it is easy to automatically generate large families of constraints	We extend dual decomposition	it is easy to automatically generate large families of constraints	82-93	94-128	We extend dual decomposition to perform prediction subject to soft constraints .	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	1<2	none	elab-aspect	elab-aspect
P14-1056_anno1	106-115	116-125	it is easy to automatically generate large families of constraints	and learn their costs with a simple convex optimization problem	it is easy to automatically generate large families of constraints	and learn their costs with a simple convex optimization problem	94-128	94-128	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	1<2	none	joint	joint
P14-1056_anno1	116-125	126-128	and learn their costs with a simple convex optimization problem	during training .	and learn their costs with a simple convex optimization problem	during training .	94-128	94-128	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	Moreover , with a technique for performing inference given soft constraints , it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training .	1<2	none	temporal	temporal
P14-1056_anno1	82-85	129-146	We extend dual decomposition	This allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .	We extend dual decomposition	This allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .	82-93	129-146	We extend dual decomposition to perform prediction subject to soft constraints .	This allows us to obtain substantial gains in accuracy on a new , challenging citation extraction dataset .	1<2	none	evaluation	evaluation
P14-1057_anno1	1-15	78-87	An important search task in the biomedical domain is to find medical records of patients	In this paper , we focus on addressing the limitations	An important search task in the biomedical domain is to find medical records of patients	In this paper , we focus on addressing the limitations	1-22	78-108	An important search task in the biomedical domain is to find medical records of patients who are qualified for a clinical trial.	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	1>2	none	bg-goal	bg-goal
P14-1057_anno1	1-15	16-22	An important search task in the biomedical domain is to find medical records of patients	who are qualified for a clinical trial.	An important search task in the biomedical domain is to find medical records of patients	who are qualified for a clinical trial.	1-22	1-22	An important search task in the biomedical domain is to find medical records of patients who are qualified for a clinical trial.	An important search task in the biomedical domain is to find medical records of patients who are qualified for a clinical trial.	1<2	none	elab-addition	elab-addition
P14-1057_anno1	23-31	78-87	One commonly used approach is to apply NLP tools	In this paper , we focus on addressing the limitations	One commonly used approach is to apply NLP tools	In this paper , we focus on addressing the limitations	23-52	78-108	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	1>2	none	bg-compare	bg-compare
P14-1057_anno1	23-31	32-40	One commonly used approach is to apply NLP tools	to map terms from queries and documents to concepts	One commonly used approach is to apply NLP tools	to map terms from queries and documents to concepts	23-52	23-52	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	1<2	none	enablement	enablement
P14-1057_anno1	23-31	41-46	One commonly used approach is to apply NLP tools	and then compute the relevance scores	One commonly used approach is to apply NLP tools	and then compute the relevance scores	23-52	23-52	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	1<2	none	joint	joint
P14-1057_anno1	41-46	47-52	and then compute the relevance scores	based on the concept-based representation .	and then compute the relevance scores	based on the concept-based representation .	23-52	23-52	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	1<2	none	bg-general	bg-general
P14-1057_anno1	23-31	53-61	One commonly used approach is to apply NLP tools	However , the mapping results are not perfect ,	One commonly used approach is to apply NLP tools	However , the mapping results are not perfect ,	23-52	53-77	One commonly used approach is to apply NLP tools to map terms from queries and documents to concepts and then compute the relevance scores based on the concept-based representation .	However , the mapping results are not perfect , and none of previous work studied how to deal with them in the retrieval process .	1<2	none	contrast	contrast
P14-1057_anno1	53-61	62-77	However , the mapping results are not perfect ,	and none of previous work studied how to deal with them in the retrieval process .	However , the mapping results are not perfect ,	and none of previous work studied how to deal with them in the retrieval process .	53-77	53-77	However , the mapping results are not perfect , and none of previous work studied how to deal with them in the retrieval process .	However , the mapping results are not perfect , and none of previous work studied how to deal with them in the retrieval process .	1<2	none	joint	joint
P14-1057_anno1	78-87	88-93	In this paper , we focus on addressing the limitations	caused by the imperfect mapping results	In this paper , we focus on addressing the limitations	caused by the imperfect mapping results	78-108	78-108	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	1<2	none	elab-addition	elab-addition
P14-1057_anno1	78-87	94-108	In this paper , we focus on addressing the limitations	and study how to further improve the retrieval performance of the concept-based ranking methods .	In this paper , we focus on addressing the limitations	and study how to further improve the retrieval performance of the concept-based ranking methods .	78-108	78-108	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	1<2	none	joint	joint
P14-1057_anno1	94-108	109-115	and study how to further improve the retrieval performance of the concept-based ranking methods .	In particular , we apply axiomatic approaches	and study how to further improve the retrieval performance of the concept-based ranking methods .	In particular , we apply axiomatic approaches	78-108	109-133	In this paper , we focus on addressing the limitations caused by the imperfect mapping results and study how to further improve the retrieval performance of the concept-based ranking methods .	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	1<2	none	cause	cause
P14-1057_anno1	109-115	116-121	In particular , we apply axiomatic approaches	and propose two weighting regularization methods	In particular , we apply axiomatic approaches	and propose two weighting regularization methods	109-133	109-133	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	1<2	none	joint	joint
P14-1057_anno1	116-121	122-125	and propose two weighting regularization methods	that adjust the weighting	and propose two weighting regularization methods	that adjust the weighting	109-133	109-133	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	1<2	none	elab-addition	elab-addition
P14-1057_anno1	122-125	126-133	that adjust the weighting	based on the relations among the concepts .	that adjust the weighting	based on the relations among the concepts .	109-133	109-133	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	1<2	none	bg-general	bg-general
P14-1057_anno1	134-136	137-142	Experimental results show	that the proposed methods are effective	Experimental results show	that the proposed methods are effective	134-164	134-164	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	1>2	none	attribution	attribution
P14-1057_anno1	109-115	137-142	In particular , we apply axiomatic approaches	that the proposed methods are effective	In particular , we apply axiomatic approaches	that the proposed methods are effective	109-133	134-164	In particular , we apply axiomatic approaches and propose two weighting regularization methods that adjust the weighting based on the relations among the concepts .	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	1<2	none	evaluation	evaluation
P14-1057_anno1	137-142	143-148	that the proposed methods are effective	to improve the retrieval performance ,	that the proposed methods are effective	to improve the retrieval performance ,	134-164	134-164	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	1<2	none	enablement	enablement
P14-1057_anno1	137-142	149-164	that the proposed methods are effective	and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	that the proposed methods are effective	and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	134-164	134-164	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	Experimental results show that the proposed methods are effective to improve the retrieval performance , and their performances are comparable to other top-performing systems in the TREC Medical Records Track .	1<2	none	progression	progression
P14-1058_anno1	1-15	16,20-26	Although the distributional hypothesis has been applied successfully in many natural language processing tasks ,	systems <*> have been limited to a single domain	Although the distributional hypothesis has been applied successfully in many natural language processing tasks ,	systems <*> have been limited to a single domain	1-43	1-43	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	1>2	none	contrast	contrast
P14-1058_anno1	16,20-26	83-87	systems <*> have been limited to a single domain	We propose an unsupervised method	systems <*> have been limited to a single domain	We propose an unsupervised method	1-43	83-105	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	1>2	none	bg-compare	bg-compare
P14-1058_anno1	16,20-26	17-19	systems <*> have been limited to a single domain	using distributional information	systems <*> have been limited to a single domain	using distributional information	1-43	1-43	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	1<2	none	elab-addition	elab-addition
P14-1058_anno1	20-26	27-36	have been limited to a single domain	because the distribution of a word can vary between domains	have been limited to a single domain	because the distribution of a word can vary between domains	1-43	1-43	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	1<2	none	exp-reason	exp-reason
P14-1058_anno1	27-36	37-43	because the distribution of a word can vary between domains	as the word's predominant meaning changes .	because the distribution of a word can vary between domains	as the word's predominant meaning changes .	1-43	1-43	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	Although the distributional hypothesis has been applied successfully in many natural language processing tasks , systems using distributional information have been limited to a single domain because the distribution of a word can vary between domains as the word's predominant meaning changes .	1<2	none	temporal	temporal
P14-1058_anno1	44-64	65-69	However , if it were possible to predict how the distribution of a word changes from one domain to another ,	the predictions could be used	However , if it were possible to predict how the distribution of a word changes from one domain to another ,	the predictions could be used	44-82	44-82	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	1>2	none	condition	condition
P14-1058_anno1	65-69	83-87	the predictions could be used	We propose an unsupervised method	the predictions could be used	We propose an unsupervised method	44-82	83-105	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	1>2	none	bg-general	bg-general
P14-1058_anno1	65-69	70-73,78-82	the predictions could be used	to adapt a system <*> to work in another .	the predictions could be used	to adapt a system <*> to work in another .	44-82	44-82	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	1<2	none	enablement	enablement
P14-1058_anno1	70-73,78-82	74-77	to adapt a system <*> to work in another .	trained in one domain	to adapt a system <*> to work in another .	trained in one domain	44-82	44-82	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	However , if it were possible to predict how the distribution of a word changes from one domain to another , the predictions could be used to adapt a system trained in one domain to work in another .	1<2	none	elab-addition	elab-addition
P14-1058_anno1	83-87	88-98	We propose an unsupervised method	to predict the distribution of a word in one domain ,	We propose an unsupervised method	to predict the distribution of a word in one domain ,	83-105	83-105	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	1<2	none	elab-addition	elab-addition
P14-1058_anno1	88-98	99-105	to predict the distribution of a word in one domain ,	given its distribution in another domain .	to predict the distribution of a word in one domain ,	given its distribution in another domain .	83-105	83-105	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	1<2	none	bg-general	bg-general
P14-1058_anno1	83-87	106-113	We propose an unsupervised method	We evaluate our method on two tasks :	We propose an unsupervised method	We evaluate our method on two tasks :	83-105	106-121	We propose an unsupervised method to predict the distribution of a word in one domain , given its distribution in another domain .	We evaluate our method on two tasks : cross-domain part-of-speech tagging and cross-domain sentiment classification .	1<2	none	evaluation	evaluation
P14-1058_anno1	106-113	114-121	We evaluate our method on two tasks :	cross-domain part-of-speech tagging and cross-domain sentiment classification .	We evaluate our method on two tasks :	cross-domain part-of-speech tagging and cross-domain sentiment classification .	106-121	106-121	We evaluate our method on two tasks : cross-domain part-of-speech tagging and cross-domain sentiment classification .	We evaluate our method on two tasks : cross-domain part-of-speech tagging and cross-domain sentiment classification .	1<2	none	elab-enumember	elab-enumember
P14-1058_anno1	106-113	122-131	We evaluate our method on two tasks :	In both tasks , our method significantly outperforms competitive baselines	We evaluate our method on two tasks :	In both tasks , our method significantly outperforms competitive baselines	106-121	122-149	We evaluate our method on two tasks : cross-domain part-of-speech tagging and cross-domain sentiment classification .	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	1<2	none	cause	cause
P14-1058_anno1	122-131	132-134	In both tasks , our method significantly outperforms competitive baselines	and returns results	In both tasks , our method significantly outperforms competitive baselines	and returns results	122-149	122-149	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	1<2	none	joint	joint
P14-1058_anno1	132-134	135-143	and returns results	that are statistically comparable to current state-of-the-art methods ,	and returns results	that are statistically comparable to current state-of-the-art methods ,	122-149	122-149	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	1<2	none	elab-addition	elab-addition
P14-1058_anno1	135-143	144-149	that are statistically comparable to current state-of-the-art methods ,	while requiring no task-specific customisations .	that are statistically comparable to current state-of-the-art methods ,	while requiring no task-specific customisations .	122-149	122-149	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	In both tasks , our method significantly outperforms competitive baselines and returns results that are statistically comparable to current state-of-the-art methods , while requiring no task-specific customisations .	1<2	none	elab-addition	elab-addition
P14-1059_anno1	1-10	31-40	We introduce the problem of generation in distributional semantics :	We motivate this novel challenge on theoretical and practical grounds	We introduce the problem of generation in distributional semantics :	We motivate this novel challenge on theoretical and practical grounds	1-30	31-53	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions .	1>2	none	bg-goal	bg-goal
P14-1059_anno1	11-14	19-24	Given a distributional vector	how can we generate the phrase	Given a distributional vector	how can we generate the phrase	1-30	1-30	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	1>2	none	bg-general	bg-general
P14-1059_anno1	11-14	15-18	Given a distributional vector	representing some meaning ,	Given a distributional vector	representing some meaning ,	1-30	1-30	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	1<2	none	elab-addition	elab-addition
P14-1059_anno1	1-10	19-24	We introduce the problem of generation in distributional semantics :	how can we generate the phrase	We introduce the problem of generation in distributional semantics :	how can we generate the phrase	1-30	1-30	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	1<2	none	elab-addition	elab-addition
P14-1059_anno1	19-24	25-30	how can we generate the phrase	that best expresses that meaning ?	how can we generate the phrase	that best expresses that meaning ?	1-30	1-30	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	We introduce the problem of generation in distributional semantics : Given a distributional vector representing some meaning , how can we generate the phrase that best expresses that meaning ?	1<2	none	elab-addition	elab-addition
P14-1059_anno1	31-40	41-53	We motivate this novel challenge on theoretical and practical grounds	and propose a simple data-driven approach to the estimation of generation functions .	We motivate this novel challenge on theoretical and practical grounds	and propose a simple data-driven approach to the estimation of generation functions .	31-53	31-53	We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions .	We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions .	1<2	none	progression	progression
P14-1059_anno1	31-40	54-60,65-71	We motivate this novel challenge on theoretical and practical grounds	We test this in a monolingual scenario <*> as well as in a cross-lingual setting	We motivate this novel challenge on theoretical and practical grounds	We test this in a monolingual scenario <*> as well as in a cross-lingual setting	31-53	54-89	We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions .	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	1<2	none	evaluation	evaluation
P14-1059_anno1	54-60,65-71	61-64	We test this in a monolingual scenario <*> as well as in a cross-lingual setting	( paraphrase generation )	We test this in a monolingual scenario <*> as well as in a cross-lingual setting	( paraphrase generation )	54-89	54-89	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	1<2	none	elab-addition	elab-addition
P14-1059_anno1	65-71	72-73,81-89	as well as in a cross-lingual setting	( translation <*> and generating the equivalent expressions in Italian ) .	as well as in a cross-lingual setting	( translation <*> and generating the equivalent expressions in Italian ) .	54-89	54-89	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	1<2	none	elab-addition	elab-addition
P14-1059_anno1	72-73,81-89	74-80	( translation <*> and generating the equivalent expressions in Italian ) .	by synthesizing adjective-noun phrase vectors in English	( translation <*> and generating the equivalent expressions in Italian ) .	by synthesizing adjective-noun phrase vectors in English	54-89	54-89	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	We test this in a monolingual scenario ( paraphrase generation ) as well as in a cross-lingual setting ( translation by synthesizing adjective-noun phrase vectors in English and generating the equivalent expressions in Italian ) .	1<2	none	manner-means	manner-means
P14-1060_anno1	1-18	33-56	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	1-32	33-56	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items .	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	1>2	none	bg-compare	bg-compare
P14-1060_anno1	1-18	19-22	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	of modeling semantic composition	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	of modeling semantic composition	1-32	1-32	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items .	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items .	1<2	none	elab-addition	elab-addition
P14-1060_anno1	1-18	23-32	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	when dealing with structures larger than single lexical items .	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities	when dealing with structures larger than single lexical items .	1-32	1-32	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items .	Traditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items .	1<2	none	temporal	temporal
P14-1060_anno1	33-56	57-73	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents ,	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents ,	33-56	57-83	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design .	1<2	none	elab-addition	elab-addition
P14-1060_anno1	57-73	74-83	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents ,	and circumvents some problems of data sparsity by design .	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents ,	and circumvents some problems of data sparsity by design .	57-83	57-83	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design .	The framework subsumes issues such as differential compositional as well as non-compositional behavior of phrasal consituents , and circumvents some problems of data sparsity by design .	1<2	none	joint	joint
P14-1060_anno1	33-56	84-88	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	We design a segmentation model	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	We design a segmentation model	33-56	84-117	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	1<2	none	elab-addition	elab-addition
P14-1060_anno1	84-88	89-97	We design a segmentation model	to optimally partition a sentence into lineal constituents ,	We design a segmentation model	to optimally partition a sentence into lineal constituents ,	84-117	84-117	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	1<2	none	enablement	enablement
P14-1060_anno1	89-97	98-105	to optimally partition a sentence into lineal constituents ,	which can be used to define distributional contexts	to optimally partition a sentence into lineal constituents ,	which can be used to define distributional contexts	84-117	84-117	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	1<2	none	elab-addition	elab-addition
P14-1060_anno1	98-105	106-117	which can be used to define distributional contexts	that are less noisy , semantically more interpretable ,and linguistically disambiguated .	which can be used to define distributional contexts	that are less noisy , semantically more interpretable ,and linguistically disambiguated .	84-117	84-117	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	We design a segmentation model to optimally partition a sentence into lineal constituents , which can be used to define distributional contexts that are less noisy , semantically more interpretable ,and linguistically disambiguated .	1<2	none	elab-addition	elab-addition
P14-1060_anno1	33-56	118-120,125-131	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	Hellinger PCA embeddings <*> show competitive results on empirical tasks .	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	Hellinger PCA embeddings <*> show competitive results on empirical tasks .	33-56	118-131	In this work , we present a frequency-driven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents , or motifs .	Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks .	1<2	none	evaluation	evaluation
P14-1060_anno1	118-120,125-131	121-124	Hellinger PCA embeddings <*> show competitive results on empirical tasks .	learnt using the framework	Hellinger PCA embeddings <*> show competitive results on empirical tasks .	learnt using the framework	118-131	118-131	Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks .	Hellinger PCA embeddings learnt using the framework show competitive results on empirical tasks .	1<2	none	elab-addition	elab-addition
P14-1061_anno1	1-14	74-78	Representing predicates in terms of their argument distribution is common practice in NLP .	We propose a novel approach	Representing predicates in terms of their argument distribution is common practice in NLP .	We propose a novel approach	1-14	74-91	Representing predicates in terms of their argument distribution is common practice in NLP .	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	1>2	none	bg-goal	bg-goal
P14-1061_anno1	15-26	74-78	Multi-word predicates ( MWPs ) in this context are often either disregarded	We propose a novel approach	Multi-word predicates ( MWPs ) in this context are often either disregarded	We propose a novel approach	15-32	74-91	Multi-word predicates ( MWPs ) in this context are often either disregarded or considered as fixed expressions .	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	1>2	none	bg-general	bg-general
P14-1061_anno1	15-26	27-32	Multi-word predicates ( MWPs ) in this context are often either disregarded	or considered as fixed expressions .	Multi-word predicates ( MWPs ) in this context are often either disregarded	or considered as fixed expressions .	15-32	15-32	Multi-word predicates ( MWPs ) in this context are often either disregarded or considered as fixed expressions .	Multi-word predicates ( MWPs ) in this context are often either disregarded or considered as fixed expressions .	1<2	none	joint	joint
P14-1061_anno1	27-32	33-41	or considered as fixed expressions .	The latter treatment is unsatisfactory in two ways :	or considered as fixed expressions .	The latter treatment is unsatisfactory in two ways :	15-32	33-73	Multi-word predicates ( MWPs ) in this context are often either disregarded or considered as fixed expressions .	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	1<2	none	elab-addition	elab-addition
P14-1061_anno1	33-41	42-50	The latter treatment is unsatisfactory in two ways :	( 1 ) identifying MWPs is notoriously difficult ,	The latter treatment is unsatisfactory in two ways :	( 1 ) identifying MWPs is notoriously difficult ,	33-73	33-73	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	1<2	none	elab-aspect	elab-aspect
P14-1061_anno1	33-41	51-59	The latter treatment is unsatisfactory in two ways :	( 2 ) MWPs show varying degrees of compositionality	The latter treatment is unsatisfactory in two ways :	( 2 ) MWPs show varying degrees of compositionality	33-73	33-73	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	1<2	none	elab-aspect	elab-aspect
P14-1061_anno1	51-59	60-73	( 2 ) MWPs show varying degrees of compositionality	and could benefit from taking into account the identity of their component parts .	( 2 ) MWPs show varying degrees of compositionality	and could benefit from taking into account the identity of their component parts .	33-73	33-73	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	The latter treatment is unsatisfactory in two ways : ( 1 ) identifying MWPs is notoriously difficult , ( 2 ) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts .	1<2	none	joint	joint
P14-1061_anno1	74-78	79-91	We propose a novel approach	that integrates the distributional representation of multiple sub-sets of the MWP's words .	We propose a novel approach	that integrates the distributional representation of multiple sub-sets of the MWP's words .	74-91	74-91	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	1<2	none	elab-addition	elab-addition
P14-1061_anno1	74-78	92-102	We propose a novel approach	We assume a latent distribution over sub-sets of the MWP ,	We propose a novel approach	We assume a latent distribution over sub-sets of the MWP ,	74-91	92-112	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	We assume a latent distribution over sub-sets of the MWP , and estimate it relative to a downstream prediction task .	1<2	none	elab-aspect	elab-aspect
P14-1061_anno1	92-102	103-112	We assume a latent distribution over sub-sets of the MWP ,	and estimate it relative to a downstream prediction task .	We assume a latent distribution over sub-sets of the MWP ,	and estimate it relative to a downstream prediction task .	92-112	92-112	We assume a latent distribution over sub-sets of the MWP , and estimate it relative to a downstream prediction task .	We assume a latent distribution over sub-sets of the MWP , and estimate it relative to a downstream prediction task .	1<2	none	joint	joint
P14-1061_anno1	113-122	123-127	Focusing on the supervised identification of lexical inference relations ,	we compare against state-of-the-art baselines	Focusing on the supervised identification of lexical inference relations ,	we compare against state-of-the-art baselines	113-140	113-140	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	1>2	none	result	result
P14-1061_anno1	74-78	123-127	We propose a novel approach	we compare against state-of-the-art baselines	We propose a novel approach	we compare against state-of-the-art baselines	74-91	113-140	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	1<2	none	evaluation	evaluation
P14-1061_anno1	123-127	128-136	we compare against state-of-the-art baselines	that consider a single sub-set of an MWP ,	we compare against state-of-the-art baselines	that consider a single sub-set of an MWP ,	113-140	113-140	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	1<2	none	elab-addition	elab-addition
P14-1061_anno1	123-127	137-140	we compare against state-of-the-art baselines	obtaining substantial improvements .	we compare against state-of-the-art baselines	obtaining substantial improvements .	113-140	113-140	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	Focusing on the supervised identification of lexical inference relations , we compare against state-of-the-art baselines that consider a single sub-set of an MWP , obtaining substantial improvements .	1<2	none	cause	cause
P14-1061_anno1	74-78	141-149	We propose a novel approach	To our knowledge , this is the first work	We propose a novel approach	To our knowledge , this is the first work	74-91	141-164	We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP's words .	To our knowledge , this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics .	1<2	none	summary	summary
P14-1061_anno1	141-149	150-164	To our knowledge , this is the first work	to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics .	To our knowledge , this is the first work	to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics .	141-164	141-164	To our knowledge , this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics .	To our knowledge , this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics .	1<2	none	elab-addition	elab-addition
P14-1062_anno1	1-2,7-12	13-17	The ability <*> is central to language understanding .	We describe a convolutional architecture	The ability <*> is central to language understanding .	We describe a convolutional architecture	1-12	13-36	The ability to accurately represent sentences is central to language understanding .	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	1>2	none	bg-general	bg-general
P14-1062_anno1	1-2,7-12	3-6	The ability <*> is central to language understanding .	to accurately represent sentences	The ability <*> is central to language understanding .	to accurately represent sentences	1-12	1-12	The ability to accurately represent sentences is central to language understanding .	The ability to accurately represent sentences is central to language understanding .	1<2	none	elab-addition	elab-addition
P14-1062_anno1	13-17	18-26	We describe a convolutional architecture	dubbed the Dynamic Convolutional Neural Network ( DCNN )	We describe a convolutional architecture	dubbed the Dynamic Convolutional Neural Network ( DCNN )	13-36	13-36	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	1<2	none	elab-addition	elab-addition
P14-1062_anno1	18-26	27-36	dubbed the Dynamic Convolutional Neural Network ( DCNN )	that we adopt for the semantic modelling of sentences .	dubbed the Dynamic Convolutional Neural Network ( DCNN )	that we adopt for the semantic modelling of sentences .	13-36	13-36	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	1<2	none	elab-addition	elab-addition
P14-1062_anno1	13-17	37-51	We describe a convolutional architecture	The network uses Dynamic k-Max Pooling , a global pooling operation over linear sequences .	We describe a convolutional architecture	The network uses Dynamic k-Max Pooling , a global pooling operation over linear sequences .	13-36	37-51	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	The network uses Dynamic k-Max Pooling , a global pooling operation over linear sequences .	1<2	none	elab-aspect	elab-aspect
P14-1062_anno1	13-17	52-59	We describe a convolutional architecture	The network handles input sentences of varying length	We describe a convolutional architecture	The network handles input sentences of varying length	13-36	52-78	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations .	1<2	none	elab-aspect	elab-aspect
P14-1062_anno1	52-59	60-67	The network handles input sentences of varying length	and induces a feature graph over the sentence	The network handles input sentences of varying length	and induces a feature graph over the sentence	52-78	52-78	The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations .	The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations .	1<2	none	joint	joint
P14-1062_anno1	60-67	68-78	and induces a feature graph over the sentence	that is capable of explicitly capturing short and long-range relations .	and induces a feature graph over the sentence	that is capable of explicitly capturing short and long-range relations .	52-78	52-78	The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations .	The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations .	1<2	none	elab-addition	elab-addition
P14-1062_anno1	13-17	79-87	We describe a convolutional architecture	The network does not rely on a parse tree	We describe a convolutional architecture	The network does not rely on a parse tree	13-36	79-95	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	The network does not rely on a parse tree and is easily applicable to any language .	1<2	none	elab-aspect	elab-aspect
P14-1062_anno1	79-87	88-95	The network does not rely on a parse tree	and is easily applicable to any language .	The network does not rely on a parse tree	and is easily applicable to any language .	79-95	79-95	The network does not rely on a parse tree and is easily applicable to any language .	The network does not rely on a parse tree and is easily applicable to any language .	1<2	none	joint	joint
P14-1062_anno1	13-17	96-103	We describe a convolutional architecture	We test the DCNN in four experiments :	We describe a convolutional architecture	We test the DCNN in four experiments :	13-36	96-122	We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network ( DCNN ) that we adopt for the semantic modelling of sentences .	We test the DCNN in four experiments : small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	1<2	none	evaluation	evaluation
P14-1062_anno1	96-103	104-122	We test the DCNN in four experiments :	small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	We test the DCNN in four experiments :	small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	96-122	96-122	We test the DCNN in four experiments : small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	We test the DCNN in four experiments : small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	1<2	none	elab-enumember	elab-enumember
P14-1062_anno1	96-103	123-132	We test the DCNN in four experiments :	The network achieves excellent performance in the first three tasks	We test the DCNN in four experiments :	The network achieves excellent performance in the first three tasks	96-122	123-151	We test the DCNN in four experiments : small scale binary and multi-class sentiment prediction , six-way question classification and Twitter sentiment prediction by distant supervision .	The network achieves excellent performance in the first three tasks and a greater than 25 % error reduction in the last task with respect to the strongest baseline .	1<2	none	cause	cause
P14-1062_anno1	123-132	133-151	The network achieves excellent performance in the first three tasks	and a greater than 25 % error reduction in the last task with respect to the strongest baseline .	The network achieves excellent performance in the first three tasks	and a greater than 25 % error reduction in the last task with respect to the strongest baseline .	123-151	123-151	The network achieves excellent performance in the first three tasks and a greater than 25 % error reduction in the last task with respect to the strongest baseline .	The network achieves excellent performance in the first three tasks and a greater than 25 % error reduction in the last task with respect to the strongest baseline .	1<2	none	joint	joint
P14-1063_anno1	1-6	7-11	We propose an online learning algorithm	based on tensor-space models .	We propose an online learning algorithm	based on tensor-space models .	1-11	1-11	We propose an online learning algorithm based on tensor-space models .	We propose an online learning algorithm based on tensor-space models .	1<2	none	manner-means	manner-means
P14-1063_anno1	1-6	12-21	We propose an online learning algorithm	A tensor-space model represents data in a compact way ,	We propose an online learning algorithm	A tensor-space model represents data in a compact way ,	1-11	12-52	We propose an online learning algorithm based on tensor-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1<2	none	elab-aspect	elab-aspect
P14-1063_anno1	22-25	26-34	and via rank-1 approximation	the weight tensor can be made highly structured ,	and via rank-1 approximation	the weight tensor can be made highly structured ,	12-52	12-52	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1>2	none	manner-means	manner-means
P14-1063_anno1	12-21	26-34	A tensor-space model represents data in a compact way ,	the weight tensor can be made highly structured ,	A tensor-space model represents data in a compact way ,	the weight tensor can be made highly structured ,	12-52	12-52	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1<2	none	joint	joint
P14-1063_anno1	26-34	35-43	the weight tensor can be made highly structured ,	resulting in a significantly smaller number of free parameters	the weight tensor can be made highly structured ,	resulting in a significantly smaller number of free parameters	12-52	12-52	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1<2	none	cause	cause
P14-1063_anno1	35-43	44-46	resulting in a significantly smaller number of free parameters	to be estimated	resulting in a significantly smaller number of free parameters	to be estimated	12-52	12-52	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1<2	none	elab-addition	elab-addition
P14-1063_anno1	35-43	47-52	resulting in a significantly smaller number of free parameters	than in comparable vector-space models .	resulting in a significantly smaller number of free parameters	than in comparable vector-space models .	12-52	12-52	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	1<2	none	comparison	comparison
P14-1063_anno1	35-43	53-57	resulting in a significantly smaller number of free parameters	This regularizes the model complexity	resulting in a significantly smaller number of free parameters	This regularizes the model complexity	12-52	53-82	A tensor-space model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models .	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	1<2	none	cause	cause
P14-1063_anno1	53-57	58-66	This regularizes the model complexity	and makes the tensor model highly effective in situations	This regularizes the model complexity	and makes the tensor model highly effective in situations	53-82	53-82	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	1<2	none	joint	joint
P14-1063_anno1	58-66	67-73	and makes the tensor model highly effective in situations	where a large feature set is defined	and makes the tensor model highly effective in situations	where a large feature set is defined	53-82	53-82	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	1<2	none	elab-addition	elab-addition
P14-1063_anno1	67-73	74-82	where a large feature set is defined	but very limited resources are available for training .	where a large feature set is defined	but very limited resources are available for training .	53-82	53-82	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	This regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training .	1<2	none	contrast	contrast
P14-1063_anno1	1-6	83-93	We propose an online learning algorithm	We apply with the proposed algorithm to a parsing task ,	We propose an online learning algorithm	We apply with the proposed algorithm to a parsing task ,	1-11	83-128	We propose an online learning algorithm based on tensor-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1<2	none	evaluation	evaluation
P14-1063_anno1	94-95	96-105,111-113	and show	that even with very little training data the learning algorithm <*> performs well ,	and show	that even with very little training data the learning algorithm <*> performs well ,	83-128	83-128	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1>2	none	attribution	attribution
P14-1063_anno1	83-93	96-105,111-113	We apply with the proposed algorithm to a parsing task ,	that even with very little training data the learning algorithm <*> performs well ,	We apply with the proposed algorithm to a parsing task ,	that even with very little training data the learning algorithm <*> performs well ,	83-128	83-128	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1<2	none	cause	cause
P14-1063_anno1	96-105,111-113	106-110	that even with very little training data the learning algorithm <*> performs well ,	based on a tensor model	that even with very little training data the learning algorithm <*> performs well ,	based on a tensor model	83-128	83-128	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1<2	none	manner-means	manner-means
P14-1063_anno1	111-113	114-122	performs well ,	and gives significantly better results than standard learning algorithms	performs well ,	and gives significantly better results than standard learning algorithms	83-128	83-128	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1<2	none	joint	joint
P14-1063_anno1	114-122	123-128	and gives significantly better results than standard learning algorithms	based on traditional vector-space models .	and gives significantly better results than standard learning algorithms	based on traditional vector-space models .	83-128	83-128	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	We apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vector-space models .	1<2	none	manner-means	manner-means
P14-1064_anno1	1-10	27-36	Statistical phrase-based translation learns translation rules from bilingual corpora ,	In this work , we present a semi-supervised graph-based approach	Statistical phrase-based translation learns translation rules from bilingual corpora ,	In this work , we present a semi-supervised graph-based approach	1-26	27-48	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	1>2	none	bg-compare	bg-compare
P14-1064_anno1	1-10	11-17	Statistical phrase-based translation learns translation rules from bilingual corpora ,	and has traditionally only used monolingual evidence	Statistical phrase-based translation learns translation rules from bilingual corpora ,	and has traditionally only used monolingual evidence	1-26	1-26	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	1<2	none	joint	joint
P14-1064_anno1	11-17	18-20	and has traditionally only used monolingual evidence	to construct features	and has traditionally only used monolingual evidence	to construct features	1-26	1-26	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	1<2	none	enablement	enablement
P14-1064_anno1	18-20	21-26	to construct features	that rescore existing translation candidates .	to construct features	that rescore existing translation candidates .	1-26	1-26	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	Statistical phrase-based translation learns translation rules from bilingual corpora , and has traditionally only used monolingual evidence to construct features that rescore existing translation candidates .	1<2	none	elab-addition	elab-addition
P14-1064_anno1	27-36	37-41	In this work , we present a semi-supervised graph-based approach	for generating new translation rules	In this work , we present a semi-supervised graph-based approach	for generating new translation rules	27-48	27-48	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	1<2	none	enablement	enablement
P14-1064_anno1	37-41	42-48	for generating new translation rules	that leverages bilingual and monolingual data .	for generating new translation rules	that leverages bilingual and monolingual data .	27-48	27-48	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	1<2	none	elab-addition	elab-addition
P14-1064_anno1	27-36	49-55	In this work , we present a semi-supervised graph-based approach	The proposed technique first constructs phrase graphs	In this work , we present a semi-supervised graph-based approach	The proposed technique first constructs phrase graphs	27-48	49-64	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	The proposed technique first constructs phrase graphs using both source and target language monolingual corpora .	1<2	none	elab-process_step	elab-process_step
P14-1064_anno1	49-55	56-64	The proposed technique first constructs phrase graphs	using both source and target language monolingual corpora .	The proposed technique first constructs phrase graphs	using both source and target language monolingual corpora .	49-64	49-64	The proposed technique first constructs phrase graphs using both source and target language monolingual corpora .	The proposed technique first constructs phrase graphs using both source and target language monolingual corpora .	1<2	none	manner-means	manner-means
P14-1064_anno1	27-36	65-72	In this work , we present a semi-supervised graph-based approach	Next , graph propagation identifies translations of phrases	In this work , we present a semi-supervised graph-based approach	Next , graph propagation identifies translations of phrases	27-48	65-89	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	1<2	none	elab-process_step	elab-process_step
P14-1064_anno1	65-72	73-81	Next , graph propagation identifies translations of phrases	that were not observed in the bilingual corpus ,	Next , graph propagation identifies translations of phrases	that were not observed in the bilingual corpus ,	65-89	65-89	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	1<2	none	manner-means	manner-means
P14-1064_anno1	82	83-89	assuming	that similar phrases have similar translations .	assuming	that similar phrases have similar translations .	65-89	65-89	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	1>2	none	attribution	attribution
P14-1064_anno1	65-72	83-89	Next , graph propagation identifies translations of phrases	that similar phrases have similar translations .	Next , graph propagation identifies translations of phrases	that similar phrases have similar translations .	65-89	65-89	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	Next , graph propagation identifies translations of phrases that were not observed in the bilingual corpus , assuming that similar phrases have similar translations .	1<2	none	condition	condition
P14-1064_anno1	27-36	90-103	In this work , we present a semi-supervised graph-based approach	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	In this work , we present a semi-supervised graph-based approach	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	27-48	90-103	In this work , we present a semi-supervised graph-based approach for generating new translation rules that leverages bilingual and monolingual data .	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	1<2	none	evaluation	evaluation
P14-1064_anno1	90-103	104-115	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	Our proposed approach significantly improves the performance of competitive phrase-based systems ,	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	Our proposed approach significantly improves the performance of competitive phrase-based systems ,	90-103	104-130	We report results on a large Arabic-English system and a medium-sized Urdu-English system .	Our proposed approach significantly improves the performance of competitive phrase-based systems , leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets .	1<2	none	cause	cause
P14-1064_anno1	104-115	116-130	Our proposed approach significantly improves the performance of competitive phrase-based systems ,	leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets .	Our proposed approach significantly improves the performance of competitive phrase-based systems ,	leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets .	104-130	104-130	Our proposed approach significantly improves the performance of competitive phrase-based systems , leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets .	Our proposed approach significantly improves the performance of competitive phrase-based systems , leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets .	1<2	none	elab-addition	elab-addition
P14-1065_anno1	1-7	8-13	We present experiments in using discourse structure	for improving machine translation evaluation .	We present experiments in using discourse structure	for improving machine translation evaluation .	1-13	1-13	We present experiments in using discourse structure for improving machine translation evaluation .	We present experiments in using discourse structure for improving machine translation evaluation .	1<2	none	enablement	enablement
P14-1065_anno1	1-7	14-21	We present experiments in using discourse structure	We first design two discourse-aware similarity measures ,	We present experiments in using discourse structure	We first design two discourse-aware similarity measures ,	1-13	14-38	We present experiments in using discourse structure for improving machine translation evaluation .	We first design two discourse-aware similarity measures , which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	1<2	none	elab-aspect	elab-aspect
P14-1065_anno1	14-21	22-25	We first design two discourse-aware similarity measures ,	which use all-subtree kernels	We first design two discourse-aware similarity measures ,	which use all-subtree kernels	14-38	14-38	We first design two discourse-aware similarity measures , which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	We first design two discourse-aware similarity measures , which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	1<2	none	elab-addition	elab-addition
P14-1065_anno1	22-25	26-38	which use all-subtree kernels	to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	which use all-subtree kernels	to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	14-38	14-38	We first design two discourse-aware similarity measures , which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	We first design two discourse-aware similarity measures , which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory .	1<2	none	enablement	enablement
P14-1065_anno1	39-42	43-65	Then , we show	that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	Then , we show	that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	39-65	39-65	Then , we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	Then , we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	1>2	none	attribution	attribution
P14-1065_anno1	1-7	43-65	We present experiments in using discourse structure	that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	We present experiments in using discourse structure	that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	1-13	39-65	We present experiments in using discourse structure for improving machine translation evaluation .	Then , we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level .	1<2	none	evaluation	evaluation
P14-1065_anno1	66-73	76-86	Rather than proposing a single new metric ,	that discourse information is complementary to the state-of-the-art evaluation metrics ,	Rather than proposing a single new metric ,	that discourse information is complementary to the state-of-the-art evaluation metrics ,	66-102	66-102	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	1>2	none	contrast	contrast
P14-1065_anno1	74-75	76-86	we show	that discourse information is complementary to the state-of-the-art evaluation metrics ,	we show	that discourse information is complementary to the state-of-the-art evaluation metrics ,	66-102	66-102	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	1>2	none	attribution	attribution
P14-1065_anno1	1-7	76-86	We present experiments in using discourse structure	that discourse information is complementary to the state-of-the-art evaluation metrics ,	We present experiments in using discourse structure	that discourse information is complementary to the state-of-the-art evaluation metrics ,	1-13	66-102	We present experiments in using discourse structure for improving machine translation evaluation .	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	1<2	none	elab-aspect	elab-aspect
P14-1065_anno1	76-86	87-102	that discourse information is complementary to the state-of-the-art evaluation metrics ,	and thus should be taken into account in the development of future richer evaluation metrics .	that discourse information is complementary to the state-of-the-art evaluation metrics ,	and thus should be taken into account in the development of future richer evaluation metrics .	66-102	66-102	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	Rather than proposing a single new metric , we show that discourse information is complementary to the state-of-the-art evaluation metrics , and thus should be taken into account in the development of future richer evaluation metrics .	1<2	none	progression	progression
P14-1066_anno1	1-6	7-11	This paper tackles the sparsity problem	in estimating phrase translation probabilities	This paper tackles the sparsity problem	in estimating phrase translation probabilities	1-30	1-30	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	1<2	none	elab-addition	elab-addition
P14-1066_anno1	1-6	12-17	This paper tackles the sparsity problem	by learning continuous phrase representations ,	This paper tackles the sparsity problem	by learning continuous phrase representations ,	1-30	1-30	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	1<2	none	manner-means	manner-means
P14-1066_anno1	12-17	18-30	by learning continuous phrase representations ,	whose distributed nature enables the sharing of related phrases in their representations .	by learning continuous phrase representations ,	whose distributed nature enables the sharing of related phrases in their representations .	1-30	1-30	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	1<2	none	elab-addition	elab-addition
P14-1066_anno1	12-17	31-49	by learning continuous phrase representations ,	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	by learning continuous phrase representations ,	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	1-30	31-66	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space , where their translation score is computed by the distance between the pair in this new space .	1<2	none	elab-aspect	elab-aspect
P14-1066_anno1	31-49	50-66	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	where their translation score is computed by the distance between the pair in this new space .	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	where their translation score is computed by the distance between the pair in this new space .	31-66	31-66	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space , where their translation score is computed by the distance between the pair in this new space .	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space , where their translation score is computed by the distance between the pair in this new space .	1<2	none	elab-addition	elab-addition
P14-1066_anno1	31-49	67-74	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	The projection is performed by a neural network	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space ,	The projection is performed by a neural network	31-66	67-83	A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space , where their translation score is computed by the distance between the pair in this new space .	The projection is performed by a neural network whose weights are learned on parallel training data .	1<2	none	manner-means	manner-means
P14-1066_anno1	67-74	75-83	The projection is performed by a neural network	whose weights are learned on parallel training data .	The projection is performed by a neural network	whose weights are learned on parallel training data .	67-83	67-83	The projection is performed by a neural network whose weights are learned on parallel training data .	The projection is performed by a neural network whose weights are learned on parallel training data .	1<2	none	elab-addition	elab-addition
P14-1066_anno1	12-17	84-94	by learning continuous phrase representations ,	Experimental evaluation has been performed on two WMT translation tasks .	by learning continuous phrase representations ,	Experimental evaluation has been performed on two WMT translation tasks .	1-30	84-94	This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .	Experimental evaluation has been performed on two WMT translation tasks .	1<2	none	evaluation	evaluation
P14-1066_anno1	84-94	95-108	Experimental evaluation has been performed on two WMT translation tasks .	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system	Experimental evaluation has been performed on two WMT translation tasks .	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system	84-94	95-121	Experimental evaluation has been performed on two WMT translation tasks .	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points .	1<2	none	cause	cause
P14-1066_anno1	95-108	109-121	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system	trained on WMT 2012 French-English data by up to 1.3 BLEU points .	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system	trained on WMT 2012 French-English data by up to 1.3 BLEU points .	95-121	95-121	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points .	Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	1-15	107-114	The automatic estimation of machine translation ( MT ) output quality is a hard task	we propose an online framework for adaptive QE	The automatic estimation of machine translation ( MT ) output quality is a hard task	we propose an online framework for adaptive QE	1-38	103-125	The automatic estimation of machine translation ( MT ) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	1>2	none	bg-goal	bg-goal
P14-1067_anno1	1-15	16-38	The automatic estimation of machine translation ( MT ) output quality is a hard task	in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	The automatic estimation of machine translation ( MT ) output quality is a hard task	in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	1-38	1-38	The automatic estimation of machine translation ( MT ) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	The automatic estimation of machine translation ( MT ) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	39-47	48-53	When moving from controlled lab evaluations to real-life scenarios	the task becomes even harder .	When moving from controlled lab evaluations to real-life scenarios	the task becomes even harder .	39-53	39-53	When moving from controlled lab evaluations to real-life scenarios the task becomes even harder .	When moving from controlled lab evaluations to real-life scenarios the task becomes even harder .	1>2	none	temporal	temporal
P14-1067_anno1	1-15	48-53	The automatic estimation of machine translation ( MT ) output quality is a hard task	the task becomes even harder .	The automatic estimation of machine translation ( MT ) output quality is a hard task	the task becomes even harder .	1-38	39-53	The automatic estimation of machine translation ( MT ) output quality is a hard task in which the selection of the appropriate algorithm and the most predictive features over reasonably sized training sets plays a crucial role .	When moving from controlled lab evaluations to real-life scenarios the task becomes even harder .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	54-69	107-114	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	we propose an online framework for adaptive QE	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	we propose an online framework for adaptive QE	54-76	103-125	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty to model user and domain changes .	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	1>2	none	bg-compare	bg-compare
P14-1067_anno1	54-69	70-76	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	to model user and domain changes .	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	to model user and domain changes .	54-76	54-76	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty to model user and domain changes .	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty to model user and domain changes .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	54-69	77-87,92-95	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	Indeed , the instability of the systems with respect to data <*> calls for adaptive solutions	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty	Indeed , the instability of the systems with respect to data <*> calls for adaptive solutions	54-76	77-102	For current MT quality estimation ( QE ) systems , additional complexity comes from the difficulty to model user and domain changes .	Indeed , the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions .	1<2	none	cause	cause
P14-1067_anno1	77-87,92-95	88-91	Indeed , the instability of the systems with respect to data <*> calls for adaptive solutions	coming from different distributions	Indeed , the instability of the systems with respect to data <*> calls for adaptive solutions	coming from different distributions	77-102	77-102	Indeed , the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions .	Indeed , the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	92-95	96-102	calls for adaptive solutions	that react to new operating conditions .	calls for adaptive solutions	that react to new operating conditions .	77-102	77-102	Indeed , the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions .	Indeed , the instability of the systems with respect to data coming from different distributions calls for adaptive solutions that react to new operating conditions .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	103-106	107-114	To tackle this issue	we propose an online framework for adaptive QE	To tackle this issue	we propose an online framework for adaptive QE	103-125	103-125	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	1>2	none	enablement	enablement
P14-1067_anno1	107-114	115-125	we propose an online framework for adaptive QE	that targets reactivity and robustness to user and domain changes .	we propose an online framework for adaptive QE	that targets reactivity and robustness to user and domain changes .	103-125	103-125	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	1<2	none	elab-addition	elab-addition
P14-1067_anno1	107-114	126-131,137-143	we propose an online framework for adaptive QE	Contrastive experiments in different testing conditions <*> demonstrate the effectiveness of our approach .	we propose an online framework for adaptive QE	Contrastive experiments in different testing conditions <*> demonstrate the effectiveness of our approach .	103-125	126-143	To tackle this issue we propose an online framework for adaptive QE that targets reactivity and robustness to user and domain changes .	Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach .	1<2	none	evaluation	evaluation
P14-1067_anno1	126-131,137-143	132-136	Contrastive experiments in different testing conditions <*> demonstrate the effectiveness of our approach .	involving user and domain changes	Contrastive experiments in different testing conditions <*> demonstrate the effectiveness of our approach .	involving user and domain changes	126-143	126-143	Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach .	Contrastive experiments in different testing conditions involving user and domain changes demonstrate the effectiveness of our approach .	1<2	none	elab-example	elab-example
P14-1068_anno1	1-15	16-20	In this paper we address the problem of grounding distributional representations of lexical meaning .	We introduce a new model	In this paper we address the problem of grounding distributional representations of lexical meaning .	We introduce a new model	1-15	16-34	In this paper we address the problem of grounding distributional representations of lexical meaning .	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	1>2	none	bg-goal	bg-goal
P14-1068_anno1	16-20	21-24	We introduce a new model	which uses stacked autoencoders	We introduce a new model	which uses stacked autoencoders	16-34	16-34	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	1<2	none	elab-addition	elab-addition
P14-1068_anno1	21-24	25-34	which uses stacked autoencoders	to learn higher-level embeddings from textual and visual input .	which uses stacked autoencoders	to learn higher-level embeddings from textual and visual input .	16-34	16-34	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	1<2	none	enablement	enablement
P14-1068_anno1	16-20	35-43	We introduce a new model	The two modalities are encoded as vectors of attributes	We introduce a new model	The two modalities are encoded as vectors of attributes	16-34	35-54	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	The two modalities are encoded as vectors of attributes and are obtained automatically from text and images , respectively .	1<2	none	elab-aspect	elab-aspect
P14-1068_anno1	35-43	44-54	The two modalities are encoded as vectors of attributes	and are obtained automatically from text and images , respectively .	The two modalities are encoded as vectors of attributes	and are obtained automatically from text and images , respectively .	35-54	35-54	The two modalities are encoded as vectors of attributes and are obtained automatically from text and images , respectively .	The two modalities are encoded as vectors of attributes and are obtained automatically from text and images , respectively .	1<2	none	joint	joint
P14-1068_anno1	16-20	55-61	We introduce a new model	We evaluate our model on its ability	We introduce a new model	We evaluate our model on its ability	16-34	55-69	We introduce a new model which uses stacked autoencoders to learn higher-level embeddings from textual and visual input .	We evaluate our model on its ability to simulate similarity judgments and concept categorization .	1<2	none	evaluation	evaluation
P14-1068_anno1	55-61	62-69	We evaluate our model on its ability	to simulate similarity judgments and concept categorization .	We evaluate our model on its ability	to simulate similarity judgments and concept categorization .	55-69	55-69	We evaluate our model on its ability to simulate similarity judgments and concept categorization .	We evaluate our model on its ability to simulate similarity judgments and concept categorization .	1<2	none	elab-addition	elab-addition
P14-1068_anno1	55-61	70-81	We evaluate our model on its ability	On both tasks , our approach outperforms baselines and related models .	We evaluate our model on its ability	On both tasks , our approach outperforms baselines and related models .	55-69	70-81	We evaluate our model on its ability to simulate similarity judgments and concept categorization .	On both tasks , our approach outperforms baselines and related models .	1<2	none	cause	cause
P14-1069_anno1	1-4	5-14	We propose three improvements	to address the drawbacks of state-of-the-art transition-based constituent parsers .	We propose three improvements	to address the drawbacks of state-of-the-art transition-based constituent parsers .	1-14	1-14	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	1<2	none	enablement	enablement
P14-1069_anno1	15-28	29-38	First , to resolve the error propagation problem of the traditional pipeline approach ,	we incorporate POS tagging into the syntactic parsing process .	First , to resolve the error propagation problem of the traditional pipeline approach ,	we incorporate POS tagging into the syntactic parsing process .	15-38	15-38	First , to resolve the error propagation problem of the traditional pipeline approach , we incorporate POS tagging into the syntactic parsing process .	First , to resolve the error propagation problem of the traditional pipeline approach , we incorporate POS tagging into the syntactic parsing process .	1>2	none	enablement	enablement
P14-1069_anno1	1-4	29-38	We propose three improvements	we incorporate POS tagging into the syntactic parsing process .	We propose three improvements	we incorporate POS tagging into the syntactic parsing process .	1-14	15-38	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	First , to resolve the error propagation problem of the traditional pipeline approach , we incorporate POS tagging into the syntactic parsing process .	1<2	none	elab-aspect	elab-aspect
P14-1069_anno1	39-53	54-61	Second , to alleviate the negative influence of size differences among competing action sequences ,	we align parser states during beam-search decoding .	Second , to alleviate the negative influence of size differences among competing action sequences ,	we align parser states during beam-search decoding .	39-61	39-61	Second , to alleviate the negative influence of size differences among competing action sequences , we align parser states during beam-search decoding .	Second , to alleviate the negative influence of size differences among competing action sequences , we align parser states during beam-search decoding .	1>2	none	enablement	enablement
P14-1069_anno1	1-4	54-61	We propose three improvements	we align parser states during beam-search decoding .	We propose three improvements	we align parser states during beam-search decoding .	1-14	39-61	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	Second , to alleviate the negative influence of size differences among competing action sequences , we align parser states during beam-search decoding .	1<2	none	elab-aspect	elab-aspect
P14-1069_anno1	62-71	72-85	Third , to enhance the power of parsing models ,	we enlarge the feature set with non-local features and semi-supervised word cluster features .	Third , to enhance the power of parsing models ,	we enlarge the feature set with non-local features and semi-supervised word cluster features .	62-85	62-85	Third , to enhance the power of parsing models , we enlarge the feature set with non-local features and semi-supervised word cluster features .	Third , to enhance the power of parsing models , we enlarge the feature set with non-local features and semi-supervised word cluster features .	1>2	none	enablement	enablement
P14-1069_anno1	1-4	72-85	We propose three improvements	we enlarge the feature set with non-local features and semi-supervised word cluster features .	We propose three improvements	we enlarge the feature set with non-local features and semi-supervised word cluster features .	1-14	62-85	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	Third , to enhance the power of parsing models , we enlarge the feature set with non-local features and semi-supervised word cluster features .	1<2	none	elab-aspect	elab-aspect
P14-1069_anno1	86-88	89-96	Experimental results show	that these modifications improve parsing performance significantly .	Experimental results show	that these modifications improve parsing performance significantly .	86-96	86-96	Experimental results show that these modifications improve parsing performance significantly .	Experimental results show that these modifications improve parsing performance significantly .	1>2	none	attribution	attribution
P14-1069_anno1	1-4	89-96	We propose three improvements	that these modifications improve parsing performance significantly .	We propose three improvements	that these modifications improve parsing performance significantly .	1-14	86-96	We propose three improvements to address the drawbacks of state-of-the-art transition-based constituent parsers .	Experimental results show that these modifications improve parsing performance significantly .	1<2	none	evaluation	evaluation
P14-1069_anno1	97-105	106-114	Evaluated on the Chinese TreeBank ( CTB ) ,	our final performance reaches 86.3 % ( F1 )	Evaluated on the Chinese TreeBank ( CTB ) ,	our final performance reaches 86.3 % ( F1 )	97-137	97-137	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1>2	none	bg-general	bg-general
P14-1069_anno1	89-96	106-114	that these modifications improve parsing performance significantly .	our final performance reaches 86.3 % ( F1 )	that these modifications improve parsing performance significantly .	our final performance reaches 86.3 % ( F1 )	86-96	97-137	Experimental results show that these modifications improve parsing performance significantly .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1<2	none	exp-evidence	exp-evidence
P14-1069_anno1	106-114	115-120	our final performance reaches 86.3 % ( F1 )	when trained on CTB 5.1 ,	our final performance reaches 86.3 % ( F1 )	when trained on CTB 5.1 ,	97-137	97-137	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1<2	none	temporal	temporal
P14-1069_anno1	106-114	121-123	our final performance reaches 86.3 % ( F1 )	and 87.1 %	our final performance reaches 86.3 % ( F1 )	and 87.1 %	97-137	97-137	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1<2	none	joint	joint
P14-1069_anno1	121-123	124-129	and 87.1 %	when trained on CTB 6.0 ,	and 87.1 %	when trained on CTB 6.0 ,	97-137	97-137	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1<2	none	temporal	temporal
P14-1069_anno1	106-114	130-137	our final performance reaches 86.3 % ( F1 )	and these results outperform all state-of-the-art parsers .	our final performance reaches 86.3 % ( F1 )	and these results outperform all state-of-the-art parsers .	97-137	97-137	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	Evaluated on the Chinese TreeBank ( CTB ) , our final performance reaches 86.3 % ( F1 ) when trained on CTB 5.1 , and 87.1 % when trained on CTB 6.0 , and these results outperform all state-of-the-art parsers .	1<2	none	summary	summary
P14-1070_anno1	1-8	9-23	In this paper , we investigate various strategies	to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition ,	In this paper , we investigate various strategies	to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition ,	1-52	1-52	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	1<2	none	elab-addition	elab-addition
P14-1070_anno1	1-8	24-37	In this paper , we investigate various strategies	testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) ,	In this paper , we investigate various strategies	testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) ,	1-52	1-52	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	1<2	none	elab-addition	elab-addition
P14-1070_anno1	24-37	38-52	testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) ,	as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) ,	as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	1-52	1-52	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	1<2	none	comparison	comparison
P14-1070_anno1	1-8	53-65	In this paper , we investigate various strategies	Our work focuses on using an alternative representation of syntactically regular MWEs ,	In this paper , we investigate various strategies	Our work focuses on using an alternative representation of syntactically regular MWEs ,	1-52	53-72	In this paper , we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression ( MWE ) recognition , testing them on the dependency version of French Tree-bank (Abeillé and Barrier, 2004) , as instantiated in the SPMRL Shared Task ( Seddah et al. , 2013 ) .	Our work focuses on using an alternative representation of syntactically regular MWEs , which captures their syntactic internal structure .	1<2	none	elab-aspect	elab-aspect
P14-1070_anno1	53-65	66-72	Our work focuses on using an alternative representation of syntactically regular MWEs ,	which captures their syntactic internal structure .	Our work focuses on using an alternative representation of syntactically regular MWEs ,	which captures their syntactic internal structure .	53-72	53-72	Our work focuses on using an alternative representation of syntactically regular MWEs , which captures their syntactic internal structure .	Our work focuses on using an alternative representation of syntactically regular MWEs , which captures their syntactic internal structure .	1<2	none	elab-addition	elab-addition
P14-1070_anno1	53-65	73-88	Our work focuses on using an alternative representation of syntactically regular MWEs ,	We obtain a system with comparable performance to that of previous works on this dataset ,	Our work focuses on using an alternative representation of syntactically regular MWEs ,	We obtain a system with comparable performance to that of previous works on this dataset ,	53-72	73-101	Our work focuses on using an alternative representation of syntactically regular MWEs , which captures their syntactic internal structure .	We obtain a system with comparable performance to that of previous works on this dataset , but which predicts both syntactic dependencies and the internal structure of MWEs .	1<2	none	evaluation	evaluation
P14-1070_anno1	73-88	89-101	We obtain a system with comparable performance to that of previous works on this dataset ,	but which predicts both syntactic dependencies and the internal structure of MWEs .	We obtain a system with comparable performance to that of previous works on this dataset ,	but which predicts both syntactic dependencies and the internal structure of MWEs .	73-101	73-101	We obtain a system with comparable performance to that of previous works on this dataset , but which predicts both syntactic dependencies and the internal structure of MWEs .	We obtain a system with comparable performance to that of previous works on this dataset , but which predicts both syntactic dependencies and the internal structure of MWEs .	1<2	none	elab-addition	elab-addition
P14-1070_anno1	73-88	102-116	We obtain a system with comparable performance to that of previous works on this dataset ,	This can be useful for capturing the various degrees of semantic compositionality of MWEs .	We obtain a system with comparable performance to that of previous works on this dataset ,	This can be useful for capturing the various degrees of semantic compositionality of MWEs .	73-101	102-116	We obtain a system with comparable performance to that of previous works on this dataset , but which predicts both syntactic dependencies and the internal structure of MWEs .	This can be useful for capturing the various degrees of semantic compositionality of MWEs .	1<2	none	cause	cause
P14-1071_anno1	1-6	7-10	This paper presents a novel framework	called error case frames	This paper presents a novel framework	called error case frames	1-15	1-15	This paper presents a novel framework called error case frames for correcting preposition errors .	This paper presents a novel framework called error case frames for correcting preposition errors .	1<2	none	elab-addition	elab-addition
P14-1071_anno1	1-6	11-15	This paper presents a novel framework	for correcting preposition errors .	This paper presents a novel framework	for correcting preposition errors .	1-15	1-15	This paper presents a novel framework called error case frames for correcting preposition errors .	This paper presents a novel framework called error case frames for correcting preposition errors .	1<2	none	enablement	enablement
P14-1071_anno1	1-6	16-19	This paper presents a novel framework	They are case frames	This paper presents a novel framework	They are case frames	1-15	16-28	This paper presents a novel framework called error case frames for correcting preposition errors .	They are case frames specially designed for describing and correcting preposition errors .	1<2	none	elab-aspect	elab-aspect
P14-1071_anno1	16-19	20-28	They are case frames	specially designed for describing and correcting preposition errors .	They are case frames	specially designed for describing and correcting preposition errors .	16-28	16-28	They are case frames specially designed for describing and correcting preposition errors .	They are case frames specially designed for describing and correcting preposition errors .	1<2	none	elab-addition	elab-addition
P14-1071_anno1	1-6	29-41	This paper presents a novel framework	Their most distinct advantage is that they can correct errors with feedback messages	This paper presents a novel framework	Their most distinct advantage is that they can correct errors with feedback messages	1-15	29-48	This paper presents a novel framework called error case frames for correcting preposition errors .	Their most distinct advantage is that they can correct errors with feedback messages explaining why the preposition is erroneous .	1<2	none	elab-aspect	elab-aspect
P14-1071_anno1	29-41	42-48	Their most distinct advantage is that they can correct errors with feedback messages	explaining why the preposition is erroneous .	Their most distinct advantage is that they can correct errors with feedback messages	explaining why the preposition is erroneous .	29-48	29-48	Their most distinct advantage is that they can correct errors with feedback messages explaining why the preposition is erroneous .	Their most distinct advantage is that they can correct errors with feedback messages explaining why the preposition is erroneous .	1<2	none	elab-addition	elab-addition
P14-1071_anno1	1-6	49-53	This paper presents a novel framework	This paper proposes a method	This paper presents a novel framework	This paper proposes a method	1-15	49-64	This paper presents a novel framework called error case frames for correcting preposition errors .	This paper proposes a method for automatically generating them by comparing learner and native corpora .	1<2	none	elab-aspect	elab-aspect
P14-1071_anno1	49-53	54-57	This paper proposes a method	for automatically generating them	This paper proposes a method	for automatically generating them	49-64	49-64	This paper proposes a method for automatically generating them by comparing learner and native corpora .	This paper proposes a method for automatically generating them by comparing learner and native corpora .	1<2	none	elab-addition	elab-addition
P14-1071_anno1	54-57	58-64	for automatically generating them	by comparing learner and native corpora .	for automatically generating them	by comparing learner and native corpora .	49-64	49-64	This paper proposes a method for automatically generating them by comparing learner and native corpora .	This paper proposes a method for automatically generating them by comparing learner and native corpora .	1<2	none	manner-means	manner-means
P14-1071_anno1	65-66	67-82	Experiments show	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	Experiments show	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	65-115	65-115	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	1>2	none	attribution	attribution
P14-1071_anno1	1-6	67-82	This paper presents a novel framework	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	This paper presents a novel framework	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	1-15	65-115	This paper presents a novel framework called error case frames for correcting preposition errors .	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	1<2	none	evaluation	evaluation
P14-1071_anno1	67-82	83-98	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ;	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ;	65-115	65-115	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	1<2	none	joint	joint
P14-1071_anno1	83-98	99-115	( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ;	( iii ) feedback messages provided by error case frames are effective in language learning assistance .	( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ;	( iii ) feedback messages provided by error case frames are effective in language learning assistance .	65-115	65-115	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	1<2	none	joint	joint
P14-1071_anno1	116-121	136-151	Considering these advantages and the fact	error case frames will likely be one of the major approaches for preposition error correction .	Considering these advantages and the fact	error case frames will likely be one of the major approaches for preposition error correction .	116-151	116-151	Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules , error case frames will likely be one of the major approaches for preposition error correction .	Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules , error case frames will likely be one of the major approaches for preposition error correction .	1>2	none	bg-general	bg-general
P14-1071_anno1	116-121	122-135	Considering these advantages and the fact	that it has been difficult to provide feedback messages by automatically generated rules ,	Considering these advantages and the fact	that it has been difficult to provide feedback messages by automatically generated rules ,	116-151	116-151	Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules , error case frames will likely be one of the major approaches for preposition error correction .	Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules , error case frames will likely be one of the major approaches for preposition error correction .	1<2	none	elab-addition	elab-addition
P14-1071_anno1	67-82	136-151	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	error case frames will likely be one of the major approaches for preposition error correction .	( i ) automatically generated error case frames achieve a performance comparable to conventional methods ;	error case frames will likely be one of the major approaches for preposition error correction .	65-115	116-151	Experiments show ( i ) automatically generated error case frames achieve a performance comparable to conventional methods ; ( ii ) error case frames are intuitively interpretable and manually modifiable to improve them ; ( iii ) feedback messages provided by error case frames are effective in language learning assistance .	Considering these advantages and the fact that it has been difficult to provide feedback messages by automatically generated rules , error case frames will likely be one of the major approaches for preposition error correction .	1<2	none	cause	cause
P14-1072_anno1	1-8	9-26	Widely used in speech and language processing ,	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	Widely used in speech and language processing ,	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	1-26	1-26	Widely used in speech and language processing , Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	Widely used in speech and language processing , Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	1>2	none	elab-addition	elab-addition
P14-1072_anno1	9-26	44-54	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	In this paper , we propose a generalization of KN smoothing	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	In this paper , we propose a generalization of KN smoothing	1-26	44-70	Widely used in speech and language processing , Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	In this paper , we propose a generalization of KN smoothing that operates on fractional counts , or , more precisely , on distributions over counts .	1>2	none	bg-compare	bg-compare
P14-1072_anno1	9-26	27-34	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	However , KN smoothing assumes integer counts ,	Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	However , KN smoothing assumes integer counts ,	1-26	27-43	Widely used in speech and language processing , Kneser-Ney ( KN ) smoothing has consistently been shown to be one of the best-performing smoothing methods .	However , KN smoothing assumes integer counts , limiting its potential uses—for example , inside Expectation-Maximization .	1<2	none	contrast	contrast
P14-1072_anno1	27-34	35-43	However , KN smoothing assumes integer counts ,	limiting its potential uses—for example , inside Expectation-Maximization .	However , KN smoothing assumes integer counts ,	limiting its potential uses—for example , inside Expectation-Maximization .	27-43	27-43	However , KN smoothing assumes integer counts , limiting its potential uses—for example , inside Expectation-Maximization .	However , KN smoothing assumes integer counts , limiting its potential uses—for example , inside Expectation-Maximization .	1<2	none	cause	cause
P14-1072_anno1	44-54	55-70	In this paper , we propose a generalization of KN smoothing	that operates on fractional counts , or , more precisely , on distributions over counts .	In this paper , we propose a generalization of KN smoothing	that operates on fractional counts , or , more precisely , on distributions over counts .	44-70	44-70	In this paper , we propose a generalization of KN smoothing that operates on fractional counts , or , more precisely , on distributions over counts .	In this paper , we propose a generalization of KN smoothing that operates on fractional counts , or , more precisely , on distributions over counts .	1<2	none	elab-addition	elab-addition
P14-1072_anno1	44-54	71-88	In this paper , we propose a generalization of KN smoothing	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts ,	In this paper , we propose a generalization of KN smoothing	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts ,	44-70	71-115	In this paper , we propose a generalization of KN smoothing that operates on fractional counts , or , more precisely , on distributions over counts .	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	1<2	none	elab-aspect	elab-aspect
P14-1072_anno1	44-54	89-94	In this paper , we propose a generalization of KN smoothing	and apply it to two tasks	In this paper , we propose a generalization of KN smoothing	and apply it to two tasks	44-70	71-115	In this paper , we propose a generalization of KN smoothing that operates on fractional counts , or , more precisely , on distributions over counts .	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	1<2	none	evaluation	evaluation
P14-1072_anno1	89-94	95-102	and apply it to two tasks	where KN smoothing was not applicable before :	and apply it to two tasks	where KN smoothing was not applicable before :	71-115	71-115	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	1<2	none	elab-addition	elab-addition
P14-1072_anno1	95-102	103-115	where KN smoothing was not applicable before :	one in language model adaptation , and the other in word alignment .	where KN smoothing was not applicable before :	one in language model adaptation , and the other in word alignment .	71-115	71-115	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	1<2	none	elab-enumember	elab-enumember
P14-1072_anno1	89-94	116-125	and apply it to two tasks	In both cases , our method improves performance significantly .	and apply it to two tasks	In both cases , our method improves performance significantly .	71-115	116-125	We rederive all the steps of KN smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where KN smoothing was not applicable before : one in language model adaptation , and the other in word alignment .	In both cases , our method improves performance significantly .	1<2	none	cause	cause
P14-1073_anno1	1-14	36-47	Entity clustering must determine when two named-entity mentions refer to the same entity .	In this paper , we propose a model for cross-document coreference resolution	Entity clustering must determine when two named-entity mentions refer to the same entity .	In this paper , we propose a model for cross-document coreference resolution	1-14	36-57	Entity clustering must determine when two named-entity mentions refer to the same entity .	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	1>2	none	bg-goal	bg-goal
P14-1073_anno1	15-20	36-47	Typical approaches use a pipeline architecture	In this paper , we propose a model for cross-document coreference resolution	Typical approaches use a pipeline architecture	In this paper , we propose a model for cross-document coreference resolution	15-35	36-57	Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity .	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	1>2	none	bg-compare	bg-compare
P14-1073_anno1	15-20	21-24	Typical approaches use a pipeline architecture	that clusters the mentions	Typical approaches use a pipeline architecture	that clusters the mentions	15-35	15-35	Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity .	Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity .	1<2	none	elab-addition	elab-addition
P14-1073_anno1	21-24	25-35	that clusters the mentions	using fixed or learned measures of name and context similarity .	that clusters the mentions	using fixed or learned measures of name and context similarity .	15-35	15-35	Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity .	Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity .	1<2	none	manner-means	manner-means
P14-1073_anno1	36-47	48-50	In this paper , we propose a model for cross-document coreference resolution	that achieves robustness	In this paper , we propose a model for cross-document coreference resolution	that achieves robustness	36-57	36-57	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	1<2	none	elab-addition	elab-addition
P14-1073_anno1	48-50	51-57	that achieves robustness	by learning similarity from unlabeled data .	that achieves robustness	by learning similarity from unlabeled data .	36-57	36-57	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	1<2	none	manner-means	manner-means
P14-1073_anno1	58-61	62-79	The generative process assumes	that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	The generative process assumes	that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	58-79	58-79	The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	1>2	none	attribution	attribution
P14-1073_anno1	36-47	62-79	In this paper , we propose a model for cross-document coreference resolution	that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	In this paper , we propose a model for cross-document coreference resolution	that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	36-57	58-79	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context .	1<2	none	elab-aspect	elab-aspect
P14-1073_anno1	36-47	80-91	In this paper , we propose a model for cross-document coreference resolution	Clustering the mentions into entities depends on recovering this copying tree jointly	In this paper , we propose a model for cross-document coreference resolution	Clustering the mentions into entities depends on recovering this copying tree jointly	36-57	80-103	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process .	1<2	none	elab-aspect	elab-aspect
P14-1073_anno1	80-91	92-103	Clustering the mentions into entities depends on recovering this copying tree jointly	with estimating models of the mutation process and parent selection process .	Clustering the mentions into entities depends on recovering this copying tree jointly	with estimating models of the mutation process and parent selection process .	80-103	80-103	Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process .	Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process .	1<2	none	joint	joint
P14-1073_anno1	36-47	104-120	In this paper , we propose a model for cross-document coreference resolution	We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets .	In this paper , we propose a model for cross-document coreference resolution	We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets .	36-57	104-120	In this paper , we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data .	We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets .	1<2	none	elab-aspect	elab-aspect
P14-1074_anno1	1-7	8-22	We introduce three linguistically motivated structured regularizers	based on parse trees , topics , and hierarchical word clusters for text categorization .	We introduce three linguistically motivated structured regularizers	based on parse trees , topics , and hierarchical word clusters for text categorization .	1-22	1-22	We introduce three linguistically motivated structured regularizers based on parse trees , topics , and hierarchical word clusters for text categorization .	We introduce three linguistically motivated structured regularizers based on parse trees , topics , and hierarchical word clusters for text categorization .	1<2	none	elab-addition	elab-addition
P14-1074_anno1	1-7	23-31	We introduce three linguistically motivated structured regularizers	These regularizers impose linguistic bias in feature weights ,	We introduce three linguistically motivated structured regularizers	These regularizers impose linguistic bias in feature weights ,	1-22	23-42	We introduce three linguistically motivated structured regularizers based on parse trees , topics , and hierarchical word clusters for text categorization .	These regularizers impose linguistic bias in feature weights , enabling us to incorporate prior knowledge into conventional bag-of-words models .	1<2	none	elab-aspect	elab-aspect
P14-1074_anno1	23-31	32-42	These regularizers impose linguistic bias in feature weights ,	enabling us to incorporate prior knowledge into conventional bag-of-words models .	These regularizers impose linguistic bias in feature weights ,	enabling us to incorporate prior knowledge into conventional bag-of-words models .	23-42	23-42	These regularizers impose linguistic bias in feature weights , enabling us to incorporate prior knowledge into conventional bag-of-words models .	These regularizers impose linguistic bias in feature weights , enabling us to incorporate prior knowledge into conventional bag-of-words models .	1<2	none	cause	cause
P14-1074_anno1	43-44	45-52	We show	that our structured regularizers consistently improve classification accuracies	We show	that our structured regularizers consistently improve classification accuracies	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1>2	none	attribution	attribution
P14-1074_anno1	1-7	45-52	We introduce three linguistically motivated structured regularizers	that our structured regularizers consistently improve classification accuracies	We introduce three linguistically motivated structured regularizers	that our structured regularizers consistently improve classification accuracies	1-22	43-93	We introduce three linguistically motivated structured regularizers based on parse trees , topics , and hierarchical word clusters for text categorization .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	evaluation	evaluation
P14-1074_anno1	45-52	53-56	that our structured regularizers consistently improve classification accuracies	compared to standard regularizers	that our structured regularizers consistently improve classification accuracies	compared to standard regularizers	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	comparison	comparison
P14-1074_anno1	53-56	57-61	compared to standard regularizers	that penalize features in isolation	compared to standard regularizers	that penalize features in isolation	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	elab-addition	elab-addition
P14-1074_anno1	53-56	62-73	compared to standard regularizers	( such as lasso , ridge , and elastic net regularizers )	compared to standard regularizers	( such as lasso , ridge , and elastic net regularizers )	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	elab-example	elab-example
P14-1074_anno1	45-52	74-84	that our structured regularizers consistently improve classification accuracies	on a range of datasets for various text prediction problems :	that our structured regularizers consistently improve classification accuracies	on a range of datasets for various text prediction problems :	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	elab-addition	elab-addition
P14-1074_anno1	74-84	85-93	on a range of datasets for various text prediction problems :	topic classification , sentiment analysis , and forecasting .	on a range of datasets for various text prediction problems :	topic classification , sentiment analysis , and forecasting .	43-93	43-93	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	We show that our structured regularizers consistently improve classification accuracies compared to standard regularizers that penalize features in isolation ( such as lasso , ridge , and elastic net regularizers ) on a range of datasets for various text prediction problems : topic classification , sentiment analysis , and forecasting .	1<2	none	elab-enumember	elab-enumember
P14-1075_anno1	1-13	14-18	This paper studies the idea of removing low-frequency words from a corpus ,	which is a common practice	This paper studies the idea of removing low-frequency words from a corpus ,	which is a common practice	1-28	1-28	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	1<2	none	elab-addition	elab-addition
P14-1075_anno1	14-18	19-28	which is a common practice	to reduce computational costs , from a theoretical standpoint .	which is a common practice	to reduce computational costs , from a theoretical standpoint .	1-28	1-28	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	1<2	none	enablement	enablement
P14-1075_anno1	29-32	33-39	Based on the assumption	that a corpus follows Zipf's law ,	Based on the assumption	that a corpus follows Zipf's law ,	29-62	29-62	Based on the assumption that a corpus follows Zipf's law , we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	Based on the assumption that a corpus follows Zipf's law , we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	1>2	none	attribution	attribution
P14-1075_anno1	33-39	40-62	that a corpus follows Zipf's law ,	we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	that a corpus follows Zipf's law ,	we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	29-62	29-62	Based on the assumption that a corpus follows Zipf's law , we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	Based on the assumption that a corpus follows Zipf's law , we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	1>2	none	bg-general	bg-general
P14-1075_anno1	1-13	40-62	This paper studies the idea of removing low-frequency words from a corpus ,	we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	This paper studies the idea of removing low-frequency words from a corpus ,	we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	1-28	29-62	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	Based on the assumption that a corpus follows Zipf's law , we derive trade-off formulae of the perplexity of k-gram models and topic models with respect to the size of the reduced vocabulary .	1<2	none	elab-aspect	elab-aspect
P14-1075_anno1	1-13	63-77	This paper studies the idea of removing low-frequency words from a corpus ,	In addition , we show an approximate behavior of each formula under certain conditions .	This paper studies the idea of removing low-frequency words from a corpus ,	In addition , we show an approximate behavior of each formula under certain conditions .	1-28	63-77	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	In addition , we show an approximate behavior of each formula under certain conditions .	1<2	none	elab-aspect	elab-aspect
P14-1075_anno1	1-13	78-87	This paper studies the idea of removing low-frequency words from a corpus ,	We verify the correctness of our theory on synthetic corpora	This paper studies the idea of removing low-frequency words from a corpus ,	We verify the correctness of our theory on synthetic corpora	1-28	78-99	This paper studies the idea of removing low-frequency words from a corpus , which is a common practice to reduce computational costs , from a theoretical standpoint .	We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora .	1<2	none	evaluation	evaluation
P14-1075_anno1	78-87	88-99	We verify the correctness of our theory on synthetic corpora	and examine the gap between theory and practice on real corpora .	We verify the correctness of our theory on synthetic corpora	and examine the gap between theory and practice on real corpora .	78-99	78-99	We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora .	We verify the correctness of our theory on synthetic corpora and examine the gap between theory and practice on real corpora .	1<2	none	joint	joint
P14-1076_anno1	1-5	6-19	We propose a two-phase framework	to adapt existing relation extraction classifiers to extract relations for new target domains .	We propose a two-phase framework	to adapt existing relation extraction classifiers to extract relations for new target domains .	1-19	1-19	We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains .	We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains .	1<2	none	enablement	enablement
P14-1076_anno1	1-5	20-24	We propose a two-phase framework	We address two challenges :	We propose a two-phase framework	We address two challenges :	1-19	20-67	We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	bg-goal	bg-goal
P14-1076_anno1	20-24	25-26	We address two challenges :	negative transfer	We address two challenges :	negative transfer	20-67	20-67	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	elab-enumember	elab-enumember
P14-1076_anno1	25-26	27-33	negative transfer	when knowledge in source domains is used	negative transfer	when knowledge in source domains is used	20-67	20-67	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	temporal	temporal
P14-1076_anno1	27-33	34-41	when knowledge in source domains is used	without considering the differences in relation distributions ;	when knowledge in source domains is used	without considering the differences in relation distributions ;	20-67	20-67	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	25-26	42-55	negative transfer	and lack of adequate labeled samples for rarer relations in the new domain ,	negative transfer	and lack of adequate labeled samples for rarer relations in the new domain ,	20-67	20-67	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	joint	joint
P14-1076_anno1	42-55	56-67	and lack of adequate labeled samples for rarer relations in the new domain ,	due to a small labeled data set and imbalance relation distributions .	and lack of adequate labeled samples for rarer relations in the new domain ,	due to a small labeled data set and imbalance relation distributions .	20-67	20-67	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	We address two challenges : negative transfer when knowledge in source domains is used without considering the differences in relation distributions ; and lack of adequate labeled samples for rarer relations in the new domain , due to a small labeled data set and imbalance relation distributions .	1<2	none	result	result
P14-1076_anno1	1-5	68-81	We propose a two-phase framework	Our framework leverages on both labeled and unlabeled data in the target domain .	We propose a two-phase framework	Our framework leverages on both labeled and unlabeled data in the target domain .	1-19	68-81	We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains .	Our framework leverages on both labeled and unlabeled data in the target domain .	1<2	none	elab-aspect	elab-aspect
P14-1076_anno1	68-81	82-100	Our framework leverages on both labeled and unlabeled data in the target domain .	First , we determine the relevance of each source domain to the target domain for each relation type ,	Our framework leverages on both labeled and unlabeled data in the target domain .	First , we determine the relevance of each source domain to the target domain for each relation type ,	68-81	82-125	Our framework leverages on both labeled and unlabeled data in the target domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	elab-process_step	elab-process_step
P14-1076_anno1	82-100	101-106	First , we determine the relevance of each source domain to the target domain for each relation type ,	using the consistency between the clustering	First , we determine the relevance of each source domain to the target domain for each relation type ,	using the consistency between the clustering	82-125	82-125	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	manner-means	manner-means
P14-1076_anno1	101-106	107-112	using the consistency between the clustering	given by the target domain labels	using the consistency between the clustering	given by the target domain labels	82-125	82-125	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	101-106	113-115	using the consistency between the clustering	and the clustering	using the consistency between the clustering	and the clustering	82-125	82-125	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	joint	joint
P14-1076_anno1	113-115	116-119	and the clustering	given by the predictors	and the clustering	given by the predictors	82-125	82-125	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	116-119	120-125	given by the predictors	trained for the source domain .	given by the predictors	trained for the source domain .	82-125	82-125	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	126-136	137-151	To overcome the lack of labeled samples for rarer relations ,	these clusterings operate on both the labeled and unlabeled data in the target domain .	To overcome the lack of labeled samples for rarer relations ,	these clusterings operate on both the labeled and unlabeled data in the target domain .	126-151	126-151	To overcome the lack of labeled samples for rarer relations , these clusterings operate on both the labeled and unlabeled data in the target domain .	To overcome the lack of labeled samples for rarer relations , these clusterings operate on both the labeled and unlabeled data in the target domain .	1>2	none	enablement	enablement
P14-1076_anno1	101-106	137-151	using the consistency between the clustering	these clusterings operate on both the labeled and unlabeled data in the target domain .	using the consistency between the clustering	these clusterings operate on both the labeled and unlabeled data in the target domain .	82-125	126-151	First , we determine the relevance of each source domain to the target domain for each relation type , using the consistency between the clustering given by the target domain labels and the clustering given by the predictors trained for the source domain .	To overcome the lack of labeled samples for rarer relations , these clusterings operate on both the labeled and unlabeled data in the target domain .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	68-81	152-166	Our framework leverages on both labeled and unlabeled data in the target domain .	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	Our framework leverages on both labeled and unlabeled data in the target domain .	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	68-81	152-166	Our framework leverages on both labeled and unlabeled data in the target domain .	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	1<2	none	elab-process_step	elab-process_step
P14-1076_anno1	167-174	175-184	Again , to overcome the imbalance distribution ,	the source-domain predictors operate on the unlabeled target data .	Again , to overcome the imbalance distribution ,	the source-domain predictors operate on the unlabeled target data .	167-184	167-184	Again , to overcome the imbalance distribution , the source-domain predictors operate on the unlabeled target data .	Again , to overcome the imbalance distribution , the source-domain predictors operate on the unlabeled target data .	1>2	none	enablement	enablement
P14-1076_anno1	152-166	175-184	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	the source-domain predictors operate on the unlabeled target data .	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	the source-domain predictors operate on the unlabeled target data .	152-166	167-184	Second , we trade-off between using relevance-weighted source-domain predictors and the labeled target data .	Again , to overcome the imbalance distribution , the source-domain predictors operate on the unlabeled target data .	1<2	none	elab-addition	elab-addition
P14-1076_anno1	1-5	185-201	We propose a two-phase framework	Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO .	We propose a two-phase framework	Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO .	1-19	185-201	We propose a two-phase framework to adapt existing relation extraction classifiers to extract relations for new target domains .	Our method outperforms numerous baselines and a weakly-supervised relation extraction method on ACE 2004 and YAGO .	1<2	none	evaluation	evaluation
P14-1077_anno1	1-15	39-48	Most existing relation extraction models make predictions for each entity pair locally and individually ,	In this paper , we propose a joint inference framework	Most existing relation extraction models make predictions for each entity pair locally and individually ,	In this paper , we propose a joint inference framework	1-38	39-60	Most existing relation extraction models make predictions for each entity pair locally and individually , while ignoring implicit global clues available in the knowledge base , sometimes leading to conflicts among local predictions from different entity pairs .	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	1>2	none	bg-compare	bg-compare
P14-1077_anno1	1-15	16-26	Most existing relation extraction models make predictions for each entity pair locally and individually ,	while ignoring implicit global clues available in the knowledge base ,	Most existing relation extraction models make predictions for each entity pair locally and individually ,	while ignoring implicit global clues available in the knowledge base ,	1-38	1-38	Most existing relation extraction models make predictions for each entity pair locally and individually , while ignoring implicit global clues available in the knowledge base , sometimes leading to conflicts among local predictions from different entity pairs .	Most existing relation extraction models make predictions for each entity pair locally and individually , while ignoring implicit global clues available in the knowledge base , sometimes leading to conflicts among local predictions from different entity pairs .	1<2	none	contrast	contrast
P14-1077_anno1	16-26	27-38	while ignoring implicit global clues available in the knowledge base ,	sometimes leading to conflicts among local predictions from different entity pairs .	while ignoring implicit global clues available in the knowledge base ,	sometimes leading to conflicts among local predictions from different entity pairs .	1-38	1-38	Most existing relation extraction models make predictions for each entity pair locally and individually , while ignoring implicit global clues available in the knowledge base , sometimes leading to conflicts among local predictions from different entity pairs .	Most existing relation extraction models make predictions for each entity pair locally and individually , while ignoring implicit global clues available in the knowledge base , sometimes leading to conflicts among local predictions from different entity pairs .	1<2	none	cause	cause
P14-1077_anno1	39-48	49-60	In this paper , we propose a joint inference framework	that utilizes these global clues to resolve disagreements among local predictions .	In this paper , we propose a joint inference framework	that utilizes these global clues to resolve disagreements among local predictions .	39-60	39-60	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	1<2	none	elab-addition	elab-addition
P14-1077_anno1	39-48	61-66	In this paper , we propose a joint inference framework	We exploit two kinds of clues	In this paper , we propose a joint inference framework	We exploit two kinds of clues	39-60	61-82	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation .	1<2	none	elab-aspect	elab-aspect
P14-1077_anno1	61-66	67-69	We exploit two kinds of clues	to generate constraints	We exploit two kinds of clues	to generate constraints	61-82	61-82	We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation .	We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation .	1<2	none	enablement	enablement
P14-1077_anno1	67-69	70-82	to generate constraints	which can capture the implicit type and cardinality requirements of a relation .	to generate constraints	which can capture the implicit type and cardinality requirements of a relation .	61-82	61-82	We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation .	We exploit two kinds of clues to generate constraints which can capture the implicit type and cardinality requirements of a relation .	1<2	none	elab-addition	elab-addition
P14-1077_anno1	83-95	96-104	Experimental results on three datasets , in both English and Chinese , show	that our framework outperforms the state-of-the-art relation extraction models	Experimental results on three datasets , in both English and Chinese , show	that our framework outperforms the state-of-the-art relation extraction models	83-113	83-113	Experimental results on three datasets , in both English and Chinese , show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets .	Experimental results on three datasets , in both English and Chinese , show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets .	1>2	none	attribution	attribution
P14-1077_anno1	39-48	96-104	In this paper , we propose a joint inference framework	that our framework outperforms the state-of-the-art relation extraction models	In this paper , we propose a joint inference framework	that our framework outperforms the state-of-the-art relation extraction models	39-60	83-113	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	Experimental results on three datasets , in both English and Chinese , show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets .	1<2	none	evaluation	evaluation
P14-1077_anno1	96-104	105-113	that our framework outperforms the state-of-the-art relation extraction models	when such clues are applicable to the datasets .	that our framework outperforms the state-of-the-art relation extraction models	when such clues are applicable to the datasets .	83-113	83-113	Experimental results on three datasets , in both English and Chinese , show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets .	Experimental results on three datasets , in both English and Chinese , show that our framework outperforms the state-of-the-art relation extraction models when such clues are applicable to the datasets .	1<2	none	temporal	temporal
P14-1077_anno1	114-117	118-120,127-130	And , we find	that the clues <*> perform comparably to those	And , we find	that the clues <*> perform comparably to those	114-134	114-134	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	1>2	none	attribution	attribution
P14-1077_anno1	39-48	118-120,127-130	In this paper , we propose a joint inference framework	that the clues <*> perform comparably to those	In this paper , we propose a joint inference framework	that the clues <*> perform comparably to those	39-60	114-134	In this paper , we propose a joint inference framework that utilizes these global clues to resolve disagreements among local predictions .	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	1<2	none	evaluation	evaluation
P14-1077_anno1	118-120,127-130	121-126	that the clues <*> perform comparably to those	learnt automatically from existing knowledge bases	that the clues <*> perform comparably to those	learnt automatically from existing knowledge bases	114-134	114-134	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	1<2	none	elab-addition	elab-addition
P14-1077_anno1	127-130	131-134	perform comparably to those	refined by human .	perform comparably to those	refined by human .	114-134	114-134	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	And , we find that the clues learnt automatically from existing knowledge bases perform comparably to those refined by human .	1<2	none	elab-addition	elab-addition
P14-1078_anno1	1-14	15-22	In this paper , we present a manifold model for medical relation extraction .	Our model is built upon a medical corpus	In this paper , we present a manifold model for medical relation extraction .	Our model is built upon a medical corpus	1-14	15-48	In this paper , we present a manifold model for medical relation extraction .	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	1<2	none	elab-aspect	elab-aspect
P14-1078_anno1	15-22	23-25	Our model is built upon a medical corpus	containing 80M sentences	Our model is built upon a medical corpus	containing 80M sentences	15-48	15-48	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	1<2	none	elab-addition	elab-addition
P14-1078_anno1	23-25	26-30	containing 80M sentences	( 11 gigabyte text )	containing 80M sentences	( 11 gigabyte text )	15-48	15-48	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	1<2	none	elab-addition	elab-addition
P14-1078_anno1	15-22	31-41	Our model is built upon a medical corpus	and designed to accurately and efciently detect the key medical relations	Our model is built upon a medical corpus	and designed to accurately and efciently detect the key medical relations	15-48	15-48	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	1<2	none	joint	joint
P14-1078_anno1	31-41	42-48	and designed to accurately and efciently detect the key medical relations	that can facilitate clinical decision making .	and designed to accurately and efciently detect the key medical relations	that can facilitate clinical decision making .	15-48	15-48	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	Our model is built upon a medical corpus containing 80M sentences ( 11 gigabyte text ) and designed to accurately and efciently detect the key medical relations that can facilitate clinical decision making .	1<2	none	elab-addition	elab-addition
P14-1078_anno1	1-14	49-58	In this paper , we present a manifold model for medical relation extraction .	Our approach integrates domain specic parsing and typing systems ,	In this paper , we present a manifold model for medical relation extraction .	Our approach integrates domain specic parsing and typing systems ,	1-14	49-68	In this paper , we present a manifold model for medical relation extraction .	Our approach integrates domain specic parsing and typing systems , and can utilize labeled as well as unlabeled examples .	1<2	none	elab-aspect	elab-aspect
P14-1078_anno1	49-58	59-68	Our approach integrates domain specic parsing and typing systems ,	and can utilize labeled as well as unlabeled examples .	Our approach integrates domain specic parsing and typing systems ,	and can utilize labeled as well as unlabeled examples .	49-68	49-68	Our approach integrates domain specic parsing and typing systems , and can utilize labeled as well as unlabeled examples .	Our approach integrates domain specic parsing and typing systems , and can utilize labeled as well as unlabeled examples .	1<2	none	joint	joint
P14-1078_anno1	69-75	76-83	To provide users with more exibility ,	we also take label weight into consideration .	To provide users with more exibility ,	we also take label weight into consideration .	69-83	69-83	To provide users with more exibility , we also take label weight into consideration .	To provide users with more exibility , we also take label weight into consideration .	1>2	none	enablement	enablement
P14-1078_anno1	1-14	76-83	In this paper , we present a manifold model for medical relation extraction .	we also take label weight into consideration .	In this paper , we present a manifold model for medical relation extraction .	we also take label weight into consideration .	1-14	69-83	In this paper , we present a manifold model for medical relation extraction .	To provide users with more exibility , we also take label weight into consideration .	1<2	none	elab-aspect	elab-aspect
P14-1078_anno1	1-14	84-94	In this paper , we present a manifold model for medical relation extraction .	Effectiveness of our model is demonstrated both theoretically with a proof	In this paper , we present a manifold model for medical relation extraction .	Effectiveness of our model is demonstrated both theoretically with a proof	1-14	84-111	In this paper , we present a manifold model for medical relation extraction .	Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments .	1<2	none	evaluation	evaluation
P14-1078_anno1	95-96	97-111	to show	that the solution is a closed-form solution and experimentally with positive results in experiments .	to show	that the solution is a closed-form solution and experimentally with positive results in experiments .	84-111	84-111	Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments .	Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments .	1>2	none	attribution	attribution
P14-1078_anno1	84-94	97-111	Effectiveness of our model is demonstrated both theoretically with a proof	that the solution is a closed-form solution and experimentally with positive results in experiments .	Effectiveness of our model is demonstrated both theoretically with a proof	that the solution is a closed-form solution and experimentally with positive results in experiments .	84-111	84-111	Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments .	Effectiveness of our model is demonstrated both theoretically with a proof to show that the solution is a closed-form solution and experimentally with positive results in experiments .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	1-22	31-36	The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features .	we propose solving the classification problem	The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features .	we propose solving the classification problem	1-22	23-46	The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features .	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	1>2	none	bg-goal	bg-goal
P14-1079_anno1	23-30	31-36	To tackle the sparsity and noise challenges ,	we propose solving the classification problem	To tackle the sparsity and noise challenges ,	we propose solving the classification problem	23-46	23-46	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	1>2	none	enablement	enablement
P14-1079_anno1	31-36	37-46	we propose solving the classification problem	using matrix completion on factorized matrix of minimized rank .	we propose solving the classification problem	using matrix completion on factorized matrix of minimized rank .	23-46	23-46	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	1<2	none	manner-means	manner-means
P14-1079_anno1	31-36	47-58,63-66	we propose solving the classification problem	We formulate relation classification as completing the unknown labels of testing items <*> in a sparse matrix	we propose solving the classification problem	We formulate relation classification as completing the unknown labels of testing items <*> in a sparse matrix	23-46	47-77	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	We formulate relation classification as completing the unknown labels of testing items ( entity pairs ) in a sparse matrix that concatenates training and testing textual features with training labels .	1<2	none	elab-aspect	elab-aspect
P14-1079_anno1	47-58,63-66	59-62	We formulate relation classification as completing the unknown labels of testing items <*> in a sparse matrix	( entity pairs )	We formulate relation classification as completing the unknown labels of testing items <*> in a sparse matrix	( entity pairs )	47-77	47-77	We formulate relation classification as completing the unknown labels of testing items ( entity pairs ) in a sparse matrix that concatenates training and testing textual features with training labels .	We formulate relation classification as completing the unknown labels of testing items ( entity pairs ) in a sparse matrix that concatenates training and testing textual features with training labels .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	63-66	67-77	in a sparse matrix	that concatenates training and testing textual features with training labels .	in a sparse matrix	that concatenates training and testing textual features with training labels .	47-77	47-77	We formulate relation classification as completing the unknown labels of testing items ( entity pairs ) in a sparse matrix that concatenates training and testing textual features with training labels .	We formulate relation classification as completing the unknown labels of testing items ( entity pairs ) in a sparse matrix that concatenates training and testing textual features with training labels .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	78-85	98-102	Our algorithmic framework is based on the assumption	We apply two optimization models	Our algorithmic framework is based on the assumption	We apply two optimization models	78-97	98-115	Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low .	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	1>2	none	result	result
P14-1079_anno1	78-85	86-97	Our algorithmic framework is based on the assumption	that the rank of item-by-feature and item-by-label joint matrix is low .	Our algorithmic framework is based on the assumption	that the rank of item-by-feature and item-by-label joint matrix is low .	78-97	78-97	Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low .	Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	31-36	98-102	we propose solving the classification problem	We apply two optimization models	we propose solving the classification problem	We apply two optimization models	23-46	98-115	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	1<2	none	elab-aspect	elab-aspect
P14-1079_anno1	98-102	103-108	We apply two optimization models	to recover the underlying low-rank matrix	We apply two optimization models	to recover the underlying low-rank matrix	98-115	98-115	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	1<2	none	enablement	enablement
P14-1079_anno1	98-102	109-115	We apply two optimization models	leveraging the sparsity of feature-label matrix .	We apply two optimization models	leveraging the sparsity of feature-label matrix .	98-115	98-115	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	31-36	116-132	we propose solving the classification problem	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm ,	we propose solving the classification problem	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm ,	23-46	116-139	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm , which can find the global optimum .	1<2	none	elab-aspect	elab-aspect
P14-1079_anno1	116-132	133-139	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm ,	which can find the global optimum .	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm ,	which can find the global optimum .	116-139	116-139	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm , which can find the global optimum .	The matrix completion problem is then solved by the fixed point continuation ( FPC ) algorithm , which can find the global optimum .	1<2	none	elab-addition	elab-addition
P14-1079_anno1	140-152	153-167	Experiments on two widely used datasets with different dimensions of textual features demonstrate	that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	Experiments on two widely used datasets with different dimensions of textual features demonstrate	that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	140-167	140-167	Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	1>2	none	attribution	attribution
P14-1079_anno1	31-36	153-167	we propose solving the classification problem	that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	we propose solving the classification problem	that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	23-46	140-167	To tackle the sparsity and noise challenges , we propose solving the classification problem using matrix completion on factorized matrix of minimized rank .	Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods .	1<2	none	evaluation	evaluation
P14-1080_anno1	1-4	27-50	Transitional expressions provide glue	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	Transitional expressions provide glue	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	1-26	27-50	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	1>2	none	contrast	contrast
P14-1080_anno1	1-4	5-11	Transitional expressions provide glue	that holds ideas together in a text	Transitional expressions provide glue	that holds ideas together in a text	1-26	1-26	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	1<2	none	elab-addition	elab-addition
P14-1080_anno1	1-4	12-17	Transitional expressions provide glue	and enhance the logical organization ,	Transitional expressions provide glue	and enhance the logical organization ,	1-26	1-26	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	1<2	none	joint	joint
P14-1080_anno1	1-4	18-26	Transitional expressions provide glue	which together help improve readability of a text .	Transitional expressions provide glue	which together help improve readability of a text .	1-26	1-26	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	Transitional expressions provide glue that holds ideas together in a text and enhance the logical organization , which together help improve readability of a text .	1<2	none	elab-addition	elab-addition
P14-1080_anno1	27-50	70-74	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	we propose two novel models	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	we propose two novel models	27-50	65-91	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	1>2	none	bg-goal	bg-goal
P14-1080_anno1	27-50	51-64	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	As a result , the translations are often hard to read and understand .	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	As a result , the translations are often hard to read and understand .	27-50	51-64	However , in most current statistical machine translation ( SMT ) systems , the outputs of compound-complex sentences still lack proper transitional expressions .	As a result , the translations are often hard to read and understand .	1<2	none	cause	cause
P14-1080_anno1	65-69	70-74	To address this issue ,	we propose two novel models	To address this issue ,	we propose two novel models	65-91	65-91	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	1>2	none	enablement	enablement
P14-1080_anno1	70-74	75-80	we propose two novel models	to encourage generating such transitional expressions	we propose two novel models	to encourage generating such transitional expressions	65-91	65-91	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	1<2	none	enablement	enablement
P14-1080_anno1	75-80	81-91	to encourage generating such transitional expressions	by introducing the source compound-complex sentence structure ( CSS ) .	to encourage generating such transitional expressions	by introducing the source compound-complex sentence structure ( CSS ) .	65-91	65-91	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	1<2	none	manner-means	manner-means
P14-1080_anno1	70-74	92-99	we propose two novel models	Our models include a CSS-based translation model ,	we propose two novel models	Our models include a CSS-based translation model ,	65-91	92-120	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	1<2	none	elab-aspect	elab-aspect
P14-1080_anno1	92-99	100-112	Our models include a CSS-based translation model ,	which generates new CSS-based translation rules , and a generative transfer model ,	Our models include a CSS-based translation model ,	which generates new CSS-based translation rules , and a generative transfer model ,	92-120	92-120	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	1<2	none	elab-addition	elab-addition
P14-1080_anno1	100-112	113-117	which generates new CSS-based translation rules , and a generative transfer model ,	which encourages producing transitional expressions	which generates new CSS-based translation rules , and a generative transfer model ,	which encourages producing transitional expressions	92-120	92-120	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	1<2	none	elab-addition	elab-addition
P14-1080_anno1	113-117	118-120	which encourages producing transitional expressions	during decoding .	which encourages producing transitional expressions	during decoding .	92-120	92-120	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	Our models include a CSS-based translation model , which generates new CSS-based translation rules , and a generative transfer model , which encourages producing transitional expressions during decoding .	1<2	none	temporal	temporal
P14-1080_anno1	70-74	121-131	we propose two novel models	The two models are integrated into a hierarchical phrase-based translation system	we propose two novel models	The two models are integrated into a hierarchical phrase-based translation system	65-91	121-136	To address this issue , we propose two novel models to encourage generating such transitional expressions by introducing the source compound-complex sentence structure ( CSS ) .	The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness .	1<2	none	evaluation	evaluation
P14-1080_anno1	121-131	132-136	The two models are integrated into a hierarchical phrase-based translation system	to evaluate their effectiveness .	The two models are integrated into a hierarchical phrase-based translation system	to evaluate their effectiveness .	121-136	121-136	The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness .	The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness .	1<2	none	enablement	enablement
P14-1080_anno1	137-140	141-149	The experimental results show	that significant improvements are achieved on various test data	The experimental results show	that significant improvements are achieved on various test data	137-158	137-158	The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .	The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .	1>2	none	enablement	enablement
P14-1080_anno1	121-131	141-149	The two models are integrated into a hierarchical phrase-based translation system	that significant improvements are achieved on various test data	The two models are integrated into a hierarchical phrase-based translation system	that significant improvements are achieved on various test data	121-136	137-158	The two models are integrated into a hierarchical phrase-based translation system to evaluate their effectiveness .	The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .	1<2	none	cause	cause
P14-1080_anno1	141-149	150-158	that significant improvements are achieved on various test data	meanwhile the translations are more cohesive and smooth .	that significant improvements are achieved on various test data	meanwhile the translations are more cohesive and smooth .	137-158	137-158	The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .	The experimental results show that significant improvements are achieved on various test data meanwhile the translations are more cohesive and smooth .	1<2	none	progression	progression
P14-1081_anno1	1-11	12-28	We present an adaptive translation quality estimation ( QE ) method	to predict the human-targeted translation error rate ( HTER ) for a document-specific machine translation model .	We present an adaptive translation quality estimation ( QE ) method	to predict the human-targeted translation error rate ( HTER ) for a document-specific machine translation model .	1-28	1-28	We present an adaptive translation quality estimation ( QE ) method to predict the human-targeted translation error rate ( HTER ) for a document-specific machine translation model .	We present an adaptive translation quality estimation ( QE ) method to predict the human-targeted translation error rate ( HTER ) for a document-specific machine translation model .	1<2	none	enablement	enablement
P14-1081_anno1	1-11	29-32	We present an adaptive translation quality estimation ( QE ) method	We first introduce features	We present an adaptive translation quality estimation ( QE ) method	We first introduce features	1-28	29-49	We present an adaptive translation quality estimation ( QE ) method to predict the human-targeted translation error rate ( HTER ) for a document-specific machine translation model .	We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis .	1<2	none	elab-aspect	elab-aspect
P14-1081_anno1	29-32	33-49	We first introduce features	derived internal to the translation decoding process as well as externally from the source sentence analysis .	We first introduce features	derived internal to the translation decoding process as well as externally from the source sentence analysis .	29-49	29-49	We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis .	We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis .	1<2	none	elab-addition	elab-addition
P14-1081_anno1	29-32	50-65	We first introduce features	We show the effectiveness of such features in both classification and regression of MT quality .	We first introduce features	We show the effectiveness of such features in both classification and regression of MT quality .	29-49	50-65	We first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis .	We show the effectiveness of such features in both classification and regression of MT quality .	1<2	none	evaluation	evaluation
P14-1081_anno1	66-77	78-90	By dynamically training the QE model for the document-specific MT model ,	we are able to achieve consistency and prediction quality across multiple documents ,	By dynamically training the QE model for the document-specific MT model ,	we are able to achieve consistency and prediction quality across multiple documents ,	66-102	66-102	By dynamically training the QE model for the document-specific MT model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	By dynamically training the QE model for the document-specific MT model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	1>2	none	manner-means	manner-means
P14-1081_anno1	50-65	78-90	We show the effectiveness of such features in both classification and regression of MT quality .	we are able to achieve consistency and prediction quality across multiple documents ,	We show the effectiveness of such features in both classification and regression of MT quality .	we are able to achieve consistency and prediction quality across multiple documents ,	50-65	66-102	We show the effectiveness of such features in both classification and regression of MT quality .	By dynamically training the QE model for the document-specific MT model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	1<2	none	elab-aspect	elab-aspect
P14-1081_anno1	78-90	91-102	we are able to achieve consistency and prediction quality across multiple documents ,	demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	we are able to achieve consistency and prediction quality across multiple documents ,	demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	66-102	66-102	By dynamically training the QE model for the document-specific MT model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	By dynamically training the QE model for the document-specific MT model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and F-scores in findingGood sentences .	1<2	none	exp-evidence	exp-evidence
P14-1081_anno1	50-65	103-117	We show the effectiveness of such features in both classification and regression of MT quality .	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study	We show the effectiveness of such features in both classification and regression of MT quality .	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study	50-65	103-136	We show the effectiveness of such features in both classification and regression of MT quality .	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference , with a 10 % increase in human translators's productivity .	1<2	none	elab-aspect	elab-aspect
P14-1081_anno1	103-117	118-136	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study	and we observe strong correlation with human preference , with a 10 % increase in human translators's productivity .	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study	and we observe strong correlation with human preference , with a 10 % increase in human translators's productivity .	103-136	103-136	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference , with a 10 % increase in human translators's productivity .	Additionally , the proposed method is applied to IBM English-to-Japanese MT post editing field study and we observe strong correlation with human preference , with a 10 % increase in human translators's productivity .	1<2	none	cause	cause
P14-1082_anno1	1-11	12-36	In this paper we present new research in translation assistance .	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	In this paper we present new research in translation assistance .	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	1-11	12-36	In this paper we present new research in translation assistance .	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	1<2	none	elab-addition	elab-addition
P14-1082_anno1	12-36	37-52	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	Practical applications of this research can be framed in the context of second language learning .	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	Practical applications of this research can be framed in the context of second language learning .	12-36	37-52	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	Practical applications of this research can be framed in the context of second language learning .	1<2	none	bg-goal	bg-goal
P14-1082_anno1	37-52	53-70	Practical applications of this research can be framed in the context of second language learning .	The type of translation assistance system under investigation here encourages language learners to write in their target language	Practical applications of this research can be framed in the context of second language learning .	The type of translation assistance system under investigation here encourages language learners to write in their target language	37-52	53-91	Practical applications of this research can be framed in the context of second language learning .	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	1<2	none	elab-addition	elab-addition
P14-1082_anno1	53-70	71-80	The type of translation assistance system under investigation here encourages language learners to write in their target language	while allowing them to fall back to their native language	The type of translation assistance system under investigation here encourages language learners to write in their target language	while allowing them to fall back to their native language	53-91	53-91	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	1<2	none	elab-addition	elab-addition
P14-1082_anno1	71-80	81-91	while allowing them to fall back to their native language	in case the correct word or expression is not known .	while allowing them to fall back to their native language	in case the correct word or expression is not known .	53-91	53-91	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	1<2	none	condition	condition
P14-1082_anno1	53-70	92-99	The type of translation assistance system under investigation here encourages language learners to write in their target language	These code switches are subsequently translated to L2	The type of translation assistance system under investigation here encourages language learners to write in their target language	These code switches are subsequently translated to L2	53-91	92-104	The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known .	These code switches are subsequently translated to L2 given the L2 context .	1<2	none	progression	progression
P14-1082_anno1	92-99	100-104	These code switches are subsequently translated to L2	given the L2 context .	These code switches are subsequently translated to L2	given the L2 context .	92-104	92-104	These code switches are subsequently translated to L2 given the L2 context .	These code switches are subsequently translated to L2 given the L2 context .	1<2	none	bg-general	bg-general
P14-1082_anno1	12-36	105-112	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	We study the feasibility of exploiting cross-lingual context	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	We study the feasibility of exploiting cross-lingual context	12-36	105-128	We describe a system capable of translating native language ( L1 ) fragments to foreign language ( L2 ) fragments in an L2 context .	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	1<2	none	elab-aspect	elab-aspect
P14-1082_anno1	105-112	113-117	We study the feasibility of exploiting cross-lingual context	to obtain high-quality translation suggestions	We study the feasibility of exploiting cross-lingual context	to obtain high-quality translation suggestions	105-128	105-128	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	1<2	none	enablement	enablement
P14-1082_anno1	113-117	118-128	to obtain high-quality translation suggestions	that improve over statistical language modelling and word-sense disambiguation baselines .	to obtain high-quality translation suggestions	that improve over statistical language modelling and word-sense disambiguation baselines .	105-128	105-128	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	1<2	none	elab-addition	elab-addition
P14-1082_anno1	105-112	129-133	We study the feasibility of exploiting cross-lingual context	A classification-based approach is presented	We study the feasibility of exploiting cross-lingual context	A classification-based approach is presented	105-128	129-158	We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines .	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	1<2	none	cause	cause
P14-1082_anno1	129-133	134-143	A classification-based approach is presented	that is indeed found to improve significantly over these baselines	A classification-based approach is presented	that is indeed found to improve significantly over these baselines	129-158	129-158	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	1<2	none	elab-addition	elab-addition
P14-1082_anno1	138-143	144-150	to improve significantly over these baselines	by making use of a contextual window	to improve significantly over these baselines	by making use of a contextual window	129-158	129-158	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	1<2	none	manner-means	manner-means
P14-1082_anno1	144-150	151-158	by making use of a contextual window	spanning a small number of neighbouring words .	by making use of a contextual window	spanning a small number of neighbouring words .	129-158	129-158	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words .	1<2	none	elab-addition	elab-addition
P14-1083_anno1	1-13	14-31	We propose a novel learning approach for statistical machine translation ( SMT )	that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	We propose a novel learning approach for statistical machine translation ( SMT )	that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	1-31	1-31	We propose a novel learning approach for statistical machine translation ( SMT ) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	We propose a novel learning approach for statistical machine translation ( SMT ) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	1<2	none	elab-addition	elab-addition
P14-1083_anno1	32-33	34-37	We show	how to generate responses	We show	how to generate responses	32-56	32-56	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	1>2	none	attribution	attribution
P14-1083_anno1	1-13	34-37	We propose a novel learning approach for statistical machine translation ( SMT )	how to generate responses	We propose a novel learning approach for statistical machine translation ( SMT )	how to generate responses	1-31	32-56	We propose a novel learning approach for statistical machine translation ( SMT ) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	1<2	none	elab-aspect	elab-aspect
P14-1083_anno1	34-37	38-43	how to generate responses	by grounding SMT in the task	how to generate responses	by grounding SMT in the task	32-56	32-56	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	1<2	none	manner-means	manner-means
P14-1083_anno1	38-43	44-56	by grounding SMT in the task	of executing a semantic parse of a translated query against a database .	by grounding SMT in the task	of executing a semantic parse of a translated query against a database .	32-56	32-56	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	We show how to generate responses by grounding SMT in the task of executing a semantic parse of a translated query against a database .	1<2	none	elab-addition	elab-addition
P14-1083_anno1	1-13	57-77	We propose a novel learning approach for statistical machine translation ( SMT )	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references	We propose a novel learning approach for statistical machine translation ( SMT )	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references	1-31	57-92	We propose a novel learning approach for statistical machine translation ( SMT ) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references only on returning the correct answer from a semantic parse of a translated query .	1<2	none	evaluation	evaluation
P14-1083_anno1	57-77	78-92	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references	only on returning the correct answer from a semantic parse of a translated query .	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references	only on returning the correct answer from a semantic parse of a translated query .	57-92	57-92	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references only on returning the correct answer from a semantic parse of a translated query .	Experiments on the GEOQUERY database show an improvement of about 6 points in F1-score for response-based learning over learning from references only on returning the correct answer from a semantic parse of a translated query .	1<2	none	elab-addition	elab-addition
P14-1083_anno1	1-13	93-104	We propose a novel learning approach for statistical machine translation ( SMT )	In general , our approach alleviates the dependency on human reference translations	We propose a novel learning approach for statistical machine translation ( SMT )	In general , our approach alleviates the dependency on human reference translations	1-31	93-115	We propose a novel learning approach for statistical machine translation ( SMT ) that allows to extract supervision signals for structured learning from an extrinsic response to a translation input .	In general , our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT .	1<2	none	elab-aspect	elab-aspect
P14-1083_anno1	93-104	105-115	In general , our approach alleviates the dependency on human reference translations	and solves the reachability problem in structured learning for SMT .	In general , our approach alleviates the dependency on human reference translations	and solves the reachability problem in structured learning for SMT .	93-115	93-115	In general , our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT .	In general , our approach alleviates the dependency on human reference translations and solves the reachability problem in structured learning for SMT .	1<2	none	joint	joint
P14-1084_anno1	1-19	60-68	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters	We compare three ways of extracting event patterns :	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters	We compare three ways of extracting event patterns :	1-59	60-74	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	We compare three ways of extracting event patterns : heuristics-based , compression-based and memory-based .	1>2	none	bg-goal	bg-goal
P14-1084_anno1	1-19	20-26	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters	in which every cluster represents an event	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters	in which every cluster represents an event	1-59	1-59	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	1<2	none	elab-addition	elab-addition
P14-1084_anno1	20-26	27-31	in which every cluster represents an event	( e.g. , marriage )	in which every cluster represents an event	( e.g. , marriage )	1-59	1-59	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	1<2	none	elab-example	elab-example
P14-1084_anno1	20-26	32-44	in which every cluster represents an event	and every pattern in the cluster is a way of expressing the event	in which every cluster represents an event	and every pattern in the cluster is a way of expressing the event	1-59	1-59	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	1<2	none	joint	joint
P14-1084_anno1	32-44	45-59	and every pattern in the cluster is a way of expressing the event	( e.g. , X married Y , X and Y tied the knot ) .	and every pattern in the cluster is a way of expressing the event	( e.g. , X married Y , X and Y tied the knot ) .	1-59	1-59	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	Abstractive text summarization of news requires a way of representing events , such as a collection of pattern clusters in which every cluster represents an event ( e.g. , marriage ) and every pattern in the cluster is a way of expressing the event ( e.g. , X married Y , X and Y tied the knot ) .	1<2	none	elab-example	elab-example
P14-1084_anno1	60-68	69-74	We compare three ways of extracting event patterns :	heuristics-based , compression-based and memory-based .	We compare three ways of extracting event patterns :	heuristics-based , compression-based and memory-based .	60-74	60-74	We compare three ways of extracting event patterns : heuristics-based , compression-based and memory-based .	We compare three ways of extracting event patterns : heuristics-based , compression-based and memory-based .	1<2	none	elab-enumember	elab-enumember
P14-1084_anno1	75-85	86-96	While the former has been used previously in multi-document abstraction ,	the latter two have never been used for this task .	While the former has been used previously in multi-document abstraction ,	the latter two have never been used for this task .	75-96	75-96	While the former has been used previously in multi-document abstraction , the latter two have never been used for this task .	While the former has been used previously in multi-document abstraction , the latter two have never been used for this task .	1>2	none	contrast	contrast
P14-1084_anno1	69-74	86-96	heuristics-based , compression-based and memory-based .	the latter two have never been used for this task .	heuristics-based , compression-based and memory-based .	the latter two have never been used for this task .	60-74	75-96	We compare three ways of extracting event patterns : heuristics-based , compression-based and memory-based .	While the former has been used previously in multi-document abstraction , the latter two have never been used for this task .	1<2	none	elab-addition	elab-addition
P14-1084_anno1	97-103	104-116	Compared with the first two techniques ,	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	Compared with the first two techniques ,	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	97-136	97-136	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	1>2	none	comparison	comparison
P14-1084_anno1	60-68	104-116	We compare three ways of extracting event patterns :	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	We compare three ways of extracting event patterns :	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	60-74	97-136	We compare three ways of extracting event patterns : heuristics-based , compression-based and memory-based .	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	1<2	none	elab-aspect	elab-aspect
P14-1084_anno1	104-116	117-136	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	97-136	97-136	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	1<2	none	elab-addition	elab-addition
P14-1084_anno1	104-116	137-149	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	To this end , we introduce a data structure and a search method	the memory-based method allows for generating significantly more grammatical and informa-tive sentences ,	To this end , we introduce a data structure and a search method	97-136	137-171	Compared with the first two techniques , the memory-based method allows for generating significantly more grammatical and informa-tive sentences , at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances .	To this end , we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances .	1<2	none	cause	cause
P14-1084_anno1	137-149	150-162	To this end , we introduce a data structure and a search method	that make it possible to efficiently extrapolate from every sentence the parse sub-trees	To this end , we introduce a data structure and a search method	that make it possible to efficiently extrapolate from every sentence the parse sub-trees	137-171	137-171	To this end , we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances .	To this end , we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances .	1<2	none	elab-addition	elab-addition
P14-1084_anno1	150-162	163-171	that make it possible to efficiently extrapolate from every sentence the parse sub-trees	that match against any of the stored utterances .	that make it possible to efficiently extrapolate from every sentence the parse sub-trees	that match against any of the stored utterances .	137-171	137-171	To this end , we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances .	To this end , we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances .	1<2	none	elab-addition	elab-addition
P14-1085_anno1	1-2	10-15	For topics	simple , short summaries are insufficient	For topics	simple , short summaries are insufficient	1-27	1-27	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	1>2	none	elab-addition	elab-addition
P14-1085_anno1	1-2	3-9	For topics	that cover large amounts of information ,	For topics	that cover large amounts of information ,	1-27	1-27	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	1<2	none	elab-addition	elab-addition
P14-1085_anno1	10-15	28-36	simple , short summaries are insufficient	We propose a new approach to scaling up summarization	simple , short summaries are insufficient	We propose a new approach to scaling up summarization	1-27	28-49	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	1>2	none	bg-goal	bg-goal
P14-1085_anno1	10-15	16-27	simple , short summaries are insufficient	- complex topics require more information and more structure to understand .	simple , short summaries are insufficient	- complex topics require more information and more structure to understand .	1-27	1-27	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	For topics that cover large amounts of information , simple , short summaries are insufficient - complex topics require more information and more structure to understand .	1<2	none	exp-reason	exp-reason
P14-1085_anno1	28-36	37-40	We propose a new approach to scaling up summarization	called hierarchical summarization ,	We propose a new approach to scaling up summarization	called hierarchical summarization ,	28-49	28-49	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	1<2	none	elab-addition	elab-addition
P14-1085_anno1	28-36	41-49	We propose a new approach to scaling up summarization	and present the first implemented system , SUMMA .	We propose a new approach to scaling up summarization	and present the first implemented system , SUMMA .	28-49	28-49	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	1<2	none	joint	joint
P14-1085_anno1	41-49	50-58	and present the first implemented system , SUMMA .	SUMMA produces a hierarchy of relatively short summaries ,	and present the first implemented system , SUMMA .	SUMMA produces a hierarchy of relatively short summaries ,	28-49	50-83	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	1<2	none	elab-aspect	elab-aspect
P14-1085_anno1	50-58	59-66	SUMMA produces a hierarchy of relatively short summaries ,	where the top level provides a general overview	SUMMA produces a hierarchy of relatively short summaries ,	where the top level provides a general overview	50-83	50-83	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	1<2	none	elab-addition	elab-addition
P14-1085_anno1	59-66	67-72	where the top level provides a general overview	and users can navigate the hierarchy	where the top level provides a general overview	and users can navigate the hierarchy	50-83	50-83	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	1<2	none	joint	joint
P14-1085_anno1	67-72	73-83	and users can navigate the hierarchy	to drill down for more details on topics of interest .	and users can navigate the hierarchy	to drill down for more details on topics of interest .	50-83	50-83	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	SUMMA produces a hierarchy of relatively short summaries , where the top level provides a general overview and users can navigate the hierarchy to drill down for more details on topics of interest .	1<2	none	enablement	enablement
P14-1085_anno1	84-89	90-96	Compared to flat multi-document summaries ,	users prefer SUMMA ten times as often	Compared to flat multi-document summaries ,	users prefer SUMMA ten times as often	84-123	84-123	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1>2	none	comparison	comparison
P14-1085_anno1	41-49	90-96	and present the first implemented system , SUMMA .	users prefer SUMMA ten times as often	and present the first implemented system , SUMMA .	users prefer SUMMA ten times as often	28-49	84-123	We propose a new approach to scaling up summarization called hierarchical summarization , and present the first implemented system , SUMMA .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1<2	none	evaluation	evaluation
P14-1085_anno1	90-96	97-102	users prefer SUMMA ten times as often	and learn just as much ,	users prefer SUMMA ten times as often	and learn just as much ,	84-123	84-123	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1<2	none	joint	joint
P14-1085_anno1	103-107	108-114	and compared to timelines ,	users prefer SUMMA three times as often	and compared to timelines ,	users prefer SUMMA three times as often	84-123	84-123	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1>2	none	comparison	comparison
P14-1085_anno1	90-96	108-114	users prefer SUMMA ten times as often	users prefer SUMMA three times as often	users prefer SUMMA ten times as often	users prefer SUMMA three times as often	84-123	84-123	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1<2	none	joint	joint
P14-1085_anno1	108-114	115-123	users prefer SUMMA three times as often	and learn more in twice as many cases .	users prefer SUMMA three times as often	and learn more in twice as many cases .	84-123	84-123	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	Compared to flat multi-document summaries , users prefer SUMMA ten times as often and learn just as much , and compared to timelines , users prefer SUMMA three times as often and learn more in twice as many cases .	1<2	none	joint	joint
P14-1086_anno1	1-8	62-70	Update summarization is a form of multi-document summarization	We introduce a new task , Query-Chain Summarization ,	Update summarization is a form of multi-document summarization	We introduce a new task , Query-Chain Summarization ,	1-26	62-102	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1>2	none	bg-general	bg-general
P14-1086_anno1	1-8	9-21	Update summarization is a form of multi-document summarization	where a document set must be summarized in the context of other documents	Update summarization is a form of multi-document summarization	where a document set must be summarized in the context of other documents	1-26	1-26	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	9-21	22-26	where a document set must be summarized in the context of other documents	assumed to be known .	where a document set must be summarized in the context of other documents	assumed to be known .	1-26	1-26	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	1-8	27-42	Update summarization is a form of multi-document summarization	Efficient update summarization must focus on identifying new information and avoiding repetition of known information .	Update summarization is a form of multi-document summarization	Efficient update summarization must focus on identifying new information and avoiding repetition of known information .	1-26	27-42	Update summarization is a form of multi-document summarization where a document set must be summarized in the context of other documents assumed to be known .	Efficient update summarization must focus on identifying new information and avoiding repetition of known information .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	43-61	62-70	In Query-focused summarization , the task is to produce a summary as an answer to a given query .	We introduce a new task , Query-Chain Summarization ,	In Query-focused summarization , the task is to produce a summary as an answer to a given query .	We introduce a new task , Query-Chain Summarization ,	43-61	62-102	In Query-focused summarization , the task is to produce a summary as an answer to a given query .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1>2	none	bg-general	bg-general
P14-1086_anno1	62-70	182-188	We introduce a new task , Query-Chain Summarization ,	We present an algorithm for Query-Chain Summarization	We introduce a new task , Query-Chain Summarization ,	We present an algorithm for Query-Chain Summarization	62-102	182-197	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant .	1>2	none	bg-goal	bg-goal
P14-1086_anno1	62-70	71-79	We introduce a new task , Query-Chain Summarization ,	which combines aspects of the two previous tasks :	We introduce a new task , Query-Chain Summarization ,	which combines aspects of the two previous tasks :	62-102	62-102	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	80-86	87-92	starting from a given document set ,	increasingly specific queries are considered ,	starting from a given document set ,	increasingly specific queries are considered ,	62-102	62-102	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1>2	none	bg-general	bg-general
P14-1086_anno1	62-70	87-92	We introduce a new task , Query-Chain Summarization ,	increasingly specific queries are considered ,	We introduce a new task , Query-Chain Summarization ,	increasingly specific queries are considered ,	62-102	62-102	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1<2	none	elab-definition	elab-definition
P14-1086_anno1	87-92	93-102	increasingly specific queries are considered ,	and a new summary is produced at each step .	increasingly specific queries are considered ,	and a new summary is produced at each step .	62-102	62-102	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	1<2	none	joint	joint
P14-1086_anno1	62-70	103-108	We introduce a new task , Query-Chain Summarization ,	This process models exploratory search :	We introduce a new task , Query-Chain Summarization ,	This process models exploratory search :	62-102	103-137	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	1<2	none	elab-aspect	elab-aspect
P14-1086_anno1	103-108	109-114	This process models exploratory search :	a user explores a new topic	This process models exploratory search :	a user explores a new topic	103-137	103-137	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	1<2	none	elab-definition	elab-definition
P14-1086_anno1	109-114	115-121	a user explores a new topic	by submitting a sequence of queries ,	a user explores a new topic	by submitting a sequence of queries ,	103-137	103-137	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	1<2	none	manner-means	manner-means
P14-1086_anno1	109-114	122-128	a user explores a new topic	inspecting a summary of the result set	a user explores a new topic	inspecting a summary of the result set	103-137	103-137	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	122-128	129-137	inspecting a summary of the result set	and phrasing a new query at each step .	inspecting a summary of the result set	and phrasing a new query at each step .	103-137	103-137	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	This process models exploratory search : a user explores a new topic by submitting a sequence of queries , inspecting a summary of the result set and phrasing a new query at each step .	1<2	none	joint	joint
P14-1086_anno1	138-142	166-181	We present a novel dataset	that summaries produced in the context of such exploratory process are different from informative summaries .	We present a novel dataset	that summaries produced in the context of such exploratory process are different from informative summaries .	138-162	163-181	We present a novel dataset comprising 22 query-chains sessions of length up to 3 with 3 matching human summaries each in the consumer-health domain .	Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries .	1>2	none	bg-general	bg-general
P14-1086_anno1	138-142	143-162	We present a novel dataset	comprising 22 query-chains sessions of length up to 3 with 3 matching human summaries each in the consumer-health domain .	We present a novel dataset	comprising 22 query-chains sessions of length up to 3 with 3 matching human summaries each in the consumer-health domain .	138-162	138-162	We present a novel dataset comprising 22 query-chains sessions of length up to 3 with 3 matching human summaries each in the consumer-health domain .	We present a novel dataset comprising 22 query-chains sessions of length up to 3 with 3 matching human summaries each in the consumer-health domain .	1<2	none	elab-addition	elab-addition
P14-1086_anno1	163-165	166-181	Our analysis demonstrates	that summaries produced in the context of such exploratory process are different from informative summaries .	Our analysis demonstrates	that summaries produced in the context of such exploratory process are different from informative summaries .	163-181	163-181	Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries .	Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries .	1>2	none	attribution	attribution
P14-1086_anno1	62-70	166-181	We introduce a new task , Query-Chain Summarization ,	that summaries produced in the context of such exploratory process are different from informative summaries .	We introduce a new task , Query-Chain Summarization ,	that summaries produced in the context of such exploratory process are different from informative summaries .	62-102	163-181	We introduce a new task , Query-Chain Summarization , which combines aspects of the two previous tasks : starting from a given document set , increasingly specific queries are considered , and a new summary is produced at each step .	Our analysis demonstrates that summaries produced in the context of such exploratory process are different from informative summaries .	1<2	none	elab-aspect	elab-aspect
P14-1086_anno1	182-188	189-197	We present an algorithm for Query-Chain Summarization	based on a new LDA topic model variant .	We present an algorithm for Query-Chain Summarization	based on a new LDA topic model variant .	182-197	182-197	We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant .	We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant .	1<2	none	manner-means	manner-means
P14-1086_anno1	198-199	200-206	Evaluation indicates	the algorithm improves on strong baselines .	Evaluation indicates	the algorithm improves on strong baselines .	198-206	198-206	Evaluation indicates the algorithm improves on strong baselines .	Evaluation indicates the algorithm improves on strong baselines .	1>2	none	attribution	attribution
P14-1086_anno1	182-188	200-206	We present an algorithm for Query-Chain Summarization	the algorithm improves on strong baselines .	We present an algorithm for Query-Chain Summarization	the algorithm improves on strong baselines .	182-197	198-206	We present an algorithm for Query-Chain Summarization based on a new LDA topic model variant .	Evaluation indicates the algorithm improves on strong baselines .	1<2	none	evaluation	evaluation
P14-1087_anno1	1-12	13-17	We study the use of temporal information in the form of timelines	to enhance multi-document summarization .	We study the use of temporal information in the form of timelines	to enhance multi-document summarization .	1-17	1-17	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	1<2	none	enablement	enablement
P14-1087_anno1	1-12	18-25	We study the use of temporal information in the form of timelines	We employ a fully automated temporal processing system	We study the use of temporal information in the form of timelines	We employ a fully automated temporal processing system	1-17	18-34	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	We employ a fully automated temporal processing system to generate a timeline for each input document .	1<2	none	elab-aspect	elab-aspect
P14-1087_anno1	18-25	26-34	We employ a fully automated temporal processing system	to generate a timeline for each input document .	We employ a fully automated temporal processing system	to generate a timeline for each input document .	18-34	18-34	We employ a fully automated temporal processing system to generate a timeline for each input document .	We employ a fully automated temporal processing system to generate a timeline for each input document .	1<2	none	enablement	enablement
P14-1087_anno1	18-25	35-42	We employ a fully automated temporal processing system	We derive three features from these timelines ,	We employ a fully automated temporal processing system	We derive three features from these timelines ,	18-34	35-65	We employ a fully automated temporal processing system to generate a timeline for each input document .	We derive three features from these timelines , and show that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	1<2	none	cause	cause
P14-1087_anno1	43-44	45-65	and show	that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	and show	that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	35-65	35-65	We derive three features from these timelines , and show that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	We derive three features from these timelines , and show that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	1>2	none	attribution	attribution
P14-1087_anno1	35-42	45-65	We derive three features from these timelines ,	that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	We derive three features from these timelines ,	that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	35-65	35-65	We derive three features from these timelines , and show that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	We derive three features from these timelines , and show that their use in supervised summarization lead to a significant 4.1 % improvement in ROUGE performance over a state-of-the-art base-line .	1<2	none	evaluation	evaluation
P14-1087_anno1	1-12	66-78	We study the use of temporal information in the form of timelines	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	We study the use of temporal information in the form of timelines	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	1-17	66-100	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	1<2	none	elab-aspect	elab-aspect
P14-1087_anno1	66-78	79-82	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	that promotes temporal diversity	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	that promotes temporal diversity	66-100	66-100	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	1<2	none	elab-addition	elab-addition
P14-1087_anno1	79-82	83-90	that promotes temporal diversity	by way of computing time span similarity ,	that promotes temporal diversity	by way of computing time span similarity ,	66-100	66-100	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	1<2	none	manner-means	manner-means
P14-1087_anno1	66-78	91-100	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	and show its utility in summarizing certain document sets .	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance	and show its utility in summarizing certain document sets .	66-100	66-100	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	In addition , we propose TIMEMMR , a modification to Maximal Marginal Relevance that promotes temporal diversity by way of computing time span similarity , and show its utility in summarizing certain document sets .	1<2	none	joint	joint
P14-1087_anno1	1-12	101-106	We study the use of temporal information in the form of timelines	We also propose a filtering metric	We study the use of temporal information in the form of timelines	We also propose a filtering metric	1-17	101-124	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	1<2	none	elab-aspect	elab-aspect
P14-1087_anno1	101-106	107-110	We also propose a filtering metric	to discard noisy timelines	We also propose a filtering metric	to discard noisy timelines	101-124	101-124	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	1<2	none	enablement	enablement
P14-1087_anno1	107-110	111-116	to discard noisy timelines	generated by our automatic processes ,	to discard noisy timelines	generated by our automatic processes ,	101-124	101-124	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	1<2	none	elab-addition	elab-addition
P14-1087_anno1	111-116	117-124	generated by our automatic processes ,	to purify the timeline input for summarization .	generated by our automatic processes ,	to purify the timeline input for summarization .	101-124	101-124	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	We also propose a filtering metric to discard noisy timelines generated by our automatic processes , to purify the timeline input for summarization .	1<2	none	enablement	enablement
P14-1087_anno1	125-128	133-143	By selectively using timelines	overall summarization performance is increased by a significant 5.9 % .	By selectively using timelines	overall summarization performance is increased by a significant 5.9 % .	125-143	125-143	By selectively using timelines guided by filtering , overall summarization performance is increased by a significant 5.9 % .	By selectively using timelines guided by filtering , overall summarization performance is increased by a significant 5.9 % .	1>2	none	manner-means	manner-means
P14-1087_anno1	125-128	129-132	By selectively using timelines	guided by filtering ,	By selectively using timelines	guided by filtering ,	125-143	125-143	By selectively using timelines guided by filtering , overall summarization performance is increased by a significant 5.9 % .	By selectively using timelines guided by filtering , overall summarization performance is increased by a significant 5.9 % .	1<2	none	elab-addition	elab-addition
P14-1087_anno1	1-12	133-143	We study the use of temporal information in the form of timelines	overall summarization performance is increased by a significant 5.9 % .	We study the use of temporal information in the form of timelines	overall summarization performance is increased by a significant 5.9 % .	1-17	125-143	We study the use of temporal information in the form of timelines to enhance multi-document summarization .	By selectively using timelines guided by filtering , overall summarization performance is increased by a significant 5.9 % .	1<2	none	evaluation	evaluation
P14-1088_anno1	1-16	17-24	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) ,	there is an increasing consensus within the field	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) ,	there is an increasing consensus within the field	1-46	1-46	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	1>2	none	bg-general	bg-general
P14-1088_anno1	17-24	47-71,77-79	there is an increasing consensus within the field	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	there is an increasing consensus within the field	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	1-46	47-84	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	1>2	none	result	result
P14-1088_anno1	25-37	38-46	that in order to properly gauge the reliability of an annotation effort ,	chance-corrected measures of inter-annotator agreement should be used .	that in order to properly gauge the reliability of an annotation effort ,	chance-corrected measures of inter-annotator agreement should be used .	1-46	1-46	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	1>2	none	enablement	enablement
P14-1088_anno1	17-24	38-46	there is an increasing consensus within the field	chance-corrected measures of inter-annotator agreement should be used .	there is an increasing consensus within the field	chance-corrected measures of inter-annotator agreement should be used .	1-46	1-46	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	Following the works of Carletta ( 1996 ) and Artstein and Poesio ( 2008 ) , there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort , chance-corrected measures of inter-annotator agreement should be used .	1<2	none	elab-addition	elab-addition
P14-1088_anno1	47-71,77-79	85-92	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	In this work we present a chance-corrected metric	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	In this work we present a chance-corrected metric	47-84	85-116	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	1>2	none	bg-general	bg-general
P14-1088_anno1	47-71,77-79	72-76	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	( for phrase structure )	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 <*> and accuracy scores	( for phrase structure )	47-84	47-84	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	1<2	none	elab-addition	elab-addition
P14-1088_anno1	77-79	80-84	and accuracy scores	( for dependencies ) .	and accuracy scores	( for dependencies ) .	47-84	47-84	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	With this in mind , it is striking that virtually all evaluations of syntactic annotation efforts use uncorrected parser evaluation metrics such as bracket F1 ( for phrase structure ) and accuracy scores ( for dependencies ) .	1<2	none	elab-addition	elab-addition
P14-1088_anno1	85-92	93-96	In this work we present a chance-corrected metric	based on Krippendorff's α,	In this work we present a chance-corrected metric	based on Krippendorff's α,	85-116	85-116	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	1<2	none	manner-means	manner-means
P14-1088_anno1	85-92	97-116	In this work we present a chance-corrected metric	adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	In this work we present a chance-corrected metric	adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	85-116	85-116	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	1<2	none	elab-addition	elab-addition
P14-1088_anno1	117-120	121-128	To evaluate our metric	we first present a number of synthetic experiments	To evaluate our metric	we first present a number of synthetic experiments	117-160	117-160	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	1>2	none	evaluation	evaluation
P14-1088_anno1	85-92	121-128	In this work we present a chance-corrected metric	we first present a number of synthetic experiments	In this work we present a chance-corrected metric	we first present a number of synthetic experiments	85-116	117-160	In this work we present a chance-corrected metric based on Krippendorff's α, adapted to the structure of syntactic annotations and applicable both to phrase structure and dependency annotation without any modifications .	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	1<2	none	evaluation	evaluation
P14-1088_anno1	121-128	129-135	we first present a number of synthetic experiments	to better control the sources of noise	we first present a number of synthetic experiments	to better control the sources of noise	117-160	117-160	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	1<2	none	enablement	enablement
P14-1088_anno1	129-135	136-141	to better control the sources of noise	and gauge the metric's responses ,	to better control the sources of noise	and gauge the metric's responses ,	117-160	117-160	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	1<2	none	joint	joint
P14-1088_anno1	121-128	142-160	we first present a number of synthetic experiments	before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	we first present a number of synthetic experiments	before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	117-160	117-160	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	To evaluate our metric we first present a number of synthetic experiments to better control the sources of noise and gauge the metric's responses , before finally contrasting the behaviour of our chance-corrected metric with that of uncorrected parser evaluation metrics on real corpora.	1<2	none	temporal	temporal
P14-1089_anno1	1-16	17-28	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	that is , an integrated taxonomy of Wikipage pages and categories .	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	that is , an integrated taxonomy of Wikipage pages and categories .	1-28	1-28	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia , that is , an integrated taxonomy of Wikipage pages and categories .	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia , that is , an integrated taxonomy of Wikipage pages and categories .	1<2	none	elab-definition	elab-definition
P14-1089_anno1	1-16	29-39	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	We leverage the information available in either one of the taxonomies	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	We leverage the information available in either one of the taxonomies	1-28	29-48	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia , that is , an integrated taxonomy of Wikipage pages and categories .	We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy .	1<2	none	elab-aspect	elab-aspect
P14-1089_anno1	29-39	40-48	We leverage the information available in either one of the taxonomies	to reinforce the creation of the other taxonomy .	We leverage the information available in either one of the taxonomies	to reinforce the creation of the other taxonomy .	29-48	29-48	We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy .	We leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy .	1<2	none	enablement	enablement
P14-1089_anno1	1-16	49-69	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia , YAGO , MENTA , WikiNet and WikiTaxonomy .	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia , YAGO , MENTA , WikiNet and WikiTaxonomy .	1-28	49-69	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia , that is , an integrated taxonomy of Wikipage pages and categories .	Our experiments show higher quality and coverage than state-of-the-art resources like DBpedia , YAGO , MENTA , WikiNet and WikiTaxonomy .	1<2	none	evaluation	evaluation
P14-1089_anno1	1-16	70-77	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	WiBi is available at http : //wibitaxonomy.org .	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia ,	WiBi is available at http : //wibitaxonomy.org .	1-28	70-77	We present WiBi , an approach to the automatic creation of a bitaxonomy for Wikipedia , that is , an integrated taxonomy of Wikipage pages and categories .	WiBi is available at http : //wibitaxonomy.org .	1<2	none	elab-aspect	elab-aspect
P14-1090_anno1	1-4,10-16	57-63,71-82	Answering natural language questions <*> has recently been explored as a platform	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	Answering natural language questions <*> has recently been explored as a platform	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	1-29	54-82	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing .	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	1>2	none	bg-goal	bg-goal
P14-1090_anno1	1-4,10-16	5-9	Answering natural language questions <*> has recently been explored as a platform	using the Freebase knowledge base	Answering natural language questions <*> has recently been explored as a platform	using the Freebase knowledge base	1-29	1-29	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing .	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing .	1<2	none	manner-means	manner-means
P14-1090_anno1	10-16	17-29	has recently been explored as a platform	for advancing the state of the art in open domain semantic parsing .	has recently been explored as a platform	for advancing the state of the art in open domain semantic parsing .	1-29	1-29	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing .	Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing .	1<2	none	enablement	enablement
P14-1090_anno1	30-37	57-63,71-82	Those efforts map questions to sophisticated meaning representations	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	Those efforts map questions to sophisticated meaning representations	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	30-53	54-82	Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base .	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	1>2	none	bg-compare	bg-compare
P14-1090_anno1	30-37	38-53	Those efforts map questions to sophisticated meaning representations	that are then attempted to be matched against viable answer candidates in the knowledge base .	Those efforts map questions to sophisticated meaning representations	that are then attempted to be matched against viable answer candidates in the knowledge base .	30-53	30-53	Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base .	Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base .	1<2	none	elab-addition	elab-addition
P14-1090_anno1	54-56	57-63,71-82	Here we show	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	Here we show	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	54-82	54-82	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	1>2	none	attribution	attribution
P14-1090_anno1	57-63,71-82	64-70	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	when paired with a web-scale corpus ,	that relatively modest information extraction techniques , <*> can outperform these sophisticated approaches by roughly 34 % relative gain .	when paired with a web-scale corpus ,	54-82	54-82	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	Here we show that relatively modest information extraction techniques , when paired with a web-scale corpus , can outperform these sophisticated approaches by roughly 34 % relative gain .	1<2	none	condition	condition
P14-1091_anno1	1-13	56-60	A typical knowledge-based question answering ( KB-QA ) system faces two challenges :	we present a translation-based approach	A typical knowledge-based question answering ( KB-QA ) system faces two challenges :	we present a translation-based approach	1-44	45-70	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	1>2	none	bg-goal	bg-goal
P14-1091_anno1	1-13	14-28	A typical knowledge-based question answering ( KB-QA ) system faces two challenges :	one is to transform natural language questions into their meaning representations ( MRs ) ;	A typical knowledge-based question answering ( KB-QA ) system faces two challenges :	one is to transform natural language questions into their meaning representations ( MRs ) ;	1-44	1-44	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	1<2	none	elab-enumember	elab-enumember
P14-1091_anno1	14-28	29-40	one is to transform natural language questions into their meaning representations ( MRs ) ;	the other is to retrieve answers from knowledge bases ( KBs )	one is to transform natural language questions into their meaning representations ( MRs ) ;	the other is to retrieve answers from knowledge bases ( KBs )	1-44	1-44	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	1<2	none	joint	joint
P14-1091_anno1	29-40	41-44	the other is to retrieve answers from knowledge bases ( KBs )	using generated MRs .	the other is to retrieve answers from knowledge bases ( KBs )	using generated MRs .	1-44	1-44	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	A typical knowledge-based question answering ( KB-QA ) system faces two challenges : one is to transform natural language questions into their meaning representations ( MRs ) ; the other is to retrieve answers from knowledge bases ( KBs ) using generated MRs .	1<2	none	manner-means	manner-means
P14-1091_anno1	45-47	56-60	Unlike previous methods	we present a translation-based approach	Unlike previous methods	we present a translation-based approach	45-70	45-70	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	1>2	none	bg-compare	bg-compare
P14-1091_anno1	45-47	48-55	Unlike previous methods	which treat them in a cascaded manner ,	Unlike previous methods	which treat them in a cascaded manner ,	45-70	45-70	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	1<2	none	elab-addition	elab-addition
P14-1091_anno1	56-60	61-70	we present a translation-based approach	to solve these two tasks in one unified framework .	we present a translation-based approach	to solve these two tasks in one unified framework .	45-70	45-70	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	1<2	none	enablement	enablement
P14-1091_anno1	56-60	71-80	we present a translation-based approach	We translate questions to answers based on CYK parsing .	we present a translation-based approach	We translate questions to answers based on CYK parsing .	45-70	71-80	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	We translate questions to answers based on CYK parsing .	1<2	none	elab-aspect	elab-aspect
P14-1091_anno1	71-80	81-86,92-99	We translate questions to answers based on CYK parsing .	Answers as translations of the span <*> are obtained by a question translation method ,	We translate questions to answers based on CYK parsing .	Answers as translations of the span <*> are obtained by a question translation method ,	71-80	81-132	We translate questions to answers based on CYK parsing .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	elab-addition	elab-addition
P14-1091_anno1	81-86,92-99	87-91	Answers as translations of the span <*> are obtained by a question translation method ,	covered by each CYK cell	Answers as translations of the span <*> are obtained by a question translation method ,	covered by each CYK cell	81-132	81-132	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	elab-addition	elab-addition
P14-1091_anno1	92-99	100-110	are obtained by a question translation method ,	which first generates formal triple queries as MRs for the span	are obtained by a question translation method ,	which first generates formal triple queries as MRs for the span	81-132	81-132	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	elab-addition	elab-addition
P14-1091_anno1	100-110	111-118	which first generates formal triple queries as MRs for the span	based on question patterns and relation expressions ,	which first generates formal triple queries as MRs for the span	based on question patterns and relation expressions ,	81-132	81-132	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	bg-general	bg-general
P14-1091_anno1	100-110	119-126	which first generates formal triple queries as MRs for the span	and then retrieves answers from a given KB	which first generates formal triple queries as MRs for the span	and then retrieves answers from a given KB	81-132	81-132	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	joint	joint
P14-1091_anno1	119-126	127-132	and then retrieves answers from a given KB	based on triple queries generated .	and then retrieves answers from a given KB	based on triple queries generated .	81-132	81-132	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	Answers as translations of the span covered by each CYK cell are obtained by a question translation method , which first generates formal triple queries as MRs for the span based on question patterns and relation expressions , and then retrieves answers from a given KB based on triple queries generated .	1<2	none	bg-general	bg-general
P14-1091_anno1	56-60	133-140	we present a translation-based approach	A linear model is defined over derivations ,	we present a translation-based approach	A linear model is defined over derivations ,	45-70	133-159	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	1<2	none	elab-aspect	elab-aspect
P14-1091_anno1	133-140	141-147	A linear model is defined over derivations ,	and minimum error rate training is used	A linear model is defined over derivations ,	and minimum error rate training is used	133-159	133-159	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	1<2	none	joint	joint
P14-1091_anno1	141-147	148-151	and minimum error rate training is used	to tune feature weights	and minimum error rate training is used	to tune feature weights	133-159	133-159	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	1<2	none	enablement	enablement
P14-1091_anno1	148-151	152-159	to tune feature weights	based on a set of question-answer pairs .	to tune feature weights	based on a set of question-answer pairs .	133-159	133-159	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	A linear model is defined over derivations , and minimum error rate training is used to tune feature weights based on a set of question-answer pairs .	1<2	none	bg-general	bg-general
P14-1091_anno1	160-164	171-176	Compared to a KB-QA system	our method achieves better results .	Compared to a KB-QA system	our method achieves better results .	160-176	160-176	Compared to a KB-QA system using a state-of-the-art semantic parser , our method achieves better results .	Compared to a KB-QA system using a state-of-the-art semantic parser , our method achieves better results .	1>2	none	comparison	comparison
P14-1091_anno1	160-164	165-170	Compared to a KB-QA system	using a state-of-the-art semantic parser ,	Compared to a KB-QA system	using a state-of-the-art semantic parser ,	160-176	160-176	Compared to a KB-QA system using a state-of-the-art semantic parser , our method achieves better results .	Compared to a KB-QA system using a state-of-the-art semantic parser , our method achieves better results .	1<2	none	manner-means	manner-means
P14-1091_anno1	56-60	171-176	we present a translation-based approach	our method achieves better results .	we present a translation-based approach	our method achieves better results .	45-70	160-176	Unlike previous methods which treat them in a cascaded manner , we present a translation-based approach to solve these two tasks in one unified framework .	Compared to a KB-QA system using a state-of-the-art semantic parser , our method achieves better results .	1<2	none	evaluation	evaluation
P14-1092_anno1	1-10	11-18	We propose a robust answer reranking model for non-factoid questions	that integrates lexical semantics with discourse information ,	We propose a robust answer reranking model for non-factoid questions	that integrates lexical semantics with discourse information ,	1-43	1-43	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	1<2	none	elab-addition	elab-addition
P14-1092_anno1	1-10	19-25	We propose a robust answer reranking model for non-factoid questions	driven by two representations of discourse :	We propose a robust answer reranking model for non-factoid questions	driven by two representations of discourse :	1-43	1-43	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	1<2	none	elab-addition	elab-addition
P14-1092_anno1	19-25	26-28,34-37	driven by two representations of discourse :	a shallow representation <*> and a deep one	driven by two representations of discourse :	a shallow representation <*> and a deep one	1-43	1-43	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	1<2	none	elab-enumember	elab-enumember
P14-1092_anno1	26-28,34-37	29-33	a shallow representation <*> and a deep one	centered around discourse markers ,	a shallow representation <*> and a deep one	centered around discourse markers ,	1-43	1-43	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	1<2	none	elab-addition	elab-addition
P14-1092_anno1	34-37	38-43	and a deep one	based on Rhetorical Structure Theory .	and a deep one	based on Rhetorical Structure Theory .	1-43	1-43	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	1<2	none	bg-general	bg-general
P14-1092_anno1	1-10	44-57	We propose a robust answer reranking model for non-factoid questions	We evaluate the proposed model on two corpora from different genres and domains :	We propose a robust answer reranking model for non-factoid questions	We evaluate the proposed model on two corpora from different genres and domains :	1-43	44-80	We propose a robust answer reranking model for non-factoid questions that integrates lexical semantics with discourse information , driven by two representations of discourse : a shallow representation centered around discourse markers , and a deep one based on Rhetorical Structure Theory .	We evaluate the proposed model on two corpora from different genres and domains : one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	1<2	none	evaluation	evaluation
P14-1092_anno1	44-57	58-80	We evaluate the proposed model on two corpora from different genres and domains :	one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	We evaluate the proposed model on two corpora from different genres and domains :	one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	44-80	44-80	We evaluate the proposed model on two corpora from different genres and domains : one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	We evaluate the proposed model on two corpora from different genres and domains : one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	1<2	none	elab-enumember	elab-enumember
P14-1092_anno1	81-83	84-92	We experimentally demonstrate	that the discourse structure of non-factoid answers provides information	We experimentally demonstrate	that the discourse structure of non-factoid answers provides information	81-124	81-124	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	1>2	none	attribution	attribution
P14-1092_anno1	44-57	84-92	We evaluate the proposed model on two corpora from different genres and domains :	that the discourse structure of non-factoid answers provides information	We evaluate the proposed model on two corpora from different genres and domains :	that the discourse structure of non-factoid answers provides information	44-80	81-124	We evaluate the proposed model on two corpora from different genres and domains : one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	1<2	none	elab-aspect	elab-aspect
P14-1092_anno1	84-92	93-104	that the discourse structure of non-factoid answers provides information	that is complementary to lexical semantic similarity between question and answer ,	that the discourse structure of non-factoid answers provides information	that is complementary to lexical semantic similarity between question and answer ,	81-124	81-124	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	1<2	none	elab-addition	elab-addition
P14-1092_anno1	93-104	105-117	that is complementary to lexical semantic similarity between question and answer ,	improving performance up to 24 % ( relative ) over a state-of-the-art model	that is complementary to lexical semantic similarity between question and answer ,	improving performance up to 24 % ( relative ) over a state-of-the-art model	81-124	81-124	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	1<2	none	exp-evidence	exp-evidence
P14-1092_anno1	105-117	118-124	improving performance up to 24 % ( relative ) over a state-of-the-art model	that exploits lexical semantic similarity alone .	improving performance up to 24 % ( relative ) over a state-of-the-art model	that exploits lexical semantic similarity alone .	81-124	81-124	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	We experimentally demonstrate that the discourse structure of non-factoid answers provides information that is complementary to lexical semantic similarity between question and answer , improving performance up to 24 % ( relative ) over a state-of-the-art model that exploits lexical semantic similarity alone .	1<2	none	elab-addition	elab-addition
P14-1092_anno1	125-134	136-146	We further demonstrate excellent domain transfer of discourse information ,	these discourse features have general utility to non-factoid question answering .	We further demonstrate excellent domain transfer of discourse information ,	these discourse features have general utility to non-factoid question answering .	125-146	125-146	We further demonstrate excellent domain transfer of discourse information , suggesting these discourse features have general utility to non-factoid question answering .	We further demonstrate excellent domain transfer of discourse information , suggesting these discourse features have general utility to non-factoid question answering .	1>2	none	exp-evidence	exp-evidence
P14-1092_anno1	135	136-146	suggesting	these discourse features have general utility to non-factoid question answering .	suggesting	these discourse features have general utility to non-factoid question answering .	125-146	125-146	We further demonstrate excellent domain transfer of discourse information , suggesting these discourse features have general utility to non-factoid question answering .	We further demonstrate excellent domain transfer of discourse information , suggesting these discourse features have general utility to non-factoid question answering .	1>2	none	attribution	attribution
P14-1092_anno1	44-57	136-146	We evaluate the proposed model on two corpora from different genres and domains :	these discourse features have general utility to non-factoid question answering .	We evaluate the proposed model on two corpora from different genres and domains :	these discourse features have general utility to non-factoid question answering .	44-80	125-146	We evaluate the proposed model on two corpora from different genres and domains : one from Yahoo ! Answers and one from the biology domain , and two types of non-factoid questions : manner and reason .	We further demonstrate excellent domain transfer of discourse information , suggesting these discourse features have general utility to non-factoid question answering .	1<2	none	elab-aspect	elab-aspect
P14-1093_anno1	1-5	6-17	We propose a supervised method	of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web	We propose a supervised method	of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web	1-31	1-31	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	6-17	18-20,25-31	of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web	using semantic relation <*> , context , and association features .	of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web	using semantic relation <*> , context , and association features .	1-31	1-31	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	1<2	none	manner-means	manner-means
P14-1093_anno1	18-20,25-31	21-24	using semantic relation <*> , context , and association features .	( between nouns )	using semantic relation <*> , context , and association features .	( between nouns )	1-31	1-31	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	32-33	34-38	Experiments show	that our method outperforms baselines	Experiments show	that our method outperforms baselines	32-45	32-45	Experiments show that our method outperforms baselines that are based on state-of-the-art methods .	Experiments show that our method outperforms baselines that are based on state-of-the-art methods .	1>2	none	attribution	attribution
P14-1093_anno1	1-5	34-38	We propose a supervised method	that our method outperforms baselines	We propose a supervised method	that our method outperforms baselines	1-31	32-45	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	Experiments show that our method outperforms baselines that are based on state-of-the-art methods .	1<2	none	evaluation	evaluation
P14-1093_anno1	34-38	39-45	that our method outperforms baselines	that are based on state-of-the-art methods .	that our method outperforms baselines	that are based on state-of-the-art methods .	32-45	32-45	Experiments show that our method outperforms baselines that are based on state-of-the-art methods .	Experiments show that our method outperforms baselines that are based on state-of-the-art methods .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	1-5	46-49	We propose a supervised method	We also propose methods	We propose a supervised method	We also propose methods	1-31	46-65	We propose a supervised method of extracting event causalities like conduct slash-and-burn agriculture→exacerbate desertification from the web using semantic relation ( between nouns ) , context , and association features .	We also propose methods of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	1<2	none	joint	joint
P14-1093_anno1	46-49	50-65	We also propose methods	of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	We also propose methods	of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	46-65	46-65	We also propose methods of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	We also propose methods of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	66-67	68-78	Experiments show	that we can generate 50,000 scenarios with 68 % precision .	Experiments show	that we can generate 50,000 scenarios with 68 % precision .	66-78	66-78	Experiments show that we can generate 50,000 scenarios with 68 % precision .	Experiments show that we can generate 50,000 scenarios with 68 % precision .	1>2	none	attribution	attribution
P14-1093_anno1	46-49	68-78	We also propose methods	that we can generate 50,000 scenarios with 68 % precision .	We also propose methods	that we can generate 50,000 scenarios with 68 % precision .	46-65	66-78	We also propose methods of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	Experiments show that we can generate 50,000 scenarios with 68 % precision .	1<2	none	evaluation	evaluation
P14-1093_anno1	46-49	79-93	We also propose methods	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	We also propose methods	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	46-65	79-108	We also propose methods of generating future scenarios like conduct slash-and-burn agriculture→exacerbate desertification→increase Asian dust (from China)→asthma gets worse .	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	1<2	none	elab-example	elab-example
P14-1093_anno1	79-93	94-104	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	which is written in no document in our input web corpus	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	which is written in no document in our input web corpus	79-108	79-108	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	94-104	105-108	which is written in no document in our input web corpus	crawled in 2007 .	which is written in no document in our input web corpus	crawled in 2007 .	79-108	79-108	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	1<2	none	elab-addition	elab-addition
P14-1093_anno1	79-93	109-112,117-126	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) ,	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	79-108	109-126	We also generated a scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) , which is written in no document in our input web corpus crawled in 2007 .	But the vibrio risk due to global warming was observed in Baker-Austin et al. ( 2013 ) .	1<2	none	evaluation	evaluation
P14-1093_anno1	109-112,117-126	113-116	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	due to global warming	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	due to global warming	109-126	109-126	But the vibrio risk due to global warming was observed in Baker-Austin et al. ( 2013 ) .	But the vibrio risk due to global warming was observed in Baker-Austin et al. ( 2013 ) .	1<2	none	exp-reason	exp-reason
P14-1093_anno1	109-112,117-126	127-139	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	Thus, we " predicted " the future event sequence in a sense .	But the vibrio risk <*> was observed in Baker-Austin et al. ( 2013 ) .	Thus, we " predicted " the future event sequence in a sense .	109-126	127-139	But the vibrio risk due to global warming was observed in Baker-Austin et al. ( 2013 ) .	Thus, we " predicted " the future event sequence in a sense .	1<2	none	summary	summary
P14-1094_anno1	1-11	22-32	Cross-narrative temporal ordering of medical events is essential to the task	We address the problem of aligning multiple medical event sequences ,	Cross-narrative temporal ordering of medical events is essential to the task	We address the problem of aligning multiple medical event sequences ,	1-21	22-85	Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient's history .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1>2	none	bg-goal	bg-goal
P14-1094_anno1	1-11	12-21	Cross-narrative temporal ordering of medical events is essential to the task	of generating a comprehensive timeline over a patient's history .	Cross-narrative temporal ordering of medical events is essential to the task	of generating a comprehensive timeline over a patient's history .	1-21	1-21	Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient's history .	Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient's history .	1<2	none	elab-addition	elab-addition
P14-1094_anno1	22-32	33-38	We address the problem of aligning multiple medical event sequences ,	corresponding to different clinical narratives ,	We address the problem of aligning multiple medical event sequences ,	corresponding to different clinical narratives ,	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	bg-general	bg-general
P14-1094_anno1	22-32	39-43	We address the problem of aligning multiple medical event sequences ,	comparing the following approaches :	We address the problem of aligning multiple medical event sequences ,	comparing the following approaches :	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	manner-means	manner-means
P14-1094_anno1	39-43	44-57	comparing the following approaches :	( 1 ) A novel weighted finite state transducer representation of medical event sequences	comparing the following approaches :	( 1 ) A novel weighted finite state transducer representation of medical event sequences	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	elab-enumember	elab-enumember
P14-1094_anno1	44-57	58-65	( 1 ) A novel weighted finite state transducer representation of medical event sequences	that enables composition and search for decoding ,	( 1 ) A novel weighted finite state transducer representation of medical event sequences	that enables composition and search for decoding ,	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	elab-addition	elab-addition
P14-1094_anno1	44-57	66-78	( 1 ) A novel weighted finite state transducer representation of medical event sequences	and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences	( 1 ) A novel weighted finite state transducer representation of medical event sequences	and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	joint	joint
P14-1094_anno1	66-78	79-85	and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences	using global and local alignment algorithms .	and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences	using global and local alignment algorithms .	22-85	22-85	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	1<2	none	manner-means	manner-means
P14-1094_anno1	39-43	86-92,98-106	comparing the following approaches :	The cross-narrative coreference and temporal relation weights <*> are learned from a corpus of clinical narratives .	comparing the following approaches :	The cross-narrative coreference and temporal relation weights <*> are learned from a corpus of clinical narratives .	22-85	86-106	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives .	1<2	none	elab-addition	elab-addition
P14-1094_anno1	86-92,98-106	93-97	The cross-narrative coreference and temporal relation weights <*> are learned from a corpus of clinical narratives .	used in both these approaches	The cross-narrative coreference and temporal relation weights <*> are learned from a corpus of clinical narratives .	used in both these approaches	86-106	86-106	The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives .	The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives .	1<2	none	elab-addition	elab-addition
P14-1094_anno1	39-43	107-109	comparing the following approaches :	We present results	comparing the following approaches :	We present results	22-85	107-138	We address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms .	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	1<2	none	cause	cause
P14-1094_anno1	107-109	110-112	We present results	using both approaches	We present results	using both approaches	107-138	107-138	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	1<2	none	manner-means	manner-means
P14-1094_anno1	113-114	115-138	and observe	that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	and observe	that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	107-138	107-138	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	1>2	none	attribution	attribution
P14-1094_anno1	107-109	115-138	We present results	that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	We present results	that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	107-138	107-138	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	We present results using both approaches and observe that the finite state transducer approach performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .	1<2	none	joint	joint
P14-1095_anno1	1-10	11-15	This paper introduces FactChecker , language-aware approach to truth-finding .	FactChecker differs from prior approaches	This paper introduces FactChecker , language-aware approach to truth-finding .	FactChecker differs from prior approaches	1-10	11-37	This paper introduces FactChecker , language-aware approach to truth-finding .	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	1<2	none	elab-aspect	elab-aspect
P14-1095_anno1	11-15	16-26	FactChecker differs from prior approaches	in that it does not rely on iterative peer voting ,	FactChecker differs from prior approaches	in that it does not rely on iterative peer voting ,	11-37	11-37	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	1<2	none	elab-addition	elab-addition
P14-1095_anno1	16-26	27-30	in that it does not rely on iterative peer voting ,	instead it leverages language	in that it does not rely on iterative peer voting ,	instead it leverages language	11-37	11-37	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	1<2	none	elab-addition	elab-addition
P14-1095_anno1	27-30	31-37	instead it leverages language	to infer believability of fact candidates .	instead it leverages language	to infer believability of fact candidates .	11-37	11-37	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	FactChecker differs from prior approaches in that it does not rely on iterative peer voting , instead it leverages language to infer believability of fact candidates .	1<2	none	enablement	enablement
P14-1095_anno1	1-10	38-46	This paper introduces FactChecker , language-aware approach to truth-finding .	In particular , FactChecker makes use of linguistic features	This paper introduces FactChecker , language-aware approach to truth-finding .	In particular , FactChecker makes use of linguistic features	1-10	38-61	This paper introduces FactChecker , language-aware approach to truth-finding .	In particular , FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated .	1<2	none	elab-aspect	elab-aspect
P14-1095_anno1	38-46	47-55	In particular , FactChecker makes use of linguistic features	to detect if a given source objectively states facts	In particular , FactChecker makes use of linguistic features	to detect if a given source objectively states facts	38-61	38-61	In particular , FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated .	In particular , FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated .	1<2	none	enablement	enablement
P14-1095_anno1	49-55	56-61	if a given source objectively states facts	or is speculative and opinionated .	if a given source objectively states facts	or is speculative and opinionated .	38-61	38-61	In particular , FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated .	In particular , FactChecker makes use of linguistic features to detect if a given source objectively states facts or is speculative and opinionated .	1<2	none	contrast	contrast
P14-1095_anno1	62-66,71-74	75-81	To ensure that fact candidates <*> have similar believability ,	FactChecker augments objectivity with a co-mention score	To ensure that fact candidates <*> have similar believability ,	FactChecker augments objectivity with a co-mention score	62-92	62-92	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	1>2	none	enablement	enablement
P14-1095_anno1	62-66,71-74	67-70	To ensure that fact candidates <*> have similar believability ,	mentioned in similar sources	To ensure that fact candidates <*> have similar believability ,	mentioned in similar sources	62-92	62-92	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	1<2	none	elab-addition	elab-addition
P14-1095_anno1	1-10	75-81	This paper introduces FactChecker , language-aware approach to truth-finding .	FactChecker augments objectivity with a co-mention score	This paper introduces FactChecker , language-aware approach to truth-finding .	FactChecker augments objectivity with a co-mention score	1-10	62-92	This paper introduces FactChecker , language-aware approach to truth-finding .	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	1<2	none	elab-aspect	elab-aspect
P14-1095_anno1	75-81	82-92	FactChecker augments objectivity with a co-mention score	to compute the overall believability score of a fact candidate .	FactChecker augments objectivity with a co-mention score	to compute the overall believability score of a fact candidate .	62-92	62-92	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	To ensure that fact candidates mentioned in similar sources have similar believability , FactChecker augments objectivity with a co-mention score to compute the overall believability score of a fact candidate .	1<2	none	enablement	enablement
P14-1095_anno1	93-98	99-107	Our experiments on various datasets show	that FactChecker yields higher accuracy than existing approaches .	Our experiments on various datasets show	that FactChecker yields higher accuracy than existing approaches .	93-107	93-107	Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches .	Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches .	1>2	none	attribution	attribution
P14-1095_anno1	1-10	99-107	This paper introduces FactChecker , language-aware approach to truth-finding .	that FactChecker yields higher accuracy than existing approaches .	This paper introduces FactChecker , language-aware approach to truth-finding .	that FactChecker yields higher accuracy than existing approaches .	1-10	93-107	This paper introduces FactChecker , language-aware approach to truth-finding .	Our experiments on various datasets show that FactChecker yields higher accuracy than existing approaches .	1<2	none	evaluation	evaluation
P14-1096_anno1	1-9	10-14	In this paper , we propose an unsupervised method	to identify noun sense changes	In this paper , we propose an unsupervised method	to identify noun sense changes	1-32	1-32	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	1<2	none	enablement	enablement
P14-1096_anno1	1-9	15-32	In this paper , we propose an unsupervised method	based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	In this paper , we propose an unsupervised method	based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	1-32	1-32	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	1<2	none	manner-means	manner-means
P14-1096_anno1	1-9	33-44	In this paper , we propose an unsupervised method	We construct distributional thesauri based networks from data at different time points	In this paper , we propose an unsupervised method	We construct distributional thesauri based networks from data at different time points	1-32	33-62	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	1<2	none	elab-process_step	elab-process_step
P14-1096_anno1	33-44	45-50	We construct distributional thesauri based networks from data at different time points	and cluster each of them separately	We construct distributional thesauri based networks from data at different time points	and cluster each of them separately	33-62	33-62	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	1<2	none	joint	joint
P14-1096_anno1	45-50	51-55	and cluster each of them separately	to obtain word-centric sense clusters	and cluster each of them separately	to obtain word-centric sense clusters	33-62	33-62	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	1<2	none	enablement	enablement
P14-1096_anno1	51-55	56-62	to obtain word-centric sense clusters	corresponding to the different time points .	to obtain word-centric sense clusters	corresponding to the different time points .	33-62	33-62	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points .	1<2	none	elab-addition	elab-addition
P14-1096_anno1	1-9	63-74	In this paper , we propose an unsupervised method	Subsequently , we compare these sense clusters of two different time points	In this paper , we propose an unsupervised method	Subsequently , we compare these sense clusters of two different time points	1-32	63-131	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1<2	none	elab-process_step	elab-process_step
P14-1096_anno1	75-76	77-88	to find	if ( i ) there is birth of a new sense or	to find	if ( i ) there is birth of a new sense or	63-131	63-131	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1>2	none	attribution	attribution
P14-1096_anno1	63-74	77-88	Subsequently , we compare these sense clusters of two different time points	if ( i ) there is birth of a new sense or	Subsequently , we compare these sense clusters of two different time points	if ( i ) there is birth of a new sense or	63-131	63-131	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1<2	none	enablement	enablement
P14-1096_anno1	77-88	89-104	if ( i ) there is birth of a new sense or	( ii ) if an older sense has got split into more than one sense or	if ( i ) there is birth of a new sense or	( ii ) if an older sense has got split into more than one sense or	63-131	63-131	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1<2	none	joint	joint
P14-1096_anno1	89-104	105-121	( ii ) if an older sense has got split into more than one sense or	( iii ) if a newer sense has been formed from the joining of older senses or	( ii ) if an older sense has got split into more than one sense or	( iii ) if a newer sense has been formed from the joining of older senses or	63-131	63-131	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1<2	none	joint	joint
P14-1096_anno1	105-121	122-131	( iii ) if a newer sense has been formed from the joining of older senses or	( iv ) if a particular sense has died .	( iii ) if a newer sense has been formed from the joining of older senses or	( iv ) if a particular sense has died .	63-131	63-131	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	Subsequently , we compare these sense clusters of two different time points to find if ( i ) there is birth of a new sense or ( ii ) if an older sense has got split into more than one sense or ( iii ) if a newer sense has been formed from the joining of older senses or ( iv ) if a particular sense has died .	1<2	none	joint	joint
P14-1096_anno1	1-9	132-150	In this paper , we propose an unsupervised method	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	In this paper , we propose an unsupervised method	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	1-32	132-150	In this paper , we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books .	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	1<2	none	evaluation	evaluation
P14-1096_anno1	151-153	154-185	Manual evaluation indicates	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Manual evaluation indicates	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	151-185	151-185	Manual evaluation indicates that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Manual evaluation indicates that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	1>2	none	attribution	attribution
P14-1096_anno1	132-150	154-185	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	132-150	151-185	We conduct a thorough evaluation of the proposed method-ology both manually as well as through comparison with WordNet .	Manual evaluation indicates that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	1<2	none	cause	cause
P14-1096_anno1	154-185	186-202	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet ,	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet ,	151-185	186-220	Manual evaluation indicates that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet , while in 46 % cases and 43 % cases split and join are respectively confirmed by WordNet .	1<2	none	progression	progression
P14-1096_anno1	186-202	203-220	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet ,	while in 46 % cases and 43 % cases split and join are respectively confirmed by WordNet .	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet ,	while in 46 % cases and 43 % cases split and join are respectively confirmed by WordNet .	186-220	186-220	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet , while in 46 % cases and 43 % cases split and join are respectively confirmed by WordNet .	Remarkably , in 44 % cases the birth of a novel sense is attested by WordNet , while in 46 % cases and 43 % cases split and join are respectively confirmed by WordNet .	1<2	none	joint	joint
P14-1096_anno1	154-185	221-241	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Our approach can be applied for lexicography , as well as for applications like word sense disambiguation or semantic search .	that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Our approach can be applied for lexicography , as well as for applications like word sense disambiguation or semantic search .	151-185	221-241	Manual evaluation indicates that the algorithm could correctly identify 60.4 % birth cases from a set of 48 randomly picked samples and 57 % split/join cases from a set of 21 randomly picked samples .	Our approach can be applied for lexicography , as well as for applications like word sense disambiguation or semantic search .	1<2	none	summary	summary
P14-1097_anno1	1-5	6-16	We present an unsupervised method	for inducing verb classes from verb uses in giga-word corpora .	We present an unsupervised method	for inducing verb classes from verb uses in giga-word corpora .	1-16	1-16	We present an unsupervised method for inducing verb classes from verb uses in giga-word corpora .	We present an unsupervised method for inducing verb classes from verb uses in giga-word corpora .	1<2	none	enablement	enablement
P14-1097_anno1	1-5	17-24	We present an unsupervised method	Our method consists of two clustering steps :	We present an unsupervised method	Our method consists of two clustering steps :	1-16	17-48	We present an unsupervised method for inducing verb classes from verb uses in giga-word corpora .	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	1<2	none	elab-aspect	elab-aspect
P14-1097_anno1	17-24	25-30	Our method consists of two clustering steps :	verb-specific semantic frames are first induced	Our method consists of two clustering steps :	verb-specific semantic frames are first induced	17-48	17-48	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	1<2	none	elab-process_step	elab-process_step
P14-1097_anno1	25-30	31-37	verb-specific semantic frames are first induced	by clustering verb uses in a corpus	verb-specific semantic frames are first induced	by clustering verb uses in a corpus	17-48	17-48	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	1<2	none	manner-means	manner-means
P14-1097_anno1	17-24	38-43	Our method consists of two clustering steps :	and then verb classes are induced	Our method consists of two clustering steps :	and then verb classes are induced	17-48	17-48	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	1<2	none	elab-process_step	elab-process_step
P14-1097_anno1	38-43	44-48	and then verb classes are induced	by clustering these frames .	and then verb classes are induced	by clustering these frames .	17-48	17-48	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	Our method consists of two clustering steps : verb-specific semantic frames are first induced by clustering verb uses in a corpus and then verb classes are induced by clustering these frames .	1<2	none	manner-means	manner-means
P14-1097_anno1	49-54	55-61	By taking this step-wise approach ,	we can not only generate verb classes	By taking this step-wise approach ,	we can not only generate verb classes	49-94	49-94	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	1>2	none	manner-means	manner-means
P14-1097_anno1	1-5	55-61	We present an unsupervised method	we can not only generate verb classes	We present an unsupervised method	we can not only generate verb classes	1-16	49-94	We present an unsupervised method for inducing verb classes from verb uses in giga-word corpora .	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	1<2	none	elab-aspect	elab-aspect
P14-1097_anno1	55-61	62-74	we can not only generate verb classes	based on a massive amount of verb uses in a scalable manner ,	we can not only generate verb classes	based on a massive amount of verb uses in a scalable manner ,	49-94	49-94	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	1<2	none	bg-general	bg-general
P14-1097_anno1	55-61	75-81	we can not only generate verb classes	but also deal with verb polysemy ,	we can not only generate verb classes	but also deal with verb polysemy ,	49-94	49-94	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	1<2	none	progression	progression
P14-1097_anno1	75-81	82-94	but also deal with verb polysemy ,	which is bypassed by most of the previous studies on verb clustering .	but also deal with verb polysemy ,	which is bypassed by most of the previous studies on verb clustering .	49-94	49-94	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	By taking this step-wise approach , we can not only generate verb classes based on a massive amount of verb uses in a scalable manner , but also deal with verb polysemy , which is bypassed by most of the previous studies on verb clustering .	1<2	none	elab-addition	elab-addition
P14-1097_anno1	95-112	118-127	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger	The effectiveness of our approach is verified through quantitative evaluations	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger	The effectiveness of our approach is verified through quantitative evaluations	95-117	118-133	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger comprising 20 billion words .	The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data .	1>2	none	bg-general	bg-general
P14-1097_anno1	95-112	113-117	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger	comprising 20 billion words .	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger	comprising 20 billion words .	95-117	95-117	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger comprising 20 billion words .	In our experiments , we acquire semantic frames and verb classes from two giga-word corpora , the larger comprising 20 billion words .	1<2	none	elab-addition	elab-addition
P14-1097_anno1	1-5	118-127	We present an unsupervised method	The effectiveness of our approach is verified through quantitative evaluations	We present an unsupervised method	The effectiveness of our approach is verified through quantitative evaluations	1-16	118-133	We present an unsupervised method for inducing verb classes from verb uses in giga-word corpora .	The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data .	1<2	none	evaluation	evaluation
P14-1097_anno1	118-127	128-133	The effectiveness of our approach is verified through quantitative evaluations	based on polysemy-aware gold-standard data .	The effectiveness of our approach is verified through quantitative evaluations	based on polysemy-aware gold-standard data .	118-133	118-133	The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data .	The effectiveness of our approach is verified through quantitative evaluations based on polysemy-aware gold-standard data .	1<2	none	bg-general	bg-general
P14-1098_anno1	1-10	11-17	We present a structured learning approach to inducing hypernym taxonomies	using a probabilistic graphical model formulation .	We present a structured learning approach to inducing hypernym taxonomies	using a probabilistic graphical model formulation .	1-17	1-17	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	1<2	none	manner-means	manner-means
P14-1098_anno1	1-10	18-29	We present a structured learning approach to inducing hypernym taxonomies	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood ,	We present a structured learning approach to inducing hypernym taxonomies	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood ,	1-17	18-45	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood , captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts .	1<2	none	elab-aspect	elab-aspect
P14-1098_anno1	18-29	30-33	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood ,	captured by semantic features	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood ,	captured by semantic features	18-45	18-45	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood , captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts .	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood , captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts .	1<2	none	elab-addition	elab-addition
P14-1098_anno1	30-33	34-45	captured by semantic features	based on patterns and statistics from Web n-grams and Wikipedia abstracts .	captured by semantic features	based on patterns and statistics from Web n-grams and Wikipedia abstracts .	18-45	18-45	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood , captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts .	Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood , captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts .	1<2	none	bg-general	bg-general
P14-1098_anno1	1-10	46-70	We present a structured learning approach to inducing hypernym taxonomies	For efficient inference over taxonomy structures , we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor .	We present a structured learning approach to inducing hypernym taxonomies	For efficient inference over taxonomy structures , we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor .	1-17	46-70	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	For efficient inference over taxonomy structures , we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor .	1<2	none	elab-aspect	elab-aspect
P14-1098_anno1	71-75	76-80	To train the system ,	we extract sub-structures of WordNet	To train the system ,	we extract sub-structures of WordNet	71-93	71-93	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	1>2	none	enablement	enablement
P14-1098_anno1	1-10	76-80	We present a structured learning approach to inducing hypernym taxonomies	we extract sub-structures of WordNet	We present a structured learning approach to inducing hypernym taxonomies	we extract sub-structures of WordNet	1-17	71-93	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	1<2	none	elab-aspect	elab-aspect
P14-1098_anno1	76-80	81-87	we extract sub-structures of WordNet	and discriminatively learn to reproduce them ,	we extract sub-structures of WordNet	and discriminatively learn to reproduce them ,	71-93	71-93	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	1<2	none	joint	joint
P14-1098_anno1	81-87	88-93	and discriminatively learn to reproduce them ,	using adaptive subgradient stochastic optimization .	and discriminatively learn to reproduce them ,	using adaptive subgradient stochastic optimization .	71-93	71-93	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	To train the system , we extract sub-structures of WordNet and discriminatively learn to reproduce them , using adaptive subgradient stochastic optimization .	1<2	none	manner-means	manner-means
P14-1098_anno1	94-96	97-102	On the task	of reproducing sub-hierarchies of WordNet ,	On the task	of reproducing sub-hierarchies of WordNet ,	94-128	94-128	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	1<2	none	elab-addition	elab-addition
P14-1098_anno1	1-10	94-96,103-115	We present a structured learning approach to inducing hypernym taxonomies	<*> On the task <*> our approach achieves a 51 % error reduction over a chance baseline ,	We present a structured learning approach to inducing hypernym taxonomies	On the task <*> our approach achieves a 51 % error reduction over a chance baseline ,	1-17	94-128	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	1<2	none	evaluation	evaluation
P14-1098_anno1	94-96,103-115	116-121	<*> On the task <*> our approach achieves a 51 % error reduction over a chance baseline ,	including a 15 % error reduction	On the task <*> our approach achieves a 51 % error reduction over a chance baseline ,	including a 15 % error reduction	94-128	94-128	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	1<2	none	elab-addition	elab-addition
P14-1098_anno1	116-121	122-128	including a 15 % error reduction	due to the non-hypernym-factored sibling features .	including a 15 % error reduction	due to the non-hypernym-factored sibling features .	94-128	94-128	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	On the task of reproducing sub-hierarchies of WordNet , our approach achieves a 51 % error reduction over a chance baseline , including a 15 % error reduction due to the non-hypernym-factored sibling features .	1<2	none	exp-reason	exp-reason
P14-1098_anno1	1-10	129-149	We present a structured learning approach to inducing hypernym taxonomies	On a comparison setup , we find up to 29 % relative error reduction over previous work on ancestor F1 .	We present a structured learning approach to inducing hypernym taxonomies	On a comparison setup , we find up to 29 % relative error reduction over previous work on ancestor F1 .	1-17	129-149	We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation .	On a comparison setup , we find up to 29 % relative error reduction over previous work on ancestor F1 .	1<2	none	evaluation	evaluation
P14-1099_anno1	1-11	12-18	We introduce a provably correct learning algorithm for latent-variable PCFGs .	The algorithm relies on two steps :	We introduce a provably correct learning algorithm for latent-variable PCFGs .	The algorithm relies on two steps :	1-11	12-67	We introduce a provably correct learning algorithm for latent-variable PCFGs .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-aspect	elab-aspect
P14-1099_anno1	12-18	19-26	The algorithm relies on two steps :	first , the use of a matrix-decomposition algorithm	The algorithm relies on two steps :	first , the use of a matrix-decomposition algorithm	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-process_step	elab-process_step
P14-1099_anno1	19-26	27-31	first , the use of a matrix-decomposition algorithm	applied to a co-occurrence matrix	first , the use of a matrix-decomposition algorithm	applied to a co-occurrence matrix	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-addition	elab-addition
P14-1099_anno1	27-31	32-41	applied to a co-occurrence matrix	estimated from the parse trees in a training sample ;	applied to a co-occurrence matrix	estimated from the parse trees in a training sample ;	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-addition	elab-addition
P14-1099_anno1	12-18	42-47	The algorithm relies on two steps :	second , the use of EM	The algorithm relies on two steps :	second , the use of EM	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-process_step	elab-process_step
P14-1099_anno1	42-47	48-52	second , the use of EM	applied to a convex objective	second , the use of EM	applied to a convex objective	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-addition	elab-addition
P14-1099_anno1	48-52	53-67	applied to a convex objective	derived from the training samples in combination with the output from the matrix decomposition .	applied to a convex objective	derived from the training samples in combination with the output from the matrix decomposition .	12-67	12-67	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	The algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition .	1<2	none	elab-addition	elab-addition
P14-1099_anno1	68-76	77-86	Experiments on parsing and a language modeling problem show	that the algorithm is efficient and effective in practice .	Experiments on parsing and a language modeling problem show	that the algorithm is efficient and effective in practice .	68-86	68-86	Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .	Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .	1>2	none	attribution	attribution
P14-1099_anno1	1-11	77-86	We introduce a provably correct learning algorithm for latent-variable PCFGs .	that the algorithm is efficient and effective in practice .	We introduce a provably correct learning algorithm for latent-variable PCFGs .	that the algorithm is efficient and effective in practice .	1-11	68-86	We introduce a provably correct learning algorithm for latent-variable PCFGs .	Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .	1<2	none	evaluation	evaluation
P14-1100_anno1	1-9	10-19	We propose a spectral approach for unsupervised constituent parsing	that comes with theoretical guarantees on latent structure recovery .	We propose a spectral approach for unsupervised constituent parsing	that comes with theoretical guarantees on latent structure recovery .	1-19	1-19	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	1<2	none	elab-addition	elab-addition
P14-1100_anno1	1-9	20-24	We propose a spectral approach for unsupervised constituent parsing	Our approach is grammar-less -	We propose a spectral approach for unsupervised constituent parsing	Our approach is grammar-less -	1-19	20-40	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	Our approach is grammar-less - we directly learn the bracketing structure of a given sentence without using a grammar model .	1<2	none	elab-aspect	elab-aspect
P14-1100_anno1	20-24	25-34	Our approach is grammar-less -	we directly learn the bracketing structure of a given sentence	Our approach is grammar-less -	we directly learn the bracketing structure of a given sentence	20-40	20-40	Our approach is grammar-less - we directly learn the bracketing structure of a given sentence without using a grammar model .	Our approach is grammar-less - we directly learn the bracketing structure of a given sentence without using a grammar model .	1<2	none	exp-reason	exp-reason
P14-1100_anno1	25-34	35-40	we directly learn the bracketing structure of a given sentence	without using a grammar model .	we directly learn the bracketing structure of a given sentence	without using a grammar model .	20-40	20-40	Our approach is grammar-less - we directly learn the bracketing structure of a given sentence without using a grammar model .	Our approach is grammar-less - we directly learn the bracketing structure of a given sentence without using a grammar model .	1<2	none	manner-means	manner-means
P14-1100_anno1	1-9	41-69	We propose a spectral approach for unsupervised constituent parsing	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case	We propose a spectral approach for unsupervised constituent parsing	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case	1-19	41-77	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples .	1<2	none	elab-aspect	elab-aspect
P14-1100_anno1	41-69	70-77	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case	where the tree structure varies across examples .	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case	where the tree structure varies across examples .	41-77	41-77	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples .	The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples .	1<2	none	elab-addition	elab-addition
P14-1100_anno1	78-88	97-101	Although finding the "minimal" latent tree is NP-hard in general ,	that it can be found	Although finding the "minimal" latent tree is NP-hard in general ,	that it can be found	78-106	78-106	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	1>2	none	contrast	contrast
P14-1100_anno1	89-96	97-101	for the case of projective trees we find	that it can be found	for the case of projective trees we find	that it can be found	78-106	78-106	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	1>2	none	attribution	attribution
P14-1100_anno1	1-9	97-101	We propose a spectral approach for unsupervised constituent parsing	that it can be found	We propose a spectral approach for unsupervised constituent parsing	that it can be found	1-19	78-106	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	1<2	none	elab-aspect	elab-aspect
P14-1100_anno1	97-101	102-106	that it can be found	using bilexical parsing algorithms .	that it can be found	using bilexical parsing algorithms .	78-106	78-106	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	Although finding the "minimal" latent tree is NP-hard in general , for the case of projective trees we find that it can be found using bilexical parsing algorithms .	1<2	none	manner-means	manner-means
P14-1100_anno1	1-9	107-112	We propose a spectral approach for unsupervised constituent parsing	Empirically , our algorithm performs favorably	We propose a spectral approach for unsupervised constituent parsing	Empirically , our algorithm performs favorably	1-19	107-132	We propose a spectral approach for unsupervised constituent parsing that comes with theoretical guarantees on latent structure recovery .	Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .	1<2	none	evaluation	evaluation
P14-1100_anno1	107-112	113-125	Empirically , our algorithm performs favorably	compared to the constituent context model of Klein and Manning ( 2002 )	Empirically , our algorithm performs favorably	compared to the constituent context model of Klein and Manning ( 2002 )	107-132	107-132	Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .	Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .	1<2	none	comparison	comparison
P14-1100_anno1	107-112	126-132	Empirically , our algorithm performs favorably	without the need for careful initialization .	Empirically , our algorithm performs favorably	without the need for careful initialization .	107-132	107-132	Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .	Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning ( 2002 ) without the need for careful initialization .	1<2	none	elab-addition	elab-addition
P16-1001_anno1	1-10	110-115	Semantic parsers map natural language statements into meaning representations ,	Imitation learning algorithms have been shown	Semantic parsers map natural language statements into meaning representations ,	Imitation learning algorithms have been shown	1-29	110-124	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Imitation learning algorithms have been shown to help these systems recover from such errors .	1>2	none	bg-goal	bg-goal
P16-1001_anno1	1-10	11-17	Semantic parsers map natural language statements into meaning representations ,	and must abstract over syntactic phenomena ,	Semantic parsers map natural language statements into meaning representations ,	and must abstract over syntactic phenomena ,	1-29	1-29	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	1<2	none	progression	progression
P16-1001_anno1	11-17	18-20	and must abstract over syntactic phenomena ,	resolve anaphora ,	and must abstract over syntactic phenomena ,	resolve anaphora ,	1-29	1-29	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	1<2	none	joint	joint
P16-1001_anno1	11-17	21-24	and must abstract over syntactic phenomena ,	and identify word senses	and must abstract over syntactic phenomena ,	and identify word senses	1-29	1-29	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	1<2	none	joint	joint
P16-1001_anno1	11-17	25-29	and must abstract over syntactic phenomena ,	to eliminate ambiguous interpretations .	and must abstract over syntactic phenomena ,	to eliminate ambiguous interpretations .	1-29	1-29	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	1<2	none	enablement	enablement
P16-1001_anno1	1-10	30-44	Semantic parsers map natural language statements into meaning representations ,	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism	Semantic parsers map natural language statements into meaning representations ,	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism	1-29	30-68	Semantic parsers map natural language statements into meaning representations , and must abstract over syntactic phenomena , resolve anaphora , and identify word senses to eliminate ambiguous interpretations .	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism which , similar to a dependency parse , utilizes a graph to represent relationships between concepts ( Banarescu et al. , 2013 ) .	1<2	none	elab-example	elab-example
P16-1001_anno1	30-44	45-55	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism	which , similar to a dependency parse , utilizes a graph	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism	which , similar to a dependency parse , utilizes a graph	30-68	30-68	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism which , similar to a dependency parse , utilizes a graph to represent relationships between concepts ( Banarescu et al. , 2013 ) .	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism which , similar to a dependency parse , utilizes a graph to represent relationships between concepts ( Banarescu et al. , 2013 ) .	1<2	none	elab-addition	elab-addition
P16-1001_anno1	45-55	56-68	which , similar to a dependency parse , utilizes a graph	to represent relationships between concepts ( Banarescu et al. , 2013 ) .	which , similar to a dependency parse , utilizes a graph	to represent relationships between concepts ( Banarescu et al. , 2013 ) .	30-68	30-68	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism which , similar to a dependency parse , utilizes a graph to represent relationships between concepts ( Banarescu et al. , 2013 ) .	Abstract meaning representation ( AMR ) is a recent example of one such semantic formalism which , similar to a dependency parse , utilizes a graph to represent relationships between concepts ( Banarescu et al. , 2013 ) .	1<2	none	enablement	enablement
P16-1001_anno1	69-73	74-83	As with dependency parsing ,	transition-based approaches are a common approach to this problem .	As with dependency parsing ,	transition-based approaches are a common approach to this problem .	69-83	69-83	As with dependency parsing , transition-based approaches are a common approach to this problem .	As with dependency parsing , transition-based approaches are a common approach to this problem .	1>2	none	manner-means	manner-means
P16-1001_anno1	74-83	84-91	transition-based approaches are a common approach to this problem .	However , when trained in the traditional manner	transition-based approaches are a common approach to this problem .	However , when trained in the traditional manner	69-83	84-109	As with dependency parsing , transition-based approaches are a common approach to this problem .	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	1>2	none	contrast	contrast
P16-1001_anno1	84-91	92-100	However , when trained in the traditional manner	these systems are susceptible to the accumulation of errors	However , when trained in the traditional manner	these systems are susceptible to the accumulation of errors	84-109	84-109	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	1>2	none	condition	condition
P16-1001_anno1	92-100	110-115	these systems are susceptible to the accumulation of errors	Imitation learning algorithms have been shown	these systems are susceptible to the accumulation of errors	Imitation learning algorithms have been shown	84-109	110-124	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	Imitation learning algorithms have been shown to help these systems recover from such errors .	1>2	none	bg-compare	bg-compare
P16-1001_anno1	92-100	101-109	these systems are susceptible to the accumulation of errors	when they find undesirable states during greedy decoding .	these systems are susceptible to the accumulation of errors	when they find undesirable states during greedy decoding .	84-109	84-109	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	However , when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding .	1<2	none	condition	condition
P16-1001_anno1	110-115	116-124	Imitation learning algorithms have been shown	to help these systems recover from such errors .	Imitation learning algorithms have been shown	to help these systems recover from such errors .	110-124	110-124	Imitation learning algorithms have been shown to help these systems recover from such errors .	Imitation learning algorithms have been shown to help these systems recover from such errors .	1<2	none	enablement	enablement
P16-1001_anno1	125-132	133-143	To effectively use these methods for AMR parsing	we find it highly beneficial to introduce two novel extensions :	To effectively use these methods for AMR parsing	we find it highly beneficial to introduce two novel extensions :	125-149	125-149	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	1>2	none	enablement	enablement
P16-1001_anno1	110-115	133-143	Imitation learning algorithms have been shown	we find it highly beneficial to introduce two novel extensions :	Imitation learning algorithms have been shown	we find it highly beneficial to introduce two novel extensions :	110-124	125-149	Imitation learning algorithms have been shown to help these systems recover from such errors .	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	1<2	none	elab-addition	elab-addition
P16-1001_anno1	133-143	144-149	we find it highly beneficial to introduce two novel extensions :	noise reduction and targeted exploration .	we find it highly beneficial to introduce two novel extensions :	noise reduction and targeted exploration .	125-149	125-149	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	1<2	none	elab-enumember	elab-enumember
P16-1001_anno1	144-149	150-159	noise reduction and targeted exploration .	The former mitigates the noise in the feature representation ,	noise reduction and targeted exploration .	The former mitigates the noise in the feature representation ,	125-149	150-168	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	The former mitigates the noise in the feature representation , a result of the complexity of the task .	1<2	none	elab-aspect	elab-aspect
P16-1001_anno1	150-159	160-168	The former mitigates the noise in the feature representation ,	a result of the complexity of the task .	The former mitigates the noise in the feature representation ,	a result of the complexity of the task .	150-168	150-168	The former mitigates the noise in the feature representation , a result of the complexity of the task .	The former mitigates the noise in the feature representation , a result of the complexity of the task .	1<2	none	elab-addition	elab-addition
P16-1001_anno1	144-149	169-179	noise reduction and targeted exploration .	The latter targets the exploration steps of imitation learning towards areas	noise reduction and targeted exploration .	The latter targets the exploration steps of imitation learning towards areas	125-149	169-195	To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions : noise reduction and targeted exploration .	The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space .	1<2	none	elab-aspect	elab-aspect
P16-1001_anno1	169-179	180-195	The latter targets the exploration steps of imitation learning towards areas	which are likely to provide the most information in the context of a large action-space .	The latter targets the exploration steps of imitation learning towards areas	which are likely to provide the most information in the context of a large action-space .	169-195	169-195	The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space .	The latter targets the exploration steps of imitation learning towards areas which are likely to provide the most information in the context of a large action-space .	1<2	none	elab-addition	elab-addition
P16-1001_anno1	110-115	196-201	Imitation learning algorithms have been shown	We achieve state-ofthe art results ,	Imitation learning algorithms have been shown	We achieve state-ofthe art results ,	110-124	196-212	Imitation learning algorithms have been shown to help these systems recover from such errors .	We achieve state-ofthe art results , and improve upon standard transition-based parsing by 4.7 F1 points .	1<2	none	evaluation	evaluation
P16-1001_anno1	196-201	202-212	We achieve state-ofthe art results ,	and improve upon standard transition-based parsing by 4.7 F1 points .	We achieve state-ofthe art results ,	and improve upon standard transition-based parsing by 4.7 F1 points .	196-212	196-212	We achieve state-ofthe art results , and improve upon standard transition-based parsing by 4.7 F1 points .	We achieve state-ofthe art results , and improve upon standard transition-based parsing by 4.7 F1 points .	1<2	none	joint	joint
P16-1002_anno1	1-10	27-35	Modeling crisp logical regularities is crucial in semantic parsing ,	In this paper , we introduce data recombination ,	Modeling crisp logical regularities is crucial in semantic parsing ,	In this paper , we introduce data recombination ,	1-26	27-47	Modeling crisp logical regularities is crucial in semantic parsing , making it difficult for neural models with no task-specific prior knowledge to achieve good results .	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	1>2	none	bg-goal	bg-goal
P16-1002_anno1	1-10	11-26	Modeling crisp logical regularities is crucial in semantic parsing ,	making it difficult for neural models with no task-specific prior knowledge to achieve good results .	Modeling crisp logical regularities is crucial in semantic parsing ,	making it difficult for neural models with no task-specific prior knowledge to achieve good results .	1-26	1-26	Modeling crisp logical regularities is crucial in semantic parsing , making it difficult for neural models with no task-specific prior knowledge to achieve good results .	Modeling crisp logical regularities is crucial in semantic parsing , making it difficult for neural models with no task-specific prior knowledge to achieve good results .	1<2	none	elab-addition	elab-addition
P16-1002_anno1	27-35	36-38	In this paper , we introduce data recombination ,	a novel framework	In this paper , we introduce data recombination ,	a novel framework	27-47	27-47	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	1<2	none	elab-definition	elab-definition
P16-1002_anno1	36-38	39-47	a novel framework	for injecting such prior knowledge into a model .	a novel framework	for injecting such prior knowledge into a model .	27-47	27-47	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	1<2	none	elab-addition	elab-addition
P16-1002_anno1	27-35	48-60	In this paper , we introduce data recombination ,	From the training data , we induce a highprecision synchronous context-free grammar ,	In this paper , we introduce data recombination ,	From the training data , we induce a highprecision synchronous context-free grammar ,	27-47	48-72	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	From the training data , we induce a highprecision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing .	1<2	none	elab-process_step	elab-process_step
P16-1002_anno1	48-60	61-66	From the training data , we induce a highprecision synchronous context-free grammar ,	which captures important conditional independence properties	From the training data , we induce a highprecision synchronous context-free grammar ,	which captures important conditional independence properties	48-72	48-72	From the training data , we induce a highprecision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing .	From the training data , we induce a highprecision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing .	1<2	none	elab-addition	elab-addition
P16-1002_anno1	61-66	67-72	which captures important conditional independence properties	commonly found in semantic parsing .	which captures important conditional independence properties	commonly found in semantic parsing .	48-72	48-72	From the training data , we induce a highprecision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing .	From the training data , we induce a highprecision synchronous context-free grammar , which captures important conditional independence properties commonly found in semantic parsing .	1<2	none	elab-addition	elab-addition
P16-1002_anno1	27-35	73-91	In this paper , we introduce data recombination ,	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	In this paper , we introduce data recombination ,	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	27-47	73-105	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints sampled from this grammar , thereby teaching the model about these structural properties .	1<2	none	elab-process_step	elab-process_step
P16-1002_anno1	73-91	92-96	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	sampled from this grammar ,	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	sampled from this grammar ,	73-105	73-105	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints sampled from this grammar , thereby teaching the model about these structural properties .	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints sampled from this grammar , thereby teaching the model about these structural properties .	1<2	none	elab-addition	elab-addition
P16-1002_anno1	73-91	97-105	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	thereby teaching the model about these structural properties .	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints	thereby teaching the model about these structural properties .	73-105	73-105	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints sampled from this grammar , thereby teaching the model about these structural properties .	We then train a sequence-to-sequence recurrent network ( RNN ) model with a novel attention-based copying mechanism on datapoints sampled from this grammar , thereby teaching the model about these structural properties .	1<2	none	progression	progression
P16-1002_anno1	27-35	106-120	In this paper , we introduce data recombination ,	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets ,	In this paper , we introduce data recombination ,	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets ,	27-47	106-136	In this paper , we introduce data recombination , a novel framework for injecting such prior knowledge into a model .	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets , leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision .	1<2	none	evaluation	evaluation
P16-1002_anno1	106-120	121-136	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets ,	leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision .	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets ,	leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision .	106-136	106-136	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets , leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision .	Data recombination improves the accuracy of our RNN model on three semantic parsing datasets , leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	1-15	45-59	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those	In this paper , we consider a much more expressive class of logical forms ,	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those	In this paper , we consider a much more expressive class of logical forms ,	1-25	45-77	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those that yield the correct denotation-from a combinatorially large space .	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	1>2	none	bg-goal	bg-goal
P16-1003_anno1	1-15	16-25	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those	that yield the correct denotation-from a combinatorially large space .	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those	that yield the correct denotation-from a combinatorially large space .	1-25	1-25	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those that yield the correct denotation-from a combinatorially large space .	A core problem in learning semantic parsers from denotations is picking out consistent logical forms-those that yield the correct denotation-from a combinatorially large space .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	26-31	32-40	To control the search space ,	previous work relied on restricted set of rules ,	To control the search space ,	previous work relied on restricted set of rules ,	26-44	26-44	To control the search space , previous work relied on restricted set of rules , which limits expressivity .	To control the search space , previous work relied on restricted set of rules , which limits expressivity .	1>2	none	enablement	enablement
P16-1003_anno1	32-40	45-59	previous work relied on restricted set of rules ,	In this paper , we consider a much more expressive class of logical forms ,	previous work relied on restricted set of rules ,	In this paper , we consider a much more expressive class of logical forms ,	26-44	45-77	To control the search space , previous work relied on restricted set of rules , which limits expressivity .	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	1>2	none	bg-compare	bg-compare
P16-1003_anno1	32-40	41-44	previous work relied on restricted set of rules ,	which limits expressivity .	previous work relied on restricted set of rules ,	which limits expressivity .	26-44	26-44	To control the search space , previous work relied on restricted set of rules , which limits expressivity .	To control the search space , previous work relied on restricted set of rules , which limits expressivity .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	60-61	62-66	and show	how to use dynamic programming	and show	how to use dynamic programming	45-77	45-77	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	1>2	none	attribution	attribution
P16-1003_anno1	45-59	62-66	In this paper , we consider a much more expressive class of logical forms ,	how to use dynamic programming	In this paper , we consider a much more expressive class of logical forms ,	how to use dynamic programming	45-77	45-77	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	1<2	none	joint	joint
P16-1003_anno1	62-66	67-77	how to use dynamic programming	to efficiently represent the complete set of consistent logical forms .	how to use dynamic programming	to efficiently represent the complete set of consistent logical forms .	45-77	45-77	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	1<2	none	enablement	enablement
P16-1003_anno1	45-59	78-85	In this paper , we consider a much more expressive class of logical forms ,	Expressivity also introduces many more spurious logical forms	In this paper , we consider a much more expressive class of logical forms ,	Expressivity also introduces many more spurious logical forms	45-77	78-102	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	78-85	86-92	Expressivity also introduces many more spurious logical forms	which are consistent with the correct denotation	Expressivity also introduces many more spurious logical forms	which are consistent with the correct denotation	78-102	78-102	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	86-92	93-102	which are consistent with the correct denotation	but do not represent the meaning of the utterance .	which are consistent with the correct denotation	but do not represent the meaning of the utterance .	78-102	78-102	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	1<2	none	contrast	contrast
P16-1003_anno1	103-106	107-110	To address this ,	we generate fictitious worlds	To address this ,	we generate fictitious worlds	103-124	103-124	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	1>2	none	enablement	enablement
P16-1003_anno1	78-85	107-110	Expressivity also introduces many more spurious logical forms	we generate fictitious worlds	Expressivity also introduces many more spurious logical forms	we generate fictitious worlds	78-102	103-124	Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance .	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	1<2	none	elab-addition	elab-addition
P16-1003_anno1	107-110	111-117	we generate fictitious worlds	and use crowdsourced denotations on these worlds	we generate fictitious worlds	and use crowdsourced denotations on these worlds	103-124	103-124	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	1<2	none	joint	joint
P16-1003_anno1	111-117	118-124	and use crowdsourced denotations on these worlds	to filter out spurious logical forms .	and use crowdsourced denotations on these worlds	to filter out spurious logical forms .	103-124	103-124	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	To address this , we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms .	1<2	none	enablement	enablement
P16-1003_anno1	45-59	125-143	In this paper , we consider a much more expressive class of logical forms ,	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % ,	In this paper , we consider a much more expressive class of logical forms ,	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % ,	45-77	125-159	In this paper , we consider a much more expressive class of logical forms , and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms .	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % , and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .	1<2	none	evaluation	evaluation
P16-1003_anno1	125-143	144-159	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % ,	and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % ,	and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .	125-159	125-159	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % , and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .	On the WIKITABLEQUESTIONS dataset , we increase the coverage of answerable questions from 53.5 % to 76 % , and the additional crowdsourced supervision lets us rule out 92.1 % of spurious logical forms .	1<2	none	joint	joint
P16-1004_anno1	1-13	33-40	Semantic parsing aims at mapping natural language to machine interpretable meaning representations .	In this paper we present a general method	Semantic parsing aims at mapping natural language to machine interpretable meaning representations .	In this paper we present a general method	1-13	33-47	Semantic parsing aims at mapping natural language to machine interpretable meaning representations .	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	1>2	none	bg-goal	bg-goal
P16-1004_anno1	14-26	33-40	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features	In this paper we present a general method	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features	In this paper we present a general method	14-32	33-47	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features which are either domainor representation-specific .	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	1>2	none	bg-compare	bg-compare
P16-1004_anno1	14-26	27-32	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features	which are either domainor representation-specific .	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features	which are either domainor representation-specific .	14-32	14-32	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features which are either domainor representation-specific .	Traditional approaches rely on high-quality lexicons , manually-built templates , and linguistic features which are either domainor representation-specific .	1<2	none	elab-addition	elab-addition
P16-1004_anno1	33-40	41-47	In this paper we present a general method	based on an attention-enhanced encoder-decoder model .	In this paper we present a general method	based on an attention-enhanced encoder-decoder model .	33-47	33-47	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	1<2	none	bg-general	bg-general
P16-1004_anno1	33-40	48-55	In this paper we present a general method	We encode input utterances into vector representations ,	In this paper we present a general method	We encode input utterances into vector representations ,	33-47	48-72	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	We encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors .	1<2	none	elab-addition	elab-addition
P16-1004_anno1	48-55	56-60	We encode input utterances into vector representations ,	and generate their logical forms	We encode input utterances into vector representations ,	and generate their logical forms	48-72	48-72	We encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors .	We encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors .	1<2	none	progression	progression
P16-1004_anno1	56-60	61-72	and generate their logical forms	by conditioning the output sequences or trees on the encoding vectors .	and generate their logical forms	by conditioning the output sequences or trees on the encoding vectors .	48-72	48-72	We encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors .	We encode input utterances into vector representations , and generate their logical forms by conditioning the output sequences or trees on the encoding vectors .	1<2	none	manner-means	manner-means
P16-1004_anno1	73-78	79-83	Experimental results on four atasets show	that our approach performs competitively	Experimental results on four atasets show	that our approach performs competitively	73-98	73-98	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	1>2	none	attribution	attribution
P16-1004_anno1	33-40	79-83	In this paper we present a general method	that our approach performs competitively	In this paper we present a general method	that our approach performs competitively	33-47	73-98	In this paper we present a general method based on an attention-enhanced encoder-decoder model .	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	1<2	none	evaluation	evaluation
P16-1004_anno1	79-83	84-87	that our approach performs competitively	without using hand-engineered features	that our approach performs competitively	without using hand-engineered features	73-98	73-98	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	1<2	none	condition	condition
P16-1004_anno1	79-83	88-98	that our approach performs competitively	and is easy to adapt across domains and meaning representations .	that our approach performs competitively	and is easy to adapt across domains and meaning representations .	73-98	73-98	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	Experimental results on four atasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations .	1<2	none	joint	joint
P16-1005_anno1	1-30	42-49	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	We propose a simple yet effective unsupervised approach	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	We propose a simple yet effective unsupervised approach	1-30	42-110	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1>2	none	bg-goal	bg-goal
P16-1005_anno1	1-30	31-41	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	Slot filling remains very challenging over the past seven years .	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	Slot filling remains very challenging over the past seven years .	1-30	31-41	Slot filling aims to extract the values ( slot fillers ) of specific attributes ( slots types ) for a given entity ( query ) from a largescale corpus .	Slot filling remains very challenging over the past seven years .	1<2	none	elab-addition	elab-addition
P16-1005_anno1	42-49	50-53	We propose a simple yet effective unsupervised approach	to extract slot fillers	We propose a simple yet effective unsupervised approach	to extract slot fillers	42-110	42-110	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1<2	none	elab-addition	elab-addition
P16-1005_anno1	42-49	54-60	We propose a simple yet effective unsupervised approach	based on the following two observations :	We propose a simple yet effective unsupervised approach	based on the following two observations :	42-110	42-110	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1<2	none	bg-general	bg-general
P16-1005_anno1	54-60	61-86	based on the following two observations :	( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ;	based on the following two observations :	( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ;	42-110	42-110	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1<2	none	elab-enumember	elab-enumember
P16-1005_anno1	61-86	87-95	( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ;	( 2 ) a relation is likely to exist	( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ;	( 2 ) a relation is likely to exist	42-110	42-110	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1<2	none	joint	joint
P16-1005_anno1	87-95	96-110	( 2 ) a relation is likely to exist	if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	( 2 ) a relation is likely to exist	if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	42-110	42-110	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	1<2	none	condition	condition
P16-1005_anno1	42-49	111-116	We propose a simple yet effective unsupervised approach	Thus we design a graph-based algorithm	We propose a simple yet effective unsupervised approach	Thus we design a graph-based algorithm	42-110	111-148	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	1<2	none	elab-addition	elab-addition
P16-1005_anno1	111-116	117-120	Thus we design a graph-based algorithm	to automatically identify triggers	Thus we design a graph-based algorithm	to automatically identify triggers	111-148	111-148	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	1<2	none	elab-addition	elab-addition
P16-1005_anno1	111-116	121-136	Thus we design a graph-based algorithm	based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair	Thus we design a graph-based algorithm	based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair	111-148	111-148	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	1<2	none	bg-general	bg-general
P16-1005_anno1	111-116	137-142	Thus we design a graph-based algorithm	and then label the slot type	Thus we design a graph-based algorithm	and then label the slot type	111-148	111-148	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	1<2	none	progression	progression
P16-1005_anno1	137-142	143-148	and then label the slot type	based on the identified triggers .	and then label the slot type	based on the identified triggers .	111-148	111-148	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	Thus we design a graph-based algorithm to automatically identify triggers based on personalized PageRank and Affinity Propagation for a given ( query , filler ) pair and then label the slot type based on the identified triggers .	1<2	none	bg-general	bg-general
P16-1005_anno1	42-49	149-164	We propose a simple yet effective unsupervised approach	Our approach achieves 11.6 % -25 % higher F-score over state-ofthe-art English slot filling methods .	We propose a simple yet effective unsupervised approach	Our approach achieves 11.6 % -25 % higher F-score over state-ofthe-art English slot filling methods .	42-110	149-164	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	Our approach achieves 11.6 % -25 % higher F-score over state-ofthe-art English slot filling methods .	1<2	none	evaluation	evaluation
P16-1005_anno1	165-168	186-199	Our experiments also demonstrate	this approach can be quickly adapted to any language and new slot types .	Our experiments also demonstrate	this approach can be quickly adapted to any language and new slot types .	165-199	165-199	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	1>2	none	attribution	attribution
P16-1005_anno1	169-185	186-199	that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist ,	this approach can be quickly adapted to any language and new slot types .	that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist ,	this approach can be quickly adapted to any language and new slot types .	165-199	165-199	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	1>2	none	condition	condition
P16-1005_anno1	42-49	186-199	We propose a simple yet effective unsupervised approach	this approach can be quickly adapted to any language and new slot types .	We propose a simple yet effective unsupervised approach	this approach can be quickly adapted to any language and new slot types .	42-110	165-199	We propose a simple yet effective unsupervised approach to extract slot fillers based on the following two observations : ( 1 ) a trigger is usually a salient node relative to the query and filler nodes in the dependency graph of a context sentence ; ( 2 ) a relation is likely to exist if the query and candidate filler nodes are strongly connected by a relation-specific trigger .	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	1<2	none	evaluation	evaluation
P16-1005_anno1	186-199	200-213	this approach can be quickly adapted to any language and new slot types .	Our promising results on Chinese slot filling can serve as a new benchmark .	this approach can be quickly adapted to any language and new slot types .	Our promising results on Chinese slot filling can serve as a new benchmark .	165-199	200-213	Our experiments also demonstrate that as long as a few trigger seeds , name tagging and dependency parsing capabilities exist , this approach can be quickly adapted to any language and new slot types .	Our promising results on Chinese slot filling can serve as a new benchmark .	1<2	none	elab-addition	elab-addition
P16-1006_anno1	1-8	9-19	When a large-scale incident or disaster occurs ,	there is often a great demand for rapidly developing a system	When a large-scale incident or disaster occurs ,	there is often a great demand for rapidly developing a system	1-32	1-32	When a large-scale incident or disaster occurs , there is often a great demand for rapidly developing a system to extract detailed and new information from lowresource languages ( LLs ) .	When a large-scale incident or disaster occurs , there is often a great demand for rapidly developing a system to extract detailed and new information from lowresource languages ( LLs ) .	1>2	none	temporal	temporal
P16-1006_anno1	9-19	33-37	there is often a great demand for rapidly developing a system	We propose a novel approach	there is often a great demand for rapidly developing a system	We propose a novel approach	1-32	33-62	When a large-scale incident or disaster occurs , there is often a great demand for rapidly developing a system to extract detailed and new information from lowresource languages ( LLs ) .	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	1>2	none	bg-goal	bg-goal
P16-1006_anno1	9-19	20-32	there is often a great demand for rapidly developing a system	to extract detailed and new information from lowresource languages ( LLs ) .	there is often a great demand for rapidly developing a system	to extract detailed and new information from lowresource languages ( LLs ) .	1-32	1-32	When a large-scale incident or disaster occurs , there is often a great demand for rapidly developing a system to extract detailed and new information from lowresource languages ( LLs ) .	When a large-scale incident or disaster occurs , there is often a great demand for rapidly developing a system to extract detailed and new information from lowresource languages ( LLs ) .	1<2	none	enablement	enablement
P16-1006_anno1	33-37	38-48	We propose a novel approach	to discover comparable documents in high-resource languages ( HLs ) ,	We propose a novel approach	to discover comparable documents in high-resource languages ( HLs ) ,	33-62	33-62	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	1<2	none	elab-addition	elab-addition
P16-1006_anno1	38-48	49-62	to discover comparable documents in high-resource languages ( HLs ) ,	and project Entity Discovery and Linking results from HLs documents back to LLs .	to discover comparable documents in high-resource languages ( HLs ) ,	and project Entity Discovery and Linking results from HLs documents back to LLs .	33-62	33-62	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	1<2	none	joint	joint
P16-1006_anno1	33-37	63-75	We propose a novel approach	We leverage a wide variety of language-independent forms from multiple data modalities ,	We propose a novel approach	We leverage a wide variety of language-independent forms from multiple data modalities ,	33-62	63-92	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	We leverage a wide variety of language-independent forms from multiple data modalities , including image processing ( image-to-image retrieval , visual similarity and face recognition ) and sound matching .	1<2	none	elab-addition	elab-addition
P16-1006_anno1	63-75	76-92	We leverage a wide variety of language-independent forms from multiple data modalities ,	including image processing ( image-to-image retrieval , visual similarity and face recognition ) and sound matching .	We leverage a wide variety of language-independent forms from multiple data modalities ,	including image processing ( image-to-image retrieval , visual similarity and face recognition ) and sound matching .	63-92	63-92	We leverage a wide variety of language-independent forms from multiple data modalities , including image processing ( image-to-image retrieval , visual similarity and face recognition ) and sound matching .	We leverage a wide variety of language-independent forms from multiple data modalities , including image processing ( image-to-image retrieval , visual similarity and face recognition ) and sound matching .	1<2	none	elab-enumember	elab-enumember
P16-1006_anno1	33-37	93-97	We propose a novel approach	We also propose novel methods	We propose a novel approach	We also propose novel methods	33-62	93-110	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	We also propose novel methods to learn entity priors from a large-scale HL corpus and knowledge base .	1<2	none	elab-addition	elab-addition
P16-1006_anno1	93-97	98-110	We also propose novel methods	to learn entity priors from a large-scale HL corpus and knowledge base .	We also propose novel methods	to learn entity priors from a large-scale HL corpus and knowledge base .	93-110	93-110	We also propose novel methods to learn entity priors from a large-scale HL corpus and knowledge base .	We also propose novel methods to learn entity priors from a large-scale HL corpus and knowledge base .	1<2	none	elab-addition	elab-addition
P16-1006_anno1	111-123	124-125	Using Hausa and Chinese as the LLs and English as the HL ,	experiments show	Using Hausa and Chinese as the LLs and English as the HL ,	experiments show	111-153	111-153	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	1>2	none	elab-addition	elab-addition
P16-1006_anno1	124-125	126-142	experiments show	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	experiments show	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	111-153	111-153	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	1>2	none	attribution	attribution
P16-1006_anno1	33-37	126-142	We propose a novel approach	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	We propose a novel approach	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	33-62	111-153	We propose a novel approach to discover comparable documents in high-resource languages ( HLs ) , and project Entity Discovery and Linking results from HLs documents back to LLs .	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	1<2	none	evaluation	evaluation
P16-1006_anno1	126-142	143-153	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model ,	and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	111-153	111-153	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	Using Hausa and Chinese as the LLs and English as the HL , experiments show that our approach achieves 36.1 % higher Hausa name tagging F-score over a costly supervised model , and 9.4 % higher Chinese-to-English Entity Linking accuracy over state-of-the-art .	1<2	none	joint	joint
P16-1007_anno1	16	17-23	suggesting	how to complete a partial translation .	suggesting	how to complete a partial translation .	1-23	1-23	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	1>2	none	attribution	attribution
P16-1007_anno1	1-15	17-23	We apply phrase-based and neural models to a core task in interactive machine translation :	how to complete a partial translation .	We apply phrase-based and neural models to a core task in interactive machine translation :	how to complete a partial translation .	1-23	1-23	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	1-15	24-34	We apply phrase-based and neural models to a core task in interactive machine translation :	For the phrase-based system , we demonstrate improvements in suggestion quality	We apply phrase-based and neural models to a core task in interactive machine translation :	For the phrase-based system , we demonstrate improvements in suggestion quality	1-23	24-50	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques , and inference algorithms tailored to this task .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	24-34	35-45	For the phrase-based system , we demonstrate improvements in suggestion quality	using novel objective functions , learning techniques , and inference algorithms	For the phrase-based system , we demonstrate improvements in suggestion quality	using novel objective functions , learning techniques , and inference algorithms	24-50	24-50	For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques , and inference algorithms tailored to this task .	For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques , and inference algorithms tailored to this task .	1<2	none	manner-means	manner-means
P16-1007_anno1	35-45	46-50	using novel objective functions , learning techniques , and inference algorithms	tailored to this task .	using novel objective functions , learning techniques , and inference algorithms	tailored to this task .	24-50	24-50	For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques , and inference algorithms tailored to this task .	For the phrase-based system , we demonstrate improvements in suggestion quality using novel objective functions , learning techniques , and inference algorithms tailored to this task .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	1-15	51-67,73-86	We apply phrase-based and neural models to a core task in interactive machine translation :	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method <*> and a tuning procedure for a hierarchical joint model of alignment and translation .	We apply phrase-based and neural models to a core task in interactive machine translation :	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method <*> and a tuning procedure for a hierarchical joint model of alignment and translation .	1-23	51-86	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method that increases suggestion diversity , and a tuning procedure for a hierarchical joint model of alignment and translation .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	51-67,73-86	68-72	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method <*> and a tuning procedure for a hierarchical joint model of alignment and translation .	that increases suggestion diversity ,	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method <*> and a tuning procedure for a hierarchical joint model of alignment and translation .	that increases suggestion diversity ,	51-86	51-86	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method that increases suggestion diversity , and a tuning procedure for a hierarchical joint model of alignment and translation .	Our contributions include new tunable metrics , an improved beam search strategy , an n-best extraction method that increases suggestion diversity , and a tuning procedure for a hierarchical joint model of alignment and translation .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	1-15	87-108	We apply phrase-based and neural models to a core task in interactive machine translation :	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	We apply phrase-based and neural models to a core task in interactive machine translation :	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	1-23	87-108	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	1<2	none	evaluation	evaluation
P16-1007_anno1	87-108	109-121	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	Our recurrent neural translation system increases accuracy yet further to 53.0 % ,	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	Our recurrent neural translation system increases accuracy yet further to 53.0 % ,	87-108	109-130	The combination of these techniques improves next-word suggestion accuracy dramatically from 28.5 % to 41.2 % in a large-scale English-German experiment .	Our recurrent neural translation system increases accuracy yet further to 53.0 % , but inference is two orders of magnitude slower .	1<2	none	elab-addition	elab-addition
P16-1007_anno1	109-121	122-130	Our recurrent neural translation system increases accuracy yet further to 53.0 % ,	but inference is two orders of magnitude slower .	Our recurrent neural translation system increases accuracy yet further to 53.0 % ,	but inference is two orders of magnitude slower .	109-130	109-130	Our recurrent neural translation system increases accuracy yet further to 53.0 % , but inference is two orders of magnitude slower .	Our recurrent neural translation system increases accuracy yet further to 53.0 % , but inference is two orders of magnitude slower .	1<2	none	contrast	contrast
P16-1007_anno1	1-15	131-142	We apply phrase-based and neural models to a core task in interactive machine translation :	Manual error analysis shows the strengths and weaknesses of both approaches .	We apply phrase-based and neural models to a core task in interactive machine translation :	Manual error analysis shows the strengths and weaknesses of both approaches .	1-23	131-142	We apply phrase-based and neural models to a core task in interactive machine translation : suggesting how to complete a partial translation .	Manual error analysis shows the strengths and weaknesses of both approaches .	1<2	none	evaluation	evaluation
P16-1008_anno1	1-11	43-50	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	we propose coverage-based NMT in this paper .	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	we propose coverage-based NMT in this paper .	1-19	38-50	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT ) by jointly learning to align and translate .	To address this problem , we propose coverage-based NMT in this paper .	1>2	none	bg-compare	bg-compare
P16-1008_anno1	1-11	12-19	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	by jointly learning to align and translate .	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	by jointly learning to align and translate .	1-19	1-19	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT ) by jointly learning to align and translate .	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT ) by jointly learning to align and translate .	1<2	none	manner-means	manner-means
P16-1008_anno1	1-11	20-27	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	It tends to ignore past alignment information ,	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT )	It tends to ignore past alignment information ,	1-19	20-37	Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT ) by jointly learning to align and translate .	It tends to ignore past alignment information , however , which often leads to over-translation and under-translation .	1<2	none	elab-addition	elab-addition
P16-1008_anno1	20-27	28-37	It tends to ignore past alignment information ,	however , which often leads to over-translation and under-translation .	It tends to ignore past alignment information ,	however , which often leads to over-translation and under-translation .	20-37	20-37	It tends to ignore past alignment information , however , which often leads to over-translation and under-translation .	It tends to ignore past alignment information , however , which often leads to over-translation and under-translation .	1<2	none	elab-addition	elab-addition
P16-1008_anno1	38-42	43-50	To address this problem ,	we propose coverage-based NMT in this paper .	To address this problem ,	we propose coverage-based NMT in this paper .	38-50	38-50	To address this problem , we propose coverage-based NMT in this paper .	To address this problem , we propose coverage-based NMT in this paper .	1>2	none	enablement	enablement
P16-1008_anno1	43-50	51-55	we propose coverage-based NMT in this paper .	We maintain a coverage vector	we propose coverage-based NMT in this paper .	We maintain a coverage vector	38-50	51-63	To address this problem , we propose coverage-based NMT in this paper .	We maintain a coverage vector to keep track of the attention history .	1<2	none	elab-addition	elab-addition
P16-1008_anno1	51-55	56-63	We maintain a coverage vector	to keep track of the attention history .	We maintain a coverage vector	to keep track of the attention history .	51-63	51-63	We maintain a coverage vector to keep track of the attention history .	We maintain a coverage vector to keep track of the attention history .	1<2	none	enablement	enablement
P16-1008_anno1	51-55	64-72	We maintain a coverage vector	The coverage vector is fed to the attention model	We maintain a coverage vector	The coverage vector is fed to the attention model	51-63	64-90	We maintain a coverage vector to keep track of the attention history .	The coverage vector is fed to the attention model to help adjust future attention , which lets NMT system to consider more about untranslated source words .	1<2	none	elab-addition	elab-addition
P16-1008_anno1	64-72	73-78	The coverage vector is fed to the attention model	to help adjust future attention ,	The coverage vector is fed to the attention model	to help adjust future attention ,	64-90	64-90	The coverage vector is fed to the attention model to help adjust future attention , which lets NMT system to consider more about untranslated source words .	The coverage vector is fed to the attention model to help adjust future attention , which lets NMT system to consider more about untranslated source words .	1<2	none	enablement	enablement
P16-1008_anno1	64-72	79-90	The coverage vector is fed to the attention model	which lets NMT system to consider more about untranslated source words .	The coverage vector is fed to the attention model	which lets NMT system to consider more about untranslated source words .	64-90	64-90	The coverage vector is fed to the attention model to help adjust future attention , which lets NMT system to consider more about untranslated source words .	The coverage vector is fed to the attention model to help adjust future attention , which lets NMT system to consider more about untranslated source words .	1<2	none	elab-addition	elab-addition
P16-1008_anno1	91-92	93-109	Experiments show	that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	Experiments show	that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	91-109	91-109	Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	1>2	none	attribution	attribution
P16-1008_anno1	43-50	93-109	we propose coverage-based NMT in this paper .	that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	we propose coverage-based NMT in this paper .	that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	38-50	91-109	To address this problem , we propose coverage-based NMT in this paper .	Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT .	1<2	none	evaluation	evaluation
P16-1009_anno1	1-16	41-51	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs ,	and we investigate the use of monolingual data for NMT .	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs ,	and we investigate the use of monolingual data for NMT .	1-24	25-51	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs , while only using parallel data for training .	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	1>2	none	bg-compare	bg-compare
P16-1009_anno1	1-16	17-24	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs ,	while only using parallel data for training .	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs ,	while only using parallel data for training .	1-24	1-24	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs , while only using parallel data for training .	Neural Machine Translation ( NMT ) has obtained state-of-the art performance for several language pairs , while only using parallel data for training .	1<2	none	condition	condition
P16-1009_anno1	25-31	41-51	Targetside monolingual data plays an important role	and we investigate the use of monolingual data for NMT .	Targetside monolingual data plays an important role	and we investigate the use of monolingual data for NMT .	25-51	25-51	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	1>2	none	joint	joint
P16-1009_anno1	25-31	32-40	Targetside monolingual data plays an important role	in boosting fluency for phrasebased statistical machine translation ,	Targetside monolingual data plays an important role	in boosting fluency for phrasebased statistical machine translation ,	25-51	25-51	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	52-57	70-77	In contrast to previous work ,	that encoder-decoder NMT architectures already have the capacity	In contrast to previous work ,	that encoder-decoder NMT architectures already have the capacity	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1>2	none	comparison	comparison
P16-1009_anno1	52-57	58-67	In contrast to previous work ,	which combines NMT models with separately trained language models ,	In contrast to previous work ,	which combines NMT models with separately trained language models ,	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	68-69	70-77	we note	that encoder-decoder NMT architectures already have the capacity	we note	that encoder-decoder NMT architectures already have the capacity	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1>2	none	attribution	attribution
P16-1009_anno1	41-51	70-77	and we investigate the use of monolingual data for NMT .	that encoder-decoder NMT architectures already have the capacity	and we investigate the use of monolingual data for NMT .	that encoder-decoder NMT architectures already have the capacity	25-51	52-103	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	70-77	78-87	that encoder-decoder NMT architectures already have the capacity	to learn the same information as a language model ,	that encoder-decoder NMT architectures already have the capacity	to learn the same information as a language model ,	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	70-77	88-91	that encoder-decoder NMT architectures already have the capacity	and we explore strategies	that encoder-decoder NMT architectures already have the capacity	and we explore strategies	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	joint	joint
P16-1009_anno1	88-91	92-96	and we explore strategies	to train with monolingual data	and we explore strategies	to train with monolingual data	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	92-96	97-103	to train with monolingual data	without changing the neural network architecture .	to train with monolingual data	without changing the neural network architecture .	52-103	52-103	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	In contrast to previous work , which combines NMT models with separately trained language models , we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model , and we explore strategies to train with monolingual data without changing the neural network architecture .	1<2	none	condition	condition
P16-1009_anno1	104-113	114-123	By pairing monolingual training data with an automatic backtranslation ,	we can treat it as additional parallel training data ,	By pairing monolingual training data with an automatic backtranslation ,	we can treat it as additional parallel training data ,	104-161	104-161	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	1>2	none	manner-means	manner-means
P16-1009_anno1	41-51	114-123	and we investigate the use of monolingual data for NMT .	we can treat it as additional parallel training data ,	and we investigate the use of monolingual data for NMT .	we can treat it as additional parallel training data ,	25-51	104-161	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	1<2	none	evaluation	evaluation
P16-1009_anno1	114-123	124-156	we can treat it as additional parallel training data ,	and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) ,	we can treat it as additional parallel training data ,	and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) ,	104-161	104-161	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	1<2	none	joint	joint
P16-1009_anno1	124-156	157-161	and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) ,	obtaining new state-of-the-art results .	and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) ,	obtaining new state-of-the-art results .	104-161	104-161	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	By pairing monolingual training data with an automatic backtranslation , we can treat it as additional parallel training data , and we obtain substantial improvements on the WMT 15 task English to German ( +2.8-3.7 BLEU ) , and for the low-resourced IWSLT 14 task Turkish to English ( +2.1-3.4 BLEU ) , obtaining new state-of-the-art results .	1<2	none	elab-addition	elab-addition
P16-1009_anno1	162-164	165-184	We also show	that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	We also show	that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	162-184	162-184	We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	1>2	none	attribution	attribution
P16-1009_anno1	41-51	165-184	and we investigate the use of monolingual data for NMT .	that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	and we investigate the use of monolingual data for NMT .	that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	25-51	162-184	Targetside monolingual data plays an important role in boosting fluency for phrasebased statistical machine translation , and we investigate the use of monolingual data for NMT .	We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English ! German .	1<2	none	evaluation	evaluation
P16-1010_anno1	1-17	25-30	One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases .	in this paper we construct graphs	One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases .	in this paper we construct graphs	1-17	18-43	One major drawback of phrase-based translation is that it segments an input sentence into continuous phrases .	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	1>2	none	bg-goal	bg-goal
P16-1010_anno1	18-24	25-30	To support linguistically informed source discontinuity ,	in this paper we construct graphs	To support linguistically informed source discontinuity ,	in this paper we construct graphs	18-43	18-43	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	1>2	none	enablement	enablement
P16-1010_anno1	25-30	31-36	in this paper we construct graphs	which combine bigram and dependency relations	in this paper we construct graphs	which combine bigram and dependency relations	18-43	18-43	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	1<2	none	elab-addition	elab-addition
P16-1010_anno1	25-30	37-43	in this paper we construct graphs	and propose a graph-based translation model .	in this paper we construct graphs	and propose a graph-based translation model .	18-43	18-43	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	1<2	none	joint	joint
P16-1010_anno1	25-30	44-53	in this paper we construct graphs	The model segments an input graph into connected subgraphs ,	in this paper we construct graphs	The model segments an input graph into connected subgraphs ,	18-43	44-62	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	The model segments an input graph into connected subgraphs , each of which may cover a discontinuous phrase .	1<2	none	elab-addition	elab-addition
P16-1010_anno1	44-53	54-62	The model segments an input graph into connected subgraphs ,	each of which may cover a discontinuous phrase .	The model segments an input graph into connected subgraphs ,	each of which may cover a discontinuous phrase .	44-62	44-62	The model segments an input graph into connected subgraphs , each of which may cover a discontinuous phrase .	The model segments an input graph into connected subgraphs , each of which may cover a discontinuous phrase .	1<2	none	elab-addition	elab-addition
P16-1010_anno1	25-30	63-66	in this paper we construct graphs	We use beam search	in this paper we construct graphs	We use beam search	18-43	63-79	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	We use beam search to combine translations of each subgraph left-to-right to produce a complete translation .	1<2	none	elab-addition	elab-addition
P16-1010_anno1	63-66	67-73	We use beam search	to combine translations of each subgraph left-to-right	We use beam search	to combine translations of each subgraph left-to-right	63-79	63-79	We use beam search to combine translations of each subgraph left-to-right to produce a complete translation .	We use beam search to combine translations of each subgraph left-to-right to produce a complete translation .	1<2	none	enablement	enablement
P16-1010_anno1	67-73	74-79	to combine translations of each subgraph left-to-right	to produce a complete translation .	to combine translations of each subgraph left-to-right	to produce a complete translation .	63-79	63-79	We use beam search to combine translations of each subgraph left-to-right to produce a complete translation .	We use beam search to combine translations of each subgraph left-to-right to produce a complete translation .	1<2	none	enablement	enablement
P16-1010_anno1	80-86	87-103	Experiments on Chinese-English and German-English tasks show	that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	Experiments on Chinese-English and German-English tasks show	that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	80-103	80-103	Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	1>2	none	attribution	attribution
P16-1010_anno1	25-30	87-103	in this paper we construct graphs	that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	in this paper we construct graphs	that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	18-43	80-103	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	Experiments on Chinese-English and German-English tasks show that our system is significantly better than the phrase-based model by up to +1.5/+0.5 BLEU scores .	1<2	none	evaluation	evaluation
P16-1010_anno1	104-110	111-120	By explicitly modeling the graph segmentation ,	our system obtains further improvement , especially on German-English .	By explicitly modeling the graph segmentation ,	our system obtains further improvement , especially on German-English .	104-120	104-120	By explicitly modeling the graph segmentation , our system obtains further improvement , especially on German-English .	By explicitly modeling the graph segmentation , our system obtains further improvement , especially on German-English .	1>2	none	manner-means	manner-means
P16-1010_anno1	25-30	111-120	in this paper we construct graphs	our system obtains further improvement , especially on German-English .	in this paper we construct graphs	our system obtains further improvement , especially on German-English .	18-43	104-120	To support linguistically informed source discontinuity , in this paper we construct graphs which combine bigram and dependency relations and propose a graph-based translation model .	By explicitly modeling the graph segmentation , our system obtains further improvement , especially on German-English .	1<2	none	evaluation	evaluation
P16-1011_anno1	1-13	14-23	As a new generation of cognitive robots start to enter our lives ,	it is important to enable robots to follow human commands	As a new generation of cognitive robots start to enter our lives ,	it is important to enable robots to follow human commands	1-33	1-33	As a new generation of cognitive robots start to enter our lives , it is important to enable robots to follow human commands and to learn new actions from human language instructions .	As a new generation of cognitive robots start to enter our lives , it is important to enable robots to follow human commands and to learn new actions from human language instructions .	1>2	none	cause	cause
P16-1011_anno1	14-23	39-43	it is important to enable robots to follow human commands	this paper presents an approach	it is important to enable robots to follow human commands	this paper presents an approach	1-33	34-64	As a new generation of cognitive robots start to enter our lives , it is important to enable robots to follow human commands and to learn new actions from human language instructions .	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	1>2	none	bg-goal	bg-goal
P16-1011_anno1	14-23	24-33	it is important to enable robots to follow human commands	and to learn new actions from human language instructions .	it is important to enable robots to follow human commands	and to learn new actions from human language instructions .	1-33	1-33	As a new generation of cognitive robots start to enter our lives , it is important to enable robots to follow human commands and to learn new actions from human language instructions .	As a new generation of cognitive robots start to enter our lives , it is important to enable robots to follow human commands and to learn new actions from human language instructions .	1<2	none	joint	joint
P16-1011_anno1	34-38	39-43	To address this issue ,	this paper presents an approach	To address this issue ,	this paper presents an approach	34-64	34-64	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	1>2	none	enablement	enablement
P16-1011_anno1	39-43	44-53	this paper presents an approach	that explicitly represents verb semantics through hypothesis spaces of fluents	this paper presents an approach	that explicitly represents verb semantics through hypothesis spaces of fluents	34-64	34-64	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	1<2	none	elab-addition	elab-addition
P16-1011_anno1	44-53	54-59	that explicitly represents verb semantics through hypothesis spaces of fluents	and automatically acquires these hypothesis spaces	that explicitly represents verb semantics through hypothesis spaces of fluents	and automatically acquires these hypothesis spaces	34-64	34-64	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	1<2	none	joint	joint
P16-1011_anno1	54-59	60-64	and automatically acquires these hypothesis spaces	by interacting with humans .	and automatically acquires these hypothesis spaces	by interacting with humans .	34-64	34-64	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	1<2	none	manner-means	manner-means
P16-1011_anno1	39-43	65-71	this paper presents an approach	The learned hypothesis spaces can be used	this paper presents an approach	The learned hypothesis spaces can be used	34-64	65-83	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	The learned hypothesis spaces can be used to automatically plan for lower-level primitive actions towards physical world interaction .	1<2	none	elab-addition	elab-addition
P16-1011_anno1	65-71	72-83	The learned hypothesis spaces can be used	to automatically plan for lower-level primitive actions towards physical world interaction .	The learned hypothesis spaces can be used	to automatically plan for lower-level primitive actions towards physical world interaction .	65-83	65-83	The learned hypothesis spaces can be used to automatically plan for lower-level primitive actions towards physical world interaction .	The learned hypothesis spaces can be used to automatically plan for lower-level primitive actions towards physical world interaction .	1<2	none	enablement	enablement
P16-1011_anno1	84-88	89-98,107-111	Our empirical results have shown	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	Our empirical results have shown	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	84-111	84-111	Our empirical results have shown that the representation of a hypothesis space of fluents , combined with the learned hypothesis selection algorithm , outperforms a previous baseline .	Our empirical results have shown that the representation of a hypothesis space of fluents , combined with the learned hypothesis selection algorithm , outperforms a previous baseline .	1>2	none	attribution	attribution
P16-1011_anno1	39-43	89-98,107-111	this paper presents an approach	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	this paper presents an approach	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	34-64	84-111	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	Our empirical results have shown that the representation of a hypothesis space of fluents , combined with the learned hypothesis selection algorithm , outperforms a previous baseline .	1<2	none	evaluation	evaluation
P16-1011_anno1	89-98,107-111	99-106	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	combined with the learned hypothesis selection algorithm ,	that the representation of a hypothesis space of fluents , <*> outperforms a previous baseline .	combined with the learned hypothesis selection algorithm ,	84-111	84-111	Our empirical results have shown that the representation of a hypothesis space of fluents , combined with the learned hypothesis selection algorithm , outperforms a previous baseline .	Our empirical results have shown that the representation of a hypothesis space of fluents , combined with the learned hypothesis selection algorithm , outperforms a previous baseline .	1<2	none	elab-addition	elab-addition
P16-1011_anno1	39-43	112-120	this paper presents an approach	In addition , our approach applies incremental learning ,	this paper presents an approach	In addition , our approach applies incremental learning ,	34-64	112-132	To address this issue , this paper presents an approach that explicitly represents verb semantics through hypothesis spaces of fluents and automatically acquires these hypothesis spaces by interacting with humans .	In addition , our approach applies incremental learning , which can contribute to life-long learning from humans in the future .	1<2	none	evaluation	evaluation
P16-1011_anno1	112-120	121-132	In addition , our approach applies incremental learning ,	which can contribute to life-long learning from humans in the future .	In addition , our approach applies incremental learning ,	which can contribute to life-long learning from humans in the future .	112-132	112-132	In addition , our approach applies incremental learning , which can contribute to life-long learning from humans in the future .	In addition , our approach applies incremental learning , which can contribute to life-long learning from humans in the future .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	1-7	8-17	We propose a framework for lexical substitution	that is able to perform transfer learning across languages .	We propose a framework for lexical substitution	that is able to perform transfer learning across languages .	1-17	1-17	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	1-7	18-28	We propose a framework for lexical substitution	Datasets for this task are available in at least three languages	We propose a framework for lexical substitution	Datasets for this task are available in at least three languages	1-17	18-37	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	Datasets for this task are available in at least three languages ( English , Italian , and German ) .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	18-28	29-37	Datasets for this task are available in at least three languages	( English , Italian , and German ) .	Datasets for this task are available in at least three languages	( English , Italian , and German ) .	18-37	18-37	Datasets for this task are available in at least three languages ( English , Italian , and German ) .	Datasets for this task are available in at least three languages ( English , Italian , and German ) .	1<2	none	elab-enumember	elab-enumember
P16-1012_anno1	38-48	49-65	Previous work has addressed each of these tasks in isolation .	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	Previous work has addressed each of these tasks in isolation .	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	38-48	49-65	Previous work has addressed each of these tasks in isolation .	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	1>2	none	contrast	contrast
P16-1012_anno1	18-28	49-65	Datasets for this task are available in at least three languages	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	Datasets for this task are available in at least three languages	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	18-37	49-65	Datasets for this task are available in at least three languages ( English , Italian , and German ) .	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	66-67	68-76	We show	that a supervised system can be trained effectively ,	We show	that a supervised system can be trained effectively ,	66-87	66-87	We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages .	We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages .	1>2	none	attribution	attribution
P16-1012_anno1	49-65	68-76	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	that a supervised system can be trained effectively ,	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	that a supervised system can be trained effectively ,	49-65	66-87	In contrast , we regard the union of three shared tasks as a combined multilingual dataset .	We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	68-76	77-87	that a supervised system can be trained effectively ,	even if training and evaluation data are from different languages .	that a supervised system can be trained effectively ,	even if training and evaluation data are from different languages .	66-87	66-87	We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages .	We show that a supervised system can be trained effectively , even if training and evaluation data are from different languages .	1<2	none	condition	condition
P16-1012_anno1	88-93	94-106	Successful transfer learning between languages suggests	that the learned model is in fact independent of the underlying language .	Successful transfer learning between languages suggests	that the learned model is in fact independent of the underlying language .	88-106	88-106	Successful transfer learning between languages suggests that the learned model is in fact independent of the underlying language .	Successful transfer learning between languages suggests that the learned model is in fact independent of the underlying language .	1>2	none	attribution	attribution
P16-1012_anno1	1-7	94-106	We propose a framework for lexical substitution	that the learned model is in fact independent of the underlying language .	We propose a framework for lexical substitution	that the learned model is in fact independent of the underlying language .	1-17	88-106	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	Successful transfer learning between languages suggests that the learned model is in fact independent of the underlying language .	1<2	none	summary	summary
P16-1012_anno1	1-7	107-111	We propose a framework for lexical substitution	We combine state-of-the-art unsupervised features	We propose a framework for lexical substitution	We combine state-of-the-art unsupervised features	1-17	107-126	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system .	1<2	none	evaluation	evaluation
P16-1012_anno1	107-111	112-119	We combine state-of-the-art unsupervised features	obtained from syntactic word embeddings and distributional thesauri	We combine state-of-the-art unsupervised features	obtained from syntactic word embeddings and distributional thesauri	107-126	107-126	We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system .	We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	107-111	120-126	We combine state-of-the-art unsupervised features	in a supervised delexicalized ranking system .	We combine state-of-the-art unsupervised features	in a supervised delexicalized ranking system .	107-126	107-126	We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system .	We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system .	1<2	none	elab-addition	elab-addition
P16-1012_anno1	1-7	127-145	We propose a framework for lexical substitution	Our system improves over state of the art in the full lexical substitution task in all three languages .	We propose a framework for lexical substitution	Our system improves over state of the art in the full lexical substitution task in all three languages .	1-17	127-145	We propose a framework for lexical substitution that is able to perform transfer learning across languages .	Our system improves over state of the art in the full lexical substitution task in all three languages .	1<2	none	evaluation	evaluation
P16-1013_anno1	1-4	5-12	We use Bayesian optimization	to learn curricula for word representation learning ,	We use Bayesian optimization	to learn curricula for word representation learning ,	1-26	1-26	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	1<2	none	enablement	enablement
P16-1013_anno1	1-4	13-17	We use Bayesian optimization	optimizing performance on downstream tasks	We use Bayesian optimization	optimizing performance on downstream tasks	1-26	1-26	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	1<2	none	elab-addition	elab-addition
P16-1013_anno1	13-17	18-26	optimizing performance on downstream tasks	that depend on the learned representations as features .	optimizing performance on downstream tasks	that depend on the learned representations as features .	1-26	1-26	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	1<2	none	elab-addition	elab-addition
P16-1013_anno1	5-12	27-35	to learn curricula for word representation learning ,	The curricula are modeled by a linear ranking function	to learn curricula for word representation learning ,	The curricula are modeled by a linear ranking function	1-26	27-66	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus .	1<2	none	elab-addition	elab-addition
P16-1013_anno1	27-35	36-50	The curricula are modeled by a linear ranking function	which is the scalar product of a learned weight vector and an engineered feature vector	The curricula are modeled by a linear ranking function	which is the scalar product of a learned weight vector and an engineered feature vector	27-66	27-66	The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus .	The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus .	1<2	none	elab-addition	elab-addition
P16-1013_anno1	36-50	51-66	which is the scalar product of a learned weight vector and an engineered feature vector	that characterizes the different aspects of the complexity of each instance in the training corpus .	which is the scalar product of a learned weight vector and an engineered feature vector	that characterizes the different aspects of the complexity of each instance in the training corpus .	27-66	27-66	The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus .	The curricula are modeled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus .	1<2	none	elab-addition	elab-addition
P16-1013_anno1	67-68	69-80	We show	that learning the curriculum improves performance on a variety of downstream tasks	We show	that learning the curriculum improves performance on a variety of downstream tasks	67-92	67-92	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	1>2	none	attribution	attribution
P16-1013_anno1	1-4	69-80	We use Bayesian optimization	that learning the curriculum improves performance on a variety of downstream tasks	We use Bayesian optimization	that learning the curriculum improves performance on a variety of downstream tasks	1-26	67-92	We use Bayesian optimization to learn curricula for word representation learning , optimizing performance on downstream tasks that depend on the learned representations as features .	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	1<2	none	evaluation	evaluation
P16-1013_anno1	69-80	81-83	that learning the curriculum improves performance on a variety of downstream tasks	over random orders	that learning the curriculum improves performance on a variety of downstream tasks	over random orders	67-92	67-92	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	1<2	none	comparison	comparison
P16-1013_anno1	81-83	84-92	over random orders	and in comparison to the natural corpus order .	over random orders	and in comparison to the natural corpus order .	67-92	67-92	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order .	1<2	none	joint	joint
P16-1014_anno1	1-11	31-35	The problem of rare and unknown words is an important issue	We propose a novel way	The problem of rare and unknown words is an important issue	We propose a novel way	1-30	31-51	The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models .	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	1>2	none	bg-goal	bg-goal
P16-1014_anno1	1-11	12-22	The problem of rare and unknown words is an important issue	that can potentially effect the performance of many NLP systems ,	The problem of rare and unknown words is an important issue	that can potentially effect the performance of many NLP systems ,	1-30	1-30	The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models .	The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	12-22	23-30	that can potentially effect the performance of many NLP systems ,	including traditional count-based and deep learning models .	that can potentially effect the performance of many NLP systems ,	including traditional count-based and deep learning models .	1-30	1-30	The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models .	The problem of rare and unknown words is an important issue that can potentially effect the performance of many NLP systems , including traditional count-based and deep learning models .	1<2	none	elab-example	elab-example
P16-1014_anno1	31-35	36-48	We propose a novel way	to deal with the rare and unseen words for the neural network models	We propose a novel way	to deal with the rare and unseen words for the neural network models	31-51	31-51	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	36-48	49-51	to deal with the rare and unseen words for the neural network models	using attention .	to deal with the rare and unseen words for the neural network models	using attention .	31-51	31-51	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	1<2	none	manner-means	manner-means
P16-1014_anno1	31-35	52-57	We propose a novel way	Our model uses two softmax layers	We propose a novel way	Our model uses two softmax layers	31-51	52-92	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	52-57	58-69	Our model uses two softmax layers	in order to predict the next word in conditional language models :	Our model uses two softmax layers	in order to predict the next word in conditional language models :	52-92	52-92	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	1<2	none	enablement	enablement
P16-1014_anno1	52-57	70-81	Our model uses two softmax layers	one predicts the location of a word in the source sentence ,	Our model uses two softmax layers	one predicts the location of a word in the source sentence ,	52-92	52-92	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	70-81	82-92	one predicts the location of a word in the source sentence ,	and the other predicts a word in the shortlist vocabulary .	one predicts the location of a word in the source sentence ,	and the other predicts a word in the shortlist vocabulary .	52-92	52-92	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	Our model uses two softmax layers in order to predict the next word in conditional language models : one predicts the location of a word in the source sentence , and the other predicts a word in the shortlist vocabulary .	1<2	none	joint	joint
P16-1014_anno1	31-35	93-98,105-110	We propose a novel way	At each timestep , the decision <*> is adaptively made by an MLP	We propose a novel way	At each timestep , the decision <*> is adaptively made by an MLP	31-51	93-117	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	At each timestep , the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	93-98,105-110	99-104	At each timestep , the decision <*> is adaptively made by an MLP	of which softmax layer to use	At each timestep , the decision <*> is adaptively made by an MLP	of which softmax layer to use	93-117	93-117	At each timestep , the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context .	At each timestep , the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	105-110	111-117	is adaptively made by an MLP	which is conditioned on the context .	is adaptively made by an MLP	which is conditioned on the context .	93-117	93-117	At each timestep , the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context .	At each timestep , the decision of which softmax layer to use is adaptively made by an MLP which is conditioned on the context .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	31-35	118-125	We propose a novel way	We motivate this work from a psychological evidence	We propose a novel way	We motivate this work from a psychological evidence	31-51	118-151	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	118-125	126-131	We motivate this work from a psychological evidence	that humans naturally have a tendency	We motivate this work from a psychological evidence	that humans naturally have a tendency	118-151	118-151	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	126-131	132-141	that humans naturally have a tendency	to point towards objects in the context or the environment	that humans naturally have a tendency	to point towards objects in the context or the environment	118-151	118-151	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	1<2	none	elab-addition	elab-addition
P16-1014_anno1	126-131	142-151	that humans naturally have a tendency	when the name of an object is not known .	that humans naturally have a tendency	when the name of an object is not known .	118-151	118-151	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	We motivate this work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known .	1<2	none	condition	condition
P16-1014_anno1	152-156	157-163	Using our proposed model ,	we observe improvements on two tasks ,	Using our proposed model ,	we observe improvements on two tasks ,	152-182	152-182	Using our proposed model , we observe improvements on two tasks , neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	Using our proposed model , we observe improvements on two tasks , neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	1>2	none	manner-means	manner-means
P16-1014_anno1	31-35	157-163	We propose a novel way	we observe improvements on two tasks ,	We propose a novel way	we observe improvements on two tasks ,	31-51	152-182	We propose a novel way to deal with the rare and unseen words for the neural network models using attention .	Using our proposed model , we observe improvements on two tasks , neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	1<2	none	evaluation	evaluation
P16-1014_anno1	157-163	164-182	we observe improvements on two tasks ,	neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	we observe improvements on two tasks ,	neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	152-182	152-182	Using our proposed model , we observe improvements on two tasks , neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	Using our proposed model , we observe improvements on two tasks , neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset .	1<2	none	elab-enumember	elab-enumember
P16-1015_anno1	1-11	12-23	In this paper , we present a generalized transition-based parsing framework	where parsers are instantiated in terms of a set of control parameters	In this paper , we present a generalized transition-based parsing framework	where parsers are instantiated in terms of a set of control parameters	1-30	1-30	In this paper , we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states .	In this paper , we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states .	1<2	none	elab-addition	elab-addition
P16-1015_anno1	12-23	24-30	where parsers are instantiated in terms of a set of control parameters	that constrain transitions between parser states .	where parsers are instantiated in terms of a set of control parameters	that constrain transitions between parser states .	1-30	1-30	In this paper , we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states .	In this paper , we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states .	1<2	none	elab-addition	elab-addition
P16-1015_anno1	1-11	31-36	In this paper , we present a generalized transition-based parsing framework	This generalization provides a unified framework	In this paper , we present a generalized transition-based parsing framework	This generalization provides a unified framework	1-30	31-52	In this paper , we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states .	This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	1<2	none	elab-addition	elab-addition
P16-1015_anno1	31-36	37-52	This generalization provides a unified framework	to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	This generalization provides a unified framework	to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	31-52	31-52	This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	1<2	none	enablement	enablement
P16-1015_anno1	37-52	53-58	to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	This includes well-known transition systems ,	to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	This includes well-known transition systems ,	31-52	53-64	This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective .	This includes well-known transition systems , but also previously unstudied systems .	1<2	none	elab-addition	elab-addition
P16-1015_anno1	53-58	59-64	This includes well-known transition systems ,	but also previously unstudied systems .	This includes well-known transition systems ,	but also previously unstudied systems .	53-64	53-64	This includes well-known transition systems , but also previously unstudied systems .	This includes well-known transition systems , but also previously unstudied systems .	1<2	none	joint	joint
P16-1016_anno1	1-5	6-17	We present a transition-based system	that jointly predicts the syntactic structure and lexical units of a sentence	We present a transition-based system	that jointly predicts the syntactic structure and lexical units of a sentence	1-43	1-43	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	6-17	18-26	that jointly predicts the syntactic structure and lexical units of a sentence	by building two structures over the input words :	that jointly predicts the syntactic structure and lexical units of a sentence	by building two structures over the input words :	1-43	1-43	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	1<2	none	manner-means	manner-means
P16-1016_anno1	18-26	27-36	by building two structures over the input words :	a syntactic dependency tree and a forest of lexical units	by building two structures over the input words :	a syntactic dependency tree and a forest of lexical units	1-43	1-43	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	1<2	none	elab-enumember	elab-enumember
P16-1016_anno1	27-36	37-43	a syntactic dependency tree and a forest of lexical units	including multiword expressions ( MWEs ) .	a syntactic dependency tree and a forest of lexical units	including multiword expressions ( MWEs ) .	1-43	1-43	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	1<2	none	elab-example	elab-example
P16-1016_anno1	1-5	44-59	We present a transition-based system	This combined representation allows us to capture both the syntactic and semantic structure of MWEs ,	We present a transition-based system	This combined representation allows us to capture both the syntactic and semantic structure of MWEs ,	1-43	44-73	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	This combined representation allows us to capture both the syntactic and semantic structure of MWEs , which in turn enables deeper downstream semantic analysis , especially for semicompositional MWEs .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	44-59	60-68	This combined representation allows us to capture both the syntactic and semantic structure of MWEs ,	which in turn enables deeper downstream semantic analysis ,	This combined representation allows us to capture both the syntactic and semantic structure of MWEs ,	which in turn enables deeper downstream semantic analysis ,	44-73	44-73	This combined representation allows us to capture both the syntactic and semantic structure of MWEs , which in turn enables deeper downstream semantic analysis , especially for semicompositional MWEs .	This combined representation allows us to capture both the syntactic and semantic structure of MWEs , which in turn enables deeper downstream semantic analysis , especially for semicompositional MWEs .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	60-68	69-73	which in turn enables deeper downstream semantic analysis ,	especially for semicompositional MWEs .	which in turn enables deeper downstream semantic analysis ,	especially for semicompositional MWEs .	44-73	44-73	This combined representation allows us to capture both the syntactic and semantic structure of MWEs , which in turn enables deeper downstream semantic analysis , especially for semicompositional MWEs .	This combined representation allows us to capture both the syntactic and semantic structure of MWEs , which in turn enables deeper downstream semantic analysis , especially for semicompositional MWEs .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	1-5	74-86	We present a transition-based system	The proposed system extends the arc-standard transition system for dependency parsing with transitions	We present a transition-based system	The proposed system extends the arc-standard transition system for dependency parsing with transitions	1-43	74-92	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	The proposed system extends the arc-standard transition system for dependency parsing with transitions for building complex lexical units .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	74-86	87-92	The proposed system extends the arc-standard transition system for dependency parsing with transitions	for building complex lexical units .	The proposed system extends the arc-standard transition system for dependency parsing with transitions	for building complex lexical units .	74-92	74-92	The proposed system extends the arc-standard transition system for dependency parsing with transitions for building complex lexical units .	The proposed system extends the arc-standard transition system for dependency parsing with transitions for building complex lexical units .	1<2	none	elab-addition	elab-addition
P16-1016_anno1	93-99	100-113	Experiments on two different data sets show	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	Experiments on two different data sets show	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	93-119	93-119	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy ) compared to existing joint approaches .	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy ) compared to existing joint approaches .	1>2	none	attribution	attribution
P16-1016_anno1	1-5	100-113	We present a transition-based system	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	We present a transition-based system	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	1-43	93-119	We present a transition-based system that jointly predicts the syntactic structure and lexical units of a sentence by building two structures over the input words : a syntactic dependency tree and a forest of lexical units including multiword expressions ( MWEs ) .	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy ) compared to existing joint approaches .	1<2	none	evaluation	evaluation
P16-1016_anno1	100-113	114-119	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	compared to existing joint approaches .	that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy )	compared to existing joint approaches .	93-119	93-119	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy ) compared to existing joint approaches .	Experiments on two different data sets show that the approach significantly improves MWE identification accuracy ( and sometimes syntactic accuracy ) compared to existing joint approaches .	1<2	none	comparison	comparison
P96-1029	1-5	6-14	We report on a method	for compiling decision trees into weighted finite-state transducers .	We report on a method	for compiling decision trees into weighted finite-state transducers .	1-14	1-14	We report on a method for compiling decision trees into weighted finite-state transducers .	We report on a method for compiling decision trees into weighted finite-state transducers .	1<2	none	enablement	enablement
P96-1029	1-5	15-32	We report on a method	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string ,	We report on a method	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string ,	1-14	15-51	We report on a method for compiling decision trees into weighted finite-state transducers .	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string , and the decision at each tree node is stateable in terms of regular expressions on the input string .	1<2	none	elab-aspect	elab-aspect
P96-1029	15-32	33-51	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string ,	and the decision at each tree node is stateable in terms of regular expressions on the input string .	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string ,	and the decision at each tree node is stateable in terms of regular expressions on the input string .	15-51	15-51	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string , and the decision at each tree node is stateable in terms of regular expressions on the input string .	The key assumptions are that the tree predictions specify how to rewrite symbols from an input string , and the decision at each tree node is stateable in terms of regular expressions on the input string .	1<2	none	joint	joint
P96-1029	1-5	52-62	We report on a method	Each leaf node can then be treated as a separate rule	We report on a method	Each leaf node can then be treated as a separate rule	1-14	52-84	We report on a method for compiling decision trees into weighted finite-state transducers .	Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf .	1<2	none	elab-aspect	elab-aspect
P96-1029	52-62	63-73	Each leaf node can then be treated as a separate rule	where the left and right contexts are constructable from the decisions	Each leaf node can then be treated as a separate rule	where the left and right contexts are constructable from the decisions	52-84	52-84	Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf .	Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf .	1<2	none	elab-addition	elab-addition
P96-1029	63-73	74-84	where the left and right contexts are constructable from the decisions	made traversing the tree from the root to the leaf .	where the left and right contexts are constructable from the decisions	made traversing the tree from the root to the leaf .	52-84	52-84	Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf .	Each leaf node can then be treated as a separate rule where the left and right contexts are constructable from the decisions made traversing the tree from the root to the leaf .	1<2	none	elab-addition	elab-addition
P96-1029	1-5	85-90	We report on a method	These rules are compiled into transducers	We report on a method	These rules are compiled into transducers	1-14	85-106	We report on a method for compiling decision trees into weighted finite-state transducers .	These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in ( Mohri and Sproat , 1996 ) .	1<2	none	elab-aspect	elab-aspect
P96-1029	85-90	91-96	These rules are compiled into transducers	using the weighted rewrite-rule rule-compilation algorithm	These rules are compiled into transducers	using the weighted rewrite-rule rule-compilation algorithm	85-106	85-106	These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in ( Mohri and Sproat , 1996 ) .	These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in ( Mohri and Sproat , 1996 ) .	1<2	none	manner-means	manner-means
P96-1029	91-96	97-106	using the weighted rewrite-rule rule-compilation algorithm	described in ( Mohri and Sproat , 1996 ) .	using the weighted rewrite-rule rule-compilation algorithm	described in ( Mohri and Sproat , 1996 ) .	85-106	85-106	These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in ( Mohri and Sproat , 1996 ) .	These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in ( Mohri and Sproat , 1996 ) .	1<2	none	elab-addition	elab-addition
P16-1017_anno1	1-14	15-23	Dynamic oracle training has shown substantial improvements for dependency parsing in various settings ,	but has not been explored for constituent parsing .	Dynamic oracle training has shown substantial improvements for dependency parsing in various settings ,	but has not been explored for constituent parsing .	1-23	1-23	Dynamic oracle training has shown substantial improvements for dependency parsing in various settings , but has not been explored for constituent parsing .	Dynamic oracle training has shown substantial improvements for dependency parsing in various settings , but has not been explored for constituent parsing .	1>2	none	contrast	contrast
P16-1017_anno1	15-23	24-35	but has not been explored for constituent parsing .	The present article introduces a dynamic oracle for transition-based constituent parsing .	but has not been explored for constituent parsing .	The present article introduces a dynamic oracle for transition-based constituent parsing .	1-23	24-35	Dynamic oracle training has shown substantial improvements for dependency parsing in various settings , but has not been explored for constituent parsing .	The present article introduces a dynamic oracle for transition-based constituent parsing .	1>2	none	bg-compare	bg-compare
P16-1017_anno1	36-45	46-54,61-63	Experiments on the 9 languages of the SPMRL dataset show	that a neural greedy parser with morphological features , <*> leads to accuracies	Experiments on the 9 languages of the SPMRL dataset show	that a neural greedy parser with morphological features , <*> leads to accuracies	36-72	36-72	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	1>2	none	attribution	attribution
P16-1017_anno1	24-35	46-54,61-63	The present article introduces a dynamic oracle for transition-based constituent parsing .	that a neural greedy parser with morphological features , <*> leads to accuracies	The present article introduces a dynamic oracle for transition-based constituent parsing .	that a neural greedy parser with morphological features , <*> leads to accuracies	24-35	36-72	The present article introduces a dynamic oracle for transition-based constituent parsing .	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	1<2	none	elab-addition	elab-addition
P16-1017_anno1	46-54,61-63	55-60	that a neural greedy parser with morphological features , <*> leads to accuracies	trained with a dynamic oracle ,	that a neural greedy parser with morphological features , <*> leads to accuracies	trained with a dynamic oracle ,	36-72	36-72	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	1<2	none	elab-addition	elab-addition
P16-1017_anno1	46-54,61-63	64-72	that a neural greedy parser with morphological features , <*> leads to accuracies	comparable with the best non-reranking and non-ensemble parsers .	that a neural greedy parser with morphological features , <*> leads to accuracies	comparable with the best non-reranking and non-ensemble parsers .	36-72	36-72	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features , trained with a dynamic oracle , leads to accuracies comparable with the best non-reranking and non-ensemble parsers .	1<2	none	comparison	comparison
P16-1018_anno1	1-7	37-41	Metaphorical expressions are pervasive in natural language	This paper is the first	Metaphorical expressions are pervasive in natural language	This paper is the first	1-16	37-55	Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics .	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	1>2	none	bg-goal	bg-goal
P16-1018_anno1	1-7	8-16	Metaphorical expressions are pervasive in natural language	and pose a substantial challenge for computational semantics .	Metaphorical expressions are pervasive in natural language	and pose a substantial challenge for computational semantics .	1-16	1-16	Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics .	Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics .	1<2	none	joint	joint
P16-1018_anno1	1-7	17-36	Metaphorical expressions are pervasive in natural language	The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models ( CDSMs ) .	Metaphorical expressions are pervasive in natural language	The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models ( CDSMs ) .	1-16	17-36	Metaphorical expressions are pervasive in natural language and pose a substantial challenge for computational semantics .	The inherent compositionality of metaphor makes it an important test case for compositional distributional semantic models ( CDSMs ) .	1<2	none	elab-addition	elab-addition
P16-1018_anno1	42-43	44-55	to investigate	whether metaphorical composition warrants a distinct treatment in the CDSM framework .	to investigate	whether metaphorical composition warrants a distinct treatment in the CDSM framework .	37-55	37-55	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	1>2	none	attribution	attribution
P16-1018_anno1	37-41	44-55	This paper is the first	whether metaphorical composition warrants a distinct treatment in the CDSM framework .	This paper is the first	whether metaphorical composition warrants a distinct treatment in the CDSM framework .	37-55	37-55	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	1<2	none	elab-addition	elab-addition
P16-1018_anno1	37-41	56-59	This paper is the first	We propose a method	This paper is the first	We propose a method	37-55	56-89	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	1<2	none	elab-addition	elab-addition
P16-1018_anno1	56-59	60-69	We propose a method	to learn metaphors as linear transformations in a vector space	We propose a method	to learn metaphors as linear transformations in a vector space	56-89	56-89	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	1<2	none	elab-addition	elab-addition
P16-1018_anno1	70-71	72-89	and find	that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	and find	that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	56-89	56-89	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	1>2	none	attribution	attribution
P16-1018_anno1	56-59	72-89	We propose a method	that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	We propose a method	that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	56-89	56-89	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	We propose a method to learn metaphors as linear transformations in a vector space and find that , across a variety of semantic domains , explicitly modeling metaphor improves the resulting semantic representations .	1<2	none	progression	progression
P16-1018_anno1	37-41	90-100	This paper is the first	We then use these representations in a metaphor identification task ,	This paper is the first	We then use these representations in a metaphor identification task ,	37-55	90-111	This paper is the first to investigate whether metaphorical composition warrants a distinct treatment in the CDSM framework .	We then use these representations in a metaphor identification task , achieving a high performance of 0.82 in terms of F-score .	1<2	none	evaluation	evaluation
P16-1018_anno1	90-100	101-111	We then use these representations in a metaphor identification task ,	achieving a high performance of 0.82 in terms of F-score .	We then use these representations in a metaphor identification task ,	achieving a high performance of 0.82 in terms of F-score .	90-111	90-111	We then use these representations in a metaphor identification task , achieving a high performance of 0.82 in terms of F-score .	We then use these representations in a metaphor identification task , achieving a high performance of 0.82 in terms of F-score .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	1-15	32-41	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases	In this work we explore the use of Skip-Thought Vectors	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases	In this work we explore the use of Skip-Thought Vectors	1-31	32-58	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase .	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	1>2	none	bg-goal	bg-goal
P16-1019_anno1	1-15	16-31	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases	whether each occurrence of a phrase is a literal or idiomatic usage of the phrase .	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases	whether each occurrence of a phrase is a literal or idiomatic usage of the phrase .	1-31	1-31	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase .	Idiom token classification is the task of deciding for a set of potentially idiomatic phrases whether each occurrence of a phrase is a literal or idiomatic usage of the phrase .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	32-41	42-45	In this work we explore the use of Skip-Thought Vectors	to create distributed representations	In this work we explore the use of Skip-Thought Vectors	to create distributed representations	32-58	32-58	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	1<2	none	enablement	enablement
P16-1019_anno1	42-45	46-48	to create distributed representations	that encode features	to create distributed representations	that encode features	32-58	32-58	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	46-48	49-58	that encode features	that are predictive with respect to idiom token classification .	that encode features	that are predictive with respect to idiom token classification .	32-58	32-58	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	59-60	61-62,66-68	We show	that classifiers <*> have competitive performance	We show	that classifiers <*> have competitive performance	59-80	59-80	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	1>2	none	attribution	attribution
P16-1019_anno1	32-41	61-62,66-68	In this work we explore the use of Skip-Thought Vectors	that classifiers <*> have competitive performance	In this work we explore the use of Skip-Thought Vectors	that classifiers <*> have competitive performance	32-58	59-80	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	61-62,66-68	63-65	that classifiers <*> have competitive performance	using these representations	that classifiers <*> have competitive performance	using these representations	59-80	59-80	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	61-62,66-68	69-80	that classifiers <*> have competitive performance	compared with the state of the art in idiom token classification .	that classifiers <*> have competitive performance	compared with the state of the art in idiom token classification .	59-80	59-80	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	We show that classifiers using these representations have competitive performance compared with the state of the art in idiom token classification .	1<2	none	comparison	comparison
P16-1019_anno1	32-41	81-90	In this work we explore the use of Skip-Thought Vectors	Importantly , however , our models use only the sentence	In this work we explore the use of Skip-Thought Vectors	Importantly , however , our models use only the sentence	32-58	81-112	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	Importantly , however , our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	81-90	91-96	Importantly , however , our models use only the sentence	containing the target phrase as input	Importantly , however , our models use only the sentence	containing the target phrase as input	81-112	81-112	Importantly , however , our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	Importantly , however , our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	81-90	97-112	Importantly , however , our models use only the sentence	and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	Importantly , however , our models use only the sentence	and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	81-112	81-112	Importantly , however , our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	Importantly , however , our models use only the sentence containing the target phrase as input and are thus less dependent on a potentially inaccurate or incomplete model of discourse context .	1<2	none	result	result
P16-1019_anno1	32-41	113-121	In this work we explore the use of Skip-Thought Vectors	We further demonstrate the feasibility of using these representations	In this work we explore the use of Skip-Thought Vectors	We further demonstrate the feasibility of using these representations	32-58	113-130	In this work we explore the use of Skip-Thought Vectors to create distributed representations that encode features that are predictive with respect to idiom token classification .	We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier .	1<2	none	elab-addition	elab-addition
P16-1019_anno1	113-121	122-130	We further demonstrate the feasibility of using these representations	to train a competitive general idiom token classifier .	We further demonstrate the feasibility of using these representations	to train a competitive general idiom token classifier .	113-130	113-130	We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier .	We further demonstrate the feasibility of using these representations to train a competitive general idiom token classifier .	1<2	none	enablement	enablement
P16-1020_anno1	1-5	6-13	We present a novel method	for jointly learning compositional and noncompositional phrase embeddings	We present a novel method	for jointly learning compositional and noncompositional phrase embeddings	1-26	1-26	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	1<2	none	elab-addition	elab-addition
P16-1020_anno1	6-13	14-20	for jointly learning compositional and noncompositional phrase embeddings	by adaptively weighting both types of embeddings	for jointly learning compositional and noncompositional phrase embeddings	by adaptively weighting both types of embeddings	1-26	1-26	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	1<2	none	manner-means	manner-means
P16-1020_anno1	14-20	21-26	by adaptively weighting both types of embeddings	using a compositionality scoring function .	by adaptively weighting both types of embeddings	using a compositionality scoring function .	1-26	1-26	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	1<2	none	manner-means	manner-means
P16-1020_anno1	1-5	27-31	We present a novel method	The scoring function is used	We present a novel method	The scoring function is used	1-26	27-58	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	1<2	none	elab-addition	elab-addition
P16-1020_anno1	27-31	32-41	The scoring function is used	to quantify the level of compositionality of each phrase ,	The scoring function is used	to quantify the level of compositionality of each phrase ,	27-58	27-58	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	1<2	none	enablement	enablement
P16-1020_anno1	27-31	42-53	The scoring function is used	and the parameters of the function are jointly optimized with the objective	The scoring function is used	and the parameters of the function are jointly optimized with the objective	27-58	27-58	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	1<2	none	joint	joint
P16-1020_anno1	42-53	54-58	and the parameters of the function are jointly optimized with the objective	for learning phrase embeddings .	and the parameters of the function are jointly optimized with the objective	for learning phrase embeddings .	27-58	27-58	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	The scoring function is used to quantify the level of compositionality of each phrase , and the parameters of the function are jointly optimized with the objective for learning phrase embeddings .	1<2	none	elab-addition	elab-addition
P16-1020_anno1	1-5	59-71	We present a novel method	In experiments , we apply the adaptive joint learning method to the task	We present a novel method	In experiments , we apply the adaptive joint learning method to the task	1-26	59-104	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	1<2	none	evaluation	evaluation
P16-1020_anno1	59-71	72-79	In experiments , we apply the adaptive joint learning method to the task	of learning embeddings of transitive verb phrases ,	In experiments , we apply the adaptive joint learning method to the task	of learning embeddings of transitive verb phrases ,	59-104	59-104	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	1<2	none	elab-addition	elab-addition
P16-1020_anno1	80-81	82-95	and show	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	and show	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	59-104	59-104	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	1>2	none	attribution	attribution
P16-1020_anno1	59-71	82-95	In experiments , we apply the adaptive joint learning method to the task	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	In experiments , we apply the adaptive joint learning method to the task	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	59-104	59-104	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	1<2	none	progression	progression
P16-1020_anno1	82-95	96-104	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	substantially outperforming the previous state of the art .	that the compositionality scores have strong correlation with human ratings for verb-object compositionality ,	substantially outperforming the previous state of the art .	59-104	59-104	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	In experiments , we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases , and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality , substantially outperforming the previous state of the art .	1<2	none	elab-addition	elab-addition
P16-1020_anno1	1-5	105-121	We present a novel method	Moreover , our embeddings improve upon the previous best model on a transitive verb disambiguation task .	We present a novel method	Moreover , our embeddings improve upon the previous best model on a transitive verb disambiguation task .	1-26	105-121	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	Moreover , our embeddings improve upon the previous best model on a transitive verb disambiguation task .	1<2	none	evaluation	evaluation
P16-1020_anno1	122-124	125-137	We also show	that a simple ensemble technique further improves the results for both tasks .	We also show	that a simple ensemble technique further improves the results for both tasks .	122-137	122-137	We also show that a simple ensemble technique further improves the results for both tasks .	We also show that a simple ensemble technique further improves the results for both tasks .	1>2	none	attribution	attribution
P16-1020_anno1	1-5	125-137	We present a novel method	that a simple ensemble technique further improves the results for both tasks .	We present a novel method	that a simple ensemble technique further improves the results for both tasks .	1-26	122-137	We present a novel method for jointly learning compositional and noncompositional phrase embeddings by adaptively weighting both types of embeddings using a compositionality scoring function .	We also show that a simple ensemble technique further improves the results for both tasks .	1<2	none	evaluation	evaluation
P16-1021_anno1	1-9	69-73	Metaphor is a common linguistic tool in communication ,	We present a new approach	Metaphor is a common linguistic tool in communication ,	We present a new approach	1-22	69-107	Metaphor is a common linguistic tool in communication , making its detection in discourse a crucial task for natural language understanding .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1>2	none	bg-goal	bg-goal
P16-1021_anno1	1-9	10-22	Metaphor is a common linguistic tool in communication ,	making its detection in discourse a crucial task for natural language understanding .	Metaphor is a common linguistic tool in communication ,	making its detection in discourse a crucial task for natural language understanding .	1-22	1-22	Metaphor is a common linguistic tool in communication , making its detection in discourse a crucial task for natural language understanding .	Metaphor is a common linguistic tool in communication , making its detection in discourse a crucial task for natural language understanding .	1<2	none	result	result
P16-1021_anno1	23-45	69-73	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	We present a new approach	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	We present a new approach	23-45	69-107	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1>2	none	bg-compare	bg-compare
P16-1021_anno1	46-51	52-59	While these methods are effective ,	they tend to overclassify target words as metaphorical	While these methods are effective ,	they tend to overclassify target words as metaphorical	46-68	46-68	While these methods are effective , they tend to overclassify target words as metaphorical when they deviate in meaning from its context .	While these methods are effective , they tend to overclassify target words as metaphorical when they deviate in meaning from its context .	1>2	none	condition	condition
P16-1021_anno1	23-45	52-59	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	they tend to overclassify target words as metaphorical	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	they tend to overclassify target words as metaphorical	23-45	46-68	One popular approach to this challenge is to capture semantic incohesion between a metaphor and the dominant topic of the surrounding text .	While these methods are effective , they tend to overclassify target words as metaphorical when they deviate in meaning from its context .	1<2	none	elab-addition	elab-addition
P16-1021_anno1	52-59	60-68	they tend to overclassify target words as metaphorical	when they deviate in meaning from its context .	they tend to overclassify target words as metaphorical	when they deviate in meaning from its context .	46-68	46-68	While these methods are effective , they tend to overclassify target words as metaphorical when they deviate in meaning from its context .	While these methods are effective , they tend to overclassify target words as metaphorical when they deviate in meaning from its context .	1<2	none	condition	condition
P16-1021_anno1	69-73	74-85	We present a new approach	that ( 1 ) distinguishes literal and non-literal use of target words	We present a new approach	that ( 1 ) distinguishes literal and non-literal use of target words	69-107	69-107	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1<2	none	elab-addition	elab-addition
P16-1021_anno1	74-85	86-90	that ( 1 ) distinguishes literal and non-literal use of target words	by examining sentence-level topic transitions	that ( 1 ) distinguishes literal and non-literal use of target words	by examining sentence-level topic transitions	69-107	69-107	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1<2	none	manner-means	manner-means
P16-1021_anno1	74-85	91-99	that ( 1 ) distinguishes literal and non-literal use of target words	and ( 2 ) captures the motivation of speakers	that ( 1 ) distinguishes literal and non-literal use of target words	and ( 2 ) captures the motivation of speakers	69-107	69-107	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1<2	none	joint	joint
P16-1021_anno1	91-99	100-107	and ( 2 ) captures the motivation of speakers	to express emotions and abstract concepts metaphorically .	and ( 2 ) captures the motivation of speakers	to express emotions and abstract concepts metaphorically .	69-107	69-107	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	1<2	none	enablement	enablement
P16-1021_anno1	69-73	108-127	We present a new approach	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	We present a new approach	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	69-107	108-127	We present a new approach that ( 1 ) distinguishes literal and non-literal use of target words by examining sentence-level topic transitions and ( 2 ) captures the motivation of speakers to express emotions and abstract concepts metaphorically .	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	1<2	none	evaluation	evaluation
P16-1021_anno1	108-127	128-145	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	These experimental results also reveal a tendency toward metaphor usage in personal topics and certain emotional contexts .	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	These experimental results also reveal a tendency toward metaphor usage in personal topics and certain emotional contexts .	108-127	128-145	Experiments on an online breast cancer discussion forum dataset demonstrate a significant improvement in metaphor detection over the state-of-theart .	These experimental results also reveal a tendency toward metaphor usage in personal topics and certain emotional contexts .	1<2	none	elab-addition	elab-addition
P16-1022_anno1	1-11	71-86	Neural networks are among the state-ofthe-art techniques for language modeling .	In this paper , we propose to compress neural language models by sparse word representations .	Neural networks are among the state-ofthe-art techniques for language modeling .	In this paper , we propose to compress neural language models by sparse word representations .	1-11	71-86	Neural networks are among the state-ofthe-art techniques for language modeling .	In this paper , we propose to compress neural language models by sparse word representations .	1>2	none	bg-compare	bg-compare
P16-1022_anno1	1-11	12-26	Neural networks are among the state-ofthe-art techniques for language modeling .	Existing neural language models typically map discrete words to distributed , dense vector representations .	Neural networks are among the state-ofthe-art techniques for language modeling .	Existing neural language models typically map discrete words to distributed , dense vector representations .	1-11	12-26	Neural networks are among the state-ofthe-art techniques for language modeling .	Existing neural language models typically map discrete words to distributed , dense vector representations .	1<2	none	elab-addition	elab-addition
P16-1022_anno1	27-38	39-49	After information processing of the preceding context words by hidden layers ,	an output layer estimates the probability of the next word .	After information processing of the preceding context words by hidden layers ,	an output layer estimates the probability of the next word .	27-49	27-49	After information processing of the preceding context words by hidden layers , an output layer estimates the probability of the next word .	After information processing of the preceding context words by hidden layers , an output layer estimates the probability of the next word .	1>2	none	temporal	temporal
P16-1022_anno1	12-26	39-49	Existing neural language models typically map discrete words to distributed , dense vector representations .	an output layer estimates the probability of the next word .	Existing neural language models typically map discrete words to distributed , dense vector representations .	an output layer estimates the probability of the next word .	12-26	27-49	Existing neural language models typically map discrete words to distributed , dense vector representations .	After information processing of the preceding context words by hidden layers , an output layer estimates the probability of the next word .	1<2	none	elab-addition	elab-addition
P16-1022_anno1	12-26	50-55	Existing neural language models typically map discrete words to distributed , dense vector representations .	Such approaches are time- and memory-intensive	Existing neural language models typically map discrete words to distributed , dense vector representations .	Such approaches are time- and memory-intensive	12-26	50-70	Existing neural language models typically map discrete words to distributed , dense vector representations .	Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer .	1<2	none	elab-addition	elab-addition
P16-1022_anno1	50-55	56-70	Such approaches are time- and memory-intensive	because of the large numbers of parameters for word embeddings and the output layer .	Such approaches are time- and memory-intensive	because of the large numbers of parameters for word embeddings and the output layer .	50-70	50-70	Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer .	Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer .	1<2	none	exp-reason	exp-reason
P16-1022_anno1	71-86	87-108	In this paper , we propose to compress neural language models by sparse word representations .	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size ,	In this paper , we propose to compress neural language models by sparse word representations .	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size ,	71-86	87-113	In this paper , we propose to compress neural language models by sparse word representations .	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size , which is almost imperceptible .	1<2	none	evaluation	evaluation
P16-1022_anno1	87-108	109-113	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size ,	which is almost imperceptible .	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size ,	which is almost imperceptible .	87-113	87-113	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size , which is almost imperceptible .	In the experiments , the number of parameters in our model increases very slowly with the growth of the vocabulary size , which is almost imperceptible .	1<2	none	elab-addition	elab-addition
P16-1022_anno1	71-86	114-128	In this paper , we propose to compress neural language models by sparse word representations .	Moreover , our approach not only reduces the parameter space to a large extent ,	In this paper , we propose to compress neural language models by sparse word representations .	Moreover , our approach not only reduces the parameter space to a large extent ,	71-86	114-140	In this paper , we propose to compress neural language models by sparse word representations .	Moreover , our approach not only reduces the parameter space to a large extent , but also improves the performance in terms of the perplexity measure .	1<2	none	evaluation	evaluation
P16-1022_anno1	114-128	129-140	Moreover , our approach not only reduces the parameter space to a large extent ,	but also improves the performance in terms of the perplexity measure .	Moreover , our approach not only reduces the parameter space to a large extent ,	but also improves the performance in terms of the perplexity measure .	114-140	114-140	Moreover , our approach not only reduces the parameter space to a large extent , but also improves the performance in terms of the perplexity measure .	Moreover , our approach not only reduces the parameter space to a large extent , but also improves the performance in terms of the perplexity measure .	1<2	none	progression	progression
P16-1023_anno1	1-12	13-19	We introduce a new methodology for intrinsic evaluation of word representations .	Specifically , we identify four fundamental criteria	We introduce a new methodology for intrinsic evaluation of word representations .	Specifically , we identify four fundamental criteria	1-12	13-52	We introduce a new methodology for intrinsic evaluation of word representations .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	elab-addition	elab-addition
P16-1023_anno1	13-19	20-26	Specifically , we identify four fundamental criteria	based on the characteristics of natural language	Specifically , we identify four fundamental criteria	based on the characteristics of natural language	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	bg-general	bg-general
P16-1023_anno1	20-26	27-33	based on the characteristics of natural language	that pose difficulties to NLP systems ;	based on the characteristics of natural language	that pose difficulties to NLP systems ;	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	elab-addition	elab-addition
P16-1023_anno1	13-19	34-36	Specifically , we identify four fundamental criteria	and develop tests	Specifically , we identify four fundamental criteria	and develop tests	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	joint	joint
P16-1023_anno1	37-39	40-47	that directly show	whether or not representations contain the subspaces necessary	that directly show	whether or not representations contain the subspaces necessary	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1>2	none	attribution	attribution
P16-1023_anno1	34-36	40-47	and develop tests	whether or not representations contain the subspaces necessary	and develop tests	whether or not representations contain the subspaces necessary	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	elab-addition	elab-addition
P16-1023_anno1	40-47	48-52	whether or not representations contain the subspaces necessary	to satisfy these criteria .	whether or not representations contain the subspaces necessary	to satisfy these criteria .	13-52	13-52	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	Specifically , we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems ; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria .	1<2	none	elab-addition	elab-addition
P16-1023_anno1	1-12	53-67	We introduce a new methodology for intrinsic evaluation of word representations .	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words	We introduce a new methodology for intrinsic evaluation of word representations .	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words	1-12	53-75	We introduce a new methodology for intrinsic evaluation of word representations .	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points .	1<2	none	evaluation	evaluation
P16-1023_anno1	53-67	68-75	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words	and thus view vector representations as points .	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words	and thus view vector representations as points .	53-75	53-75	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points .	Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points .	1<2	none	result	result
P16-1023_anno1	1-12	76-85	We introduce a new methodology for intrinsic evaluation of word representations .	We show the limits of these point-based intrinsic evaluations .	We introduce a new methodology for intrinsic evaluation of word representations .	We show the limits of these point-based intrinsic evaluations .	1-12	76-85	We introduce a new methodology for intrinsic evaluation of word representations .	We show the limits of these point-based intrinsic evaluations .	1<2	none	evaluation	evaluation
P16-1023_anno1	1-12	86-103	We introduce a new methodology for intrinsic evaluation of word representations .	We apply our evaluation methodology to the comparison of a count vector model and several neural network models	We introduce a new methodology for intrinsic evaluation of word representations .	We apply our evaluation methodology to the comparison of a count vector model and several neural network models	1-12	86-111	We introduce a new methodology for intrinsic evaluation of word representations .	We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models .	1<2	none	evaluation	evaluation
P16-1023_anno1	86-103	104-111	We apply our evaluation methodology to the comparison of a count vector model and several neural network models	and demonstrate important properties of these models .	We apply our evaluation methodology to the comparison of a count vector model and several neural network models	and demonstrate important properties of these models .	86-111	86-111	We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models .	We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models .	1<2	none	progression	progression
P16-1024_anno1	1-23	58-77	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	1-23	58-94	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions ( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	1>2	none	bg-compare	bg-compare
P16-1024_anno1	1-23	24-41	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces ,	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces ,	1-23	24-57	A shared bilingual word embedding space ( SBWES ) is an indispensable resource in a variety of cross-language NLP and IR tasks .	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces , where the mapping critically relies on a seed word lexicon used in the learning process .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	24-41	42-51	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces ,	where the mapping critically relies on a seed word lexicon	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces ,	where the mapping critically relies on a seed word lexicon	24-57	24-57	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces , where the mapping critically relies on a seed word lexicon used in the learning process .	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces , where the mapping critically relies on a seed word lexicon used in the learning process .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	42-51	52-57	where the mapping critically relies on a seed word lexicon	used in the learning process .	where the mapping critically relies on a seed word lexicon	used in the learning process .	24-57	24-57	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces , where the mapping critically relies on a seed word lexicon used in the learning process .	A common approach to the SBWES induction is to learn a mapping function between monolingual semantic spaces , where the mapping critically relies on a seed word lexicon used in the learning process .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	58-77	78-94	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	58-94	58-94	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions ( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions ( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	1<2	none	elab-enumember	elab-enumember
P16-1024_anno1	95-101	102-116	On the basis of our analysis ,	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	On the basis of our analysis ,	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	95-116	95-116	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	1>2	none	bg-general	bg-general
P16-1024_anno1	58-77	102-116	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	58-94	95-116	In this work , we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions ( i.e. , lexicon source , lexicon size , translation method , translation pair reliability ) .	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	102-116	117-129	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces	95-116	117-143	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from a seed document-level embedding space .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	117-129	130-143	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces	using only highly reliable symmetric translation pairs from a seed document-level embedding space .	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces	using only highly reliable symmetric translation pairs from a seed document-level embedding space .	117-143	117-143	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from a seed document-level embedding space .	This model ( HYBWE ) learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from a seed document-level embedding space .	1<2	none	manner-means	manner-means
P16-1024_anno1	102-116	144-155	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs	95-116	144-183	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	1<2	none	evaluation	evaluation
P16-1024_anno1	156-157	165-174	and show	our new HYBWE model outperforms benchmarking BWE learning models ,	and show	our new HYBWE model outperforms benchmarking BWE learning models ,	144-183	144-183	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	1>2	none	attribution	attribution
P16-1024_anno1	158-164	165-174	that by carefully selecting reliable translation pairs	our new HYBWE model outperforms benchmarking BWE learning models ,	that by carefully selecting reliable translation pairs	our new HYBWE model outperforms benchmarking BWE learning models ,	144-183	144-183	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	1>2	none	manner-means	manner-means
P16-1024_anno1	144-155	165-174	We perform bilingual lexicon learning ( BLL ) with 3 language pairs	our new HYBWE model outperforms benchmarking BWE learning models ,	We perform bilingual lexicon learning ( BLL ) with 3 language pairs	our new HYBWE model outperforms benchmarking BWE learning models ,	144-183	144-183	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	1<2	none	progression	progression
P16-1024_anno1	165-174	175-183	our new HYBWE model outperforms benchmarking BWE learning models ,	all of which use more expensive bilingual signals .	our new HYBWE model outperforms benchmarking BWE learning models ,	all of which use more expensive bilingual signals .	144-183	144-183	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	We perform bilingual lexicon learning ( BLL ) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models , all of which use more expensive bilingual signals .	1<2	none	elab-addition	elab-addition
P16-1024_anno1	184-187	188-193	Effectively , we demonstrate	that a SBWES may be induced	Effectively , we demonstrate	that a SBWES may be induced	184-210	184-210	Effectively , we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	Effectively , we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	1>2	none	attribution	attribution
P16-1024_anno1	102-116	188-193	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	that a SBWES may be induced	we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	that a SBWES may be induced	95-116	184-210	On the basis of our analysis , we propose a simple but effective hybrid bilingual word embedding ( BWE ) model .	Effectively , we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	1<2	none	evaluation	evaluation
P16-1024_anno1	188-193	194-210	that a SBWES may be induced	by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	that a SBWES may be induced	by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	184-210	184-210	Effectively , we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	Effectively , we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal ( document alignments ) along with monolingual data .	1<2	none	manner-means	manner-means
P16-1025_anno1	1-11	12-24	We propose a brand new `` Liberal '' Event Extraction paradigm	to extract events and discover event schemas from any input corpus simultaneously .	We propose a brand new `` Liberal '' Event Extraction paradigm	to extract events and discover event schemas from any input corpus simultaneously .	1-24	1-24	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	1<2	none	enablement	enablement
P16-1025_anno1	1-11	25-27,35-37	We propose a brand new `` Liberal '' Event Extraction paradigm	We incorporate symbolic <*> and distributional semantics	We propose a brand new `` Liberal '' Event Extraction paradigm	We incorporate symbolic <*> and distributional semantics	1-24	25-63	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	elab-addition	elab-addition
P16-1025_anno1	25-27,35-37	28-34	We incorporate symbolic <*> and distributional semantics	( e.g. , Abstract Meaning Representation )	We incorporate symbolic <*> and distributional semantics	( e.g. , Abstract Meaning Representation )	25-63	25-63	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	elab-example	elab-example
P16-1025_anno1	25-27,35-37	38-43	We incorporate symbolic <*> and distributional semantics	to detect and represent event structures	We incorporate symbolic <*> and distributional semantics	to detect and represent event structures	25-63	25-63	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	enablement	enablement
P16-1025_anno1	25-27,35-37	44-49	We incorporate symbolic <*> and distributional semantics	and adopt a joint typing framework	We incorporate symbolic <*> and distributional semantics	and adopt a joint typing framework	25-63	25-63	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	joint	joint
P16-1025_anno1	44-49	50-57	and adopt a joint typing framework	to simultaneously extract event types and argument roles	and adopt a joint typing framework	to simultaneously extract event types and argument roles	25-63	25-63	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	enablement	enablement
P16-1025_anno1	50-57	58-63	to simultaneously extract event types and argument roles	and discover an event schema .	to simultaneously extract event types and argument roles	and discover an event schema .	25-63	25-63	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	We incorporate symbolic ( e.g. , Abstract Meaning Representation ) and distributional semantics to detect and represent event structures and adopt a joint typing framework to simultaneously extract event types and argument roles and discover an event schema .	1<2	none	joint	joint
P16-1025_anno1	64-70	71-85	Experiments on general and specific domains demonstrate	that this framework can construct high-quality schemas with many event and argument role types ,	Experiments on general and specific domains demonstrate	that this framework can construct high-quality schemas with many event and argument role types ,	64-100	64-100	Experiments on general and specific domains demonstrate that this framework can construct high-quality schemas with many event and argument role types , covering a high proportion of event types and argument roles in manually defined schemas .	Experiments on general and specific domains demonstrate that this framework can construct high-quality schemas with many event and argument role types , covering a high proportion of event types and argument roles in manually defined schemas .	1>2	none	attribution	attribution
P16-1025_anno1	1-11	71-85	We propose a brand new `` Liberal '' Event Extraction paradigm	that this framework can construct high-quality schemas with many event and argument role types ,	We propose a brand new `` Liberal '' Event Extraction paradigm	that this framework can construct high-quality schemas with many event and argument role types ,	1-24	64-100	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	Experiments on general and specific domains demonstrate that this framework can construct high-quality schemas with many event and argument role types , covering a high proportion of event types and argument roles in manually defined schemas .	1<2	none	evaluation	evaluation
P16-1025_anno1	71-85	86-100	that this framework can construct high-quality schemas with many event and argument role types ,	covering a high proportion of event types and argument roles in manually defined schemas .	that this framework can construct high-quality schemas with many event and argument role types ,	covering a high proportion of event types and argument roles in manually defined schemas .	64-100	64-100	Experiments on general and specific domains demonstrate that this framework can construct high-quality schemas with many event and argument role types , covering a high proportion of event types and argument roles in manually defined schemas .	Experiments on general and specific domains demonstrate that this framework can construct high-quality schemas with many event and argument role types , covering a high proportion of event types and argument roles in manually defined schemas .	1<2	none	elab-example	elab-example
P16-1025_anno1	101-102	103-105,109-113	We show	that extraction performance <*> is comparable to supervised models	We show	that extraction performance <*> is comparable to supervised models	101-127	101-127	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	1>2	none	attribution	attribution
P16-1025_anno1	1-11	103-105,109-113	We propose a brand new `` Liberal '' Event Extraction paradigm	that extraction performance <*> is comparable to supervised models	We propose a brand new `` Liberal '' Event Extraction paradigm	that extraction performance <*> is comparable to supervised models	1-24	101-127	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	1<2	none	evaluation	evaluation
P16-1025_anno1	103-105,109-113	106-108	that extraction performance <*> is comparable to supervised models	using discovered schemas	that extraction performance <*> is comparable to supervised models	using discovered schemas	101-127	101-127	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	1<2	none	manner-means	manner-means
P16-1025_anno1	109-113	114-120	is comparable to supervised models	trained from a large amount of data	is comparable to supervised models	trained from a large amount of data	101-127	101-127	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	1<2	none	elab-addition	elab-addition
P16-1025_anno1	114-120	121-127	trained from a large amount of data	labeled according to predefined event types .	trained from a large amount of data	labeled according to predefined event types .	101-127	101-127	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	We show that extraction performance using discovered schemas is comparable to supervised models trained from a large amount of data labeled according to predefined event types .	1<2	none	elab-addition	elab-addition
P16-1025_anno1	1-11	128-138	We propose a brand new `` Liberal '' Event Extraction paradigm	The extraction quality of new event types is also promising .	We propose a brand new `` Liberal '' Event Extraction paradigm	The extraction quality of new event types is also promising .	1-24	128-138	We propose a brand new `` Liberal '' Event Extraction paradigm to extract events and discover event schemas from any input corpus simultaneously .	The extraction quality of new event types is also promising .	1<2	none	evaluation	evaluation
P16-1026_anno1	1-9	35-43	Event extraction from texts aims to detect structured information	In this paper , we propose a novel approach	Event extraction from texts aims to detect structured information	In this paper , we propose a novel approach	1-22	35-63	Event extraction from texts aims to detect structured information such as what has happened , to whom , where and when .	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	1>2	none	bg-goal	bg-goal
P16-1026_anno1	1-9	10-22	Event extraction from texts aims to detect structured information	such as what has happened , to whom , where and when .	Event extraction from texts aims to detect structured information	such as what has happened , to whom , where and when .	1-22	1-22	Event extraction from texts aims to detect structured information such as what has happened , to whom , where and when .	Event extraction from texts aims to detect structured information such as what has happened , to whom , where and when .	1<2	none	elab-example	elab-example
P16-1026_anno1	1-9	23-34	Event extraction from texts aims to detect structured information	Event extraction and visualization are typically considered as two different tasks .	Event extraction from texts aims to detect structured information	Event extraction and visualization are typically considered as two different tasks .	1-22	23-34	Event extraction from texts aims to detect structured information such as what has happened , to whom , where and when .	Event extraction and visualization are typically considered as two different tasks .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	35-43	44-47	In this paper , we propose a novel approach	based on probabilistic modelling	In this paper , we propose a novel approach	based on probabilistic modelling	35-63	35-63	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	1<2	none	bg-general	bg-general
P16-1026_anno1	35-43	48-55	In this paper , we propose a novel approach	to jointly extract and visualize events from tweets	In this paper , we propose a novel approach	to jointly extract and visualize events from tweets	35-63	35-63	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	1<2	none	enablement	enablement
P16-1026_anno1	48-55	56-63	to jointly extract and visualize events from tweets	where both tasks benefit from each other .	to jointly extract and visualize events from tweets	where both tasks benefit from each other .	35-63	35-63	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	35-43	64-84	In this paper , we propose a novel approach	We model each event as a joint distribution over named entities , a date , a location and event-related keywords .	In this paper , we propose a novel approach	We model each event as a joint distribution over named entities , a date , a location and event-related keywords .	35-63	64-84	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	We model each event as a joint distribution over named entities , a date , a location and event-related keywords .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	35-43	85-100	In this paper , we propose a novel approach	Moreover , both tweets and event instances are associated with coordinates in the visualization space .	In this paper , we propose a novel approach	Moreover , both tweets and event instances are associated with coordinates in the visualization space .	35-63	85-100	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	Moreover , both tweets and event instances are associated with coordinates in the visualization space .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	35-43	101-103,120-125	In this paper , we propose a novel approach	The manifold assumption <*> is incorporated into the learning framework	In this paper , we propose a novel approach	The manifold assumption <*> is incorporated into the learning framework	35-63	101-129	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	The manifold assumption that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	101-103,120-125	104-119	The manifold assumption <*> is incorporated into the learning framework	that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space	The manifold assumption <*> is incorporated into the learning framework	that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space	101-129	101-129	The manifold assumption that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization .	The manifold assumption that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization .	1<2	none	elab-addition	elab-addition
P16-1026_anno1	101-103,120-125	126-129	The manifold assumption <*> is incorporated into the learning framework	using a regularization .	The manifold assumption <*> is incorporated into the learning framework	using a regularization .	101-129	101-129	The manifold assumption that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization .	The manifold assumption that the intrinsic geometry of tweets is a low-rank , non-linear manifold within the high-dimensional space is incorporated into the learning framework using a regularization .	1<2	none	manner-means	manner-means
P16-1026_anno1	130-132	133-145	Experimental results show	that the proposed approach can effectively deal with both event extraction and visualization	Experimental results show	that the proposed approach can effectively deal with both event extraction and visualization	130-166	130-166	Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	1>2	none	attribution	attribution
P16-1026_anno1	35-43	133-145	In this paper , we propose a novel approach	that the proposed approach can effectively deal with both event extraction and visualization	In this paper , we propose a novel approach	that the proposed approach can effectively deal with both event extraction and visualization	35-63	130-166	In this paper , we propose a novel approach based on probabilistic modelling to jointly extract and visualize events from tweets where both tasks benefit from each other .	Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	1<2	none	evaluation	evaluation
P16-1026_anno1	133-145	146-166	that the proposed approach can effectively deal with both event extraction and visualization	and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	that the proposed approach can effectively deal with both event extraction and visualization	and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	130-166	130-166	Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	Experimental results show that the proposed approach can effectively deal with both event extraction and visualization and performs remarkably better than both the state-of-the-art event extraction method and a pipeline approach for event extraction and visualization .	1<2	none	joint	joint
P16-1027_anno1	1-17	41-50	There is a small but growing body of research on statistical scripts , models of event sequences	We compare these systems with recent Recurrent Neural Net models	There is a small but growing body of research on statistical scripts , models of event sequences	We compare these systems with recent Recurrent Neural Net models	1-27	41-79	There is a small but growing body of research on statistical scripts , models of event sequences that allow probabilistic inference of implicit events from documents .	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	1>2	none	bg-compare	bg-compare
P16-1027_anno1	1-17	18-27	There is a small but growing body of research on statistical scripts , models of event sequences	that allow probabilistic inference of implicit events from documents .	There is a small but growing body of research on statistical scripts , models of event sequences	that allow probabilistic inference of implicit events from documents .	1-27	1-27	There is a small but growing body of research on statistical scripts , models of event sequences that allow probabilistic inference of implicit events from documents .	There is a small but growing body of research on statistical scripts , models of event sequences that allow probabilistic inference of implicit events from documents .	1<2	none	elab-addition	elab-addition
P16-1027_anno1	1-17	28-34	There is a small but growing body of research on statistical scripts , models of event sequences	These systems operate on structured verb-argument events	There is a small but growing body of research on statistical scripts , models of event sequences	These systems operate on structured verb-argument events	1-27	28-40	There is a small but growing body of research on statistical scripts , models of event sequences that allow probabilistic inference of implicit events from documents .	These systems operate on structured verb-argument events produced by an NLP pipeline .	1<2	none	elab-addition	elab-addition
P16-1027_anno1	28-34	35-40	These systems operate on structured verb-argument events	produced by an NLP pipeline .	These systems operate on structured verb-argument events	produced by an NLP pipeline .	28-40	28-40	These systems operate on structured verb-argument events produced by an NLP pipeline .	These systems operate on structured verb-argument events produced by an NLP pipeline .	1<2	none	elab-addition	elab-addition
P16-1027_anno1	41-50	51-56	We compare these systems with recent Recurrent Neural Net models	that directly operate on raw tokens	We compare these systems with recent Recurrent Neural Net models	that directly operate on raw tokens	41-79	41-79	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	1<2	none	elab-addition	elab-addition
P16-1027_anno1	41-50	57-60	We compare these systems with recent Recurrent Neural Net models	to predict sentences ,	We compare these systems with recent Recurrent Neural Net models	to predict sentences ,	41-79	41-79	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	1<2	none	enablement	enablement
P16-1027_anno1	41-50	61-70	We compare these systems with recent Recurrent Neural Net models	finding the latter to be roughly comparable to the former	We compare these systems with recent Recurrent Neural Net models	finding the latter to be roughly comparable to the former	41-79	41-79	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	1<2	none	result	result
P16-1027_anno1	61-70	71-79	finding the latter to be roughly comparable to the former	in terms of predicting missing events in documents .	finding the latter to be roughly comparable to the former	in terms of predicting missing events in documents .	41-79	41-79	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences , finding the latter to be roughly comparable to the former in terms of predicting missing events in documents .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	1-9	40-44	Natural language understanding often requires deep semantic knowledge .	We develop two distinct models	Natural language understanding often requires deep semantic knowledge .	We develop two distinct models	1-9	40-63	Natural language understanding often requires deep semantic knowledge .	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	1>2	none	bg-goal	bg-goal
P16-1028_anno1	10-14	17-30	Expanding on previous proposals ,	that some important aspects of semantic knowledge can be modeled as a language model	Expanding on previous proposals ,	that some important aspects of semantic knowledge can be modeled as a language model	10-39	10-39	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	1>2	none	bg-general	bg-general
P16-1028_anno1	15-16	17-30	we suggest	that some important aspects of semantic knowledge can be modeled as a language model	we suggest	that some important aspects of semantic knowledge can be modeled as a language model	10-39	10-39	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	1>2	none	attribution	attribution
P16-1028_anno1	1-9	17-30	Natural language understanding often requires deep semantic knowledge .	that some important aspects of semantic knowledge can be modeled as a language model	Natural language understanding often requires deep semantic knowledge .	that some important aspects of semantic knowledge can be modeled as a language model	1-9	10-39	Natural language understanding often requires deep semantic knowledge .	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	17-30	31-39	that some important aspects of semantic knowledge can be modeled as a language model	if done at an appropriate level of abstraction .	that some important aspects of semantic knowledge can be modeled as a language model	if done at an appropriate level of abstraction .	10-39	10-39	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	Expanding on previous proposals , we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction .	1<2	none	condition	condition
P16-1028_anno1	40-44	45-52	We develop two distinct models	that capture semantic frame chains and discourse information	We develop two distinct models	that capture semantic frame chains and discourse information	40-63	40-63	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	45-52	53-63	that capture semantic frame chains and discourse information	while abstracting over the specific mentions of predicates and entities .	that capture semantic frame chains and discourse information	while abstracting over the specific mentions of predicates and entities .	40-63	40-63	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	1<2	none	temporal	temporal
P16-1028_anno1	40-44	64-72	We develop two distinct models	For each model , we investigate four implementations :	We develop two distinct models	For each model , we investigate four implementations :	40-63	64-95	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	For each model , we investigate four implementations : a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models that generate embeddings for semantic frames .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	64-72	73-88	For each model , we investigate four implementations :	a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models	For each model , we investigate four implementations :	a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models	64-95	64-95	For each model , we investigate four implementations : a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models that generate embeddings for semantic frames .	For each model , we investigate four implementations : a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models that generate embeddings for semantic frames .	1<2	none	elab-enumember	elab-enumember
P16-1028_anno1	73-88	89-95	a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models	that generate embeddings for semantic frames .	a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models	that generate embeddings for semantic frames .	64-95	64-95	For each model , we investigate four implementations : a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models that generate embeddings for semantic frames .	For each model , we investigate four implementations : a `` standard '' N-gram language model and three discriminatively trained `` neural '' language models that generate embeddings for semantic frames .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	40-44	96-110	We develop two distinct models	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	We develop two distinct models	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	40-63	96-142	We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1<2	none	evaluation	evaluation
P16-1028_anno1	96-110	111-117	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	using perplexity and a narrative cloze test	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	using perplexity and a narrative cloze test	96-142	96-142	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1<2	none	manner-means	manner-means
P16-1028_anno1	96-110	118-120	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	and extrinsically -	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically ,	and extrinsically -	96-142	96-142	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1<2	none	joint	joint
P16-1028_anno1	121-122	123-134	we show	that our SemLM helps improve performance on semantic natural language processing tasks	we show	that our SemLM helps improve performance on semantic natural language processing tasks	96-142	96-142	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1>2	none	attribution	attribution
P16-1028_anno1	118-120	123-134	and extrinsically -	that our SemLM helps improve performance on semantic natural language processing tasks	and extrinsically -	that our SemLM helps improve performance on semantic natural language processing tasks	96-142	96-142	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1<2	none	elab-addition	elab-addition
P16-1028_anno1	123-134	135-142	that our SemLM helps improve performance on semantic natural language processing tasks	such as co-reference resolution and discourse parsing .	that our SemLM helps improve performance on semantic natural language processing tasks	such as co-reference resolution and discourse parsing .	96-142	96-142	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	The quality of the semantic language models ( SemLM ) is evaluated both intrinsically , using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing .	1<2	none	elab-example	elab-example
P16-1029_anno1	1-12	30-40	Domain adaptation is an important research topic in sentiment analysis area .	In this paper , we propose a new domain adaptation approach	Domain adaptation is an important research topic in sentiment analysis area .	In this paper , we propose a new domain adaptation approach	1-12	30-50	Domain adaptation is an important research topic in sentiment analysis area .	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	1>2	none	bg-compare	bg-compare
P16-1029_anno1	1-12	13-29	Domain adaptation is an important research topic in sentiment analysis area .	Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain .	Domain adaptation is an important research topic in sentiment analysis area .	Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain .	1-12	13-29	Domain adaptation is an important research topic in sentiment analysis area .	Existing domain adaptation methods usually transfer sentiment knowledge from only one source domain to target domain .	1<2	none	elab-addition	elab-addition
P16-1029_anno1	30-40	41-50	In this paper , we propose a new domain adaptation approach	which can exploit sentiment knowledge from multiple source domains .	In this paper , we propose a new domain adaptation approach	which can exploit sentiment knowledge from multiple source domains .	30-50	30-50	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	1<2	none	elab-addition	elab-addition
P16-1029_anno1	30-40	51-66	In this paper , we propose a new domain adaptation approach	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains	In this paper , we propose a new domain adaptation approach	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains	30-50	51-70	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning .	1<2	none	elab-process_step	elab-process_step
P16-1029_anno1	51-66	67-70	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains	using multi-task learning .	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains	using multi-task learning .	51-70	51-70	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning .	We first extract both global and domain-specific sentiment knowledge from the data of multiple source domains using multi-task learning .	1<2	none	manner-means	manner-means
P16-1029_anno1	30-40	71-77	In this paper , we propose a new domain adaptation approach	Then we transfer them to target domain	In this paper , we propose a new domain adaptation approach	Then we transfer them to target domain	30-50	71-94	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	Then we transfer them to target domain with the help of words ' sentiment polarity relations extracted from the unlabeled target domain data .	1<2	none	elab-process_step	elab-process_step
P16-1029_anno1	71-77	78-86	Then we transfer them to target domain	with the help of words ' sentiment polarity relations	Then we transfer them to target domain	with the help of words' sentiment polarity relations	71-94	71-94	Then we transfer them to target domain with the help of words ' sentiment polarity relations extracted from the unlabeled target domain data .	Then we transfer them to target domain with the help of words ' sentiment polarity relations extracted from the unlabeled target domain data .	1<2	none	manner-means	manner-means
P16-1029_anno1	78-86	87-94	with the help of words ' sentiment polarity relations	extracted from the unlabeled target domain data .	with the help of words' sentiment polarity relations	extracted from the unlabeled target domain data .	71-94	71-94	Then we transfer them to target domain with the help of words ' sentiment polarity relations extracted from the unlabeled target domain data .	Then we transfer them to target domain with the help of words ' sentiment polarity relations extracted from the unlabeled target domain data .	1<2	none	elab-addition	elab-addition
P16-1029_anno1	30-40	95-111	In this paper , we propose a new domain adaptation approach	The similarities between target domain and different source domains are also incorporated into the adaptation process .	In this paper , we propose a new domain adaptation approach	The similarities between target domain and different source domains are also incorporated into the adaptation process .	30-50	95-111	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	The similarities between target domain and different source domains are also incorporated into the adaptation process .	1<2	none	elab-process_step	elab-process_step
P16-1029_anno1	30-40	112-122	In this paper , we propose a new domain adaptation approach	Experimental results on benchmark dataset show the effectiveness of our approach	In this paper , we propose a new domain adaptation approach	Experimental results on benchmark dataset show the effectiveness of our approach	30-50	112-129	In this paper , we propose a new domain adaptation approach which can exploit sentiment knowledge from multiple source domains .	Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .	1<2	none	evaluation	evaluation
P16-1029_anno1	112-122	123-129	Experimental results on benchmark dataset show the effectiveness of our approach	in improving cross-domain sentiment classification performance .	Experimental results on benchmark dataset show the effectiveness of our approach	in improving cross-domain sentiment classification performance .	112-129	112-129	Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .	Experimental results on benchmark dataset show the effectiveness of our approach in improving cross-domain sentiment classification performance .	1<2	none	elab-addition	elab-addition
P16-1030	1-7	16-34	Through a particular choice of a predicate	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	Through a particular choice of a predicate	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1>2	none	manner-means	manner-means
P16-1030	1-7	8-15	Through a particular choice of a predicate	( e.g. , ��x violated y�� ) ,	Through a particular choice of a predicate	( e.g. , ��x violated y�� ) ,	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	16-34	99-106	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	We introduce connotation frames as a representation formalism	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	We introduce connotation frames as a representation formalism	1-98	99-117	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	1>2	none	bg-goal	bg-goal
P16-1030	16-34	35-41	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	: ( 1 ) writer��s perspective :	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	: ( 1 ) writer��s perspective :	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	35-41	42-52	: ( 1 ) writer��s perspective :	projecting x as an ��antagonist�� and y as a ��victim�� ,	: ( 1 ) writer��s perspective :	projecting x as an ��antagonist�� and y as a ��victim�� ,	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-addition	elab-addition
P16-1030	16-34	53-63	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 2 ) entities�� perspective : y probably dislikes x ,	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 2 ) entities�� perspective : y probably dislikes x ,	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	16-34	64-74	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 3 ) effect : something bad happened to y ,	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 3 ) effect : something bad happened to y ,	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	16-34	75-84	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 4 ) value : y is something valuable ,	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	( 4 ) value : y is something valuable ,	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	16-34	85-98	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	and ( 5 ) mental state : y is distressed by the event .	a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y	and ( 5 ) mental state : y is distressed by the event .	1-98	1-98	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	Through a particular choice of a predicate ( e.g. , ��x violated y�� ) , a writer can subtly connote a range of implied sentiment and presupposed facts about the entities x and y : ( 1 ) writer��s perspective : projecting x as an ��antagonist�� and y as a ��victim�� , ( 2 ) entities�� perspective : y probably dislikes x , ( 3 ) effect : something bad happened to y , ( 4 ) value : y is something valuable , and ( 5 ) mental state : y is distressed by the event .	1<2	none	elab-example	elab-example
P16-1030	99-106	107-113	We introduce connotation frames as a representation formalism	to organize these rich dimensions of connotation	We introduce connotation frames as a representation formalism	to organize these rich dimensions of connotation	99-117	99-117	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	1<2	none	enablement	enablement
P16-1030	107-113	114-117	to organize these rich dimensions of connotation	using typed relations .	to organize these rich dimensions of connotation	using typed relations .	99-117	99-117	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	1<2	none	manner-means	manner-means
P16-1030	99-106	118-123	We introduce connotation frames as a representation formalism	First , we investigate the feasibility	We introduce connotation frames as a representation formalism	First , we investigate the feasibility	99-117	118-131	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	First , we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments .	1<2	none	elab-process_step	elab-process_step
P16-1030	118-123	124-131	First , we investigate the feasibility	of obtaining connotative labels through crowdsourcing experiments .	First , we investigate the feasibility	of obtaining connotative labels through crowdsourcing experiments .	118-131	118-131	First , we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments .	First , we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments .	1<2	none	elab-addition	elab-addition
P16-1030	99-106	132-135	We introduce connotation frames as a representation formalism	We then present models	We introduce connotation frames as a representation formalism	We then present models	99-117	132-159	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations .	1<2	none	elab-process_step	elab-process_step
P16-1030	132-135	136-143	We then present models	for predicting the connotation frames of verb predicates	We then present models	for predicting the connotation frames of verb predicates	132-159	132-159	We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations .	We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations .	1<2	none	elab-addition	elab-addition
P16-1030	136-143	144-159	for predicting the connotation frames of verb predicates	based on their distributional word representations and the interplay between different types of connotative relations .	for predicting the connotation frames of verb predicates	based on their distributional word representations and the interplay between different types of connotative relations .	132-159	132-159	We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations .	We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations .	1<2	none	bg-general	bg-general
P16-1030	160-162	163-172	Empirical results confirm	that connotation frames can be induced from various data sources	Empirical results confirm	that connotation frames can be induced from various data sources	160-181	160-181	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	1>2	none	attribution	attribution
P16-1030	99-106	163-172	We introduce connotation frames as a representation formalism	that connotation frames can be induced from various data sources	We introduce connotation frames as a representation formalism	that connotation frames can be induced from various data sources	99-117	160-181	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	1<2	none	evaluation	evaluation
P16-1030	173-174	175-181	that reflect	how language is used in context .	that reflect	how language is used in context .	160-181	160-181	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	1>2	none	attribution	attribution
P16-1030	163-172	175-181	that connotation frames can be induced from various data sources	how language is used in context .	that connotation frames can be induced from various data sources	how language is used in context .	160-181	160-181	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	Empirical results confirm that connotation frames can be induced from various data sources that reflect how language is used in context .	1<2	none	elab-addition	elab-addition
P16-1030	99-106	182-186	We introduce connotation frames as a representation formalism	We conclude with analytical results	We introduce connotation frames as a representation formalism	We conclude with analytical results	99-117	182-203	We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations .	We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .	1<2	none	evaluation	evaluation
P16-1030	182-186	187-194	We conclude with analytical results	that show the potential use of connotation frames	We conclude with analytical results	that show the potential use of connotation frames	182-203	182-203	We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .	We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .	1<2	none	elab-addition	elab-addition
P16-1030	187-194	195-203	that show the potential use of connotation frames	for analyzing subtle biases in online news media .	that show the potential use of connotation frames	for analyzing subtle biases in online news media .	182-203	182-203	We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .	We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	1-8	82-89	Sentiment classification aims to automatically predict sentiment polarity	we propose a novel domain adaptation method ,	Sentiment classification aims to automatically predict sentiment polarity	we propose a novel domain adaptation method ,	1-28	76-98	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	1>2	none	bg-goal	bg-goal
P16-1031_anno1	1-8	9-15	Sentiment classification aims to automatically predict sentiment polarity	( e.g. , positive or negative )	Sentiment classification aims to automatically predict sentiment polarity	( e.g. , positive or negative )	1-28	1-28	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	1<2	none	elab-example	elab-example
P16-1031_anno1	1-8	16-28	Sentiment classification aims to automatically predict sentiment polarity	of user generated sentiment data ( e.g. , reviews , blogs ) .	Sentiment classification aims to automatically predict sentiment polarity	of user generated sentiment data ( e.g. , reviews , blogs ) .	1-28	1-28	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	29-36	37-39,44-47	Due to the mismatch among different domains ,	a sentiment classifier <*> may not work well	Due to the mismatch among different domains ,	a sentiment classifier <*> may not work well	29-54	29-54	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	1>2	none	cause	cause
P16-1031_anno1	1-8	37-39,44-47	Sentiment classification aims to automatically predict sentiment polarity	a sentiment classifier <*> may not work well	Sentiment classification aims to automatically predict sentiment polarity	a sentiment classifier <*> may not work well	1-28	29-54	Sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user generated sentiment data ( e.g. , reviews , blogs ) .	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	37-39,44-47	40-43	a sentiment classifier <*> may not work well	trained in one domain	a sentiment classifier <*> may not work well	trained in one domain	29-54	29-54	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	37-39,44-47	48-54	a sentiment classifier <*> may not work well	when directly applied to other domains .	a sentiment classifier <*> may not work well	when directly applied to other domains .	29-54	29-54	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	1<2	none	condition	condition
P16-1031_anno1	37-39,44-47	55-75	a sentiment classifier <*> may not work well	Thus , domain adaptation for sentiment classification algorithms are highly desirable to reduce the domain discrepancy and manual labeling costs .	a sentiment classifier <*> may not work well	Thus , domain adaptation for sentiment classification algorithms are highly desirable to reduce the domain discrepancy and manual labeling costs .	29-54	55-75	Due to the mismatch among different domains , a sentiment classifier trained in one domain may not work well when directly applied to other domains .	Thus , domain adaptation for sentiment classification algorithms are highly desirable to reduce the domain discrepancy and manual labeling costs .	1<2	none	result	result
P16-1031_anno1	76-81	82-89	To address the above challenge ,	we propose a novel domain adaptation method ,	To address the above challenge ,	we propose a novel domain adaptation method ,	76-98	76-98	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	1>2	none	enablement	enablement
P16-1031_anno1	82-89	90-98	we propose a novel domain adaptation method ,	called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	we propose a novel domain adaptation method ,	called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	76-98	76-98	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	82-89	99-113	we propose a novel domain adaptation method ,	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	we propose a novel domain adaptation method ,	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	76-98	99-125	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	The proposed BTDNNs attempts to transfer the source domain examples to the target domain , and also transfer the target domain examples to the source domain .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	99-113	114-125	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	and also transfer the target domain examples to the source domain .	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	and also transfer the target domain examples to the source domain .	99-125	99-125	The proposed BTDNNs attempts to transfer the source domain examples to the target domain , and also transfer the target domain examples to the source domain .	The proposed BTDNNs attempts to transfer the source domain examples to the target domain , and also transfer the target domain examples to the source domain .	1<2	none	joint	joint
P16-1031_anno1	99-113	126-138	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	The proposed BTDNNs attempts to transfer the source domain examples to the target domain ,	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	99-125	126-159	The proposed BTDNNs attempts to transfer the source domain examples to the target domain , and also transfer the target domain examples to the source domain .	The linear transformation of BTDNNs ensures the feasibility of transferring between domains , and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	126-138	139-159	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	126-159	126-159	The linear transformation of BTDNNs ensures the feasibility of transferring between domains , and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	The linear transformation of BTDNNs ensures the feasibility of transferring between domains , and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	1<2	none	joint	joint
P16-1031_anno1	126-138	160-169	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	As a result , the transferred source domain is supervised	The linear transformation of BTDNNs ensures the feasibility of transferring between domains ,	As a result , the transferred source domain is supervised	126-159	160-178	The linear transformation of BTDNNs ensures the feasibility of transferring between domains , and the distribution consistency between the transferred domain and the desirable domain is constrained with a linear data reconstruction manner .	As a result , the transferred source domain is supervised and follows similar distribution as the target domain .	1<2	none	result	result
P16-1031_anno1	160-169	170-178	As a result , the transferred source domain is supervised	and follows similar distribution as the target domain .	As a result , the transferred source domain is supervised	and follows similar distribution as the target domain .	160-178	160-178	As a result , the transferred source domain is supervised and follows similar distribution as the target domain .	As a result , the transferred source domain is supervised and follows similar distribution as the target domain .	1<2	none	joint	joint
P16-1031_anno1	160-169	179-191	As a result , the transferred source domain is supervised	Therefore , any supervised method can be used on the transferred source domain	As a result , the transferred source domain is supervised	Therefore , any supervised method can be used on the transferred source domain	160-178	179-203	As a result , the transferred source domain is supervised and follows similar distribution as the target domain .	Therefore , any supervised method can be used on the transferred source domain to train a classifier for sentiment classification in a target domain .	1<2	none	result	result
P16-1031_anno1	179-191	192-203	Therefore , any supervised method can be used on the transferred source domain	to train a classifier for sentiment classification in a target domain .	Therefore , any supervised method can be used on the transferred source domain	to train a classifier for sentiment classification in a target domain .	179-203	179-203	Therefore , any supervised method can be used on the transferred source domain to train a classifier for sentiment classification in a target domain .	Therefore , any supervised method can be used on the transferred source domain to train a classifier for sentiment classification in a target domain .	1<2	none	enablement	enablement
P16-1031_anno1	204-209	223-233	We conduct experiments on a benchmark	that our proposed approach significantly outperforms the several baseline methods ,	We conduct experiments on a benchmark	that our proposed approach significantly outperforms the several baseline methods ,	204-219	220-248	We conduct experiments on a benchmark composed of reviews of 4 types of Amazon products .	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	1>2	none	progression	progression
P16-1031_anno1	204-209	210-219	We conduct experiments on a benchmark	composed of reviews of 4 types of Amazon products .	We conduct experiments on a benchmark	composed of reviews of 4 types of Amazon products .	204-219	204-219	We conduct experiments on a benchmark composed of reviews of 4 types of Amazon products .	We conduct experiments on a benchmark composed of reviews of 4 types of Amazon products .	1<2	none	elab-addition	elab-addition
P16-1031_anno1	220-222	223-233	Experimental results show	that our proposed approach significantly outperforms the several baseline methods ,	Experimental results show	that our proposed approach significantly outperforms the several baseline methods ,	220-248	220-248	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	1>2	none	attribution	attribution
P16-1031_anno1	82-89	223-233	we propose a novel domain adaptation method ,	that our proposed approach significantly outperforms the several baseline methods ,	we propose a novel domain adaptation method ,	that our proposed approach significantly outperforms the several baseline methods ,	76-98	220-248	To address the above challenge , we propose a novel domain adaptation method , called Bi-Transferring Deep Neural Networks ( BTDNNs ) .	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	1<2	none	evaluation	evaluation
P16-1031_anno1	223-233	234-237	that our proposed approach significantly outperforms the several baseline methods ,	and achieves an accuracy	that our proposed approach significantly outperforms the several baseline methods ,	and achieves an accuracy	220-248	220-248	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	1<2	none	joint	joint
P16-1031_anno1	234-237	238-248	and achieves an accuracy	which is competitive with the state-of-the-art method for domain adaptation .	and achieves an accuracy	which is competitive with the state-of-the-art method for domain adaptation .	220-248	220-248	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	Experimental results show that our proposed approach significantly outperforms the several baseline methods , and achieves an accuracy which is competitive with the state-of-the-art method for domain adaptation .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	1-10	11-18,28-30	We present a new approach for documentlevel sentiment inference ,	where the goal is to predict directed opinions <*> for all entities	We present a new approach for documentlevel sentiment inference ,	where the goal is to predict directed opinions <*> for all entities	1-35	1-35	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	11-18,28-30	19-27	where the goal is to predict directed opinions <*> for all entities	( who feels positively or negatively towards whom )	where the goal is to predict directed opinions <*> for all entities	( who feels positively or negatively towards whom )	1-35	1-35	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	28-30	31-35	for all entities	mentioned in a text .	for all entities	mentioned in a text .	1-35	1-35	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	36-43	44-47	To encourage more complete and consistent predictions ,	we introduce an ILP	To encourage more complete and consistent predictions ,	we introduce an ILP	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1>2	none	enablement	enablement
P16-1032_anno1	1-10	44-47	We present a new approach for documentlevel sentiment inference ,	we introduce an ILP	We present a new approach for documentlevel sentiment inference ,	we introduce an ILP	1-35	36-89	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	44-47	48-50	we introduce an ILP	that jointly models	we introduce an ILP	that jointly models	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	48-50	51-59	that jointly models	( 1 ) sentence- and discourse-level sentiment cues ,	that jointly models	( 1 ) sentence- and discourse-level sentiment cues ,	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-enumember	elab-enumember
P16-1032_anno1	48-50	60-69	that jointly models	( 2 ) factual evidence about entity factions , and	that jointly models	( 2 ) factual evidence about entity factions , and	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-enumember	elab-enumember
P16-1032_anno1	48-50	70-74	that jointly models	( 3 ) global constraints	that jointly models	( 3 ) global constraints	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-enumember	elab-enumember
P16-1032_anno1	48-50	75-79	that jointly models	based on social science theories	that jointly models	based on social science theories	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	bg-general	bg-general
P16-1032_anno1	75-79	80-89	based on social science theories	such as homophily , social balance , and reciprocity .	based on social science theories	such as homophily , social balance , and reciprocity .	36-89	36-89	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	1<2	none	elab-example	elab-example
P16-1032_anno1	44-47	90-102	we introduce an ILP	Together , these cues allow for rich inference across groups of entities ,	we introduce an ILP	Together , these cues allow for rich inference across groups of entities ,	36-89	90-121	To encourage more complete and consistent predictions , we introduce an ILP that jointly models ( 1 ) sentence- and discourse-level sentiment cues , ( 2 ) factual evidence about entity factions , and ( 3 ) global constraints based on social science theories such as homophily , social balance , and reciprocity .	Together , these cues allow for rich inference across groups of entities , including for example that CEOs and the companies they lead are likely to have similar sentiment towards others .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	90-102	103-110,113-121	Together , these cues allow for rich inference across groups of entities ,	including for example that CEOs and the companies <*> are likely to have similar sentiment towards others .	Together , these cues allow for rich inference across groups of entities ,	including for example that CEOs and the companies <*> are likely to have similar sentiment towards others .	90-121	90-121	Together , these cues allow for rich inference across groups of entities , including for example that CEOs and the companies they lead are likely to have similar sentiment towards others .	Together , these cues allow for rich inference across groups of entities , including for example that CEOs and the companies they lead are likely to have similar sentiment towards others .	1<2	none	elab-example	elab-example
P16-1032_anno1	103-110,113-121	111-112	including for example that CEOs and the companies <*> are likely to have similar sentiment towards others .	they lead	including for example that CEOs and the companies <*> are likely to have similar sentiment towards others .	they lead	90-121	90-121	Together , these cues allow for rich inference across groups of entities , including for example that CEOs and the companies they lead are likely to have similar sentiment towards others .	Together , these cues allow for rich inference across groups of entities , including for example that CEOs and the companies they lead are likely to have similar sentiment towards others .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	122-130	153-160	We evaluate performance on new , densely labeled data	that the global model outperforms sentence-level baselines ,	We evaluate performance on new , densely labeled data	that the global model outperforms sentence-level baselines ,	122-150	151-171	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	1>2	none	progression	progression
P16-1032_anno1	122-130	131-137	We evaluate performance on new , densely labeled data	that provides supervision for all pairs ,	We evaluate performance on new , densely labeled data	that provides supervision for all pairs ,	122-150	122-150	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	122-130	138-140	We evaluate performance on new , densely labeled data	complementing previous work	We evaluate performance on new , densely labeled data	complementing previous work	122-150	122-150	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	138-140	141-144	complementing previous work	that only labeled pairs	complementing previous work	that only labeled pairs	122-150	122-150	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	141-144	145-150	that only labeled pairs	mentioned in the same sentence .	that only labeled pairs	mentioned in the same sentence .	122-150	122-150	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	We evaluate performance on new , densely labeled data that provides supervision for all pairs , complementing previous work that only labeled pairs mentioned in the same sentence .	1<2	none	elab-addition	elab-addition
P16-1032_anno1	151-152	153-160	Experiments demonstrate	that the global model outperforms sentence-level baselines ,	Experiments demonstrate	that the global model outperforms sentence-level baselines ,	151-171	151-171	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	1>2	none	attribution	attribution
P16-1032_anno1	1-10	153-160	We present a new approach for documentlevel sentiment inference ,	that the global model outperforms sentence-level baselines ,	We present a new approach for documentlevel sentiment inference ,	that the global model outperforms sentence-level baselines ,	1-35	151-171	We present a new approach for documentlevel sentiment inference , where the goal is to predict directed opinions ( who feels positively or negatively towards whom ) for all entities mentioned in a text .	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	1<2	none	evaluation	evaluation
P16-1032_anno1	153-160	161-171	that the global model outperforms sentence-level baselines ,	by providing more coherent predictions across sets of related entities .	that the global model outperforms sentence-level baselines ,	by providing more coherent predictions across sets of related entities .	151-171	151-171	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	Experiments demonstrate that the global model outperforms sentence-level baselines , by providing more coherent predictions across sets of related entities .	1<2	none	exp-evidence	exp-evidence
P16-1033_anno1	1-5	15-34	Different from traditional active learning	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Different from traditional active learning	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	1-34	1-34	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	1>2	none	contrast	contrast
P16-1033_anno1	1-5	6-14	Different from traditional active learning	based on sentence-wise full annotation ( FA ) ,	Different from traditional active learning	based on sentence-wise full annotation ( FA ) ,	1-34	1-34	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	1<2	none	bg-general	bg-general
P16-1033_anno1	15-34	35-51	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	At each iteration , we select a few most uncertain words from an unlabeled data pool ,	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	At each iteration , we select a few most uncertain words from an unlabeled data pool ,	1-34	35-69	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	At each iteration , we select a few most uncertain words from an unlabeled data pool , manually annotate their syntactic heads , and add the partial trees into labeled data for parser retraining .	1<2	none	elab-addition	elab-addition
P16-1033_anno1	35-51	52-57	At each iteration , we select a few most uncertain words from an unlabeled data pool ,	manually annotate their syntactic heads ,	At each iteration , we select a few most uncertain words from an unlabeled data pool ,	manually annotate their syntactic heads ,	35-69	35-69	At each iteration , we select a few most uncertain words from an unlabeled data pool , manually annotate their syntactic heads , and add the partial trees into labeled data for parser retraining .	At each iteration , we select a few most uncertain words from an unlabeled data pool , manually annotate their syntactic heads , and add the partial trees into labeled data for parser retraining .	1<2	none	progression	progression
P16-1033_anno1	52-57	58-69	manually annotate their syntactic heads ,	and add the partial trees into labeled data for parser retraining .	manually annotate their syntactic heads ,	and add the partial trees into labeled data for parser retraining .	35-69	35-69	At each iteration , we select a few most uncertain words from an unlabeled data pool , manually annotate their syntactic heads , and add the partial trees into labeled data for parser retraining .	At each iteration , we select a few most uncertain words from an unlabeled data pool , manually annotate their syntactic heads , and add the partial trees into labeled data for parser retraining .	1<2	none	progression	progression
P16-1033_anno1	70-74	75-83	Compared with sentence-wise FA ,	dependency-wise PA gives us more flexibility in task selection	Compared with sentence-wise FA ,	dependency-wise PA gives us more flexibility in task selection	70-95	70-95	Compared with sentence-wise FA , dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence .	Compared with sentence-wise FA , dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence .	1>2	none	comparison	comparison
P16-1033_anno1	15-34	75-83	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	dependency-wise PA gives us more flexibility in task selection	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	dependency-wise PA gives us more flexibility in task selection	1-34	70-95	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Compared with sentence-wise FA , dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence .	1<2	none	elab-addition	elab-addition
P16-1033_anno1	75-83	84-95	dependency-wise PA gives us more flexibility in task selection	and avoids wasting time on annotating trivial tasks in a sentence .	dependency-wise PA gives us more flexibility in task selection	and avoids wasting time on annotating trivial tasks in a sentence .	70-95	70-95	Compared with sentence-wise FA , dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence .	Compared with sentence-wise FA , dependency-wise PA gives us more flexibility in task selection and avoids wasting time on annotating trivial tasks in a sentence .	1<2	none	joint	joint
P16-1033_anno1	15-34	96-102	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Our work makes the following contributions .	this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Our work makes the following contributions .	1-34	96-102	Different from traditional active learning based on sentence-wise full annotation ( FA ) , this paper proposes active learning with dependency-wise partial annotation ( PA ) as a finer-grained unit for dependency parsing .	Our work makes the following contributions .	1<2	none	evaluation	evaluation
P16-1033_anno1	96-102	103-120	Our work makes the following contributions .	First , we are the first to apply a probabilistic model to active learning for dependency parsing ,	Our work makes the following contributions .	First , we are the first to apply a probabilistic model to active learning for dependency parsing ,	96-102	103-151	Our work makes the following contributions .	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	1<2	none	elab-enumember	elab-enumember
P16-1033_anno1	103-120	121-136	First , we are the first to apply a probabilistic model to active learning for dependency parsing ,	which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics ,	First , we are the first to apply a probabilistic model to active learning for dependency parsing ,	which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics ,	103-151	103-151	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	1<2	none	elab-addition	elab-addition
P16-1033_anno1	121-136	137-144	which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics ,	and 2 ) directly learn parameters from PA	which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics ,	and 2 ) directly learn parameters from PA	103-151	103-151	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	1<2	none	joint	joint
P16-1033_anno1	137-144	145-151	and 2 ) directly learn parameters from PA	based on a forest-based training objective .	and 2 ) directly learn parameters from PA	based on a forest-based training objective .	103-151	103-151	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	First , we are the first to apply a probabilistic model to active learning for dependency parsing , which can 1 ) provide tree probabilities and dependency marginal probabilities as principled uncertainty metrics , and 2 ) directly learn parameters from PA based on a forest-based training objective .	1<2	none	bg-goal	bg-goal
P16-1033_anno1	96-102	152-169	Our work makes the following contributions .	Second , we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English .	Our work makes the following contributions .	Second , we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English .	96-102	152-169	Our work makes the following contributions .	Second , we propose and compare several uncertainty metrics through simulation experiments on both Chinese and English .	1<2	none	elab-enumember	elab-enumember
P16-1033_anno1	96-102	170-176	Our work makes the following contributions .	Finally , we conduct human annotation experiments	Our work makes the following contributions .	Finally , we conduct human annotation experiments	96-102	170-188	Our work makes the following contributions .	Finally , we conduct human annotation experiments to compare FA and PA on real annotation time and quality .	1<2	none	elab-enumember	elab-enumember
P16-1033_anno1	170-176	177-188	Finally , we conduct human annotation experiments	to compare FA and PA on real annotation time and quality .	Finally , we conduct human annotation experiments	to compare FA and PA on real annotation time and quality .	170-188	170-188	Finally , we conduct human annotation experiments to compare FA and PA on real annotation time and quality .	Finally , we conduct human annotation experiments to compare FA and PA on real annotation time and quality .	1<2	none	enablement	enablement
P16-1034_anno1	1-7	8-16	We present a novel dependency parsing method	which enforces two structural properties on dependency trees :	We present a novel dependency parsing method	which enforces two structural properties on dependency trees :	1-22	1-22	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	1<2	none	elab-addition	elab-addition
P16-1034_anno1	8-16	17-22	which enforces two structural properties on dependency trees :	bounded block degree and well-nestedness .	which enforces two structural properties on dependency trees :	bounded block degree and well-nestedness .	1-22	1-22	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	1<2	none	elab-enumember	elab-enumember
P16-1034_anno1	8-16	23-37	which enforces two structural properties on dependency trees :	These properties are useful to better represent the set of admissible dependency structures in treebanks	which enforces two structural properties on dependency trees :	These properties are useful to better represent the set of admissible dependency structures in treebanks	1-22	23-46	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms .	1<2	none	elab-addition	elab-addition
P16-1034_anno1	23-37	38-46	These properties are useful to better represent the set of admissible dependency structures in treebanks	and connect dependency parsing to context-sensitive grammatical formalisms .	These properties are useful to better represent the set of admissible dependency structures in treebanks	and connect dependency parsing to context-sensitive grammatical formalisms .	23-46	23-46	These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms .	These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms .	1<2	none	joint	joint
P16-1034_anno1	23-37	47-55	These properties are useful to better represent the set of admissible dependency structures in treebanks	We cast this problem as an Integer Linear Program	These properties are useful to better represent the set of admissible dependency structures in treebanks	We cast this problem as an Integer Linear Program	23-46	47-77	These properties are useful to better represent the set of admissible dependency structures in treebanks and connect dependency parsing to context-sensitive grammatical formalisms .	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	1<2	none	elab-addition	elab-addition
P16-1034_anno1	47-55	56-61	We cast this problem as an Integer Linear Program	that we solve with Lagrangian Relaxation	We cast this problem as an Integer Linear Program	that we solve with Lagrangian Relaxation	47-77	47-77	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	1<2	none	elab-addition	elab-addition
P16-1034_anno1	56-61	62-71	that we solve with Lagrangian Relaxation	from which we derive a heuristic and an exact method	that we solve with Lagrangian Relaxation	from which we derive a heuristic and an exact method	47-77	47-77	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	1<2	none	elab-addition	elab-addition
P16-1034_anno1	62-71	72-77	from which we derive a heuristic and an exact method	based on a Branch-and-Bound search .	from which we derive a heuristic and an exact method	based on a Branch-and-Bound search .	47-77	47-77	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	We cast this problem as an Integer Linear Program that we solve with Lagrangian Relaxation from which we derive a heuristic and an exact method based on a Branch-and-Bound search .	1<2	none	bg-general	bg-general
P16-1034_anno1	78-81	82-88	Experimentally , we see	that these methods are efficient and competitive	Experimentally , we see	that these methods are efficient and competitive	78-103	78-103	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	1>2	none	attribution	attribution
P16-1034_anno1	1-7	82-88	We present a novel dependency parsing method	that these methods are efficient and competitive	We present a novel dependency parsing method	that these methods are efficient and competitive	1-22	78-103	We present a novel dependency parsing method which enforces two structural properties on dependency trees : bounded block degree and well-nestedness .	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	1<2	none	evaluation	evaluation
P16-1034_anno1	82-88	89-95	that these methods are efficient and competitive	compared to a baseline unconstrained parser ,	that these methods are efficient and competitive	compared to a baseline unconstrained parser ,	78-103	78-103	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	1<2	none	comparison	comparison
P16-1034_anno1	82-88	96-103	that these methods are efficient and competitive	while enforcing structural properties in all cases .	that these methods are efficient and competitive	while enforcing structural properties in all cases .	78-103	78-103	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	Experimentally , we see that these methods are efficient and competitive compared to a baseline unconstrained parser , while enforcing structural properties in all cases .	1<2	none	condition	condition
P16-1035_anno1	1-23	32-50	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	1-31	32-50	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships .	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	1>2	none	bg-goal	bg-goal
P16-1035_anno1	1-23	24-31	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability	to model term similarity and other relationships .	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability	to model term similarity and other relationships .	1-31	1-31	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships .	Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships .	1<2	none	enablement	enablement
P16-1035_anno1	51-52	53-61,66-75	We demonstrate	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	We demonstrate	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	51-75	51-75	We demonstrate that word embeddings such as word2vec and GloVe , when trained globally , underperform corpus and query specific embeddings for retrieval tasks .	We demonstrate that word embeddings such as word2vec and GloVe , when trained globally , underperform corpus and query specific embeddings for retrieval tasks .	1>2	none	attribution	attribution
P16-1035_anno1	32-50	53-61,66-75	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	32-50	51-75	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	We demonstrate that word embeddings such as word2vec and GloVe , when trained globally , underperform corpus and query specific embeddings for retrieval tasks .	1<2	none	elab-addition	elab-addition
P16-1035_anno1	53-61,66-75	62-65	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	when trained globally ,	that word embeddings such as word2vec and GloVe , <*> underperform corpus and query specific embeddings for retrieval tasks .	when trained globally ,	51-75	51-75	We demonstrate that word embeddings such as word2vec and GloVe , when trained globally , underperform corpus and query specific embeddings for retrieval tasks .	We demonstrate that word embeddings such as word2vec and GloVe , when trained globally , underperform corpus and query specific embeddings for retrieval tasks .	1<2	none	condition	condition
P16-1035_anno1	76-78	79-81,86-92	These results suggest	that other tasks <*> may also benefit from local embeddings .	These results suggest	that other tasks <*> may also benefit from local embeddings .	76-92	76-92	These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings .	These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings .	1>2	none	attribution	attribution
P16-1035_anno1	32-50	79-81,86-92	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	that other tasks <*> may also benefit from local embeddings .	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	that other tasks <*> may also benefit from local embeddings .	32-50	76-92	We study the use of term relatedness in the context of query expansion for ad hoc information retrieval .	These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings .	1<2	none	evaluation	evaluation
P16-1035_anno1	79-81,86-92	82-85	that other tasks <*> may also benefit from local embeddings .	benefiting from global embeddings	that other tasks <*> may also benefit from local embeddings .	benefiting from global embeddings	76-92	76-92	These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings .	These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	1-7,19-25	85-93	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	In this paper , we propose a novel approach	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	In this paper , we propose a novel approach	1-38	85-118	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	1>2	none	bg-goal	bg-goal
P16-1036_anno1	1-7,19-25	8-18	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc.	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc.	1-38	1-38	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	1<2	none	elab-example	elab-example
P16-1036_anno1	19-25	26-38	provide a platform for interaction with experts	and help users to obtain precise and accurate answers to their questions .	provide a platform for interaction with experts	and help users to obtain precise and accurate answers to their questions .	1-38	1-38	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	1<2	none	joint	joint
P16-1036_anno1	1-7,19-25	39-44,52-54	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	The time lag between the user <*> could be reduced	Community Question Answering ( cQA ) services <*> provide a platform for interaction with experts	The time lag between the user <*> could be reduced	1-38	39-64	Community Question Answering ( cQA ) services like Yahoo Answers , Baidu Zhidao , Quora , StackOverflow etc. provide a platform for interaction with experts and help users to obtain precise and accurate answers to their questions .	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	39-44,52-54	45-47	The time lag between the user <*> could be reduced	posting a question	The time lag between the user <*> could be reduced	posting a question	39-64	39-64	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	45-47	48-51	posting a question	and receiving its answer	posting a question	and receiving its answer	39-64	39-64	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	1<2	none	joint	joint
P16-1036_anno1	52-54	55-64	could be reduced	by retrieving similar historic questions from the cQA archives .	could be reduced	by retrieving similar historic questions from the cQA archives .	39-64	39-64	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	1<2	none	manner-means	manner-means
P16-1036_anno1	39-44,52-54	65-84	The time lag between the user <*> could be reduced	The main challenge in this task is the `` lexicosyntactic '' gap between the current and the previous questions .	The time lag between the user <*> could be reduced	The main challenge in this task is the `` lexicosyntactic '' gap between the current and the previous questions .	39-64	65-84	The time lag between the user posting a question and receiving its answer could be reduced by retrieving similar historic questions from the cQA archives .	The main challenge in this task is the `` lexicosyntactic '' gap between the current and the previous questions .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	85-93	94-105	In this paper , we propose a novel approach	called `` Siamese Convolutional Neural Network for cQA ( SCQA ) ''	In this paper , we propose a novel approach	called `` Siamese Convolutional Neural Network for cQA ( SCQA ) ''	85-118	85-118	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	85-93	106-118	In this paper , we propose a novel approach	to find the semantic similarity between the current and the archived questions .	In this paper , we propose a novel approach	to find the semantic similarity between the current and the archived questions .	85-118	85-118	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	1<2	none	enablement	enablement
P16-1036_anno1	85-93	119-133	In this paper , we propose a novel approach	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function	In this paper , we propose a novel approach	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function	85-118	119-136	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function joining them .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	119-133	134-136	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function	joining them .	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function	joining them .	119-136	119-136	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function joining them .	SCQA consist of twin convolutional neural networks with shared parameters and a contrastive loss function joining them .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	85-93	137-144	In this paper , we propose a novel approach	SCQA learns the similarity metric for question-question pairs	In this paper , we propose a novel approach	SCQA learns the similarity metric for question-question pairs	85-118	137-155	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	137-144	145-155	SCQA learns the similarity metric for question-question pairs	by leveraging the question-answer pairs available in cQA forum archives .	SCQA learns the similarity metric for question-question pairs	by leveraging the question-answer pairs available in cQA forum archives .	137-155	137-155	SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives .	SCQA learns the similarity metric for question-question pairs by leveraging the question-answer pairs available in cQA forum archives .	1<2	none	manner-means	manner-means
P16-1036_anno1	85-93	156-180	In this paper , we propose a novel approach	The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space .	In this paper , we propose a novel approach	The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space .	85-118	156-180	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	The model projects semantically similar question pairs nearer to each other and dissimilar question pairs farther away from each other in the semantic space .	1<2	none	elab-addition	elab-addition
P16-1036_anno1	181-191	192-197	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals	that SCQA outperforms current state-of-theart approaches	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals	that SCQA outperforms current state-of-theart approaches	181-215	181-215	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	1>2	none	attribution	attribution
P16-1036_anno1	85-93	192-197	In this paper , we propose a novel approach	that SCQA outperforms current state-of-theart approaches	In this paper , we propose a novel approach	that SCQA outperforms current state-of-theart approaches	85-118	181-215	In this paper , we propose a novel approach called `` Siamese Convolutional Neural Network for cQA ( SCQA ) '' to find the semantic similarity between the current and the archived questions .	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	1<2	none	evaluation	evaluation
P16-1036_anno1	192-197	198-210	that SCQA outperforms current state-of-theart approaches	based on translation models , topic models and deep neural network based models	that SCQA outperforms current state-of-theart approaches	based on translation models , topic models and deep neural network based models	181-215	181-215	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	1<2	none	bg-general	bg-general
P16-1036_anno1	198-210	211-215	based on translation models , topic models and deep neural network based models	which use non-shared parameters .	based on translation models , topic models and deep neural network based models	which use non-shared parameters .	181-215	181-215	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	Experiments on large scale reallife `` Yahoo Answers '' dataset reveals that SCQA outperforms current state-of-theart approaches based on translation models , topic models and deep neural network based models which use non-shared parameters .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	1-14	64-71	In this work , we focus on the problem of news citation recommendation .	In this paper , we explore word embedding	In this work , we focus on the problem of news citation recommendation .	In this paper , we explore word embedding	1-14	64-96	In this work , we focus on the problem of news citation recommendation .	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	1>2	none	bg-goal	bg-goal
P16-1037_anno1	1-14	15-26	In this work , we focus on the problem of news citation recommendation .	The task aims to recommend news citations for both authors and readers	In this work , we focus on the problem of news citation recommendation .	The task aims to recommend news citations for both authors and readers	1-14	15-33	In this work , we focus on the problem of news citation recommendation .	The task aims to recommend news citations for both authors and readers to create and search news references .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	15-26	27-33	The task aims to recommend news citations for both authors and readers	to create and search news references .	The task aims to recommend news citations for both authors and readers	to create and search news references .	15-33	15-33	The task aims to recommend news citations for both authors and readers to create and search news references .	The task aims to recommend news citations for both authors and readers to create and search news references .	1<2	none	enablement	enablement
P16-1037_anno1	34-45	52-63	Due to the sparsity issue of news citations and the engineering difficulty	we focus on content similarity-based methods instead of collaborative filtering-based approaches .	Due to the sparsity issue of news citations and the engineering difficulty	we focus on content similarity-based methods instead of collaborative filtering-based approaches .	34-63	34-63	Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches .	Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches .	1>2	none	cause	cause
P16-1037_anno1	34-45	46-51	Due to the sparsity issue of news citations and the engineering difficulty	in obtaining information on authors ,	Due to the sparsity issue of news citations and the engineering difficulty	in obtaining information on authors ,	34-63	34-63	Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches .	Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	15-26	52-63	The task aims to recommend news citations for both authors and readers	we focus on content similarity-based methods instead of collaborative filtering-based approaches .	The task aims to recommend news citations for both authors and readers	we focus on content similarity-based methods instead of collaborative filtering-based approaches .	15-33	34-63	The task aims to recommend news citations for both authors and readers to create and search news references .	Due to the sparsity issue of news citations and the engineering difficulty in obtaining information on authors , we focus on content similarity-based methods instead of collaborative filtering-based approaches .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	64-71	72-77	In this paper , we explore word embedding	( i.e. , implicit semantics )	In this paper , we explore word embedding	( i.e. , implicit semantics )	64-96	64-96	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	64-71	78-80	In this paper , we explore word embedding	and grounded entities	In this paper , we explore word embedding	and grounded entities	64-96	64-96	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	1<2	none	joint	joint
P16-1037_anno1	78-80	81-86	and grounded entities	( i.e. , explicit semantics )	and grounded entities	( i.e. , explicit semantics )	64-96	64-96	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	64-71	87-96	In this paper , we explore word embedding	to address the variety and ambiguity issues of language .	In this paper , we explore word embedding	to address the variety and ambiguity issues of language .	64-96	64-96	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	1<2	none	enablement	enablement
P16-1037_anno1	64-71	97-104	In this paper , we explore word embedding	We formulate the problem as a reranking task	In this paper , we explore word embedding	We formulate the problem as a reranking task	64-96	97-116	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	We formulate the problem as a reranking task and integrate different similarity measures under the learning to rank framework .	1<2	none	elab-addition	elab-addition
P16-1037_anno1	97-104	105-116	We formulate the problem as a reranking task	and integrate different similarity measures under the learning to rank framework .	We formulate the problem as a reranking task	and integrate different similarity measures under the learning to rank framework .	97-116	97-116	We formulate the problem as a reranking task and integrate different similarity measures under the learning to rank framework .	We formulate the problem as a reranking task and integrate different similarity measures under the learning to rank framework .	1<2	none	joint	joint
P16-1037_anno1	117-125	126-135	We evaluate our approach on a real-world dataset .	The experimental results show the efficacy of our method .	We evaluate our approach on a real-world dataset .	The experimental results show the efficacy of our method .	117-125	126-135	We evaluate our approach on a real-world dataset .	The experimental results show the efficacy of our method .	1>2	none	progression	progression
P16-1037_anno1	64-71	126-135	In this paper , we explore word embedding	The experimental results show the efficacy of our method .	In this paper , we explore word embedding	The experimental results show the efficacy of our method .	64-96	126-135	In this paper , we explore word embedding ( i.e. , implicit semantics ) and grounded entities ( i.e. , explicit semantics ) to address the variety and ambiguity issues of language .	The experimental results show the efficacy of our method .	1<2	none	evaluation	evaluation
P16-1038_anno1	1-12	26-41	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages ,	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages ,	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	1-25	26-41	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages , as the creation of training and evaluation data is expensive and time-consuming .	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	1>2	none	bg-goal	bg-goal
P16-1038_anno1	1-12	13-25	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages ,	as the creation of training and evaluation data is expensive and time-consuming .	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages ,	as the creation of training and evaluation data is expensive and time-consuming .	1-25	1-25	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages , as the creation of training and evaluation data is expensive and time-consuming .	Grapheme-to-phoneme ( g2p ) models are rarely available in low-resource languages , as the creation of training and evaluation data is expensive and time-consuming .	1<2	none	cause	cause
P16-1038_anno1	26-41	42-49	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We then develop phoneme and language distance metrics	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We then develop phoneme and language distance metrics	26-41	42-74	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	1<2	none	progression	progression
P16-1038_anno1	42-49	50-56	We then develop phoneme and language distance metrics	based on phonological and linguistic knowledge ;	We then develop phoneme and language distance metrics	based on phonological and linguistic knowledge ;	42-74	42-74	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	1<2	none	bg-general	bg-general
P16-1038_anno1	57-59	60-66	applying those ,	we adapt g2p models for highresource languages	applying those ,	we adapt g2p models for highresource languages	42-74	42-74	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	1>2	none	result	result
P16-1038_anno1	42-49	60-66	We then develop phoneme and language distance metrics	we adapt g2p models for highresource languages	We then develop phoneme and language distance metrics	we adapt g2p models for highresource languages	42-74	42-74	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	1<2	none	progression	progression
P16-1038_anno1	60-66	67-74	we adapt g2p models for highresource languages	to create models for related low-resource languages .	we adapt g2p models for highresource languages	to create models for related low-resource languages .	42-74	42-74	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	We then develop phoneme and language distance metrics based on phonological and linguistic knowledge ; applying those , we adapt g2p models for highresource languages to create models for related low-resource languages .	1<2	none	enablement	enablement
P16-1038_anno1	26-41	75-84	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We provide results for models for 229 adapted languages .	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We provide results for models for 229 adapted languages .	26-41	75-84	We use Wiktionary to obtain more than 650k word-pronunciation pairs in more than 500 languages .	We provide results for models for 229 adapted languages .	1<2	none	result	result
P16-1039_anno1	1-16	37-46	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task	In this paper , we propose a novel neural framework	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task	In this paper , we propose a novel neural framework	1-36	37-58	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured .	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	1>2	none	bg-goal	bg-goal
P16-1039_anno1	1-16	17-36	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task	so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured .	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task	so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured .	1-36	1-36	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured .	Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task so that only contextual information within fixed sized local windows and simple interactions between adjacent tags can be captured .	1<2	none	result	result
P16-1039_anno1	37-46	47-51	In this paper , we propose a novel neural framework	which thoroughly eliminates context windows	In this paper , we propose a novel neural framework	which thoroughly eliminates context windows	37-58	37-58	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	1<2	none	elab-addition	elab-addition
P16-1039_anno1	47-51	52-58	which thoroughly eliminates context windows	and can utilize complete segmentation history .	which thoroughly eliminates context windows	and can utilize complete segmentation history .	37-58	37-58	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	1<2	none	joint	joint
P16-1039_anno1	37-46	59-68	In this paper , we propose a novel neural framework	Our model employs a gated combination neural network over characters	In this paper , we propose a novel neural framework	Our model employs a gated combination neural network over characters	37-58	59-92	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to a long shortterm memory ( LSTM ) language scoring model .	1<2	none	elab-addition	elab-addition
P16-1039_anno1	59-68	69-76	Our model employs a gated combination neural network over characters	to produce distributed representations of word candidates ,	Our model employs a gated combination neural network over characters	to produce distributed representations of word candidates ,	59-92	59-92	Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to a long shortterm memory ( LSTM ) language scoring model .	Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to a long shortterm memory ( LSTM ) language scoring model .	1<2	none	enablement	enablement
P16-1039_anno1	69-76	77-92	to produce distributed representations of word candidates ,	which are then given to a long shortterm memory ( LSTM ) language scoring model .	to produce distributed representations of word candidates ,	which are then given to a long shortterm memory ( LSTM ) language scoring model .	59-92	59-92	Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to a long shortterm memory ( LSTM ) language scoring model .	Our model employs a gated combination neural network over characters to produce distributed representations of word candidates , which are then given to a long shortterm memory ( LSTM ) language scoring model .	1<2	none	elab-addition	elab-addition
P16-1039_anno1	93-98	111-122	Experiments on the benchmark datasets show	our models achieve competitive or better performances with previous stateof-the-art methods .	Experiments on the benchmark datasets show	our models achieve competitive or better performances with previous stateof-the-art methods .	93-122	93-122	Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous stateof-the-art methods .	Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous stateof-the-art methods .	1>2	none	attribution	attribution
P16-1039_anno1	99-110	111-122	that without the help of feature engineering as most existing approaches ,	our models achieve competitive or better performances with previous stateof-the-art methods .	that without the help of feature engineering as most existing approaches ,	our models achieve competitive or better performances with previous stateof-the-art methods .	93-122	93-122	Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous stateof-the-art methods .	Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous stateof-the-art methods .	1>2	none	condition	condition
P16-1039_anno1	37-46	111-122	In this paper , we propose a novel neural framework	our models achieve competitive or better performances with previous stateof-the-art methods .	In this paper , we propose a novel neural framework	our models achieve competitive or better performances with previous stateof-the-art methods .	37-58	93-122	In this paper , we propose a novel neural framework which thoroughly eliminates context windows and can utilize complete segmentation history .	Experiments on the benchmark datasets show that without the help of feature engineering as most existing approaches , our models achieve competitive or better performances with previous stateof-the-art methods .	1<2	none	evaluation	evaluation
P16-1040_anno1	1-16	73-87	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation ,	In this paper , we study a neural model for word-based Chinese word segmentation ,	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation ,	In this paper , we study a neural model for word-based Chinese word segmentation ,	1-44	73-102	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	In this paper , we study a neural model for word-based Chinese word segmentation , by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	1>2	none	bg-goal	bg-goal
P16-1040_anno1	1-16	17-24	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation ,	the former exploiting sequence labeling models over characters	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation ,	the former exploiting sequence labeling models over characters	1-44	1-44	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	1<2	none	elab-addition	elab-addition
P16-1040_anno1	17-24	25-33	the former exploiting sequence labeling models over characters	and the latter typically exploiting a transition-based model ,	the former exploiting sequence labeling models over characters	and the latter typically exploiting a transition-based model ,	1-44	1-44	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	1<2	none	joint	joint
P16-1040_anno1	25-33	34-36	and the latter typically exploiting a transition-based model ,	with the advantages	and the latter typically exploiting a transition-based model ,	with the advantages	1-44	1-44	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	1<2	none	elab-addition	elab-addition
P16-1040_anno1	34-36	37-44	with the advantages	that word-level features can be easily utilized .	with the advantages	that word-level features can be easily utilized .	1-44	1-44	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	Character-based and word-based methods are two main types of statistical models for Chinese word segmentation , the former exploiting sequence labeling models over characters and the latter typically exploiting a transition-based model , with the advantages that word-level features can be easily utilized .	1<2	none	elab-addition	elab-addition
P16-1040_anno1	45-55	73-87	Neural models have been exploited for character-based Chinese word segmentation ,	In this paper , we study a neural model for word-based Chinese word segmentation ,	Neural models have been exploited for character-based Chinese word segmentation ,	In this paper , we study a neural model for word-based Chinese word segmentation ,	45-72	73-102	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	In this paper , we study a neural model for word-based Chinese word segmentation , by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	1>2	none	bg-compare	bg-compare
P16-1040_anno1	45-55	56-58	Neural models have been exploited for character-based Chinese word segmentation ,	giving high accuracies	Neural models have been exploited for character-based Chinese word segmentation ,	giving high accuracies	45-72	45-72	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	1<2	none	result	result
P16-1040_anno1	56-58	59-66	giving high accuracies	by making use of external character embeddings ,	giving high accuracies	by making use of external character embeddings ,	45-72	45-72	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	1<2	none	manner-means	manner-means
P16-1040_anno1	56-58	67-72	giving high accuracies	yet requiring less feature engineering .	giving high accuracies	yet requiring less feature engineering .	45-72	45-72	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	Neural models have been exploited for character-based Chinese word segmentation , giving high accuracies by making use of external character embeddings , yet requiring less feature engineering .	1<2	none	contrast	contrast
P16-1040_anno1	73-87	88-102	In this paper , we study a neural model for word-based Chinese word segmentation ,	by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	In this paper , we study a neural model for word-based Chinese word segmentation ,	by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	73-102	73-102	In this paper , we study a neural model for word-based Chinese word segmentation , by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	In this paper , we study a neural model for word-based Chinese word segmentation , by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	1<2	none	manner-means	manner-means
P16-1040_anno1	103-105	106-120	Experimental results demonstrate	that word features lead to comparable performances to the best systems in the literature ,	Experimental results demonstrate	that word features lead to comparable performances to the best systems in the literature ,	103-133	103-133	Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies .	Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies .	1>2	none	attribution	attribution
P16-1040_anno1	73-87	106-120	In this paper , we study a neural model for word-based Chinese word segmentation ,	that word features lead to comparable performances to the best systems in the literature ,	In this paper , we study a neural model for word-based Chinese word segmentation ,	that word features lead to comparable performances to the best systems in the literature ,	73-102	103-133	In this paper , we study a neural model for word-based Chinese word segmentation , by replacing the manuallydesigned discrete features with neural features in a word-based segmentation framework .	Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies .	1<2	none	evaluation	evaluation
P16-1040_anno1	106-120	121-133	that word features lead to comparable performances to the best systems in the literature ,	and a further combination of discrete and neural features gives top accuracies .	that word features lead to comparable performances to the best systems in the literature ,	and a further combination of discrete and neural features gives top accuracies .	103-133	103-133	Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies .	Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature , and a further combination of discrete and neural features gives top accuracies .	1<2	none	joint	joint
P16-1041_anno1	1-12	27-40	Understanding unstructured text is a major goal within natural language processing .	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	Understanding unstructured text is a major goal within natural language processing .	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	1-12	27-40	Understanding unstructured text is a major goal within natural language processing .	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	1>2	none	bg-goal	bg-goal
P16-1041_anno1	1-12	13-16	Understanding unstructured text is a major goal within natural language processing .	Comprehension tests pose questions	Understanding unstructured text is a major goal within natural language processing .	Comprehension tests pose questions	1-12	13-26	Understanding unstructured text is a major goal within natural language processing .	Comprehension tests pose questions based on short text passages to evaluate such understanding .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	13-16	17-21	Comprehension tests pose questions	based on short text passages	Comprehension tests pose questions	based on short text passages	13-26	13-26	Comprehension tests pose questions based on short text passages to evaluate such understanding .	Comprehension tests pose questions based on short text passages to evaluate such understanding .	1<2	none	bg-general	bg-general
P16-1041_anno1	13-16	22-26	Comprehension tests pose questions	to evaluate such understanding .	Comprehension tests pose questions	to evaluate such understanding .	13-26	13-26	Comprehension tests pose questions based on short text passages to evaluate such understanding .	Comprehension tests pose questions based on short text passages to evaluate such understanding .	1<2	none	enablement	enablement
P16-1041_anno1	41-47	48-59	Partly because of its limited size ,	prior work on MCTest has focused mainly on engineering better features .	Partly because of its limited size ,	prior work on MCTest has focused mainly on engineering better features .	41-59	41-59	Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .	Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .	1>2	none	cause	cause
P16-1041_anno1	27-40	48-59	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	prior work on MCTest has focused mainly on engineering better features .	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	prior work on MCTest has focused mainly on engineering better features .	27-40	41-59	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	27-40	60-68	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	We tackle the dataset with a neural approach ,	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	We tackle the dataset with a neural approach ,	27-40	60-78	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	60-68	69-72	We tackle the dataset with a neural approach ,	harnessing simple neural networks	We tackle the dataset with a neural approach ,	harnessing simple neural networks	60-78	60-78	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	69-72	73-78	harnessing simple neural networks	arranged in a parallel hierarchy .	harnessing simple neural networks	arranged in a parallel hierarchy .	60-78	60-78	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	73-78	79-100	arranged in a parallel hierarchy .	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	arranged in a parallel hierarchy .	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	60-78	79-112	We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	79-100	101-112	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	as opposed to using a manually designed , rigid feature set .	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	as opposed to using a manually designed , rigid feature set .	79-112	79-112	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .	1<2	none	comparison	comparison
P16-1041_anno1	79-100	113-126	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	Perspectives range from the word level to sentence fragments to sequences of sentences ;	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives ,	Perspectives range from the word level to sentence fragments to sequences of sentences ;	79-112	113-136	The parallel hierarchy enables our model to compare the passage , question , and answer from a variety of trainable perspectives , as opposed to using a manually designed , rigid feature set .	Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word-embedding representations of text .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	113-126	127-136	Perspectives range from the word level to sentence fragments to sequences of sentences ;	the networks operate only on word-embedding representations of text .	Perspectives range from the word level to sentence fragments to sequences of sentences ;	the networks operate only on word-embedding representations of text .	113-136	113-136	Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word-embedding representations of text .	Perspectives range from the word level to sentence fragments to sequences of sentences ; the networks operate only on word-embedding representations of text .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	137-141	151-163	When trained with a methodology	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	When trained with a methodology	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	137-183	137-183	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1>2	none	condition	condition
P16-1041_anno1	137-141	142-150	When trained with a methodology	designed to help cope with limited training data ,	When trained with a methodology	designed to help cope with limited training data ,	137-183	137-183	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1<2	none	elab-addition	elab-addition
P16-1041_anno1	27-40	151-163	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	27-40	137-183	In this work , we investigate machine comprehension on the challenging MCTest benchmark .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1<2	none	evaluation	evaluation
P16-1041_anno1	151-163	164-168	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	outperforming previous feature-engineered approaches slightly	our Parallel-Hierarchical model sets a new state of the art for MCTest ,	outperforming previous feature-engineered approaches slightly	137-183	137-183	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1<2	none	exp-evidence	exp-evidence
P16-1041_anno1	164-168	169-176	outperforming previous feature-engineered approaches slightly	and previous neural approaches by a significant margin	outperforming previous feature-engineered approaches slightly	and previous neural approaches by a significant margin	137-183	137-183	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1<2	none	joint	joint
P16-1041_anno1	169-176	177-183	and previous neural approaches by a significant margin	( over 15 percentage points ) .	and previous neural approaches by a significant margin	( over 15 percentage points ) .	137-183	137-183	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	When trained with a methodology designed to help cope with limited training data , our Parallel-Hierarchical model sets a new state of the art for MCTest , outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin ( over 15 percentage points ) .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	1-15	35-38	Broad domain question answering is often difficult in the absence of structured knowledge bases ,	We propose an approach	Broad domain question answering is often difficult in the absence of structured knowledge bases ,	We propose an approach	1-34	35-53	Broad domain question answering is often difficult in the absence of structured knowledge bases , and can benefit from shallow lexical methods ( broad coverage ) and logical reasoning ( high precision ) .	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	1>2	none	bg-goal	bg-goal
P16-1042_anno1	1-15	16-34	Broad domain question answering is often difficult in the absence of structured knowledge bases ,	and can benefit from shallow lexical methods ( broad coverage ) and logical reasoning ( high precision ) .	Broad domain question answering is often difficult in the absence of structured knowledge bases ,	and can benefit from shallow lexical methods ( broad coverage ) and logical reasoning ( high precision ) .	1-34	1-34	Broad domain question answering is often difficult in the absence of structured knowledge bases , and can benefit from shallow lexical methods ( broad coverage ) and logical reasoning ( high precision ) .	Broad domain question answering is often difficult in the absence of structured knowledge bases , and can benefit from shallow lexical methods ( broad coverage ) and logical reasoning ( high precision ) .	1<2	none	joint	joint
P16-1042_anno1	35-38	39-48	We propose an approach	for incorporating both of these signals in a unified framework	We propose an approach	for incorporating both of these signals in a unified framework	35-53	35-53	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	39-48	49-53	for incorporating both of these signals in a unified framework	based on natural logic .	for incorporating both of these signals in a unified framework	based on natural logic .	35-53	35-53	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	1<2	none	bg-general	bg-general
P16-1042_anno1	35-38	54-59	We propose an approach	We extend the breadth of inferences	We propose an approach	We extend the breadth of inferences	35-53	54-93	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	54-59	60-63	We extend the breadth of inferences	afforded by natural logic	We extend the breadth of inferences	afforded by natural logic	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	54-59	64-67	We extend the breadth of inferences	to include relational entailment	We extend the breadth of inferences	to include relational entailment	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	enablement	enablement
P16-1042_anno1	64-67	68-74	to include relational entailment	( e.g. , buy -> own )	to include relational entailment	( e.g. , buy -> own )	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	elab-example	elab-example
P16-1042_anno1	64-67	75-76	to include relational entailment	and meronymy	to include relational entailment	and meronymy	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	joint	joint
P16-1042_anno1	75-76	77-81,86-93	and meronymy	( e.g. , a person <*> is born the city 's country ) .	and meronymy	( e.g. , a person <*> is born the city's country ) .	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	elab-example	elab-example
P16-1042_anno1	77-81,86-93	82-85	( e.g. , a person <*> is born the city 's country ) .	born in a city	( e.g. , a person <*> is born the city's country ) .	born in a city	54-93	54-93	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	We extend the breadth of inferences afforded by natural logic to include relational entailment ( e.g. , buy -> own ) and meronymy ( e.g. , a person born in a city is born the city 's country ) .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	35-38	94-100	We propose an approach	Furthermore , we train an evaluation function	We propose an approach	Furthermore , we train an evaluation function	35-53	94-117	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	Furthermore , we train an evaluation function - akin to gameplaying - to evaluate the expected truth of candidate premises on the fly .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	94-100	101-105	Furthermore , we train an evaluation function	- akin to gameplaying -	Furthermore , we train an evaluation function	- akin to gameplaying -	94-117	94-117	Furthermore , we train an evaluation function - akin to gameplaying - to evaluate the expected truth of candidate premises on the fly .	Furthermore , we train an evaluation function - akin to gameplaying - to evaluate the expected truth of candidate premises on the fly .	1<2	none	elab-addition	elab-addition
P16-1042_anno1	94-100	106-117	Furthermore , we train an evaluation function	to evaluate the expected truth of candidate premises on the fly .	Furthermore , we train an evaluation function	to evaluate the expected truth of candidate premises on the fly .	94-117	94-117	Furthermore , we train an evaluation function - akin to gameplaying - to evaluate the expected truth of candidate premises on the fly .	Furthermore , we train an evaluation function - akin to gameplaying - to evaluate the expected truth of candidate premises on the fly .	1<2	none	enablement	enablement
P16-1042_anno1	35-38	118-128	We propose an approach	We evaluate our approach on answering multiple choice science questions ,	We propose an approach	We evaluate our approach on answering multiple choice science questions ,	35-53	118-135	We propose an approach for incorporating both of these signals in a unified framework based on natural logic .	We evaluate our approach on answering multiple choice science questions , achieving strong results on the dataset .	1<2	none	evaluation	evaluation
P16-1042_anno1	118-128	129-135	We evaluate our approach on answering multiple choice science questions ,	achieving strong results on the dataset .	We evaluate our approach on answering multiple choice science questions ,	achieving strong results on the dataset .	118-135	118-135	We evaluate our approach on answering multiple choice science questions , achieving strong results on the dataset .	We evaluate our approach on answering multiple choice science questions , achieving strong results on the dataset .	1<2	none	result	result
P16-1043_anno1	1-7	45-55	Cognitive science researchers have emphasized the importance	Recent works in machine learning have explored a curriculum learning approach	Cognitive science researchers have emphasized the importance	Recent works in machine learning have explored a curriculum learning approach	1-21	45-86	Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems .	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	1>2	none	elab-addition	elab-addition
P16-1043_anno1	1-7	8-21	Cognitive science researchers have emphasized the importance	of ordering a complex task into a sequence of easy to hard problems .	Cognitive science researchers have emphasized the importance	of ordering a complex task into a sequence of easy to hard problems .	1-21	1-21	Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems .	Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems .	1<2	none	elab-addition	elab-addition
P16-1043_anno1	8-21	22-30	of ordering a complex task into a sequence of easy to hard problems .	Such an ordering provides an easier path to learning	of ordering a complex task into a sequence of easy to hard problems .	Such an ordering provides an easier path to learning	1-21	22-44	Cognitive science researchers have emphasized the importance of ordering a complex task into a sequence of easy to hard problems .	Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning .	1<2	none	elab-addition	elab-addition
P16-1043_anno1	22-30	31-39	Such an ordering provides an easier path to learning	and increases the speed of acquisition of the task	Such an ordering provides an easier path to learning	and increases the speed of acquisition of the task	22-44	22-44	Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning .	Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning .	1<2	none	joint	joint
P16-1043_anno1	31-39	40-44	and increases the speed of acquisition of the task	compared to conventional learning .	and increases the speed of acquisition of the task	compared to conventional learning .	22-44	22-44	Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning .	Such an ordering provides an easier path to learning and increases the speed of acquisition of the task compared to conventional learning .	1<2	none	comparison	comparison
P16-1043_anno1	45-55	87-92	Recent works in machine learning have explored a curriculum learning approach	We introduce a number of heuristics	Recent works in machine learning have explored a curriculum learning approach	We introduce a number of heuristics	45-86	87-98	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	We introduce a number of heuristics that improve upon selfpaced learning .	1>2	none	bg-compare	bg-compare
P16-1043_anno1	45-55	56-58	Recent works in machine learning have explored a curriculum learning approach	called selfpaced learning	Recent works in machine learning have explored a curriculum learning approach	called selfpaced learning	45-86	45-86	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	1<2	none	elab-addition	elab-addition
P16-1043_anno1	45-55	59-66	Recent works in machine learning have explored a curriculum learning approach	which orders data samples on the easiness scale	Recent works in machine learning have explored a curriculum learning approach	which orders data samples on the easiness scale	45-86	45-86	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	1<2	none	elab-addition	elab-addition
P16-1043_anno1	59-66	67-78	which orders data samples on the easiness scale	so that easy samples can be introduced to the learning algorithm first	which orders data samples on the easiness scale	so that easy samples can be introduced to the learning algorithm first	45-86	45-86	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	1<2	none	result	result
P16-1043_anno1	67-78	79-86	so that easy samples can be introduced to the learning algorithm first	and harder samples can be introduced successively .	so that easy samples can be introduced to the learning algorithm first	and harder samples can be introduced successively .	45-86	45-86	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	Recent works in machine learning have explored a curriculum learning approach called selfpaced learning which orders data samples on the easiness scale so that easy samples can be introduced to the learning algorithm first and harder samples can be introduced successively .	1<2	none	progression	progression
P16-1043_anno1	87-92	93-98	We introduce a number of heuristics	that improve upon selfpaced learning .	We introduce a number of heuristics	that improve upon selfpaced learning .	87-98	87-98	We introduce a number of heuristics that improve upon selfpaced learning .	We introduce a number of heuristics that improve upon selfpaced learning .	1<2	none	elab-addition	elab-addition
P16-1043_anno1	99-102	103-118	Then , we argue	that incorporating easy , yet , a diverse set of samples can further improve learning .	Then , we argue	that incorporating easy , yet , a diverse set of samples can further improve learning .	99-118	99-118	Then , we argue that incorporating easy , yet , a diverse set of samples can further improve learning .	Then , we argue that incorporating easy , yet , a diverse set of samples can further improve learning .	1>2	none	attribution	attribution
P16-1043_anno1	87-92	103-118	We introduce a number of heuristics	that incorporating easy , yet , a diverse set of samples can further improve learning .	We introduce a number of heuristics	that incorporating easy , yet , a diverse set of samples can further improve learning .	87-98	99-118	We introduce a number of heuristics that improve upon selfpaced learning .	Then , we argue that incorporating easy , yet , a diverse set of samples can further improve learning .	1<2	none	progression	progression
P16-1043_anno1	119-133	136-146	We compare these curriculum learning proposals in the context of four non-convex models for QA	that they lead to real improvements in each of them .	We compare these curriculum learning proposals in the context of four non-convex models for QA	that they lead to real improvements in each of them .	119-146	119-146	We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them .	We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them .	1>2	none	progression	progression
P16-1043_anno1	134-135	136-146	and show	that they lead to real improvements in each of them .	and show	that they lead to real improvements in each of them .	119-146	119-146	We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them .	We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them .	1>2	none	attribution	attribution
P16-1043_anno1	87-92	136-146	We introduce a number of heuristics	that they lead to real improvements in each of them .	We introduce a number of heuristics	that they lead to real improvements in each of them .	87-98	119-146	We introduce a number of heuristics that improve upon selfpaced learning .	We compare these curriculum learning proposals in the context of four non-convex models for QA and show that they lead to real improvements in each of them .	1<2	none	evaluation	evaluation
P16-1044_anno1	1-8	25-36	Passage-level question answer matching is a challenging task	In this work , we propose a series of deep learning models	Passage-level question answer matching is a challenging task	In this work , we propose a series of deep learning models	1-24	25-42	Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .	In this work , we propose a series of deep learning models to address passage answer selection .	1>2	none	bg-goal	bg-goal
P16-1044_anno1	1-8	9-13	Passage-level question answer matching is a challenging task	since it requires effective representations	Passage-level question answer matching is a challenging task	since it requires effective representations	1-24	1-24	Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .	Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .	1<2	none	exp-reason	exp-reason
P16-1044_anno1	9-13	14-24	since it requires effective representations	that capture the complex semantic relations between questions and answers .	since it requires effective representations	that capture the complex semantic relations between questions and answers .	1-24	1-24	Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .	Passage-level question answer matching is a challenging task since it requires effective representations that capture the complex semantic relations between questions and answers .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	25-36	37-42	In this work , we propose a series of deep learning models	to address passage answer selection .	In this work , we propose a series of deep learning models	to address passage answer selection .	25-42	25-42	In this work , we propose a series of deep learning models to address passage answer selection .	In this work , we propose a series of deep learning models to address passage answer selection .	1<2	none	enablement	enablement
P16-1044_anno1	43-48	67-70	To match passage answers to questions	we develop hybrid models	To match passage answers to questions	we develop hybrid models	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1>2	none	enablement	enablement
P16-1044_anno1	43-48	49-54	To match passage answers to questions	accommodating their complex semantic relations ,	To match passage answers to questions	accommodating their complex semantic relations ,	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	bg-general	bg-general
P16-1044_anno1	55-58	67-70	unlike most previous work	we develop hybrid models	unlike most previous work	we develop hybrid models	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1>2	none	comparison	comparison
P16-1044_anno1	55-58	59-66	unlike most previous work	that utilizes a single deep learning structure ,	unlike most previous work	that utilizes a single deep learning structure ,	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	25-36	67-70	In this work , we propose a series of deep learning models	we develop hybrid models	In this work , we propose a series of deep learning models	we develop hybrid models	25-42	43-93	In this work , we propose a series of deep learning models to address passage answer selection .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	67-70	71-74	we develop hybrid models	that process the text	we develop hybrid models	that process the text	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	71-74	75-82	that process the text	using both convolutional and recurrent neural networks ,	that process the text	using both convolutional and recurrent neural networks ,	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	manner-means	manner-means
P16-1044_anno1	75-82	83-93	using both convolutional and recurrent neural networks ,	combining the merits on extracting linguistic information from both structures .	using both convolutional and recurrent neural networks ,	combining the merits on extracting linguistic information from both structures .	43-93	43-93	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	To match passage answers to questions accommodating their complex semantic relations , unlike most previous work that utilizes a single deep learning structure , we develop hybrid models that process the text using both convolutional and recurrent neural networks , combining the merits on extracting linguistic information from both structures .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	25-36	94-104	In this work , we propose a series of deep learning models	Additionally , we also develop a simple but effective attention mechanism	In this work , we propose a series of deep learning models	Additionally , we also develop a simple but effective attention mechanism	25-42	94-128	In this work , we propose a series of deep learning models to address passage answer selection .	Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	94-104	105-118	Additionally , we also develop a simple but effective attention mechanism	for the purpose of constructing better answer representations according to the input question ,	Additionally , we also develop a simple but effective attention mechanism	for the purpose of constructing better answer representations according to the input question ,	94-128	94-128	Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .	Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .	1<2	none	enablement	enablement
P16-1044_anno1	105-118	119-128	for the purpose of constructing better answer representations according to the input question ,	which is imperative for better modeling long answer sequences .	for the purpose of constructing better answer representations according to the input question ,	which is imperative for better modeling long answer sequences .	94-128	94-128	Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .	Additionally , we also develop a simple but effective attention mechanism for the purpose of constructing better answer representations according to the input question , which is imperative for better modeling long answer sequences .	1<2	none	elab-addition	elab-addition
P16-1044_anno1	129-141	142-152	The results on two public benchmark datasets , InsuranceQA and TREC-QA , show	that our proposed models outperform a variety of strong baselines .	The results on two public benchmark datasets , InsuranceQA and TREC-QA , show	that our proposed models outperform a variety of strong baselines .	129-152	129-152	The results on two public benchmark datasets , InsuranceQA and TREC-QA , show that our proposed models outperform a variety of strong baselines .	The results on two public benchmark datasets , InsuranceQA and TREC-QA , show that our proposed models outperform a variety of strong baselines .	1>2	none	attribution	attribution
P16-1044_anno1	25-36	142-152	In this work , we propose a series of deep learning models	that our proposed models outperform a variety of strong baselines .	In this work , we propose a series of deep learning models	that our proposed models outperform a variety of strong baselines .	25-42	129-152	In this work , we propose a series of deep learning models to address passage answer selection .	The results on two public benchmark datasets , InsuranceQA and TREC-QA , show that our proposed models outperform a variety of strong baselines .	1<2	none	evaluation	evaluation
P16-1045_anno1	1-8	49-58	Question answering requires access to a knowledge base	In this paper we explore tables as a semi-structured formalism	Question answering requires access to a knowledge base	In this paper we explore tables as a semi-structured formalism	1-16	49-67	Question answering requires access to a knowledge base to check facts and reason about information .	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	1>2	none	bg-goal	bg-goal
P16-1045_anno1	1-8	9-16	Question answering requires access to a knowledge base	to check facts and reason about information .	Question answering requires access to a knowledge base	to check facts and reason about information .	1-16	1-16	Question answering requires access to a knowledge base to check facts and reason about information .	Question answering requires access to a knowledge base to check facts and reason about information .	1<2	none	enablement	enablement
P16-1045_anno1	17-29	30-35	Knowledge in the form of natural language text is easy to acquire ,	but difficult for automated reasoning .	Knowledge in the form of natural language text is easy to acquire ,	but difficult for automated reasoning .	17-35	17-35	Knowledge in the form of natural language text is easy to acquire , but difficult for automated reasoning .	Knowledge in the form of natural language text is easy to acquire , but difficult for automated reasoning .	1>2	none	contrast	contrast
P16-1045_anno1	1-8	30-35	Question answering requires access to a knowledge base	but difficult for automated reasoning .	Question answering requires access to a knowledge base	but difficult for automated reasoning .	1-16	17-35	Question answering requires access to a knowledge base to check facts and reason about information .	Knowledge in the form of natural language text is easy to acquire , but difficult for automated reasoning .	1<2	none	elab-addition	elab-addition
P16-1045_anno1	36-42	43-48	Highly-structured knowledge bases can facilitate reasoning ,	but are difficult to acquire .	Highly-structured knowledge bases can facilitate reasoning ,	but are difficult to acquire .	36-48	36-48	Highly-structured knowledge bases can facilitate reasoning , but are difficult to acquire .	Highly-structured knowledge bases can facilitate reasoning , but are difficult to acquire .	1>2	none	contrast	contrast
P16-1045_anno1	1-8	43-48	Question answering requires access to a knowledge base	but are difficult to acquire .	Question answering requires access to a knowledge base	but are difficult to acquire .	1-16	36-48	Question answering requires access to a knowledge base to check facts and reason about information .	Highly-structured knowledge bases can facilitate reasoning , but are difficult to acquire .	1<2	none	elab-addition	elab-addition
P16-1045_anno1	49-58	59-67	In this paper we explore tables as a semi-structured formalism	that provides a balanced compromise to this tradeoff .	In this paper we explore tables as a semi-structured formalism	that provides a balanced compromise to this tradeoff .	49-67	49-67	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	1<2	none	elab-addition	elab-addition
P16-1045_anno1	49-58	68-74	In this paper we explore tables as a semi-structured formalism	We first use the structure of tables	In this paper we explore tables as a semi-structured formalism	We first use the structure of tables	49-67	68-97	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations , easily and efficiently via crowd-sourcing .	1<2	none	elab-process_step	elab-process_step
P16-1045_anno1	68-74	75-91	We first use the structure of tables	to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations ,	We first use the structure of tables	to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations ,	68-97	68-97	We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations , easily and efficiently via crowd-sourcing .	We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations , easily and efficiently via crowd-sourcing .	1<2	none	enablement	enablement
P16-1045_anno1	75-91	92-97	to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations ,	easily and efficiently via crowd-sourcing .	to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations ,	easily and efficiently via crowd-sourcing .	68-97	68-97	We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations , easily and efficiently via crowd-sourcing .	We first use the structure of tables to guide the construction of a dataset of over 9000 multiple-choice questions with rich alignment annotations , easily and efficiently via crowd-sourcing .	1<2	none	elab-addition	elab-addition
P16-1045_anno1	49-58	98-103	In this paper we explore tables as a semi-structured formalism	We then use this annotated data	In this paper we explore tables as a semi-structured formalism	We then use this annotated data	49-67	98-120	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base .	1<2	none	elab-process_step	elab-process_step
P16-1045_anno1	98-103	104-112	We then use this annotated data	to train a semistructured feature-driven model for question answering	We then use this annotated data	to train a semistructured feature-driven model for question answering	98-120	98-120	We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base .	We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base .	1<2	none	enablement	enablement
P16-1045_anno1	104-112	113-120	to train a semistructured feature-driven model for question answering	that uses tables as a knowledge base .	to train a semistructured feature-driven model for question answering	that uses tables as a knowledge base .	98-120	98-120	We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base .	We then use this annotated data to train a semistructured feature-driven model for question answering that uses tables as a knowledge base .	1<2	none	elab-addition	elab-addition
P16-1045_anno1	49-58	121-141	In this paper we explore tables as a semi-structured formalism	In benchmark evaluations , we significantly outperform both a strong unstructured retrieval baseline and a highlystructured Markov Logic Network model .	In this paper we explore tables as a semi-structured formalism	In benchmark evaluations , we significantly outperform both a strong unstructured retrieval baseline and a highlystructured Markov Logic Network model .	49-67	121-141	In this paper we explore tables as a semi-structured formalism that provides a balanced compromise to this tradeoff .	In benchmark evaluations , we significantly outperform both a strong unstructured retrieval baseline and a highlystructured Markov Logic Network model .	1<2	none	evaluation	evaluation
P16-1046_anno1	1-11	12-19	Traditional approaches to extractive summarization rely heavily on humanengineered features .	In this work we propose a data-driven approach	Traditional approaches to extractive summarization rely heavily on humanengineered features .	In this work we propose a data-driven approach	1-11	12-28	Traditional approaches to extractive summarization rely heavily on humanengineered features .	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	1>2	none	bg-compare	bg-compare
P16-1046_anno1	12-19	20-28	In this work we propose a data-driven approach	based on neural networks and continuous sentence features .	In this work we propose a data-driven approach	based on neural networks and continuous sentence features .	12-28	12-28	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	1<2	none	bg-general	bg-general
P16-1046_anno1	12-19	29-36	In this work we propose a data-driven approach	We develop a general framework for single-document summarization	In this work we propose a data-driven approach	We develop a general framework for single-document summarization	12-28	29-47	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor .	1<2	none	elab-addition	elab-addition
P16-1046_anno1	29-36	37-47	We develop a general framework for single-document summarization	composed of a hierarchical document encoder and an attention-based extractor .	We develop a general framework for single-document summarization	composed of a hierarchical document encoder and an attention-based extractor .	29-47	29-47	We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor .	We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor .	1<2	none	elab-enumember	elab-enumember
P16-1046_anno1	37-47	48-58	composed of a hierarchical document encoder and an attention-based extractor .	This architecture allows us to develop different classes of summarization models	composed of a hierarchical document encoder and an attention-based extractor .	This architecture allows us to develop different classes of summarization models	29-47	48-65	We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor .	This architecture allows us to develop different classes of summarization models which can extract sentences or words .	1<2	none	elab-addition	elab-addition
P16-1046_anno1	48-58	59-65	This architecture allows us to develop different classes of summarization models	which can extract sentences or words .	This architecture allows us to develop different classes of summarization models	which can extract sentences or words .	48-65	48-65	This architecture allows us to develop different classes of summarization models which can extract sentences or words .	This architecture allows us to develop different classes of summarization models which can extract sentences or words .	1<2	none	elab-addition	elab-addition
P16-1046_anno1	12-19	66-73	In this work we propose a data-driven approach	We train our models on large scale corpora	In this work we propose a data-driven approach	We train our models on large scale corpora	12-28	66-81	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	We train our models on large scale corpora containing hundreds of thousands of document-summary pairs1 .	1<2	none	elab-addition	elab-addition
P16-1046_anno1	66-73	74-81	We train our models on large scale corpora	containing hundreds of thousands of document-summary pairs1 .	We train our models on large scale corpora	containing hundreds of thousands of document-summary pairs1 .	66-81	66-81	We train our models on large scale corpora containing hundreds of thousands of document-summary pairs1 .	We train our models on large scale corpora containing hundreds of thousands of document-summary pairs1 .	1<2	none	elab-addition	elab-addition
P16-1046_anno1	82-88	89-100	Experimental results on two summarization datasets demonstrate	that our models obtain results comparable to the state of the art	Experimental results on two summarization datasets demonstrate	that our models obtain results comparable to the state of the art	82-107	82-107	Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .	Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .	1>2	none	attribution	attribution
P16-1046_anno1	12-19	89-100	In this work we propose a data-driven approach	that our models obtain results comparable to the state of the art	In this work we propose a data-driven approach	that our models obtain results comparable to the state of the art	12-28	82-107	In this work we propose a data-driven approach based on neural networks and continuous sentence features .	Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .	1<2	none	evaluation	evaluation
P16-1046_anno1	89-100	101-107	that our models obtain results comparable to the state of the art	without any access to linguistic annotation .	that our models obtain results comparable to the state of the art	without any access to linguistic annotation .	82-107	82-107	Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .	Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation .	1<2	none	condition	condition
P16-1047_anno1	1-7	18-25	Automatic negation scope detection is a task	Most systems are however 1 ) highly-engineered ,	Automatic negation scope detection is a task	Most systems are however 1 ) highly-engineered ,	1-17	18-43	Automatic negation scope detection is a task that has been tackled using different classifiers and heuristics .	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	1>2	none	bg-goal	bg-goal
P16-1047_anno1	1-7	8-11	Automatic negation scope detection is a task	that has been tackled	Automatic negation scope detection is a task	that has been tackled	1-17	1-17	Automatic negation scope detection is a task that has been tackled using different classifiers and heuristics .	Automatic negation scope detection is a task that has been tackled using different classifiers and heuristics .	1<2	none	elab-addition	elab-addition
P16-1047_anno1	8-11	12-17	that has been tackled	using different classifiers and heuristics .	that has been tackled	using different classifiers and heuristics .	1-17	1-17	Automatic negation scope detection is a task that has been tackled using different classifiers and heuristics .	Automatic negation scope detection is a task that has been tackled using different classifiers and heuristics .	1<2	none	manner-means	manner-means
P16-1047_anno1	18-25	26-29	Most systems are however 1 ) highly-engineered ,	2 ) English-specific ,	Most systems are however 1 ) highly-engineered ,	2 ) English-specific ,	18-43	18-43	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	1<2	none	joint	joint
P16-1047_anno1	26-29	30-38	2 ) English-specific ,	and 3 ) only tested on the same genre	2 ) English-specific ,	and 3 ) only tested on the same genre	18-43	18-43	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	1<2	none	joint	joint
P16-1047_anno1	30-38	39-43	and 3 ) only tested on the same genre	they were trained on .	and 3 ) only tested on the same genre	they were trained on .	18-43	18-43	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	1<2	none	elab-addition	elab-addition
P16-1047_anno1	18-25	44-52	Most systems are however 1 ) highly-engineered ,	We start by addressing 1 ) and 2 )	Most systems are however 1 ) highly-engineered ,	We start by addressing 1 ) and 2 )	18-43	44-58	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	We start by addressing 1 ) and 2 ) using a neural network architecture .	1<2	none	elab-process_step	elab-process_step
P16-1047_anno1	44-52	53-58	We start by addressing 1 ) and 2 )	using a neural network architecture .	We start by addressing 1 ) and 2 )	using a neural network architecture .	44-58	44-58	We start by addressing 1 ) and 2 ) using a neural network architecture .	We start by addressing 1 ) and 2 ) using a neural network architecture .	1<2	none	manner-means	manner-means
P16-1047_anno1	59,73	74-80,86-96	Results <*> show	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	Results <*> show	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	59-101	59-101	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	1>2	none	attribution	attribution
P16-1047_anno1	59,73	60-72	Results <*> show	obtained on data from the * SEM2012 shared task on negation scope detection	Results <*> show	obtained on data from the * SEM2012 shared task on negation scope detection	59-101	59-101	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	1<2	none	elab-addition	elab-addition
P16-1047_anno1	44-52	74-80,86-96	We start by addressing 1 ) and 2 )	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	We start by addressing 1 ) and 2 )	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	44-58	59-101	We start by addressing 1 ) and 2 ) using a neural network architecture .	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	1<2	none	evaluation	evaluation
P16-1047_anno1	74-80,86-96	81-85	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	using word-embedding features alone ,	that even a simple feed-forward neural network <*> performs on par with earlier classifiers , with a bi-directional LSTM	using word-embedding features alone ,	59-101	59-101	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	1<2	none	elab-addition	elab-addition
P16-1047_anno1	86-96	97-101	performs on par with earlier classifiers , with a bi-directional LSTM	outperforming all of them .	performs on par with earlier classifiers , with a bi-directional LSTM	outperforming all of them .	59-101	59-101	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	Results obtained on data from the * SEM2012 shared task on negation scope detection show that even a simple feed-forward neural network using word-embedding features alone , performs on par with earlier classifiers , with a bi-directional LSTM outperforming all of them .	1<2	none	elab-addition	elab-addition
P16-1047_anno1	18-25	102-115	Most systems are however 1 ) highly-engineered ,	We then address 3 ) by means of a specially-designed synthetic test set ;	Most systems are however 1 ) highly-engineered ,	We then address 3 ) by means of a specially-designed synthetic test set ;	18-43	102-148	Most systems are however 1 ) highly-engineered , 2 ) English-specific , and 3 ) only tested on the same genre they were trained on .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1<2	none	elab-process_step	elab-process_step
P16-1047_anno1	116-119	120-131	in doing so ,	we explore the problem of detecting the negation scope more in depth	in doing so ,	we explore the problem of detecting the negation scope more in depth	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1>2	none	manner-means	manner-means
P16-1047_anno1	102-115	120-131	We then address 3 ) by means of a specially-designed synthetic test set ;	we explore the problem of detecting the negation scope more in depth	We then address 3 ) by means of a specially-designed synthetic test set ;	we explore the problem of detecting the negation scope more in depth	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1<2	none	evaluation	evaluation
P16-1047_anno1	132-133	134-139	and show	that performance suffers from genre effects	and show	that performance suffers from genre effects	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1>2	none	attribution	attribution
P16-1047_anno1	120-131	134-139	we explore the problem of detecting the negation scope more in depth	that performance suffers from genre effects	we explore the problem of detecting the negation scope more in depth	that performance suffers from genre effects	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1<2	none	joint	joint
P16-1047_anno1	134-139	140-146	that performance suffers from genre effects	and differs with the type of negation	that performance suffers from genre effects	and differs with the type of negation	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1<2	none	joint	joint
P16-1047_anno1	140-146	147-148	and differs with the type of negation	considered .	and differs with the type of negation	considered .	102-148	102-148	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	We then address 3 ) by means of a specially-designed synthetic test set ; in doing so , we explore the problem of detecting the negation scope more in depth and show that performance suffers from genre effects and differs with the type of negation considered .	1<2	none	elab-addition	elab-addition
P16-1048_anno1	1-8	34-37	Most sentence embedding models typically represent each sentence	we employ conceptualization model	Most sentence embedding models typically represent each sentence	we employ conceptualization model	1-24	25-59	Most sentence embedding models typically represent each sentence only using word surface , which makes these models indiscriminative for ubiquitous homonymy and polysemy .	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	1>2	none	bg-compare	bg-compare
P16-1048_anno1	1-8	9-13	Most sentence embedding models typically represent each sentence	only using word surface ,	Most sentence embedding models typically represent each sentence	only using word surface ,	1-24	1-24	Most sentence embedding models typically represent each sentence only using word surface , which makes these models indiscriminative for ubiquitous homonymy and polysemy .	Most sentence embedding models typically represent each sentence only using word surface , which makes these models indiscriminative for ubiquitous homonymy and polysemy .	1<2	none	manner-means	manner-means
P16-1048_anno1	1-8	14-24	Most sentence embedding models typically represent each sentence	which makes these models indiscriminative for ubiquitous homonymy and polysemy .	Most sentence embedding models typically represent each sentence	which makes these models indiscriminative for ubiquitous homonymy and polysemy .	1-24	1-24	Most sentence embedding models typically represent each sentence only using word surface , which makes these models indiscriminative for ubiquitous homonymy and polysemy .	Most sentence embedding models typically represent each sentence only using word surface , which makes these models indiscriminative for ubiquitous homonymy and polysemy .	1<2	none	elab-addition	elab-addition
P16-1048_anno1	25-33	34-37	In order to enhance representation capability of sentence ,	we employ conceptualization model	In order to enhance representation capability of sentence ,	we employ conceptualization model	25-59	25-59	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	1>2	none	enablement	enablement
P16-1048_anno1	34-37	38-49	we employ conceptualization model	to assign associated concepts for each sentence in the text corpus ,	we employ conceptualization model	to assign associated concepts for each sentence in the text corpus ,	25-59	25-59	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	1<2	none	enablement	enablement
P16-1048_anno1	34-37	50-59	we employ conceptualization model	and then learn conceptual sentence embedding ( CSE ) .	we employ conceptualization model	and then learn conceptual sentence embedding ( CSE ) .	25-59	25-59	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	1<2	none	progression	progression
P16-1048_anno1	34-37	60-73	we employ conceptualization model	Hence , this semantic representation is more expressive than some widely-used text representation models	we employ conceptualization model	Hence , this semantic representation is more expressive than some widely-used text representation models	25-59	60-83	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	Hence , this semantic representation is more expressive than some widely-used text representation models such as latent topic model , especially for short-text .	1<2	none	result	result
P16-1048_anno1	60-73	74-79	Hence , this semantic representation is more expressive than some widely-used text representation models	such as latent topic model ,	Hence , this semantic representation is more expressive than some widely-used text representation models	such as latent topic model ,	60-83	60-83	Hence , this semantic representation is more expressive than some widely-used text representation models such as latent topic model , especially for short-text .	Hence , this semantic representation is more expressive than some widely-used text representation models such as latent topic model , especially for short-text .	1<2	none	elab-example	elab-example
P16-1048_anno1	74-79	80-83	such as latent topic model ,	especially for short-text .	such as latent topic model ,	especially for short-text .	60-83	60-83	Hence , this semantic representation is more expressive than some widely-used text representation models such as latent topic model , especially for short-text .	Hence , this semantic representation is more expressive than some widely-used text representation models such as latent topic model , especially for short-text .	1<2	none	elab-addition	elab-addition
P16-1048_anno1	34-37	84-90	we employ conceptualization model	Moreover , we further extend CSE models	we employ conceptualization model	Moreover , we further extend CSE models	25-59	84-109	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	1<2	none	elab-addition	elab-addition
P16-1048_anno1	84-90	91-96	Moreover , we further extend CSE models	by utilizing a local attention-based model	Moreover , we further extend CSE models	by utilizing a local attention-based model	84-109	84-109	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	1<2	none	manner-means	manner-means
P16-1048_anno1	91-96	97-103	by utilizing a local attention-based model	that select relevant words within the context	by utilizing a local attention-based model	that select relevant words within the context	84-109	84-109	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	1<2	none	elab-addition	elab-addition
P16-1048_anno1	97-103	104-109	that select relevant words within the context	to make more efficient prediction .	that select relevant words within the context	to make more efficient prediction .	84-109	84-109	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	Moreover , we further extend CSE models by utilizing a local attention-based model that select relevant words within the context to make more efficient prediction .	1<2	none	enablement	enablement
P16-1048_anno1	110-122	133-142	In the experiments , we evaluate the CSE models on two tasks ,	that the proposed models outperform typical sentence embed-ding models .	In the experiments , we evaluate the CSE models on two tasks ,	that the proposed models outperform typical sentence embed-ding models .	110-128	129-142	In the experiments , we evaluate the CSE models on two tasks , text classification and information retrieval .	The experimental results show that the proposed models outperform typical sentence embed-ding models .	1>2	none	elab-addition	elab-addition
P16-1048_anno1	110-122	123-128	In the experiments , we evaluate the CSE models on two tasks ,	text classification and information retrieval .	In the experiments , we evaluate the CSE models on two tasks ,	text classification and information retrieval .	110-128	110-128	In the experiments , we evaluate the CSE models on two tasks , text classification and information retrieval .	In the experiments , we evaluate the CSE models on two tasks , text classification and information retrieval .	1<2	none	elab-enumember	elab-enumember
P16-1048_anno1	129-132	133-142	The experimental results show	that the proposed models outperform typical sentence embed-ding models .	The experimental results show	that the proposed models outperform typical sentence embed-ding models .	129-142	129-142	The experimental results show that the proposed models outperform typical sentence embed-ding models .	The experimental results show that the proposed models outperform typical sentence embed-ding models .	1>2	none	attribution	attribution
P16-1048_anno1	34-37	133-142	we employ conceptualization model	that the proposed models outperform typical sentence embed-ding models .	we employ conceptualization model	that the proposed models outperform typical sentence embed-ding models .	25-59	129-142	In order to enhance representation capability of sentence , we employ conceptualization model to assign associated concepts for each sentence in the text corpus , and then learn conceptual sentence embedding ( CSE ) .	The experimental results show that the proposed models outperform typical sentence embed-ding models .	1<2	none	evaluation	evaluation
P16-1049_anno1	1-11	23-30	Most current chatbot engines are designed to reply to user utterances	In this paper , we present DocChat ,	Most current chatbot engines are designed to reply to user utterances	In this paper , we present DocChat ,	1-22	23-54	Most current chatbot engines are designed to reply to user utterances based on existing utterance-response ( or Q-R ) 1 pairs .	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	1>2	none	bg-compare	bg-compare
P16-1049_anno1	1-11	12-22	Most current chatbot engines are designed to reply to user utterances	based on existing utterance-response ( or Q-R ) 1 pairs .	Most current chatbot engines are designed to reply to user utterances	based on existing utterance-response ( or Q-R ) 1 pairs .	1-22	1-22	Most current chatbot engines are designed to reply to user utterances based on existing utterance-response ( or Q-R ) 1 pairs .	Most current chatbot engines are designed to reply to user utterances based on existing utterance-response ( or Q-R ) 1 pairs .	1<2	none	bg-general	bg-general
P16-1049_anno1	23-30	31-38	In this paper , we present DocChat ,	a novel information retrieval approach for chatbot engines	In this paper , we present DocChat ,	a novel information retrieval approach for chatbot engines	23-54	23-54	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	1<2	none	elab-definition	elab-definition
P16-1049_anno1	31-38	39-49	a novel information retrieval approach for chatbot engines	that can leverage unstructured documents , instead of Q-R pairs ,	a novel information retrieval approach for chatbot engines	that can leverage unstructured documents , instead of Q-R pairs ,	23-54	23-54	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	1<2	none	elab-addition	elab-addition
P16-1049_anno1	23-30	50-54	In this paper , we present DocChat ,	to respond to utterances .	In this paper , we present DocChat ,	to respond to utterances .	23-54	23-54	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	1<2	none	enablement	enablement
P16-1049_anno1	23-30	55-61,68-69	In this paper , we present DocChat ,	A learning to rank model with features <*> is proposed	In this paper , we present DocChat ,	A learning to rank model with features <*> is proposed	23-54	55-79	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly .	1<2	none	elab-addition	elab-addition
P16-1049_anno1	55-61,68-69	62-67	A learning to rank model with features <*> is proposed	designed at different levels of granularity	A learning to rank model with features <*> is proposed	designed at different levels of granularity	55-79	55-79	A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly .	A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly .	1<2	none	elab-addition	elab-addition
P16-1049_anno1	55-61,68-69	70-79	A learning to rank model with features <*> is proposed	to measure the relevance between utterances and responses directly .	A learning to rank model with features <*> is proposed	to measure the relevance between utterances and responses directly .	55-79	55-79	A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly .	A learning to rank model with features designed at different levels of granularity is proposed to measure the relevance between utterances and responses directly .	1<2	none	enablement	enablement
P16-1049_anno1	23-30	80-89	In this paper , we present DocChat ,	We evaluate our proposed approach in both English and Chinese	In this paper , we present DocChat ,	We evaluate our proposed approach in both English and Chinese	23-54	80-117	In this paper , we present DocChat , a novel information retrieval approach for chatbot engines that can leverage unstructured documents , instead of Q-R pairs , to respond to utterances .	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	1<2	none	evaluation	evaluation
P16-1049_anno1	90-104	118-125	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	Reasonable improvements and good adaptability are observed .	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	Reasonable improvements and good adaptability are observed .	80-117	118-125	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	Reasonable improvements and good adaptability are observed .	1>2	none	elab-addition	elab-addition
P16-1049_anno1	90-104	105-110	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	two answer sentence selection tasks ,	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	two answer sentence selection tasks ,	80-117	80-117	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	1<2	none	elab-addition	elab-addition
P16-1049_anno1	90-104	111-117	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	and compare it with state-of-the-art methods .	: ( i ) For English , we evaluate DocChat on WikiQA and QASent ,	and compare it with state-of-the-art methods .	80-117	80-117	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	1<2	none	joint	joint
P16-1049_anno1	80-89	118-125	We evaluate our proposed approach in both English and Chinese	Reasonable improvements and good adaptability are observed .	We evaluate our proposed approach in both English and Chinese	Reasonable improvements and good adaptability are observed .	80-117	118-125	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	Reasonable improvements and good adaptability are observed .	1<2	none	elab-aspect	elab-aspect
P16-1049_anno1	126-137	149-157	( ii ) For Chinese , we compare DocChat with XiaoIce2 ,	that DocChat is a perfect complement for chatbot engines	( ii ) For Chinese , we compare DocChat with XiaoIce2 ,	that DocChat is a perfect complement for chatbot engines	126-166	126-166	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	1>2	none	progression	progression
P16-1049_anno1	126-137	138-144	( ii ) For Chinese , we compare DocChat with XiaoIce2 ,	a famous chitchat engine in China ,	( ii ) For Chinese , we compare DocChat with XiaoIce2 ,	a famous chitchat engine in China ,	126-166	126-166	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	1<2	none	elab-addition	elab-addition
P16-1049_anno1	145-148	149-157	and side-by-side evaluation shows	that DocChat is a perfect complement for chatbot engines	and side-by-side evaluation shows	that DocChat is a perfect complement for chatbot engines	126-166	126-166	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	1>2	none	attribution	attribution
P16-1049_anno1	80-89	149-157	We evaluate our proposed approach in both English and Chinese	that DocChat is a perfect complement for chatbot engines	We evaluate our proposed approach in both English and Chinese	that DocChat is a perfect complement for chatbot engines	80-117	126-166	We evaluate our proposed approach in both English and Chinese : ( i ) For English , we evaluate DocChat on WikiQA and QASent , two answer sentence selection tasks , and compare it with state-of-the-art methods .	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	1<2	none	elab-aspect	elab-aspect
P16-1049_anno1	149-157	158-166	that DocChat is a perfect complement for chatbot engines	using Q-R pairs as main source of responses .	that DocChat is a perfect complement for chatbot engines	using Q-R pairs as main source of responses .	126-166	126-166	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	( ii ) For Chinese , we compare DocChat with XiaoIce2 , a famous chitchat engine in China , and side-by-side evaluation shows that DocChat is a perfect complement for chatbot engines using Q-R pairs as main source of responses .	1<2	none	manner-means	manner-means
P16-1050_anno1	1-17	36-48	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners ,	We focus here on `` linguistic alignment , '' changes in word choice	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners ,	We focus here on `` linguistic alignment , '' changes in word choice	1-35	36-54	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners , changing the style and substance of their communications to be more similar to their partners ' utterances .	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	1>2	none	bg-goal	bg-goal
P16-1050_anno1	1-17	18-25	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners ,	changing the style and substance of their communications	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners ,	changing the style and substance of their communications	1-35	1-35	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners , changing the style and substance of their communications to be more similar to their partners ' utterances .	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners , changing the style and substance of their communications to be more similar to their partners ' utterances .	1<2	none	result	result
P16-1050_anno1	18-25	26-35	changing the style and substance of their communications	to be more similar to their partners ' utterances .	changing the style and substance of their communications	to be more similar to their partners' utterances .	1-35	1-35	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners , changing the style and substance of their communications to be more similar to their partners ' utterances .	In conversation , speakers tend to `` accommodate '' or `` align '' to their partners , changing the style and substance of their communications to be more similar to their partners ' utterances .	1<2	none	enablement	enablement
P16-1050_anno1	36-48	49-54	We focus here on `` linguistic alignment , '' changes in word choice	based on others ' choices .	We focus here on `` linguistic alignment , '' changes in word choice	based on others' choices .	36-54	36-54	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	1<2	none	bg-general	bg-general
P16-1050_anno1	55-63	78-83	Although linguistic alignment is observed across many different contexts	its sources are still uncertain .	Although linguistic alignment is observed across many different contexts	its sources are still uncertain .	55-83	55-83	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	1>2	none	contrast	contrast
P16-1050_anno1	55-63	64-71	Although linguistic alignment is observed across many different contexts	and its degree correlates with important social factors	Although linguistic alignment is observed across many different contexts	and its degree correlates with important social factors	55-83	55-83	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	1<2	none	joint	joint
P16-1050_anno1	64-71	72-77	and its degree correlates with important social factors	such as power and likability ,	and its degree correlates with important social factors	such as power and likability ,	55-83	55-83	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	1<2	none	elab-example	elab-example
P16-1050_anno1	36-48	78-83	We focus here on `` linguistic alignment , '' changes in word choice	its sources are still uncertain .	We focus here on `` linguistic alignment , '' changes in word choice	its sources are still uncertain .	36-54	55-83	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	Although linguistic alignment is observed across many different contexts and its degree correlates with important social factors such as power and likability , its sources are still uncertain .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	36-48	84-93	We focus here on `` linguistic alignment , '' changes in word choice	We build on a recent probabilistic model of alignment ,	We focus here on `` linguistic alignment , '' changes in word choice	We build on a recent probabilistic model of alignment ,	36-54	84-106	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	We build on a recent probabilistic model of alignment , using it to separate out alignment attributable to words versus word categories .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	84-93	94-106	We build on a recent probabilistic model of alignment ,	using it to separate out alignment attributable to words versus word categories .	We build on a recent probabilistic model of alignment ,	using it to separate out alignment attributable to words versus word categories .	84-106	84-106	We build on a recent probabilistic model of alignment , using it to separate out alignment attributable to words versus word categories .	We build on a recent probabilistic model of alignment , using it to separate out alignment attributable to words versus word categories .	1<2	none	manner-means	manner-means
P16-1050_anno1	84-93	107-113	We build on a recent probabilistic model of alignment ,	We model alignment in two contexts :	We build on a recent probabilistic model of alignment ,	We model alignment in two contexts :	84-106	107-119	We build on a recent probabilistic model of alignment , using it to separate out alignment attributable to words versus word categories .	We model alignment in two contexts : telephone conversations and microblog replies .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	107-113	114-119	We model alignment in two contexts :	telephone conversations and microblog replies .	We model alignment in two contexts :	telephone conversations and microblog replies .	107-119	107-119	We model alignment in two contexts : telephone conversations and microblog replies .	We model alignment in two contexts : telephone conversations and microblog replies .	1<2	none	elab-enumember	elab-enumember
P16-1050_anno1	107-113	120-126	We model alignment in two contexts :	Our results show evidence of alignment ,	We model alignment in two contexts :	Our results show evidence of alignment ,	107-119	120-135	We model alignment in two contexts : telephone conversations and microblog replies .	Our results show evidence of alignment , but it is primarily lexical rather than categorical .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	120-126	127-135	Our results show evidence of alignment ,	but it is primarily lexical rather than categorical .	Our results show evidence of alignment ,	but it is primarily lexical rather than categorical .	120-135	120-135	Our results show evidence of alignment , but it is primarily lexical rather than categorical .	Our results show evidence of alignment , but it is primarily lexical rather than categorical .	1<2	none	contrast	contrast
P16-1050_anno1	136-139	140-146	Furthermore , we find	that discourse acts modulate alignment substantially .	Furthermore , we find	that discourse acts modulate alignment substantially .	136-146	136-146	Furthermore , we find that discourse acts modulate alignment substantially .	Furthermore , we find that discourse acts modulate alignment substantially .	1>2	none	attribution	attribution
P16-1050_anno1	140-146	147-151	that discourse acts modulate alignment substantially .	This evidence supports the view	that discourse acts modulate alignment substantially .	This evidence supports the view	136-146	147-165	Furthermore , we find that discourse acts modulate alignment substantially .	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	1>2	none	exp-evidence	exp-evidence
P16-1050_anno1	36-48	147-151	We focus here on `` linguistic alignment , '' changes in word choice	This evidence supports the view	We focus here on `` linguistic alignment , '' changes in word choice	This evidence supports the view	36-54	147-165	We focus here on `` linguistic alignment , '' changes in word choice based on others ' choices .	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	147-151	152-159	This evidence supports the view	that alignment is shaped by strategic communicative processes	This evidence supports the view	that alignment is shaped by strategic communicative processes	147-165	147-165	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	1<2	none	elab-addition	elab-addition
P16-1050_anno1	152-159	160-165	that alignment is shaped by strategic communicative processes	related to the ongoing discourse .	that alignment is shaped by strategic communicative processes	related to the ongoing discourse .	147-165	147-165	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	This evidence supports the view that alignment is shaped by strategic communicative processes related to the ongoing discourse .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	1-16	24-38	The applicability of entropy rate constancy to dialogue is examined on two spoken dialogue corpora .	however , new entropy change patterns within the topic episodes of dialogue are described ,	The applicability of entropy rate constancy to dialogue is examined on two spoken dialogue corpora .	however , new entropy change patterns within the topic episodes of dialogue are described ,	1-16	17-45	The applicability of entropy rate constancy to dialogue is examined on two spoken dialogue corpora .	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	1>2	none	bg-compare	bg-compare
P16-1051_anno1	17-23	24-38	The principle is found to hold ;	however , new entropy change patterns within the topic episodes of dialogue are described ,	The principle is found to hold ;	however , new entropy change patterns within the topic episodes of dialogue are described ,	17-45	17-45	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	1>2	none	contrast	contrast
P16-1051_anno1	24-38	39-45	however , new entropy change patterns within the topic episodes of dialogue are described ,	which are different from written text .	however , new entropy change patterns within the topic episodes of dialogue are described ,	which are different from written text .	17-45	17-45	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	24-38	46-65	however , new entropy change patterns within the topic episodes of dialogue are described ,	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	however , new entropy change patterns within the topic episodes of dialogue are described ,	Speaker's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	17-45	46-78	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively , which results in local convergence between these speakers in each topic episode .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	46-65	66-78	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	which results in local convergence between these speakers in each topic episode .	Speaker's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	which results in local convergence between these speakers in each topic episode .	46-78	46-78	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively , which results in local convergence between these speakers in each topic episode .	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively , which results in local convergence between these speakers in each topic episode .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	79-80	81-91	This implies	that the sentence entropy in dialogue is conditioned on different contexts	This implies	that the sentence entropy in dialogue is conditioned on different contexts	79-98	79-98	This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker 's roles .	This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker 's roles .	1>2	none	attribution	attribution
P16-1051_anno1	46-65	81-91	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	that the sentence entropy in dialogue is conditioned on different contexts	Speaker's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively ,	that the sentence entropy in dialogue is conditioned on different contexts	46-78	79-98	Speaker 's dynamic roles as topic initiators and topic responders are associated with decreasing and increasing entropy , respectively , which results in local convergence between these speakers in each topic episode .	This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker 's roles .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	81-91	92-98	that the sentence entropy in dialogue is conditioned on different contexts	determined by the speaker 's roles .	that the sentence entropy in dialogue is conditioned on different contexts	determined by the speaker's roles .	79-98	79-98	This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker 's roles .	This implies that the sentence entropy in dialogue is conditioned on different contexts determined by the speaker 's roles .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	24-38	99-111	however , new entropy change patterns within the topic episodes of dialogue are described ,	Explanations from the perspectives of grounding theory and interactive alignment are discussed ,	however , new entropy change patterns within the topic episodes of dialogue are described ,	Explanations from the perspectives of grounding theory and interactive alignment are discussed ,	17-45	99-122	The principle is found to hold ; however , new entropy change patterns within the topic episodes of dialogue are described , which are different from written text .	Explanations from the perspectives of grounding theory and interactive alignment are discussed , resulting in a novel , unified informationtheoretic approach of dialogue .	1<2	none	elab-addition	elab-addition
P16-1051_anno1	99-111	112-122	Explanations from the perspectives of grounding theory and interactive alignment are discussed ,	resulting in a novel , unified informationtheoretic approach of dialogue .	Explanations from the perspectives of grounding theory and interactive alignment are discussed ,	resulting in a novel , unified informationtheoretic approach of dialogue .	99-122	99-122	Explanations from the perspectives of grounding theory and interactive alignment are discussed , resulting in a novel , unified informationtheoretic approach of dialogue .	Explanations from the perspectives of grounding theory and interactive alignment are discussed , resulting in a novel , unified informationtheoretic approach of dialogue .	1<2	none	result	result
P16-1052_anno1	1-6	7-19	To establish sophisticated dialogue systems ,	text planning needs to cope with congruent as well as incongruent interlocutor interests	To establish sophisticated dialogue systems ,	text planning needs to cope with congruent as well as incongruent interlocutor interests	1-25	1-25	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	1>2	none	enablement	enablement
P16-1052_anno1	7-19	76-81	text planning needs to cope with congruent as well as incongruent interlocutor interests	We introduce the concept of fairness	text planning needs to cope with congruent as well as incongruent interlocutor interests	We introduce the concept of fairness	1-25	76-97	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	1>2	none	bg-compare	bg-compare
P16-1052_anno1	7-19	20-25	text planning needs to cope with congruent as well as incongruent interlocutor interests	as given in everyday dialogues .	text planning needs to cope with congruent as well as incongruent interlocutor interests	as given in everyday dialogues .	1-25	1-25	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	7-19	26-40	text planning needs to cope with congruent as well as incongruent interlocutor interests	Little attention has been given to this topic in text planning in contrast to dialogues	text planning needs to cope with congruent as well as incongruent interlocutor interests	Little attention has been given to this topic in text planning in contrast to dialogues	1-25	26-49	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	Little attention has been given to this topic in text planning in contrast to dialogues that are fully aligned with anticipated user interests .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	26-40	41-49	Little attention has been given to this topic in text planning in contrast to dialogues	that are fully aligned with anticipated user interests .	Little attention has been given to this topic in text planning in contrast to dialogues	that are fully aligned with anticipated user interests .	26-49	26-49	Little attention has been given to this topic in text planning in contrast to dialogues that are fully aligned with anticipated user interests .	Little attention has been given to this topic in text planning in contrast to dialogues that are fully aligned with anticipated user interests .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	50-59	60-66	When considering dialogues with congruent and incongruent interlocutor interests ,	dialogue partners are facing the constant challenge	When considering dialogues with congruent and incongruent interlocutor interests ,	dialogue partners are facing the constant challenge	50-75	50-75	When considering dialogues with congruent and incongruent interlocutor interests , dialogue partners are facing the constant challenge of finding a balance between cooperation and competition .	When considering dialogues with congruent and incongruent interlocutor interests , dialogue partners are facing the constant challenge of finding a balance between cooperation and competition .	1>2	none	condition	condition
P16-1052_anno1	7-19	60-66	text planning needs to cope with congruent as well as incongruent interlocutor interests	dialogue partners are facing the constant challenge	text planning needs to cope with congruent as well as incongruent interlocutor interests	dialogue partners are facing the constant challenge	1-25	50-75	To establish sophisticated dialogue systems , text planning needs to cope with congruent as well as incongruent interlocutor interests as given in everyday dialogues .	When considering dialogues with congruent and incongruent interlocutor interests , dialogue partners are facing the constant challenge of finding a balance between cooperation and competition .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	60-66	67-75	dialogue partners are facing the constant challenge	of finding a balance between cooperation and competition .	dialogue partners are facing the constant challenge	of finding a balance between cooperation and competition .	50-75	50-75	When considering dialogues with congruent and incongruent interlocutor interests , dialogue partners are facing the constant challenge of finding a balance between cooperation and competition .	When considering dialogues with congruent and incongruent interlocutor interests , dialogue partners are facing the constant challenge of finding a balance between cooperation and competition .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	76-81	82-88	We introduce the concept of fairness	that operationalize an equal and adequate ,	We introduce the concept of fairness	that operationalize an equal and adequate ,	76-97	76-97	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	76-81	89-97	We introduce the concept of fairness	i.e. equitable satisfaction of all interlocutors ' interests .	We introduce the concept of fairness	i.e. equitable satisfaction of all interlocutors' interests .	76-97	76-97	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	1<2	none	elab-definition	elab-definition
P16-1052_anno1	98-105	106-111	Focusing on Question-Answering ( QA ) settings ,	we describe an answer planning approach	Focusing on Question-Answering ( QA ) settings ,	we describe an answer planning approach	98-122	98-122	Focusing on Question-Answering ( QA ) settings , we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests .	Focusing on Question-Answering ( QA ) settings , we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests .	1>2	none	elab-addition	elab-addition
P16-1052_anno1	76-81	106-111	We introduce the concept of fairness	we describe an answer planning approach	We introduce the concept of fairness	we describe an answer planning approach	76-97	98-122	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	Focusing on Question-Answering ( QA ) settings , we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	106-111	112-122	we describe an answer planning approach	that support fair dialogues under congruent and incongruent interlocutor interests .	we describe an answer planning approach	that support fair dialogues under congruent and incongruent interlocutor interests .	98-122	98-122	Focusing on Question-Answering ( QA ) settings , we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests .	Focusing on Question-Answering ( QA ) settings , we describe an answer planning approach that support fair dialogues under congruent and incongruent interlocutor interests .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	123-126	133-143	Due to the fact	we present promising results from an empirical study ( N=107 )	Due to the fact	we present promising results from an empirical study ( N=107 )	123-157	123-157	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	1>2	none	cause	cause
P16-1052_anno1	123-126	127-132	Due to the fact	that fairness is subjective perse ,	Due to the fact	that fairness is subjective perse ,	123-157	123-157	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	76-81	133-143	We introduce the concept of fairness	we present promising results from an empirical study ( N=107 )	We introduce the concept of fairness	we present promising results from an empirical study ( N=107 )	76-97	123-157	We introduce the concept of fairness that operationalize an equal and adequate , i.e. equitable satisfaction of all interlocutors ' interests .	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	1<2	none	evaluation	evaluation
P16-1052_anno1	133-143	144-152	we present promising results from an empirical study ( N=107 )	in which human subjects interacted with a QA system	we present promising results from an empirical study ( N=107 )	in which human subjects interacted with a QA system	123-157	123-157	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	1<2	none	elab-addition	elab-addition
P16-1052_anno1	144-152	153-157	in which human subjects interacted with a QA system	implementing the proposed approach .	in which human subjects interacted with a QA system	implementing the proposed approach .	123-157	123-157	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	Due to the fact that fairness is subjective perse , we present promising results from an empirical study ( N=107 ) in which human subjects interacted with a QA system implementing the proposed approach .	1<2	none	elab-addition	elab-addition
P16-1053_anno1	1-15	58-70	Modeling interactions between two sentences is crucial for a number of natural language processing tasks	In this work , we propose a Sentence Interaction Network ( SIN )	Modeling interactions between two sentences is crucial for a number of natural language processing tasks	In this work , we propose a Sentence Interaction Network ( SIN )	1-25	58-79	Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection , Dialogue Act Analysis , etc. .	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	1>2	none	bg-goal	bg-goal
P16-1053_anno1	1-15	16-25	Modeling interactions between two sentences is crucial for a number of natural language processing tasks	including Answer Selection , Dialogue Act Analysis , etc. .	Modeling interactions between two sentences is crucial for a number of natural language processing tasks	including Answer Selection , Dialogue Act Analysis , etc. .	1-25	1-25	Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection , Dialogue Act Analysis , etc. .	Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection , Dialogue Act Analysis , etc. .	1<2	none	elab-example	elab-example
P16-1053_anno1	26-47	48-57	While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling ,	prior studies paid less attention on interactions between sentences .	While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling ,	prior studies paid less attention on interactions between sentences .	26-57	26-57	While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling , prior studies paid less attention on interactions between sentences .	While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling , prior studies paid less attention on interactions between sentences .	1>2	none	contrast	contrast
P16-1053_anno1	48-57	58-70	prior studies paid less attention on interactions between sentences .	In this work , we propose a Sentence Interaction Network ( SIN )	prior studies paid less attention on interactions between sentences .	In this work , we propose a Sentence Interaction Network ( SIN )	26-57	58-79	While deep learning methods like Recurrent Neural Network or Convolutional Neural Network have been proved to be powerful for sentence modeling , prior studies paid less attention on interactions between sentences .	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	1>2	none	bg-compare	bg-compare
P16-1053_anno1	58-70	71-79	In this work , we propose a Sentence Interaction Network ( SIN )	for modeling the complex interactions between two sentences .	In this work , we propose a Sentence Interaction Network ( SIN )	for modeling the complex interactions between two sentences .	58-79	58-79	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	1<2	none	elab-addition	elab-addition
P16-1053_anno1	80-91	92-104	By introducing `` interaction states '' for word and phrase pairs ,	SIN is powerful and flexible in capturing sentence interactions for different tasks .	By introducing `` interaction states '' for word and phrase pairs ,	SIN is powerful and flexible in capturing sentence interactions for different tasks .	80-104	80-104	By introducing `` interaction states '' for word and phrase pairs , SIN is powerful and flexible in capturing sentence interactions for different tasks .	By introducing `` interaction states '' for word and phrase pairs , SIN is powerful and flexible in capturing sentence interactions for different tasks .	1>2	none	manner-means	manner-means
P16-1053_anno1	58-70	92-104	In this work , we propose a Sentence Interaction Network ( SIN )	SIN is powerful and flexible in capturing sentence interactions for different tasks .	In this work , we propose a Sentence Interaction Network ( SIN )	SIN is powerful and flexible in capturing sentence interactions for different tasks .	58-79	80-104	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	By introducing `` interaction states '' for word and phrase pairs , SIN is powerful and flexible in capturing sentence interactions for different tasks .	1<2	none	elab-addition	elab-addition
P16-1053_anno1	58-70	105-115	In this work , we propose a Sentence Interaction Network ( SIN )	We obtain significant improvements on Answer Selection and Dialogue Act Analysis	In this work , we propose a Sentence Interaction Network ( SIN )	We obtain significant improvements on Answer Selection and Dialogue Act Analysis	58-79	105-120	In this work , we propose a Sentence Interaction Network ( SIN ) for modeling the complex interactions between two sentences .	We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering .	1<2	none	evaluation	evaluation
P16-1053_anno1	105-115	116-120	We obtain significant improvements on Answer Selection and Dialogue Act Analysis	without any feature engineering .	We obtain significant improvements on Answer Selection and Dialogue Act Analysis	without any feature engineering .	105-120	105-120	We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering .	We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering .	1<2	none	condition	condition
P16-1054_anno1	1-9	10-14	In this study , we introduce a nondeterministic method	for referring expression generation .	In this study , we introduce a nondeterministic method	for referring expression generation .	1-14	1-14	In this study , we introduce a nondeterministic method for referring expression generation .	In this study , we introduce a nondeterministic method for referring expression generation .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	1-9	15-18	In this study , we introduce a nondeterministic method	We describe two models	In this study , we introduce a nondeterministic method	We describe two models	1-14	15-44	In this study , we introduce a nondeterministic method for referring expression generation .	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	15-18	19-34	We describe two models	that account for individual variation in the choice of referential form in automatically generated text :	We describe two models	that account for individual variation in the choice of referential form in automatically generated text :	15-44	15-44	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	15-18	35-44	We describe two models	a Naive Bayes model and a Recurrent Neural Network .	We describe two models	a Naive Bayes model and a Recurrent Neural Network .	15-44	15-44	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	1<2	none	elab-enumember	elab-enumember
P16-1054_anno1	15-18	45-47	We describe two models	Both are evaluated	We describe two models	Both are evaluated	15-44	45-52	We describe two models that account for individual variation in the choice of referential form in automatically generated text : a Naive Bayes model and a Recurrent Neural Network .	Both are evaluated using the VaREG corpus .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	45-47	48-52	Both are evaluated	using the VaREG corpus .	Both are evaluated	using the VaREG corpus .	45-52	45-52	Both are evaluated using the VaREG corpus .	Both are evaluated using the VaREG corpus .	1<2	none	manner-means	manner-means
P16-1054_anno1	45-47	53-59	Both are evaluated	Then we select the best performing model	Both are evaluated	Then we select the best performing model	45-52	53-103	Both are evaluated using the VaREG corpus .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	53-59	60-69	Then we select the best performing model	to generate referential forms in texts from the GREC-2.0 corpus	Then we select the best performing model	to generate referential forms in texts from the GREC-2.0 corpus	53-103	53-103	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	enablement	enablement
P16-1054_anno1	53-59	70-74	Then we select the best performing model	and conduct an evaluation experiment	Then we select the best performing model	and conduct an evaluation experiment	53-103	53-103	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	progression	progression
P16-1054_anno1	70-74	75-87	and conduct an evaluation experiment	in which humans judge the coherence and comprehensibility of the generated texts ,	and conduct an evaluation experiment	in which humans judge the coherence and comprehensibility of the generated texts ,	53-103	53-103	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	75-87	88-96	in which humans judge the coherence and comprehensibility of the generated texts ,	comparing them both with the original references and those	in which humans judge the coherence and comprehensibility of the generated texts ,	comparing them both with the original references and those	53-103	53-103	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	elab-addition	elab-addition
P16-1054_anno1	88-96	97-103	comparing them both with the original references and those	produced by a random baseline model .	comparing them both with the original references and those	produced by a random baseline model .	53-103	53-103	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts , comparing them both with the original references and those produced by a random baseline model .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	1-8	18-27	How much is 131 million US dollars ?	we propose a new task of automatically generating short descriptions	How much is 131 million US dollars ?	we propose a new task of automatically generating short descriptions	1-8	9-51	How much is 131 million US dollars ?	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	1>2	none	bg-goal	bg-goal
P16-1055_anno1	9-17	18-27	To help readers put such numbers in context ,	we propose a new task of automatically generating short descriptions	To help readers put such numbers in context ,	we propose a new task of automatically generating short descriptions	9-51	9-51	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	1>2	none	enablement	enablement
P16-1055_anno1	18-27	28-31	we propose a new task of automatically generating short descriptions	known as perspectives ,	we propose a new task of automatically generating short descriptions	known as perspectives ,	9-51	9-51	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	28-31	32-40	known as perspectives ,	e.g. `` $ 131 million is about the cost	known as perspectives ,	e.g. `` $ 131 million is about the cost	9-51	9-51	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	1<2	none	elab-example	elab-example
P16-1055_anno1	32-40	41-51	e.g. `` $ 131 million is about the cost	to employ everyone in Texas over a lunch period '' .	e.g. `` $ 131 million is about the cost	to employ everyone in Texas over a lunch period '' .	9-51	9-51	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	1<2	none	enablement	enablement
P16-1055_anno1	18-27	52-64	we propose a new task of automatically generating short descriptions	First , we collect a dataset of numeric mentions in news articles ,	we propose a new task of automatically generating short descriptions	First , we collect a dataset of numeric mentions in news articles ,	9-51	52-76	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	First , we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	52-64	65-76	First , we collect a dataset of numeric mentions in news articles ,	where each mention is labeled with a set of rated perspectives .	First , we collect a dataset of numeric mentions in news articles ,	where each mention is labeled with a set of rated perspectives .	52-76	52-76	First , we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives .	First , we collect a dataset of numeric mentions in news articles , where each mention is labeled with a set of rated perspectives .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	18-27	77-81	we propose a new task of automatically generating short descriptions	We then propose a system	we propose a new task of automatically generating short descriptions	We then propose a system	9-51	77-96	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	77-81	82-85	We then propose a system	to generate these descriptions	We then propose a system	to generate these descriptions	77-96	77-96	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	1<2	none	enablement	enablement
P16-1055_anno1	82-85	86-90	to generate these descriptions	consisting of two steps :	to generate these descriptions	consisting of two steps :	77-96	77-96	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	1<2	none	elab-addition	elab-addition
P16-1055_anno1	86-90	91-96	consisting of two steps :	formula construction and description generation .	consisting of two steps :	formula construction and description generation .	77-96	77-96	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	1<2	none	elab-enumember	elab-enumember
P16-1055_anno1	91-96	97-109	formula construction and description generation .	In construction , we compose formulae from numeric facts in a knowledge base	formula construction and description generation .	In construction , we compose formulae from numeric facts in a knowledge base	77-96	97-124	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	In construction , we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility .	1<2	none	elab-aspect	elab-aspect
P16-1055_anno1	97-109	110-114	In construction , we compose formulae from numeric facts in a knowledge base	and rank the resulting formulas	In construction , we compose formulae from numeric facts in a knowledge base	and rank the resulting formulas	97-124	97-124	In construction , we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility .	In construction , we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility .	1<2	none	joint	joint
P16-1055_anno1	110-114	115-124	and rank the resulting formulas	based on familiarity , numeric proximity and semantic compatibility .	and rank the resulting formulas	based on familiarity , numeric proximity and semantic compatibility .	97-124	97-124	In construction , we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility .	In construction , we compose formulae from numeric facts in a knowledge base and rank the resulting formulas based on familiarity , numeric proximity and semantic compatibility .	1<2	none	bg-general	bg-general
P16-1055_anno1	91-96	125-134	formula construction and description generation .	In generation , we convert a formula into natural language	formula construction and description generation .	In generation , we convert a formula into natural language	77-96	125-141	We then propose a system to generate these descriptions consisting of two steps : formula construction and description generation .	In generation , we convert a formula into natural language using a sequence-to-sequence recurrent neural network .	1<2	none	elab-aspect	elab-aspect
P16-1055_anno1	125-134	135-141	In generation , we convert a formula into natural language	using a sequence-to-sequence recurrent neural network .	In generation , we convert a formula into natural language	using a sequence-to-sequence recurrent neural network .	125-141	125-141	In generation , we convert a formula into natural language using a sequence-to-sequence recurrent neural network .	In generation , we convert a formula into natural language using a sequence-to-sequence recurrent neural network .	1<2	none	manner-means	manner-means
P16-1055_anno1	18-27	142-168	we propose a new task of automatically generating short descriptions	Our system obtains a 15.2 % F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation .	we propose a new task of automatically generating short descriptions	Our system obtains a 15.2 % F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation .	9-51	142-168	To help readers put such numbers in context , we propose a new task of automatically generating short descriptions known as perspectives , e.g. `` $ 131 million is about the cost to employ everyone in Texas over a lunch period '' .	Our system obtains a 15.2 % F1 improvement over a non-compositional baseline at formula construction and a 12.5 BLEU point improvement over a baseline description generation .	1<2	none	evaluation	evaluation
P16-1056_anno1	1-19	20-33	Over the past decade , large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances .	However , to this date , there are no large-scale question-answer corpora available .	Over the past decade , large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances .	However , to this date , there are no large-scale question-answer corpora available .	1-19	20-33	Over the past decade , large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances .	However , to this date , there are no large-scale question-answer corpora available .	1>2	none	contrast	contrast
P16-1056_anno1	20-33	34-44	However , to this date , there are no large-scale question-answer corpora available .	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	However , to this date , there are no large-scale question-answer corpora available .	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	20-33	34-70	However , to this date , there are no large-scale question-answer corpora available .	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	1>2	none	bg-goal	bg-goal
P16-1056_anno1	34-44	45-49	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	an enormous question-answer pair corpus	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	an enormous question-answer pair corpus	34-70	34-70	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	1<2	none	elab-definition	elab-definition
P16-1056_anno1	45-49	50-62	an enormous question-answer pair corpus	produced by applying a novel neural network architecture on the knowledge base Freebase	an enormous question-answer pair corpus	produced by applying a novel neural network architecture on the knowledge base Freebase	34-70	34-70	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	1<2	none	elab-addition	elab-addition
P16-1056_anno1	50-62	63-70	produced by applying a novel neural network architecture on the knowledge base Freebase	to transduce facts into natural language questions .	produced by applying a novel neural network architecture on the knowledge base Freebase	to transduce facts into natural language questions .	34-70	34-70	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	1<2	none	enablement	enablement
P16-1056_anno1	34-44	71-76	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	The produced question-answer pairs are evaluated	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	The produced question-answer pairs are evaluated	34-70	71-95	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	1<2	none	elab-addition	elab-addition
P16-1056_anno1	71-76	77-80	The produced question-answer pairs are evaluated	both by human evaluators	The produced question-answer pairs are evaluated	both by human evaluators	71-95	71-95	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	1<2	none	manner-means	manner-means
P16-1056_anno1	77-80	81-86	both by human evaluators	and using automatic evaluation metrics ,	both by human evaluators	and using automatic evaluation metrics ,	71-95	71-95	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	1<2	none	joint	joint
P16-1056_anno1	81-86	87-95	and using automatic evaluation metrics ,	including well-established machine translation and sentence similarity metrics .	and using automatic evaluation metrics ,	including well-established machine translation and sentence similarity metrics .	71-95	71-95	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	The produced question-answer pairs are evaluated both by human evaluators and using automatic evaluation metrics , including well-established machine translation and sentence similarity metrics .	1<2	none	elab-example	elab-example
P16-1056_anno1	34-44	96-108	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	Across all evaluation criteria the question-generation model outperforms the competing template-based baseline .	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	Across all evaluation criteria the question-generation model outperforms the competing template-based baseline .	34-70	96-108	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	Across all evaluation criteria the question-generation model outperforms the competing template-based baseline .	1<2	none	evaluation	evaluation
P16-1056_anno1	109-116	117-130	Furthermore , when presented to human evaluators ,	the generated questions appear to be comparable in quality to real human-generated questions .	Furthermore , when presented to human evaluators ,	the generated questions appear to be comparable in quality to real human-generated questions .	109-130	109-130	Furthermore , when presented to human evaluators , the generated questions appear to be comparable in quality to real human-generated questions .	Furthermore , when presented to human evaluators , the generated questions appear to be comparable in quality to real human-generated questions .	1>2	none	condition	condition
P16-1056_anno1	34-44	117-130	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	the generated questions appear to be comparable in quality to real human-generated questions .	In this paper we present the 30M Factoid QuestionAnswer Corpus ,	the generated questions appear to be comparable in quality to real human-generated questions .	34-70	109-130	In this paper we present the 30M Factoid QuestionAnswer Corpus , an enormous question-answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions .	Furthermore , when presented to human evaluators , the generated questions appear to be comparable in quality to real human-generated questions .	1<2	none	evaluation	evaluation
P16-1057_anno1	1-9	18-24	Many language generation tasks require the production of text	We present a novel neural network architecture	Many language generation tasks require the production of text	We present a novel neural network architecture	1-17	18-38	Many language generation tasks require the production of text conditioned on both structured and unstructured inputs .	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	1>2	none	bg-compare	bg-compare
P16-1057_anno1	1-9	10-17	Many language generation tasks require the production of text	conditioned on both structured and unstructured inputs .	Many language generation tasks require the production of text	conditioned on both structured and unstructured inputs .	1-17	1-17	Many language generation tasks require the production of text conditioned on both structured and unstructured inputs .	Many language generation tasks require the production of text conditioned on both structured and unstructured inputs .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	18-24	25-29	We present a novel neural network architecture	which generates an output sequence	We present a novel neural network architecture	which generates an output sequence	18-38	18-38	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	25-29	30-38	which generates an output sequence	conditioned on an arbitrary number of input functions .	which generates an output sequence	conditioned on an arbitrary number of input functions .	18-38	18-38	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	18-24	39-55	We present a novel neural network architecture	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	We present a novel neural network architecture	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	18-38	39-72	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	39-55	56-61	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	for example characters or tokens ,	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	for example characters or tokens ,	39-72	39-72	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	1<2	none	elab-example	elab-example
P16-1057_anno1	39-55	62-65	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	to be marginalised ,	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	to be marginalised ,	39-72	39-72	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	1<2	none	enablement	enablement
P16-1057_anno1	39-55	66-72	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	thus permitting scalable and effective training .	Crucially , our approach allows both the choice of conditioning context and the granularity of generation ,	thus permitting scalable and effective training .	39-72	39-72	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	Crucially , our approach allows both the choice of conditioning context and the granularity of generation , for example characters or tokens , to be marginalised , thus permitting scalable and effective training .	1<2	none	result	result
P16-1057_anno1	73-76	77-80	Using this framework ,	we address the problem	Using this framework ,	we address the problem	73-93	73-93	Using this framework , we address the problem of generating programming code from a mixed natural language and structured specification .	Using this framework , we address the problem of generating programming code from a mixed natural language and structured specification .	1>2	none	manner-means	manner-means
P16-1057_anno1	18-24	77-80	We present a novel neural network architecture	we address the problem	We present a novel neural network architecture	we address the problem	18-38	73-93	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	Using this framework , we address the problem of generating programming code from a mixed natural language and structured specification .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	77-80	81-93	we address the problem	of generating programming code from a mixed natural language and structured specification .	we address the problem	of generating programming code from a mixed natural language and structured specification .	73-93	73-93	Using this framework , we address the problem of generating programming code from a mixed natural language and structured specification .	Using this framework , we address the problem of generating programming code from a mixed natural language and structured specification .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	18-24	94-102	We present a novel neural network architecture	We create two new data sets for this paradigm	We present a novel neural network architecture	We create two new data sets for this paradigm	18-38	94-115	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	94-102	103-115	We create two new data sets for this paradigm	derived from the collectible trading card games Magic the Gathering and Hearthstone .	We create two new data sets for this paradigm	derived from the collectible trading card games Magic the Gathering and Hearthstone .	94-115	94-115	We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone .	We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone .	1<2	none	elab-addition	elab-addition
P16-1057_anno1	116-126	127-138	On these , and a third preexisting corpus , we demonstrate	that marginalising multiple predictors allows our model to outperform strong benchmarks .	On these , and a third preexisting corpus , we demonstrate	that marginalising multiple predictors allows our model to outperform strong benchmarks .	116-138	116-138	On these , and a third preexisting corpus , we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks .	On these , and a third preexisting corpus , we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks .	1>2	none	attribution	attribution
P16-1057_anno1	18-24	127-138	We present a novel neural network architecture	that marginalising multiple predictors allows our model to outperform strong benchmarks .	We present a novel neural network architecture	that marginalising multiple predictors allows our model to outperform strong benchmarks .	18-38	116-138	We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions .	On these , and a third preexisting corpus , we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks .	1<2	none	evaluation	evaluation
P16-1058_anno1	1-16	29-48	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' ,	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' ,	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	1-28	29-60	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' , where the aim is to generate a single , discriminating expression .	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	1>2	none	contrast	contrast
P16-1058_anno1	1-16	17-28	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' ,	where the aim is to generate a single , discriminating expression .	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' ,	where the aim is to generate a single , discriminating expression .	1-28	1-28	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' , where the aim is to generate a single , discriminating expression .	Research on generating referring expressions has so far mostly focussed on `` oneshot reference '' , where the aim is to generate a single , discriminating expression .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	29-48	63-72	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	that this strategy can also be advantageous in technical systems	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	that this strategy can also be advantageous in technical systems	29-60	61-83	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	1>2	none	bg-goal	bg-goal
P16-1058_anno1	29-48	49-54	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	where referring information is offered piecewise	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' ,	where referring information is offered piecewise	29-60	29-60	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	49-54	55-60	where referring information is offered piecewise	until success has been confirmed .	where referring information is offered piecewise	until success has been confirmed .	29-60	29-60	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	In interactive settings , however , it is not uncommon for reference to be established in `` installments '' , where referring information is offered piecewise until success has been confirmed .	1<2	none	temporal	temporal
P16-1058_anno1	61-62	63-72	We show	that this strategy can also be advantageous in technical systems	We show	that this strategy can also be advantageous in technical systems	61-83	61-83	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	1>2	none	attribution	attribution
P16-1058_anno1	63-72	73-83	that this strategy can also be advantageous in technical systems	that only have uncertain access to object attributes and categories .	that this strategy can also be advantageous in technical systems	that only have uncertain access to object attributes and categories .	61-83	61-83	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	63-72	84-103	that this strategy can also be advantageous in technical systems	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	that this strategy can also be advantageous in technical systems	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	61-83	84-111	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	84-103	104-111	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	and learn to predict semantically appropriate expressions .	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	and learn to predict semantically appropriate expressions .	84-111	84-111	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions .	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions .	1<2	none	joint	joint
P16-1058_anno1	112-118	119-126	In a human evaluation , we observe	that users are sensitive to inadequate object names-	In a human evaluation , we observe	that users are sensitive to inadequate object names-	112-139	112-139	In a human evaluation , we observe that users are sensitive to inadequate object names- which unfortunately are not unlikely to be generated from low-level visual input .	In a human evaluation , we observe that users are sensitive to inadequate object names- which unfortunately are not unlikely to be generated from low-level visual input .	1>2	none	attribution	attribution
P16-1058_anno1	84-103	119-126	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	that users are sensitive to inadequate object names-	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images	that users are sensitive to inadequate object names-	84-111	112-139	We train a recently introduced model of grounded word meaning on a data set of REs for objects in images and learn to predict semantically appropriate expressions .	In a human evaluation , we observe that users are sensitive to inadequate object names- which unfortunately are not unlikely to be generated from low-level visual input .	1<2	none	evaluation	evaluation
P16-1058_anno1	119-126	127-139	that users are sensitive to inadequate object names-	which unfortunately are not unlikely to be generated from low-level visual input .	that users are sensitive to inadequate object names-	which unfortunately are not unlikely to be generated from low-level visual input .	112-139	112-139	In a human evaluation , we observe that users are sensitive to inadequate object names- which unfortunately are not unlikely to be generated from low-level visual input .	In a human evaluation , we observe that users are sensitive to inadequate object names- which unfortunately are not unlikely to be generated from low-level visual input .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	63-72	140-143	that this strategy can also be advantageous in technical systems	We propose a solution	that this strategy can also be advantageous in technical systems	We propose a solution	61-83	140-159	We show that this strategy can also be advantageous in technical systems that only have uncertain access to object attributes and categories .	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	140-143	144-148	We propose a solution	inspired from human task-oriented interaction	We propose a solution	inspired from human task-oriented interaction	140-159	140-159	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	144-148	149-151	inspired from human task-oriented interaction	and implement strategies	inspired from human task-oriented interaction	and implement strategies	140-159	140-159	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	1<2	none	joint	joint
P16-1058_anno1	140-143	152-159	We propose a solution	for avoiding and repairing semantically inaccurate words .	We propose a solution	for avoiding and repairing semantically inaccurate words .	140-159	140-159	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	1<2	none	elab-addition	elab-addition
P16-1058_anno1	140-143	160-169	We propose a solution	We enhance a word-based REG with contextaware , referential installments	We propose a solution	We enhance a word-based REG with contextaware , referential installments	140-159	160-182	We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words .	We enhance a word-based REG with contextaware , referential installments and find that they substantially improve the referential success of the system .	1<2	none	evaluation	evaluation
P16-1058_anno1	170-171	172-182	and find	that they substantially improve the referential success of the system .	and find	that they substantially improve the referential success of the system .	160-182	160-182	We enhance a word-based REG with contextaware , referential installments and find that they substantially improve the referential success of the system .	We enhance a word-based REG with contextaware , referential installments and find that they substantially improve the referential success of the system .	1>2	none	attribution	attribution
P16-1058_anno1	160-169	172-182	We enhance a word-based REG with contextaware , referential installments	that they substantially improve the referential success of the system .	We enhance a word-based REG with contextaware , referential installments	that they substantially improve the referential success of the system .	160-182	160-182	We enhance a word-based REG with contextaware , referential installments and find that they substantially improve the referential success of the system .	We enhance a word-based REG with contextaware , referential installments and find that they substantially improve the referential success of the system .	1<2	none	joint	joint
P16-1059_anno1	1-5	50-56	Entity resolution is the task	We explore attentionlike mechanisms for coherence ,	Entity resolution is the task	We explore attentionlike mechanisms for coherence ,	1-26	50-83	Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	We explore attentionlike mechanisms for coherence , where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	1>2	none	bg-goal	bg-goal
P16-1059_anno1	1-5	6-26	Entity resolution is the task	of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	Entity resolution is the task	of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	1-26	1-26	Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	1-5	27-42	Entity resolution is the task	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities	Entity resolution is the task	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities	1-26	27-49	Entity resolution is the task of linking each mention of an entity in text to the corresponding record in a knowledge base ( KB ) .	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	27-42	43-49	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities	that are related in the KB .	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities	that are related in the KB .	27-49	27-49	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB .	Coherence models for entity resolution encourage all referring expressions in a document to resolve to entities that are related in the KB .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	50-56	57-83	We explore attentionlike mechanisms for coherence ,	where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	We explore attentionlike mechanisms for coherence ,	where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	50-83	50-83	We explore attentionlike mechanisms for coherence , where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	We explore attentionlike mechanisms for coherence , where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	50-56	84-99	We explore attentionlike mechanisms for coherence ,	The rationale is that documentwide support may simply not exist for non-salient entities , or entities	We explore attentionlike mechanisms for coherence ,	The rationale is that documentwide support may simply not exist for non-salient entities , or entities	50-83	84-106	We explore attentionlike mechanisms for coherence , where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	The rationale is that documentwide support may simply not exist for non-salient entities , or entities not densely connected in the KB .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	84-99	100-106	The rationale is that documentwide support may simply not exist for non-salient entities , or entities	not densely connected in the KB .	The rationale is that documentwide support may simply not exist for non-salient entities , or entities	not densely connected in the KB .	84-106	84-106	The rationale is that documentwide support may simply not exist for non-salient entities , or entities not densely connected in the KB .	The rationale is that documentwide support may simply not exist for non-salient entities , or entities not densely connected in the KB .	1<2	none	elab-addition	elab-addition
P16-1059_anno1	50-56	107-126	We explore attentionlike mechanisms for coherence ,	Our proposed system outperforms state-of-the-art systems on the CoNLL 2003 , TAC KBP 2010 , 2011 and 2012 tasks .	We explore attentionlike mechanisms for coherence ,	Our proposed system outperforms state-of-the-art systems on the CoNLL 2003 , TAC KBP 2010 , 2011 and 2012 tasks .	50-83	107-126	We explore attentionlike mechanisms for coherence , where the evidence for each candidate is based on a small set of strong relations , rather than relations to all other entities in the document .	Our proposed system outperforms state-of-the-art systems on the CoNLL 2003 , TAC KBP 2010 , 2011 and 2012 tasks .	1<2	none	evaluation	evaluation
P16-1060_anno1	1-15	16-36	Interpretability and discriminative power are the two most basic requirements for an evaluation metric .	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	Interpretability and discriminative power are the two most basic requirements for an evaluation metric .	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	1-15	16-46	Interpretability and discriminative power are the two most basic requirements for an evaluation metric .	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	1>2	none	bg-goal	bg-goal
P16-1060_anno1	16-36	37-46	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	that makes it impossible to interpret their results properly .	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	that makes it impossible to interpret their results properly .	16-46	16-46	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	16-36	47-49,56-58	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	The only metric <*> is MUC ,	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	The only metric <*> is MUC ,	16-46	47-71	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	The only metric which is insensitive to this flaw is MUC , which , however , is known to be the least discriminative metric .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	47-49,56-58	50-55	The only metric <*> is MUC ,	which is insensitive to this flaw	The only metric <*> is MUC ,	which is insensitive to this flaw	47-71	47-71	The only metric which is insensitive to this flaw is MUC , which , however , is known to be the least discriminative metric .	The only metric which is insensitive to this flaw is MUC , which , however , is known to be the least discriminative metric .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	56-58	59-71	is MUC ,	which , however , is known to be the least discriminative metric .	is MUC ,	which , however , is known to be the least discriminative metric .	47-71	47-71	The only metric which is insensitive to this flaw is MUC , which , however , is known to be the least discriminative metric .	The only metric which is insensitive to this flaw is MUC , which , however , is known to be the least discriminative metric .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	72-76	86-102	It is a known fact	The common practice for ranking coreference resolvers is to use the average of three different metrics .	It is a known fact	The common practice for ranking coreference resolvers is to use the average of three different metrics .	72-85	86-102	It is a known fact that none of the current metrics are reliable .	The common practice for ranking coreference resolvers is to use the average of three different metrics .	1>2	none	result	result
P16-1060_anno1	72-76	77-85	It is a known fact	that none of the current metrics are reliable .	It is a known fact	that none of the current metrics are reliable .	72-85	72-85	It is a known fact that none of the current metrics are reliable .	It is a known fact that none of the current metrics are reliable .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	86-102	103-112	The common practice for ranking coreference resolvers is to use the average of three different metrics .	However , one cannot expect to obtain a reliable score	The common practice for ranking coreference resolvers is to use the average of three different metrics .	However , one cannot expect to obtain a reliable score	86-102	103-118	The common practice for ranking coreference resolvers is to use the average of three different metrics .	However , one cannot expect to obtain a reliable score by averaging three unreliable metrics .	1>2	none	contrast	contrast
P16-1060_anno1	16-36	103-112	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	However , one cannot expect to obtain a reliable score	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	However , one cannot expect to obtain a reliable score	16-46	103-118	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	However , one cannot expect to obtain a reliable score by averaging three unreliable metrics .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	103-112	113-118	However , one cannot expect to obtain a reliable score	by averaging three unreliable metrics .	However , one cannot expect to obtain a reliable score	by averaging three unreliable metrics .	103-118	103-118	However , one cannot expect to obtain a reliable score by averaging three unreliable metrics .	However , one cannot expect to obtain a reliable score by averaging three unreliable metrics .	1<2	none	manner-means	manner-means
P16-1060_anno1	16-36	119-127	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	We propose LEA , a Link-based Entity-Aware evaluation metric	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics	We propose LEA , a Link-based Entity-Aware evaluation metric	16-46	119-140	In this paper , we report the mention identification effect in the B3 , CEAF , and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly .	We propose LEA , a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	119-127	128-140	We propose LEA , a Link-based Entity-Aware evaluation metric	that is designed to overcome the shortcomings of the current evaluation metrics .	We propose LEA , a Link-based Entity-Aware evaluation metric	that is designed to overcome the shortcomings of the current evaluation metrics .	119-140	119-140	We propose LEA , a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics .	We propose LEA , a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics .	1<2	none	elab-addition	elab-addition
P16-1060_anno1	119-127	141-156	We propose LEA , a Link-based Entity-Aware evaluation metric	LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer .	We propose LEA , a Link-based Entity-Aware evaluation metric	LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer .	119-140	141-156	We propose LEA , a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics .	LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	1-15	26-33	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features	We present a neural network based coreference system	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features	We present a neural network based coreference system	1-25	26-44	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs .	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	1>2	none	bg-goal	bg-goal
P16-1061_anno1	1-15	16-25	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features	defined over clusters of mentions instead of mention pairs .	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features	defined over clusters of mentions instead of mention pairs .	1-25	1-25	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs .	A long-standing challenge in coreference resolution has been the incorporation of entity-level information - features defined over clusters of mentions instead of mention pairs .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	26-33	34-44	We present a neural network based coreference system	that produces high-dimensional vector representations for pairs of coreference clusters .	We present a neural network based coreference system	that produces high-dimensional vector representations for pairs of coreference clusters .	26-44	26-44	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	45-48	49-57	Using these representations ,	our system learns when combining clusters is desirable .	Using these representations ,	our system learns when combining clusters is desirable .	45-57	45-57	Using these representations , our system learns when combining clusters is desirable .	Using these representations , our system learns when combining clusters is desirable .	1>2	none	manner-means	manner-means
P16-1061_anno1	34-44	49-57	that produces high-dimensional vector representations for pairs of coreference clusters .	our system learns when combining clusters is desirable .	that produces high-dimensional vector representations for pairs of coreference clusters .	our system learns when combining clusters is desirable .	26-44	45-57	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	Using these representations , our system learns when combining clusters is desirable .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	26-33	58-65	We present a neural network based coreference system	We train the system with a learning-to-search algorithm	We present a neural network based coreference system	We train the system with a learning-to-search algorithm	26-44	58-84	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	We train the system with a learning-to-search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	58-65	66-68	We train the system with a learning-to-search algorithm	that teaches it	We train the system with a learning-to-search algorithm	that teaches it	58-84	58-84	We train the system with a learning-to-search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	We train the system with a learning-to-search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	58-65	69-84	We train the system with a learning-to-search algorithm	which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	We train the system with a learning-to-search algorithm	which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	58-84	58-84	We train the system with a learning-to-search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	We train the system with a learning-to-search algorithm that teaches it which local decisions ( cluster merges ) will lead to a high-scoring final coreference partition .	1<2	none	elab-addition	elab-addition
P16-1061_anno1	26-33	85-104	We present a neural network based coreference system	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset	We present a neural network based coreference system	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset	26-44	85-110	We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters .	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features .	1<2	none	evaluation	evaluation
P16-1061_anno1	85-104	105-110	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset	despite using few hand-engineered features .	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset	despite using few hand-engineered features .	85-110	85-110	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features .	The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features .	1<2	none	contrast	contrast
P16-1062_anno1	1-4,19-22	28-37	Properties of corpora , <*> impact the best way	We examine several such properties in a variety of corpora	Properties of corpora , <*> impact the best way	We examine several such properties in a variety of corpora	1-27	28-51	Properties of corpora , such as the diversity of vocabulary and how tightly related texts cluster together , impact the best way to cluster short texts .	We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms .	1>2	none	bg-goal	bg-goal
P16-1062_anno1	1-4,19-22	5-18	Properties of corpora , <*> impact the best way	such as the diversity of vocabulary and how tightly related texts cluster together ,	Properties of corpora , <*> impact the best way	such as the diversity of vocabulary and how tightly related texts cluster together ,	1-27	1-27	Properties of corpora , such as the diversity of vocabulary and how tightly related texts cluster together , impact the best way to cluster short texts .	Properties of corpora , such as the diversity of vocabulary and how tightly related texts cluster together , impact the best way to cluster short texts .	1<2	none	elab-example	elab-example
P16-1062_anno1	19-22	23-27	impact the best way	to cluster short texts .	impact the best way	to cluster short texts .	1-27	1-27	Properties of corpora , such as the diversity of vocabulary and how tightly related texts cluster together , impact the best way to cluster short texts .	Properties of corpora , such as the diversity of vocabulary and how tightly related texts cluster together , impact the best way to cluster short texts .	1<2	none	elab-addition	elab-addition
P16-1062_anno1	28-37	38-51	We examine several such properties in a variety of corpora	and track their effects on various combinations of similarity metrics and clustering algorithms .	We examine several such properties in a variety of corpora	and track their effects on various combinations of similarity metrics and clustering algorithms .	28-51	28-51	We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms .	We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms .	1<2	none	joint	joint
P16-1062_anno1	52-53	54-73	We show	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	We show	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	52-82	52-82	We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset , but do not help with less creative texts .	We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset , but do not help with less creative texts .	1>2	none	attribution	attribution
P16-1062_anno1	28-37	54-73	We examine several such properties in a variety of corpora	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	We examine several such properties in a variety of corpora	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	28-51	52-82	We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms .	We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset , but do not help with less creative texts .	1<2	none	elab-addition	elab-addition
P16-1062_anno1	54-73	74-82	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	but do not help with less creative texts .	that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset ,	but do not help with less creative texts .	52-82	52-82	We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset , but do not help with less creative texts .	We show that semantic similarity metrics outperform traditional n-gram and dependency similarity metrics for kmeans clustering of a linguistically creative dataset , but do not help with less creative texts .	1<2	none	contrast	contrast
P16-1062_anno1	28-37	83-96	We examine several such properties in a variety of corpora	Yet the choice of similarity metric interacts with the choice of clustering method .	We examine several such properties in a variety of corpora	Yet the choice of similarity metric interacts with the choice of clustering method .	28-51	83-96	We examine several such properties in a variety of corpora and track their effects on various combinations of similarity metrics and clustering algorithms .	Yet the choice of similarity metric interacts with the choice of clustering method .	1<2	none	elab-addition	elab-addition
P16-1062_anno1	97-98	99-108	We find	that graphbased clustering methods perform well on tightly clustered data	We find	that graphbased clustering methods perform well on tightly clustered data	97-115	97-115	We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data .	We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data .	1>2	none	attribution	attribution
P16-1062_anno1	83-96	99-108	Yet the choice of similarity metric interacts with the choice of clustering method .	that graphbased clustering methods perform well on tightly clustered data	Yet the choice of similarity metric interacts with the choice of clustering method .	that graphbased clustering methods perform well on tightly clustered data	83-96	97-115	Yet the choice of similarity metric interacts with the choice of clustering method .	We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data .	1<2	none	elab-addition	elab-addition
P16-1062_anno1	99-108	109-115	that graphbased clustering methods perform well on tightly clustered data	but poorly on loosely clustered data .	that graphbased clustering methods perform well on tightly clustered data	but poorly on loosely clustered data .	97-115	97-115	We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data .	We find that graphbased clustering methods perform well on tightly clustered data but poorly on loosely clustered data .	1<2	none	contrast	contrast
P16-1062_anno1	83-96	116-122	Yet the choice of similarity metric interacts with the choice of clustering method .	Semantic similarity metrics generate loosely clustered output	Yet the choice of similarity metric interacts with the choice of clustering method .	Semantic similarity metrics generate loosely clustered output	83-96	116-131	Yet the choice of similarity metric interacts with the choice of clustering method .	Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset .	1<2	none	elab-addition	elab-addition
P16-1062_anno1	116-122	123-131	Semantic similarity metrics generate loosely clustered output	even when applied to a tightly clustered dataset .	Semantic similarity metrics generate loosely clustered output	even when applied to a tightly clustered dataset .	116-131	116-131	Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset .	Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset .	1<2	none	condition	condition
P16-1062_anno1	116-122	132-144	Semantic similarity metrics generate loosely clustered output	Thus , the best performing clustering systems could not use semantic metrics .	Semantic similarity metrics generate loosely clustered output	Thus , the best performing clustering systems could not use semantic metrics .	116-131	132-144	Semantic similarity metrics generate loosely clustered output even when applied to a tightly clustered dataset .	Thus , the best performing clustering systems could not use semantic metrics .	1<2	none	result	result
P16-1063_anno1	1-10	51-58	Word embedding maps words into a lowdimensional continuous embedding space	These two types of patterns are complementary .	Word embedding maps words into a lowdimensional continuous embedding space	These two types of patterns are complementary .	1-23	51-58	Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window .	These two types of patterns are complementary .	1>2	none	summary	summary
P16-1063_anno1	1-10	11-23	Word embedding maps words into a lowdimensional continuous embedding space	by exploiting the local word collocation patterns in a small context window .	Word embedding maps words into a lowdimensional continuous embedding space	by exploiting the local word collocation patterns in a small context window .	1-23	1-23	Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window .	Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window .	1<2	none	manner-means	manner-means
P16-1063_anno1	24-38	51-58	On the other hand , topic modeling maps documents onto a low-dimensional topic space ,	These two types of patterns are complementary .	On the other hand , topic modeling maps documents onto a low-dimensional topic space ,	These two types of patterns are complementary .	24-50	51-58	On the other hand , topic modeling maps documents onto a low-dimensional topic space , by utilizing the global word collocation patterns in the same document .	These two types of patterns are complementary .	1>2	none	summary	summary
P16-1063_anno1	24-38	39-50	On the other hand , topic modeling maps documents onto a low-dimensional topic space ,	by utilizing the global word collocation patterns in the same document .	On the other hand , topic modeling maps documents onto a low-dimensional topic space ,	by utilizing the global word collocation patterns in the same document .	24-50	24-50	On the other hand , topic modeling maps documents onto a low-dimensional topic space , by utilizing the global word collocation patterns in the same document .	On the other hand , topic modeling maps documents onto a low-dimensional topic space , by utilizing the global word collocation patterns in the same document .	1<2	none	manner-means	manner-means
P16-1063_anno1	51-58	59-69	These two types of patterns are complementary .	In this paper , we propose a generative topic embedding model	These two types of patterns are complementary .	In this paper , we propose a generative topic embedding model	51-58	59-77	These two types of patterns are complementary .	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	1>2	none	bg-goal	bg-goal
P16-1063_anno1	59-69	70-77	In this paper , we propose a generative topic embedding model	to combine the two types of patterns .	In this paper , we propose a generative topic embedding model	to combine the two types of patterns .	59-77	59-77	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	1<2	none	enablement	enablement
P16-1063_anno1	59-69	78-88	In this paper , we propose a generative topic embedding model	In our model , topics are represented by embedding vectors ,	In this paper , we propose a generative topic embedding model	In our model , topics are represented by embedding vectors ,	59-77	78-94	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	In our model , topics are represented by embedding vectors , and are shared across documents .	1<2	none	elab-addition	elab-addition
P16-1063_anno1	78-88	89-94	In our model , topics are represented by embedding vectors ,	and are shared across documents .	In our model , topics are represented by embedding vectors ,	and are shared across documents .	78-94	78-94	In our model , topics are represented by embedding vectors , and are shared across documents .	In our model , topics are represented by embedding vectors , and are shared across documents .	1<2	none	joint	joint
P16-1063_anno1	78-88	95-110	In our model , topics are represented by embedding vectors ,	The probability of each word is influenced by both its local context and its topic .	In our model , topics are represented by embedding vectors ,	The probability of each word is influenced by both its local context and its topic .	78-94	95-110	In our model , topics are represented by embedding vectors , and are shared across documents .	The probability of each word is influenced by both its local context and its topic .	1<2	none	elab-addition	elab-addition
P16-1063_anno1	78-88	111-129	In our model , topics are represented by embedding vectors ,	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	In our model , topics are represented by embedding vectors ,	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	78-94	111-129	In our model , topics are represented by embedding vectors , and are shared across documents .	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	1<2	none	elab-addition	elab-addition
P16-1063_anno1	111-129	130-140	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	Jointly they represent the document in a low-dimensional continuous space .	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	Jointly they represent the document in a low-dimensional continuous space .	111-129	130-140	A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document .	Jointly they represent the document in a low-dimensional continuous space .	1<2	none	elab-addition	elab-addition
P16-1063_anno1	59-69	141-159	In this paper , we propose a generative topic embedding model	In two document classification tasks , our method performs better than eight existing methods , with fewer features .	In this paper , we propose a generative topic embedding model	In two document classification tasks , our method performs better than eight existing methods , with fewer features .	59-77	141-159	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	In two document classification tasks , our method performs better than eight existing methods , with fewer features .	1<2	none	evaluation	evaluation
P16-1063_anno1	160-167	168-174	In addition , we illustrate with an example	that our method can generate coherent topics	In addition , we illustrate with an example	that our method can generate coherent topics	160-181	160-181	In addition , we illustrate with an example that our method can generate coherent topics even based on only one document .	In addition , we illustrate with an example that our method can generate coherent topics even based on only one document .	1>2	none	attribution	attribution
P16-1063_anno1	59-69	168-174	In this paper , we propose a generative topic embedding model	that our method can generate coherent topics	In this paper , we propose a generative topic embedding model	that our method can generate coherent topics	59-77	160-181	In this paper , we propose a generative topic embedding model to combine the two types of patterns .	In addition , we illustrate with an example that our method can generate coherent topics even based on only one document .	1<2	none	evaluation	evaluation
P16-1063_anno1	168-174	175-181	that our method can generate coherent topics	even based on only one document .	that our method can generate coherent topics	even based on only one document .	160-181	160-181	In addition , we illustrate with an example that our method can generate coherent topics even based on only one document .	In addition , we illustrate with an example that our method can generate coherent topics even based on only one document .	1<2	none	bg-general	bg-general
P16-1064_anno1	1-3,10-15	16-27	News reader comments <*> are typically massive in amount .	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	News reader comments <*> are typically massive in amount .	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	1-15	16-44	News reader comments found in many on-line news websites are typically massive in amount .	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	1>2	none	bg-goal	bg-goal
P16-1064_anno1	1-3,10-15	4-9	News reader comments <*> are typically massive in amount .	found in many on-line news websites	News reader comments <*> are typically massive in amount .	found in many on-line news websites	1-15	1-15	News reader comments found in many on-line news websites are typically massive in amount .	News reader comments found in many on-line news websites are typically massive in amount .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	16-27	28-39	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	which is aimed at discovering common discussion topics from news reader comments	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	which is aimed at discovering common discussion topics from news reader comments	16-44	16-44	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	28-39	40-44	which is aimed at discovering common discussion topics from news reader comments	written in different languages .	which is aimed at discovering common discussion topics from news reader comments	written in different languages .	16-44	16-44	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	16-27	45-51	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	We propose a new probabilistic graphical model	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	We propose a new probabilistic graphical model	16-44	45-69	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	45-51	52-53	We propose a new probabilistic graphical model	called MCTA	We propose a new probabilistic graphical model	called MCTA	45-69	45-69	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	45-51	54-60	We propose a new probabilistic graphical model	which can cope with the language gap	We propose a new probabilistic graphical model	which can cope with the language gap	45-69	45-69	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	54-60	61-69	which can cope with the language gap	and capture the common semantics in different languages .	which can cope with the language gap	and capture the common semantics in different languages .	45-69	45-69	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	We propose a new probabilistic graphical model called MCTA which can cope with the language gap and capture the common semantics in different languages .	1<2	none	joint	joint
P16-1064_anno1	16-27	70-77	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	We also develop a partially collapsed Gibbs sampler	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	We also develop a partially collapsed Gibbs sampler	16-44	70-95	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	We also develop a partially collapsed Gibbs sampler which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	70-77	78-95	We also develop a partially collapsed Gibbs sampler	which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning .	We also develop a partially collapsed Gibbs sampler	which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning .	70-95	70-95	We also develop a partially collapsed Gibbs sampler which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning .	We also develop a partially collapsed Gibbs sampler which effectively incorporates the term translation relationship into the detection of cultural-common topics for model parameter learning .	1<2	none	elab-addition	elab-addition
P16-1064_anno1	16-27	96-104	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	Experimental results show improvements over the state-of-the-art model .	We investigate the task of Cultural-common Topic Detection ( CTD ) ,	Experimental results show improvements over the state-of-the-art model .	16-44	96-104	We investigate the task of Cultural-common Topic Detection ( CTD ) , which is aimed at discovering common discussion topics from news reader comments written in different languages .	Experimental results show improvements over the state-of-the-art model .	1<2	none	evaluation	evaluation
P16-1065_anno1	1-15	33-40	Document collections often have links between documents - citations , hyperlinks , or revisions -	we introduce a new topic model for documents	Document collections often have links between documents - citations , hyperlinks , or revisions -	we introduce a new topic model for documents	1-27	28-64	Document collections often have links between documents - citations , hyperlinks , or revisions - and which links are added is often based on topical similarity .	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	1>2	none	bg-goal	bg-goal
P16-1065_anno1	1-15	16-27	Document collections often have links between documents - citations , hyperlinks , or revisions -	and which links are added is often based on topical similarity .	Document collections often have links between documents - citations , hyperlinks , or revisions -	and which links are added is often based on topical similarity .	1-27	1-27	Document collections often have links between documents - citations , hyperlinks , or revisions - and which links are added is often based on topical similarity .	Document collections often have links between documents - citations , hyperlinks , or revisions - and which links are added is often based on topical similarity .	1<2	none	joint	joint
P16-1065_anno1	28-32	33-40	To model these intuitions ,	we introduce a new topic model for documents	To model these intuitions ,	we introduce a new topic model for documents	28-64	28-64	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	1>2	none	enablement	enablement
P16-1065_anno1	33-40	41-46	we introduce a new topic model for documents	situated within a network structure ,	we introduce a new topic model for documents	situated within a network structure ,	28-64	28-64	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	1<2	none	elab-addition	elab-addition
P16-1065_anno1	33-40	47-59	we introduce a new topic model for documents	integrating latent blocks of documents with a max-margin learning criterion for link prediction	we introduce a new topic model for documents	integrating latent blocks of documents with a max-margin learning criterion for link prediction	28-64	28-64	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	1<2	none	result	result
P16-1065_anno1	47-59	60-64	integrating latent blocks of documents with a max-margin learning criterion for link prediction	using topicand word-level features .	integrating latent blocks of documents with a max-margin learning criterion for link prediction	using topicand word-level features .	28-64	28-64	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	1<2	none	manner-means	manner-means
P16-1065_anno1	33-40	65-75	we introduce a new topic model for documents	Experiments on a scientific paper dataset and collection of webpages show	we introduce a new topic model for documents	Experiments on a scientific paper dataset and collection of webpages show	28-64	65-103	To model these intuitions , we introduce a new topic model for documents situated within a network structure , integrating latent blocks of documents with a max-margin learning criterion for link prediction using topicand word-level features .	Experiments on a scientific paper dataset and collection of webpages show that , by more robustly exploiting the rich link structure within a document network , our model improves link prediction , topic quality , and block distributions .	1<2	none	evaluation	evaluation
P16-1065_anno1	76-90	91-103	that , by more robustly exploiting the rich link structure within a document network ,	our model improves link prediction , topic quality , and block distributions .	that , by more robustly exploiting the rich link structure within a document network ,	our model improves link prediction , topic quality , and block distributions .	65-103	65-103	Experiments on a scientific paper dataset and collection of webpages show that , by more robustly exploiting the rich link structure within a document network , our model improves link prediction , topic quality , and block distributions .	Experiments on a scientific paper dataset and collection of webpages show that , by more robustly exploiting the rich link structure within a document network , our model improves link prediction , topic quality , and block distributions .	1>2	none	manner-means	manner-means
P16-1065_anno1	65-75	91-103	Experiments on a scientific paper dataset and collection of webpages show	our model improves link prediction , topic quality , and block distributions .	Experiments on a scientific paper dataset and collection of webpages show	our model improves link prediction , topic quality , and block distributions .	65-103	65-103	Experiments on a scientific paper dataset and collection of webpages show that , by more robustly exploiting the rich link structure within a document network , our model improves link prediction , topic quality , and block distributions .	Experiments on a scientific paper dataset and collection of webpages show that , by more robustly exploiting the rich link structure within a document network , our model improves link prediction , topic quality , and block distributions .	1<2	none	attribution	attribution
P16-1066_anno1	1-11	38-47	Sentiment Analysis ( SA ) is an active research area nowadays	however research on SA of Arabic has just flourished .	Sentiment Analysis ( SA ) is an active research area nowadays	however research on SA of Arabic has just flourished .	1-29	30-47	Sentiment Analysis ( SA ) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web .	SA of English has been thoroughly researched ; however research on SA of Arabic has just flourished .	1>2	none	elab-addition	elab-addition
P16-1066_anno1	1-11	12-21	Sentiment Analysis ( SA ) is an active research area nowadays	due to the tremendous interest in aggregating and evaluating opinions	Sentiment Analysis ( SA ) is an active research area nowadays	due to the tremendous interest in aggregating and evaluating opinions	1-29	1-29	Sentiment Analysis ( SA ) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web .	Sentiment Analysis ( SA ) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web .	1<2	none	cause	cause
P16-1066_anno1	1-11	22-29	Sentiment Analysis ( SA ) is an active research area nowadays	being disseminated by users on the Web .	Sentiment Analysis ( SA ) is an active research area nowadays	being disseminated by users on the Web .	1-29	1-29	Sentiment Analysis ( SA ) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web .	Sentiment Analysis ( SA ) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	30-37	38-47	SA of English has been thoroughly researched ;	however research on SA of Arabic has just flourished .	SA of English has been thoroughly researched ;	however research on SA of Arabic has just flourished .	30-47	30-47	SA of English has been thoroughly researched ; however research on SA of Arabic has just flourished .	SA of English has been thoroughly researched ; however research on SA of Arabic has just flourished .	1>2	none	contrast	contrast
P16-1066_anno1	38-47	71-84	however research on SA of Arabic has just flourished .	In this paper we attempt to bridge a gap in Arabic SA of Twitter	however research on SA of Arabic has just flourished .	In this paper we attempt to bridge a gap in Arabic SA of Twitter	30-47	71-101	SA of English has been thoroughly researched ; however research on SA of Arabic has just flourished .	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	1>2	none	bg-goal	bg-goal
P16-1066_anno1	38-47	48-53	however research on SA of Arabic has just flourished .	Twitter is considered a powerful tool	however research on SA of Arabic has just flourished .	Twitter is considered a powerful tool	30-47	48-70	SA of English has been thoroughly researched ; however research on SA of Arabic has just flourished .	Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	48-53	54-63	Twitter is considered a powerful tool	for disseminating information and a rich resource for opinionated text	Twitter is considered a powerful tool	for disseminating information and a rich resource for opinionated text	48-70	48-70	Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics .	Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	48-53	64-70	Twitter is considered a powerful tool	containing views on many different topics .	Twitter is considered a powerful tool	containing views on many different topics .	48-70	48-70	Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics .	Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	71-84	85-91	In this paper we attempt to bridge a gap in Arabic SA of Twitter	which is the lack of sentiment lexicons	In this paper we attempt to bridge a gap in Arabic SA of Twitter	which is the lack of sentiment lexicons	71-101	71-101	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	85-91	92-101	which is the lack of sentiment lexicons	that are tailored for the informal language of Twitter .	which is the lack of sentiment lexicons	that are tailored for the informal language of Twitter .	71-101	71-101	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	71-84	102-105	In this paper we attempt to bridge a gap in Arabic SA of Twitter	We generate two lexicons	In this paper we attempt to bridge a gap in Arabic SA of Twitter	We generate two lexicons	71-101	102-126	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	102-105	106-112	We generate two lexicons	extracted from a large dataset of tweets	We generate two lexicons	extracted from a large dataset of tweets	102-126	102-126	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	102-105	113-115	We generate two lexicons	using two approaches	We generate two lexicons	using two approaches	102-126	102-126	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	1<2	none	manner-means	manner-means
P16-1066_anno1	102-105	116-126	We generate two lexicons	and evaluate their use in a simple lexicon based method .	We generate two lexicons	and evaluate their use in a simple lexicon based method .	102-126	102-126	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method .	1<2	none	joint	joint
P16-1066_anno1	127-136	137-147	The evaluation is performed on internal and external datasets .	The performance of these automatically generated lexicons was very promising ,	The evaluation is performed on internal and external datasets .	The performance of these automatically generated lexicons was very promising ,	127-136	137-155	The evaluation is performed on internal and external datasets .	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	1>2	none	elab-addition	elab-addition
P16-1066_anno1	71-84	137-147	In this paper we attempt to bridge a gap in Arabic SA of Twitter	The performance of these automatically generated lexicons was very promising ,	In this paper we attempt to bridge a gap in Arabic SA of Twitter	The performance of these automatically generated lexicons was very promising ,	71-101	137-155	In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter .	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	1<2	none	evaluation	evaluation
P16-1066_anno1	137-147	148-151	The performance of these automatically generated lexicons was very promising ,	albeit the simple method	The performance of these automatically generated lexicons was very promising ,	albeit the simple method	137-155	137-155	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	1<2	none	contrast	contrast
P16-1066_anno1	148-151	152-155	albeit the simple method	used for classification .	albeit the simple method	used for classification .	137-155	137-155	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	1<2	none	elab-addition	elab-addition
P16-1066_anno1	137-147	156-174	The performance of these automatically generated lexicons was very promising ,	The best F-score obtained was 89.58 % on the internal dataset and 63.1-64.7 % on the external datasets .	The performance of these automatically generated lexicons was very promising ,	The best F-score obtained was 89.58 % on the internal dataset and 63.1-64.7 % on the external datasets .	137-155	156-174	The performance of these automatically generated lexicons was very promising , albeit the simple method used for classification .	The best F-score obtained was 89.58 % on the internal dataset and 63.1-64.7 % on the external datasets .	1<2	none	exp-evidence	exp-evidence
P16-1067_anno1	1-6	7-15	This paper proposes an unsupervised approach	for segmenting a multiauthor document into authorial components .	This paper proposes an unsupervised approach	for segmenting a multiauthor document into authorial components .	1-15	1-15	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	1-6	16-25	This paper proposes an unsupervised approach	The key novelty is that we utilize the sequential patterns	This paper proposes an unsupervised approach	The key novelty is that we utilize the sequential patterns	1-15	16-34	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	16-25	26-29	The key novelty is that we utilize the sequential patterns	hidden among document elements	The key novelty is that we utilize the sequential patterns	hidden among document elements	16-34	16-34	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	16-25	30-34	The key novelty is that we utilize the sequential patterns	when determining their authorships .	The key novelty is that we utilize the sequential patterns	when determining their authorships .	16-34	16-34	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	1<2	none	temporal	temporal
P16-1067_anno1	16-25	35-46	The key novelty is that we utilize the sequential patterns	For this purpose , we adopt Hidden Markov Model ( HMM )	The key novelty is that we utilize the sequential patterns	For this purpose , we adopt Hidden Markov Model ( HMM )	16-34	35-63	The key novelty is that we utilize the sequential patterns hidden among document elements when determining their authorships .	For this purpose , we adopt Hidden Markov Model ( HMM ) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	35-46	47-52	For this purpose , we adopt Hidden Markov Model ( HMM )	and construct a sequential probabilistic model	For this purpose , we adopt Hidden Markov Model ( HMM )	and construct a sequential probabilistic model	35-63	35-63	For this purpose , we adopt Hidden Markov Model ( HMM ) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships .	For this purpose , we adopt Hidden Markov Model ( HMM ) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships .	1<2	none	joint	joint
P16-1067_anno1	47-52	53-63	and construct a sequential probabilistic model	to capture the dependencies of sequential sentences and their authorships .	and construct a sequential probabilistic model	to capture the dependencies of sequential sentences and their authorships .	35-63	35-63	For this purpose , we adopt Hidden Markov Model ( HMM ) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships .	For this purpose , we adopt Hidden Markov Model ( HMM ) and construct a sequential probabilistic model to capture the dependencies of sequential sentences and their authorships .	1<2	none	enablement	enablement
P16-1067_anno1	1-6	64-69	This paper proposes an unsupervised approach	An unsupervised learning method is developed	This paper proposes an unsupervised approach	An unsupervised learning method is developed	1-15	64-75	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	An unsupervised learning method is developed to initialize the HMM parameters .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	64-69	70-75	An unsupervised learning method is developed	to initialize the HMM parameters .	An unsupervised learning method is developed	to initialize the HMM parameters .	64-75	64-75	An unsupervised learning method is developed to initialize the HMM parameters .	An unsupervised learning method is developed to initialize the HMM parameters .	1<2	none	enablement	enablement
P16-1067_anno1	1-6	76-88	This paper proposes an unsupervised approach	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea	This paper proposes an unsupervised approach	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea	1-15	76-99	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea and our approach has outperformed the state-of-the-arts on all tests .	1<2	none	evaluation	evaluation
P16-1067_anno1	76-88	89-99	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea	and our approach has outperformed the state-of-the-arts on all tests .	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea	and our approach has outperformed the state-of-the-arts on all tests .	76-99	76-99	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea and our approach has outperformed the state-of-the-arts on all tests .	Experimental results on benchmark datasets have demonstrated the significant benefit of our idea and our approach has outperformed the state-of-the-arts on all tests .	1<2	none	joint	joint
P16-1067_anno1	100-111	118-124	As an example of its applications , the proposed approach is applied	and has also shown promising results .	As an example of its applications , the proposed approach is applied	and has also shown promising results .	100-124	100-124	As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .	As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .	1>2	none	progression	progression
P16-1067_anno1	100-111	112-117	As an example of its applications , the proposed approach is applied	for attributing authorship of a document	As an example of its applications , the proposed approach is applied	for attributing authorship of a document	100-124	100-124	As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .	As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .	1<2	none	elab-addition	elab-addition
P16-1067_anno1	1-6	118-124	This paper proposes an unsupervised approach	and has also shown promising results .	This paper proposes an unsupervised approach	and has also shown promising results .	1-15	100-124	This paper proposes an unsupervised approach for segmenting a multiauthor document into authorial components .	As an example of its applications , the proposed approach is applied for attributing authorship of a document and has also shown promising results .	1<2	none	evaluation	evaluation
P16-1068_anno1	1-16	26-40	Automated Text Scoring ( ATS ) provides a cost-effective and consistent alternative to human marking .	the predictive features of the system need to be manually engineered by human experts .	Automated Text Scoring ( ATS ) provides a cost-effective and consistent alternative to human marking .	the predictive features of the system need to be manually engineered by human experts .	1-16	17-40	Automated Text Scoring ( ATS ) provides a cost-effective and consistent alternative to human marking .	However , in order to achieve good performance , the predictive features of the system need to be manually engineered by human experts .	1>2	none	contrast	contrast
P16-1068_anno1	17-25	26-40	However , in order to achieve good performance ,	the predictive features of the system need to be manually engineered by human experts .	However , in order to achieve good performance ,	the predictive features of the system need to be manually engineered by human experts .	17-40	17-40	However , in order to achieve good performance , the predictive features of the system need to be manually engineered by human experts .	However , in order to achieve good performance , the predictive features of the system need to be manually engineered by human experts .	1>2	none	enablement	enablement
P16-1068_anno1	26-40	41-44	the predictive features of the system need to be manually engineered by human experts .	We introduce a model	the predictive features of the system need to be manually engineered by human experts .	We introduce a model	17-40	41-63	However , in order to achieve good performance , the predictive features of the system need to be manually engineered by human experts .	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	1>2	none	bg-goal	bg-goal
P16-1068_anno1	41-44	45-48	We introduce a model	that forms word representations	We introduce a model	that forms word representations	41-63	41-63	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	1<2	none	elab-addition	elab-addition
P16-1068_anno1	41-44	49-52	We introduce a model	by learning the extent	We introduce a model	by learning the extent	41-63	41-63	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	1<2	none	manner-means	manner-means
P16-1068_anno1	49-52	53-63	by learning the extent	to which specific words contribute to the text 's score .	by learning the extent	to which specific words contribute to the text's score .	41-63	41-63	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	1<2	none	elab-addition	elab-addition
P16-1068_anno1	64-68	78-92	Using Long-Short Term Memory networks	that a fully automated framework is able to achieve excellent results over similar approaches .	Using Long-Short Term Memory networks	that a fully automated framework is able to achieve excellent results over similar approaches .	64-92	64-92	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	1>2	none	manner-means	manner-means
P16-1068_anno1	64-68	69-75	Using Long-Short Term Memory networks	to represent the meaning of texts ,	Using Long-Short Term Memory networks	to represent the meaning of texts ,	64-92	64-92	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	1<2	none	enablement	enablement
P16-1068_anno1	76-77	78-92	we demonstrate	that a fully automated framework is able to achieve excellent results over similar approaches .	we demonstrate	that a fully automated framework is able to achieve excellent results over similar approaches .	64-92	64-92	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	1>2	none	attribution	attribution
P16-1068_anno1	41-44	78-92	We introduce a model	that a fully automated framework is able to achieve excellent results over similar approaches .	We introduce a model	that a fully automated framework is able to achieve excellent results over similar approaches .	41-63	64-92	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	Using Long-Short Term Memory networks to represent the meaning of texts , we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches .	1<2	none	elab-addition	elab-addition
P16-1068_anno1	93-102	113-117	In an attempt to make our results more interpretable ,	we introduce a novel method	In an attempt to make our results more interpretable ,	we introduce a novel method	93-132	93-132	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	1>2	none	elab-addition	elab-addition
P16-1068_anno1	93-102	103-112	In an attempt to make our results more interpretable ,	and inspired by recent advances in visualizing neural networks ,	In an attempt to make our results more interpretable ,	and inspired by recent advances in visualizing neural networks ,	93-132	93-132	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	1<2	none	joint	joint
P16-1068_anno1	41-44	113-117	We introduce a model	we introduce a novel method	We introduce a model	we introduce a novel method	41-63	93-132	We introduce a model that forms word representations by learning the extent to which specific words contribute to the text 's score .	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	1<2	none	elab-addition	elab-addition
P16-1068_anno1	113-117	118-124	we introduce a novel method	for identifying the regions of the text	we introduce a novel method	for identifying the regions of the text	93-132	93-132	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	1<2	none	elab-addition	elab-addition
P16-1068_anno1	118-124	125-132	for identifying the regions of the text	that the model has found more discriminative .	for identifying the regions of the text	that the model has found more discriminative .	93-132	93-132	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	In an attempt to make our results more interpretable , and inspired by recent advances in visualizing neural networks , we introduce a novel method for identifying the regions of the text that the model has found more discriminative .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	1-12	34-39	Digital personal assistants are becoming both more common and more useful .	This paper focuses on understanding rules	Digital personal assistants are becoming both more common and more useful .	This paper focuses on understanding rules	1-12	34-56	Digital personal assistants are becoming both more common and more useful .	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	1>2	none	bg-goal	bg-goal
P16-1069_anno1	1-12	13-23	Digital personal assistants are becoming both more common and more useful .	The major NLP challenge for personal assistants is machine understanding :	Digital personal assistants are becoming both more common and more useful .	The major NLP challenge for personal assistants is machine understanding :	1-12	13-33	Digital personal assistants are becoming both more common and more useful .	The major NLP challenge for personal assistants is machine understanding : translating natural language user commands into an executable representation .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	13-23	24-33	The major NLP challenge for personal assistants is machine understanding :	translating natural language user commands into an executable representation .	The major NLP challenge for personal assistants is machine understanding :	translating natural language user commands into an executable representation .	13-33	13-33	The major NLP challenge for personal assistants is machine understanding : translating natural language user commands into an executable representation .	The major NLP challenge for personal assistants is machine understanding : translating natural language user commands into an executable representation .	1<2	none	elab-definition	elab-definition
P16-1069_anno1	34-39	40-44	This paper focuses on understanding rules	written as If-Then statements ,	This paper focuses on understanding rules	written as If-Then statements ,	34-56	34-56	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	34-39	45-56	This paper focuses on understanding rules	though the techniques should be portable to other semantic parsing tasks .	This paper focuses on understanding rules	though the techniques should be portable to other semantic parsing tasks .	34-56	34-56	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	1<2	none	contrast	contrast
P16-1069_anno1	34-39	57-62	This paper focuses on understanding rules	We view understanding as structure prediction	This paper focuses on understanding rules	We view understanding as structure prediction	34-56	57-75	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	We view understanding as structure prediction and show improved models using both conventional techniques and neural network models .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	57-62	63-66	We view understanding as structure prediction	and show improved models	We view understanding as structure prediction	and show improved models	57-75	57-75	We view understanding as structure prediction and show improved models using both conventional techniques and neural network models .	We view understanding as structure prediction and show improved models using both conventional techniques and neural network models .	1<2	none	joint	joint
P16-1069_anno1	63-66	67-75	and show improved models	using both conventional techniques and neural network models .	and show improved models	using both conventional techniques and neural network models .	57-75	57-75	We view understanding as structure prediction and show improved models using both conventional techniques and neural network models .	We view understanding as structure prediction and show improved models using both conventional techniques and neural network models .	1<2	none	manner-means	manner-means
P16-1069_anno1	34-39	76-80	This paper focuses on understanding rules	We also discuss various ways	This paper focuses on understanding rules	We also discuss various ways	34-56	76-104	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	76-80	81-83	We also discuss various ways	to improve generalization	We also discuss various ways	to improve generalization	76-104	76-104	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	1<2	none	elab-addition	elab-addition
P16-1069_anno1	81-83	84-87	to improve generalization	and reduce overfitting :	to improve generalization	and reduce overfitting :	76-104	76-104	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	1<2	none	joint	joint
P16-1069_anno1	84-87	88-104	and reduce overfitting :	synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	and reduce overfitting :	synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	76-104	76-104	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	We also discuss various ways to improve generalization and reduce overfitting : synthetic training data from paraphrase , grammar combinations , feature selection and ensembles of multiple systems .	1<2	none	elab-example	elab-example
P16-1069_anno1	34-39	105-123	This paper focuses on understanding rules	An ensemble of these techniques achieves a new state of the art result with 8 % accuracy improvement .	This paper focuses on understanding rules	An ensemble of these techniques achieves a new state of the art result with 8 % accuracy improvement .	34-56	105-123	This paper focuses on understanding rules written as If-Then statements , though the techniques should be portable to other semantic parsing tasks .	An ensemble of these techniques achieves a new state of the art result with 8 % accuracy improvement .	1<2	none	evaluation	evaluation
P16-1070_anno1	1-11	12-27	We introduce the Treebank of Learner English ( TLE ) ,	the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	We introduce the Treebank of Learner English ( TLE ) ,	the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	1-27	1-27	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	1<2	none	elab-addition	elab-addition
P16-1070_anno1	1-11	28-56	We introduce the Treebank of Learner English ( TLE ) ,	The TLE provides manually annotated POS tags and Universal Dependency ( UD ) trees for 5,124 sentences from the Cambridge First Certificate in English ( FCE ) corpus .	We introduce the Treebank of Learner English ( TLE ) ,	The TLE provides manually annotated POS tags and Universal Dependency ( UD ) trees for 5,124 sentences from the Cambridge First Certificate in English ( FCE ) corpus .	1-27	28-56	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	The TLE provides manually annotated POS tags and Universal Dependency ( UD ) trees for 5,124 sentences from the Cambridge First Certificate in English ( FCE ) corpus .	1<2	none	elab-addition	elab-addition
P16-1070_anno1	1-11	57-70	We introduce the Treebank of Learner English ( TLE ) ,	The UD annotations are tied to a pre-existing error annotation of the FCE ,	We introduce the Treebank of Learner English ( TLE ) ,	The UD annotations are tied to a pre-existing error annotation of the FCE ,	1-27	57-88	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	The UD annotations are tied to a pre-existing error annotation of the FCE , whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence .	1<2	none	elab-aspect	elab-aspect
P16-1070_anno1	57-70	71-88	The UD annotations are tied to a pre-existing error annotation of the FCE ,	whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence .	The UD annotations are tied to a pre-existing error annotation of the FCE ,	whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence .	57-88	57-88	The UD annotations are tied to a pre-existing error annotation of the FCE , whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence .	The UD annotations are tied to a pre-existing error annotation of the FCE , whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence .	1<2	none	elab-addition	elab-addition
P16-1070_anno1	1-11	89-96	We introduce the Treebank of Learner English ( TLE ) ,	Further on , we delineate ESL annotation guidelines	We introduce the Treebank of Learner English ( TLE ) ,	Further on , we delineate ESL annotation guidelines	1-27	89-106	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	Further on , we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English .	1<2	none	elab-aspect	elab-aspect
P16-1070_anno1	89-96	97-106	Further on , we delineate ESL annotation guidelines	that allow for consistent syntactic treatment of ungrammatical English .	Further on , we delineate ESL annotation guidelines	that allow for consistent syntactic treatment of ungrammatical English .	89-106	89-106	Further on , we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English .	Further on , we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English .	1<2	none	elab-addition	elab-addition
P16-1070_anno1	1-11	107-120	We introduce the Treebank of Learner English ( TLE ) ,	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset	We introduce the Treebank of Learner English ( TLE ) ,	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset	1-27	107-131	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy .	1<2	none	elab-aspect	elab-aspect
P16-1070_anno1	107-120	121-131	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset	and measure the effect of grammatical errors on parsing accuracy .	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset	and measure the effect of grammatical errors on parsing accuracy .	107-131	107-131	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy .	Finally , we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy .	1<2	none	joint	joint
P16-1070_anno1	1-11	132-158	We introduce the Treebank of Learner English ( TLE ) ,	We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language .	We introduce the Treebank of Learner English ( TLE ) ,	We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language .	1-27	132-158	We introduce the Treebank of Learner English ( TLE ) , the first publicly available syntactic treebank for English as a Second Language ( ESL ) .	We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language .	1<2	none	evaluation	evaluation
P16-1071_anno1	1-16	28-32	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results ,	This paper addresses the difficulty	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results ,	This paper addresses the difficulty	1-27	28-51	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results , yet some of them indicate different activations with different PoS .	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	1>2	none	bg-goal	bg-goal
P16-1071_anno1	1-16	17-27	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results ,	yet some of them indicate different activations with different PoS .	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results ,	yet some of them indicate different activations with different PoS .	1-27	1-27	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results , yet some of them indicate different activations with different PoS .	Neuro-imaging studies on reading different parts of speech ( PoS ) report somewhat mixed results , yet some of them indicate different activations with different PoS .	1<2	none	contrast	contrast
P16-1071_anno1	28-32	33-45	This paper addresses the difficulty	of using fMRI to discriminate between linguistic tokens in reading of running text	This paper addresses the difficulty	of using fMRI to discriminate between linguistic tokens in reading of running text	28-51	28-51	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	1<2	none	elab-addition	elab-addition
P16-1071_anno1	28-32	46-51	This paper addresses the difficulty	because of low temporal resolution .	This paper addresses the difficulty	because of low temporal resolution .	28-51	28-51	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	1<2	none	exp-reason	exp-reason
P16-1071_anno1	52-53	61-71	We show	fMRI data contains a signal of PoS distinctions to the extent	We show	fMRI data contains a signal of PoS distinctions to the extent	52-85	52-85	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	1>2	none	attribution	attribution
P16-1071_anno1	54-60	61-71	that once we solve this problem ,	fMRI data contains a signal of PoS distinctions to the extent	that once we solve this problem ,	fMRI data contains a signal of PoS distinctions to the extent	52-85	52-85	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	1>2	none	condition	condition
P16-1071_anno1	28-32	61-71	This paper addresses the difficulty	fMRI data contains a signal of PoS distinctions to the extent	This paper addresses the difficulty	fMRI data contains a signal of PoS distinctions to the extent	28-51	52-85	This paper addresses the difficulty of using fMRI to discriminate between linguistic tokens in reading of running text because of low temporal resolution .	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	1<2	none	evaluation	evaluation
P16-1071_anno1	61-71	72-85	fMRI data contains a signal of PoS distinctions to the extent	that it improves PoS induction with error reductions of more than 4 % .	fMRI data contains a signal of PoS distinctions to the extent	that it improves PoS induction with error reductions of more than 4 % .	52-85	52-85	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	We show that once we solve this problem , fMRI data contains a signal of PoS distinctions to the extent that it improves PoS induction with error reductions of more than 4 % .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	1-19	20-29	Relation classification is an important semantic processing task in the field of natural language processing ( NLP ) .	In this paper , we present a novel model BRCNN	Relation classification is an important semantic processing task in the field of natural language processing ( NLP ) .	In this paper , we present a novel model BRCNN	1-19	20-40	Relation classification is an important semantic processing task in the field of natural language processing ( NLP ) .	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	1>2	none	bg-goal	bg-goal
P16-1072_anno1	20-29	30-40	In this paper , we present a novel model BRCNN	to classify the relation of two entities in a sentence .	In this paper , we present a novel model BRCNN	to classify the relation of two entities in a sentence .	20-40	20-40	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	1<2	none	enablement	enablement
P16-1072_anno1	20-29	41-56	In this paper , we present a novel model BRCNN	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities	In this paper , we present a novel model BRCNN	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities	20-40	41-63	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities leveraging convolutional or recurrent neural networks .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	41-56	57-63	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities	leveraging convolutional or recurrent neural networks .	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities	leveraging convolutional or recurrent neural networks .	41-63	41-63	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities leveraging convolutional or recurrent neural networks .	Some state-of-the-art systems concentrate on modeling the shortest dependency path ( SDP ) between two entities leveraging convolutional or recurrent neural networks .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	20-29	64-80	In this paper , we present a novel model BRCNN	We further explore how to make full use of the dependency relations information in the SDP ,	In this paper , we present a novel model BRCNN	We further explore how to make full use of the dependency relations information in the SDP ,	20-40	64-100	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	We further explore how to make full use of the dependency relations information in the SDP , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( LSTM ) units .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	64-80	81-100	We further explore how to make full use of the dependency relations information in the SDP ,	by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( LSTM ) units .	We further explore how to make full use of the dependency relations information in the SDP ,	by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( LSTM ) units .	64-100	64-100	We further explore how to make full use of the dependency relations information in the SDP , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( LSTM ) units .	We further explore how to make full use of the dependency relations information in the SDP , by combining convolutional neural networks and twochannel recurrent neural networks with long short term memory ( LSTM ) units .	1<2	none	manner-means	manner-means
P16-1072_anno1	20-29	101-105	In this paper , we present a novel model BRCNN	We propose a bidirectional architecture	In this paper , we present a novel model BRCNN	We propose a bidirectional architecture	20-40	101-131	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time , which benefits classifying the direction of relations .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	101-105	106-123	We propose a bidirectional architecture	to learn relation representations with directional information along the SDP forwards and backwards at the same time ,	We propose a bidirectional architecture	to learn relation representations with directional information along the SDP forwards and backwards at the same time ,	101-131	101-131	We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time , which benefits classifying the direction of relations .	We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time , which benefits classifying the direction of relations .	1<2	none	enablement	enablement
P16-1072_anno1	106-123	124-131	to learn relation representations with directional information along the SDP forwards and backwards at the same time ,	which benefits classifying the direction of relations .	to learn relation representations with directional information along the SDP forwards and backwards at the same time ,	which benefits classifying the direction of relations .	101-131	101-131	We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time , which benefits classifying the direction of relations .	We propose a bidirectional architecture to learn relation representations with directional information along the SDP forwards and backwards at the same time , which benefits classifying the direction of relations .	1<2	none	elab-addition	elab-addition
P16-1072_anno1	132-134	135-148	Experimental results show	that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	Experimental results show	that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	132-148	132-148	Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	1>2	none	attribution	attribution
P16-1072_anno1	20-29	135-148	In this paper , we present a novel model BRCNN	that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	In this paper , we present a novel model BRCNN	that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	20-40	132-148	In this paper , we present a novel model BRCNN to classify the relation of two entities in a sentence .	Experimental results show that our method outperforms the state-of-theart approaches on the SemEval-2010 Task 8 dataset .	1<2	none	evaluation	evaluation
P16-1073_anno1	1-18	19-32	A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology .	In this paper , we propose a sentence rewriting based semantic parsing method ,	A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology .	In this paper , we propose a sentence rewriting based semantic parsing method ,	1-18	19-58	A major challenge of semantic parsing is the vocabulary mismatch problem between natural language and target ontology .	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	1>2	none	bg-goal	bg-goal
P16-1073_anno1	19-32	33-39	In this paper , we propose a sentence rewriting based semantic parsing method ,	which can effectively resolve the mismatch problem	In this paper , we propose a sentence rewriting based semantic parsing method ,	which can effectively resolve the mismatch problem	19-58	19-58	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	1<2	none	elab-addition	elab-addition
P16-1073_anno1	33-39	40-47	which can effectively resolve the mismatch problem	by rewriting a sentence into a new form	which can effectively resolve the mismatch problem	by rewriting a sentence into a new form	19-58	19-58	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	1<2	none	manner-means	manner-means
P16-1073_anno1	40-47	48-58	by rewriting a sentence into a new form	which has the same structure with its target logical form .	by rewriting a sentence into a new form	which has the same structure with its target logical form .	19-58	19-58	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	1<2	none	elab-addition	elab-addition
P16-1073_anno1	19-32	59-72	In this paper , we propose a sentence rewriting based semantic parsing method ,	Specifically , we propose two sentence-rewriting methods for two common types of mismatch :	In this paper , we propose a sentence rewriting based semantic parsing method ,	Specifically , we propose two sentence-rewriting methods for two common types of mismatch :	19-58	59-86	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	Specifically , we propose two sentence-rewriting methods for two common types of mismatch : a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch .	1<2	none	elab-addition	elab-addition
P16-1073_anno1	59-72	73-86	Specifically , we propose two sentence-rewriting methods for two common types of mismatch :	a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch .	Specifically , we propose two sentence-rewriting methods for two common types of mismatch :	a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch .	59-86	59-86	Specifically , we propose two sentence-rewriting methods for two common types of mismatch : a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch .	Specifically , we propose two sentence-rewriting methods for two common types of mismatch : a dictionary-based method for 1-N mismatch and a template-based method for N-1 mismatch .	1<2	none	elab-enumember	elab-enumember
P16-1073_anno1	87-103	107-121	We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset - WEBQUESTIONS .	that our system outperforms the base system with a 3.4 % gain in F1 ,	We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset - WEBQUESTIONS .	that our system outperforms the base system with a 3.4 % gain in F1 ,	87-103	104-133	We evaluate our sentence rewriting based semantic parser on the benchmark semantic parsing dataset - WEBQUESTIONS .	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	1>2	none	elab-addition	elab-addition
P16-1073_anno1	104-106	107-121	Experimental results show	that our system outperforms the base system with a 3.4 % gain in F1 ,	Experimental results show	that our system outperforms the base system with a 3.4 % gain in F1 ,	104-133	104-133	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	1>2	none	attribution	attribution
P16-1073_anno1	19-32	107-121	In this paper , we propose a sentence rewriting based semantic parsing method ,	that our system outperforms the base system with a 3.4 % gain in F1 ,	In this paper , we propose a sentence rewriting based semantic parsing method ,	that our system outperforms the base system with a 3.4 % gain in F1 ,	19-58	104-133	In this paper , we propose a sentence rewriting based semantic parsing method , which can effectively resolve the mismatch problem by rewriting a sentence into a new form which has the same structure with its target logical form .	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	1<2	none	evaluation	evaluation
P16-1073_anno1	107-121	122-127	that our system outperforms the base system with a 3.4 % gain in F1 ,	and generates logical forms more accurately	that our system outperforms the base system with a 3.4 % gain in F1 ,	and generates logical forms more accurately	104-133	104-133	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	1<2	none	joint	joint
P16-1073_anno1	122-127	128-133	and generates logical forms more accurately	and parses sentences more robustly .	and generates logical forms more accurately	and parses sentences more robustly .	104-133	104-133	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	Experimental results show that our system outperforms the base system with a 3.4 % gain in F1 , and generates logical forms more accurately and parses sentences more robustly .	1<2	none	joint	joint
P16-1074_anno1	1-21	22-29	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance ,	it is relatively difficult to scale them up	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance ,	it is relatively difficult to scale them up	1-54	1-54	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	1>2	none	contrast	contrast
P16-1074_anno1	22-29	60-73	it is relatively difficult to scale them up	we propose a supervised approach to AZP resolution based on deep neural networks ,	it is relatively difficult to scale them up	we propose a supervised approach to AZP resolution based on deep neural networks ,	1-54	55-92	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	1>2	none	bg-compare	bg-compare
P16-1074_anno1	22-29	30-36	it is relatively difficult to scale them up	to reach the next level of performance	it is relatively difficult to scale them up	to reach the next level of performance	1-54	1-54	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	1<2	none	enablement	enablement
P16-1074_anno1	22-29	37-54	it is relatively difficult to scale them up	due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	it is relatively difficult to scale them up	due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	1-54	1-54	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	While unsupervised anaphoric zero pronoun ( AZP ) resolvers have recently been shown to rival their supervised counterparts in performance , it is relatively difficult to scale them up to reach the next level of performance due to the large amount of feature engineering efforts involved and their ineffectiveness in exploiting lexical features .	1<2	none	exp-reason	exp-reason
P16-1074_anno1	55-59	60-73	To address these weaknesses ,	we propose a supervised approach to AZP resolution based on deep neural networks ,	To address these weaknesses ,	we propose a supervised approach to AZP resolution based on deep neural networks ,	55-92	55-92	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	1>2	none	enablement	enablement
P16-1074_anno1	60-73	74-78	we propose a supervised approach to AZP resolution based on deep neural networks ,	taking advantage of their ability	we propose a supervised approach to AZP resolution based on deep neural networks ,	taking advantage of their ability	55-92	55-92	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	1<2	none	elab-addition	elab-addition
P16-1074_anno1	74-78	79-83	taking advantage of their ability	to learn useful task-specific representations	taking advantage of their ability	to learn useful task-specific representations	55-92	55-92	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	1<2	none	enablement	enablement
P16-1074_anno1	79-83	84-92	to learn useful task-specific representations	and effectively exploit lexical features via word embeddings .	to learn useful task-specific representations	and effectively exploit lexical features via word embeddings .	55-92	55-92	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	1<2	none	joint	joint
P16-1074_anno1	60-73	93-97	we propose a supervised approach to AZP resolution based on deep neural networks ,	Our approach achieves stateof-the-art performance	we propose a supervised approach to AZP resolution based on deep neural networks ,	Our approach achieves stateof-the-art performance	55-92	93-107	To address these weaknesses , we propose a supervised approach to AZP resolution based on deep neural networks , taking advantage of their ability to learn useful task-specific representations and effectively exploit lexical features via word embeddings .	Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus .	1<2	none	evaluation	evaluation
P16-1074_anno1	93-97	98-107	Our approach achieves stateof-the-art performance	when resolving the Chinese AZPs in the OntoNotes corpus .	Our approach achieves stateof-the-art performance	when resolving the Chinese AZPs in the OntoNotes corpus .	93-107	93-107	Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus .	Our approach achieves stateof-the-art performance when resolving the Chinese AZPs in the OntoNotes corpus .	1<2	none	condition	condition
P16-1075_anno1	1-17	44-51	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	In this paper , we overcome this shortcoming	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	In this paper , we overcome this shortcoming	1-29	44-70	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data in order to make accurate predictions for a particular writing task .	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	1>2	none	bg-compare	bg-compare
P16-1075_anno1	1-17	18-29	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	in order to make accurate predictions for a particular writing task .	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	in order to make accurate predictions for a particular writing task .	1-29	1-29	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data in order to make accurate predictions for a particular writing task .	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data in order to make accurate predictions for a particular writing task .	1<2	none	enablement	enablement
P16-1075_anno1	1-17	30-43	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	This limitation hinders their utility , and consequently their deployment in real-world settings .	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data	This limitation hinders their utility , and consequently their deployment in real-world settings .	1-29	30-43	Supervised machine learning models for automated essay scoring ( AES ) usually require substantial task-specific training data in order to make accurate predictions for a particular writing task .	This limitation hinders their utility , and consequently their deployment in real-world settings .	1<2	none	result	result
P16-1075_anno1	44-51	52-58	In this paper , we overcome this shortcoming	using a constrained multi-task pairwisepreference learning approach	In this paper , we overcome this shortcoming	using a constrained multi-task pairwisepreference learning approach	44-70	44-70	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	1<2	none	manner-means	manner-means
P16-1075_anno1	52-58	59-70	using a constrained multi-task pairwisepreference learning approach	that enables the data from multiple tasks to be combined effectively .	using a constrained multi-task pairwisepreference learning approach	that enables the data from multiple tasks to be combined effectively .	44-70	44-70	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	1<2	none	elab-addition	elab-addition
P16-1075_anno1	71-78	81-96	Furthermore , contrary to some recent research ,	that high performance AES systems can be built with little or no task-specific training data .	Furthermore , contrary to some recent research ,	that high performance AES systems can be built with little or no task-specific training data .	71-96	71-96	Furthermore , contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data .	Furthermore , contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data .	1>2	none	comparison	comparison
P16-1075_anno1	79-80	81-96	we show	that high performance AES systems can be built with little or no task-specific training data .	we show	that high performance AES systems can be built with little or no task-specific training data .	71-96	71-96	Furthermore , contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data .	Furthermore , contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data .	1>2	none	attribution	attribution
P16-1075_anno1	44-51	81-96	In this paper , we overcome this shortcoming	that high performance AES systems can be built with little or no task-specific training data .	In this paper , we overcome this shortcoming	that high performance AES systems can be built with little or no task-specific training data .	44-70	71-96	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	Furthermore , contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data .	1<2	none	elab-addition	elab-addition
P16-1075_anno1	44-51	97-111	In this paper , we overcome this shortcoming	We perform a detailed study of our approach on a publicly available dataset in scenarios	In this paper , we overcome this shortcoming	We perform a detailed study of our approach on a publicly available dataset in scenarios	44-70	97-130	In this paper , we overcome this shortcoming using a constrained multi-task pairwisepreference learning approach that enables the data from multiple tasks to be combined effectively .	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	1<2	none	elab-addition	elab-addition
P16-1075_anno1	97-111	112-120	We perform a detailed study of our approach on a publicly available dataset in scenarios	where we have varying amounts of task-specific training data	We perform a detailed study of our approach on a publicly available dataset in scenarios	where we have varying amounts of task-specific training data	97-130	97-130	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	1<2	none	elab-addition	elab-addition
P16-1075_anno1	97-111	121-123	We perform a detailed study of our approach on a publicly available dataset in scenarios	and in scenarios	We perform a detailed study of our approach on a publicly available dataset in scenarios	and in scenarios	97-130	97-130	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	1<2	none	joint	joint
P16-1075_anno1	121-123	124-130	and in scenarios	where the number of tasks increases .	and in scenarios	where the number of tasks increases .	97-130	97-130	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	1-9	30-39	How can we enable computers to automatically answer questions	However , it remains a challenge to answer factoid questions	How can we enable computers to automatically answer questions	However , it remains a challenge to answer factoid questions	1-19	30-51	How can we enable computers to automatically answer questions like `` Who created the character Harry Potter '' ?	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	1>2	none	contrast	contrast
P16-1076_anno1	1-9	10-19	How can we enable computers to automatically answer questions	like `` Who created the character Harry Potter '' ?	How can we enable computers to automatically answer questions	like `` Who created the character Harry Potter '' ?	1-19	1-19	How can we enable computers to automatically answer questions like `` Who created the character Harry Potter '' ?	How can we enable computers to automatically answer questions like `` Who created the character Harry Potter '' ?	1<2	none	elab-addition	elab-addition
P16-1076_anno1	1-9	20-29	How can we enable computers to automatically answer questions	Carefully built knowledge bases provide rich sources of facts .	How can we enable computers to automatically answer questions	Carefully built knowledge bases provide rich sources of facts .	1-19	20-29	How can we enable computers to automatically answer questions like `` Who created the character Harry Potter '' ?	Carefully built knowledge bases provide rich sources of facts .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	30-39	77-85	However , it remains a challenge to answer factoid questions	We propose CFO , a Conditional Focused neuralnetwork-based approach	However , it remains a challenge to answer factoid questions	We propose CFO , a Conditional Focused neuralnetwork-based approach	30-51	77-93	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	1>2	none	bg-goal	bg-goal
P16-1076_anno1	30-39	40-43	However , it remains a challenge to answer factoid questions	raised in natural language	However , it remains a challenge to answer factoid questions	raised in natural language	30-51	30-51	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	30-39	44-51	However , it remains a challenge to answer factoid questions	due to numerous expressions of one question .	However , it remains a challenge to answer factoid questions	due to numerous expressions of one question .	30-51	30-51	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	1<2	none	exp-reason	exp-reason
P16-1076_anno1	30-39	52-63	However , it remains a challenge to answer factoid questions	In particular , we focus on the most common questions - ones	However , it remains a challenge to answer factoid questions	In particular , we focus on the most common questions - ones	30-51	52-76	However , it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question .	In particular , we focus on the most common questions - ones that can be answered with a single fact in the knowledge base .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	52-63	64-76	In particular , we focus on the most common questions - ones	that can be answered with a single fact in the knowledge base .	In particular , we focus on the most common questions - ones	that can be answered with a single fact in the knowledge base .	52-76	52-76	In particular , we focus on the most common questions - ones that can be answered with a single fact in the knowledge base .	In particular , we focus on the most common questions - ones that can be answered with a single fact in the knowledge base .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	77-85	86-93	We propose CFO , a Conditional Focused neuralnetwork-based approach	to answering factoid questions with knowledge bases .	We propose CFO , a Conditional Focused neuralnetwork-based approach	to answering factoid questions with knowledge bases .	77-93	77-93	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	1<2	none	enablement	enablement
P16-1076_anno1	77-85	94-100	We propose CFO , a Conditional Focused neuralnetwork-based approach	Our approach first zooms in a question	We propose CFO , a Conditional Focused neuralnetwork-based approach	Our approach first zooms in a question	77-93	94-120	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework .	1<2	none	elab-addition	elab-addition
P16-1076_anno1	94-100	101-108	Our approach first zooms in a question	to find more probable candidate subject mentions ,	Our approach first zooms in a question	to find more probable candidate subject mentions ,	94-120	94-120	Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework .	Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework .	1<2	none	enablement	enablement
P16-1076_anno1	94-100	109-120	Our approach first zooms in a question	and infers the final answers with a unified conditional probabilistic framework .	Our approach first zooms in a question	and infers the final answers with a unified conditional probabilistic framework .	94-120	94-120	Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework .	Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework .	1<2	none	joint	joint
P16-1076_anno1	121-130	131-153	Powered by deep recurrent neural networks and neural embeddings ,	our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	Powered by deep recurrent neural networks and neural embeddings ,	our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	121-153	121-153	Powered by deep recurrent neural networks and neural embeddings , our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	Powered by deep recurrent neural networks and neural embeddings , our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	1>2	none	elab-addition	elab-addition
P16-1076_anno1	77-85	131-153	We propose CFO , a Conditional Focused neuralnetwork-based approach	our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	We propose CFO , a Conditional Focused neuralnetwork-based approach	our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	77-93	121-153	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	Powered by deep recurrent neural networks and neural embeddings , our proposed CFO achieves an accuracy of 75.7 % on a dataset of 108k questions - the largest public one to date .	1<2	none	evaluation	evaluation
P16-1076_anno1	77-85	154-169	We propose CFO , a Conditional Focused neuralnetwork-based approach	It outperforms the current state of the art by an absolute margin of 11.8 % .	We propose CFO , a Conditional Focused neuralnetwork-based approach	It outperforms the current state of the art by an absolute margin of 11.8 % .	77-93	154-169	We propose CFO , a Conditional Focused neuralnetwork-based approach to answering factoid questions with knowledge bases .	It outperforms the current state of the art by an absolute margin of 11.8 % .	1<2	none	evaluation	evaluation
P16-1077_anno1	1-13	47-50	We revisit Levin 's theory about the correspondence of verb meaning and syntax	We address this challenge	We revisit Levin's theory about the correspondence of verb meaning and syntax	We address this challenge	1-34	47-62	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes .	1>2	none	bg-goal	bg-goal
P16-1077_anno1	1-13	14-28	We revisit Levin 's theory about the correspondence of verb meaning and syntax	and infer semantic classes from a large syntactic classification of more than 600 German verbs	We revisit Levin's theory about the correspondence of verb meaning and syntax	and infer semantic classes from a large syntactic classification of more than 600 German verbs	1-34	1-34	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	1<2	none	progression	progression
P16-1077_anno1	14-28	29-34	and infer semantic classes from a large syntactic classification of more than 600 German verbs	taking clausal and non-finite arguments .	and infer semantic classes from a large syntactic classification of more than 600 German verbs	taking clausal and non-finite arguments .	1-34	1-34	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	1<2	none	elab-addition	elab-addition
P16-1077_anno1	1-13	35-46	We revisit Levin 's theory about the correspondence of verb meaning and syntax	Grasping the meaning components of Levin-classes is known to be hard .	We revisit Levin's theory about the correspondence of verb meaning and syntax	Grasping the meaning components of Levin-classes is known to be hard .	1-34	35-46	We revisit Levin 's theory about the correspondence of verb meaning and syntax and infer semantic classes from a large syntactic classification of more than 600 German verbs taking clausal and non-finite arguments .	Grasping the meaning components of Levin-classes is known to be hard .	1<2	none	elab-addition	elab-addition
P16-1077_anno1	47-50	51-62	We address this challenge	by setting up a multi-perspective semantic characterization of the inferred classes .	We address this challenge	by setting up a multi-perspective semantic characterization of the inferred classes .	47-62	47-62	We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes .	We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes .	1<2	none	manner-means	manner-means
P16-1077_anno1	47-50	63-84	We address this challenge	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	We address this challenge	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	47-62	63-112	We address this challenge by setting up a multi-perspective semantic characterization of the inferred classes .	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	1<2	none	elab-addition	elab-addition
P16-1077_anno1	63-84	85-94	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	- the German wordnet GermaNet , VerbNet and FrameNet -	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	- the German wordnet GermaNet , VerbNet and FrameNet -	63-112	63-112	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	1<2	none	elab-enumember	elab-enumember
P16-1077_anno1	63-84	95-106	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	and perform a detailed analysis and evaluation of the resulting German-English classification	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons	and perform a detailed analysis and evaluation of the resulting German-English classification	63-112	63-112	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	1<2	none	joint	joint
P16-1077_anno1	95-106	107-112	and perform a detailed analysis and evaluation of the resulting German-English classification	( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	and perform a detailed analysis and evaluation of the resulting German-English classification	( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	63-112	63-112	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	To this end , we link the inferred classes and their English translation to independently constructed semantic classes in three different lexicons - the German wordnet GermaNet , VerbNet and FrameNet - and perform a detailed analysis and evaluation of the resulting German-English classification ( available at www.ukp.tu-darmstadt.de/modality-verbclasses/ ) .	1<2	none	elab-addition	elab-addition
P16-1078_anno1	1-18	27-35	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data	We propose a novel end-to-end syntactic NMT model ,	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data	We propose a novel end-to-end syntactic NMT model ,	1-26	27-45	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data and do not directly use syntactic information .	We propose a novel end-to-end syntactic NMT model , extending a sequenceto-sequence model with the source-side phrase structure .	1>2	none	bg-compare	bg-compare
P16-1078_anno1	1-18	19-26	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data	and do not directly use syntactic information .	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data	and do not directly use syntactic information .	1-26	1-26	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data and do not directly use syntactic information .	Most of the existing Neural Machine Translation ( NMT ) models focus on the conversion of sequential data and do not directly use syntactic information .	1<2	none	joint	joint
P16-1078_anno1	27-35	36-45	We propose a novel end-to-end syntactic NMT model ,	extending a sequenceto-sequence model with the source-side phrase structure .	We propose a novel end-to-end syntactic NMT model ,	extending a sequenceto-sequence model with the source-side phrase structure .	27-45	27-45	We propose a novel end-to-end syntactic NMT model , extending a sequenceto-sequence model with the source-side phrase structure .	We propose a novel end-to-end syntactic NMT model , extending a sequenceto-sequence model with the source-side phrase structure .	1<2	none	elab-addition	elab-addition
P16-1078_anno1	27-35	46-51	We propose a novel end-to-end syntactic NMT model ,	Our model has an attention mechanism	We propose a novel end-to-end syntactic NMT model ,	Our model has an attention mechanism	27-45	46-75	We propose a novel end-to-end syntactic NMT model , extending a sequenceto-sequence model with the source-side phrase structure .	Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence .	1<2	none	elab-addition	elab-addition
P16-1078_anno1	46-51	52-60	Our model has an attention mechanism	that enables the decoder to generate a translated word	Our model has an attention mechanism	that enables the decoder to generate a translated word	46-75	46-75	Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence .	Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence .	1<2	none	elab-addition	elab-addition
P16-1078_anno1	52-60	61-75	that enables the decoder to generate a translated word	while softly aligning it with phrases as well as words of the source sentence .	that enables the decoder to generate a translated word	while softly aligning it with phrases as well as words of the source sentence .	46-75	46-75	Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence .	Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence .	1<2	none	condition	condition
P16-1078_anno1	76-84	85-94	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	76-104	76-104	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system .	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system .	1>2	none	attribution	attribution
P16-1078_anno1	27-35	85-94	We propose a novel end-to-end syntactic NMT model ,	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	We propose a novel end-to-end syntactic NMT model ,	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	27-45	76-104	We propose a novel end-to-end syntactic NMT model , extending a sequenceto-sequence model with the source-side phrase structure .	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system .	1<2	none	evaluation	evaluation
P16-1078_anno1	85-94	95-104	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	and compares favorably with the state-of-the-art tree-to-string SMT system .	that our proposed model considerably outperforms sequence-to-sequence attentional NMT models	and compares favorably with the state-of-the-art tree-to-string SMT system .	76-104	76-104	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system .	Experimental results on the WAT '15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system .	1<2	none	joint	joint
P16-1079_anno1	1-8	49-57	Coordination is an important and common syntactic construction	In this work , we initiated manual annotation process	Coordination is an important and common syntactic construction	In this work , we initiated manual annotation process	1-20	49-62	Coordination is an important and common syntactic construction which is not handled well by state of the art parsers .	In this work , we initiated manual annotation process for solving these issues .	1>2	none	bg-compare	bg-compare
P16-1079_anno1	1-8	9-20	Coordination is an important and common syntactic construction	which is not handled well by state of the art parsers .	Coordination is an important and common syntactic construction	which is not handled well by state of the art parsers .	1-20	1-20	Coordination is an important and common syntactic construction which is not handled well by state of the art parsers .	Coordination is an important and common syntactic construction which is not handled well by state of the art parsers .	1<2	none	elab-addition	elab-addition
P16-1079_anno1	1-8	21-33	Coordination is an important and common syntactic construction	Coordinations in the Penn Treebank are missing internal structure in many cases ,	Coordination is an important and common syntactic construction	Coordinations in the Penn Treebank are missing internal structure in many cases ,	1-20	21-48	Coordination is an important and common syntactic construction which is not handled well by state of the art parsers .	Coordinations in the Penn Treebank are missing internal structure in many cases , do not include explicit marking of the conjuncts and contain various errors and inconsistencies .	1<2	none	elab-addition	elab-addition
P16-1079_anno1	21-33	34-41	Coordinations in the Penn Treebank are missing internal structure in many cases ,	do not include explicit marking of the conjuncts	Coordinations in the Penn Treebank are missing internal structure in many cases ,	do not include explicit marking of the conjuncts	21-48	21-48	Coordinations in the Penn Treebank are missing internal structure in many cases , do not include explicit marking of the conjuncts and contain various errors and inconsistencies .	Coordinations in the Penn Treebank are missing internal structure in many cases , do not include explicit marking of the conjuncts and contain various errors and inconsistencies .	1<2	none	elab-addition	elab-addition
P16-1079_anno1	34-41	42-48	do not include explicit marking of the conjuncts	and contain various errors and inconsistencies .	do not include explicit marking of the conjuncts	and contain various errors and inconsistencies .	21-48	21-48	Coordinations in the Penn Treebank are missing internal structure in many cases , do not include explicit marking of the conjuncts and contain various errors and inconsistencies .	Coordinations in the Penn Treebank are missing internal structure in many cases , do not include explicit marking of the conjuncts and contain various errors and inconsistencies .	1<2	none	joint	joint
P16-1079_anno1	49-57	58-62	In this work , we initiated manual annotation process	for solving these issues .	In this work , we initiated manual annotation process	for solving these issues .	49-62	49-62	In this work , we initiated manual annotation process for solving these issues .	In this work , we initiated manual annotation process for solving these issues .	1<2	none	enablement	enablement
P16-1079_anno1	49-57	63-71	In this work , we initiated manual annotation process	We identify the different elements in a coordination phrase	In this work , we initiated manual annotation process	We identify the different elements in a coordination phrase	49-62	63-79	In this work , we initiated manual annotation process for solving these issues .	We identify the different elements in a coordination phrase and label each element with its function .	1<2	none	elab-process_step	elab-process_step
P16-1079_anno1	63-71	72-79	We identify the different elements in a coordination phrase	and label each element with its function .	We identify the different elements in a coordination phrase	and label each element with its function .	63-79	63-79	We identify the different elements in a coordination phrase and label each element with its function .	We identify the different elements in a coordination phrase and label each element with its function .	1<2	none	joint	joint
P16-1079_anno1	49-57	80-83	In this work , we initiated manual annotation process	We add phrase boundaries	In this work , we initiated manual annotation process	We add phrase boundaries	49-62	80-95	In this work , we initiated manual annotation process for solving these issues .	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	1<2	none	elab-process_step	elab-process_step
P16-1079_anno1	80-83	84-88	We add phrase boundaries	when these are missing ,	We add phrase boundaries	when these are missing ,	80-95	80-95	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	1<2	none	condition	condition
P16-1079_anno1	84-88	89-91	when these are missing ,	unify inconsistencies ,	when these are missing ,	unify inconsistencies ,	80-95	80-95	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	1<2	none	joint	joint
P16-1079_anno1	84-88	92-95	when these are missing ,	and fix errors .	when these are missing ,	and fix errors .	80-95	80-95	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	1<2	none	joint	joint
P16-1079_anno1	80-83	96-103	We add phrase boundaries	The outcome is an extension of the PTB	We add phrase boundaries	The outcome is an extension of the PTB	80-95	96-112	We add phrase boundaries when these are missing , unify inconsistencies , and fix errors .	The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations .	1<2	none	elab-addition	elab-addition
P16-1079_anno1	96-103	104-112	The outcome is an extension of the PTB	that includes consistent and detailed structures for coordinations .	The outcome is an extension of the PTB	that includes consistent and detailed structures for coordinations .	96-112	96-112	The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations .	The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations .	1<2	none	elab-addition	elab-addition
P16-1079_anno1	49-57	113-120	In this work , we initiated manual annotation process	We make the coordination annotation publicly available ,	In this work , we initiated manual annotation process	We make the coordination annotation publicly available ,	49-62	113-132	In this work , we initiated manual annotation process for solving these issues .	We make the coordination annotation publicly available , in hope that they will facilitate further research into coordination disambiguation .	1<2	none	elab-process_step	elab-process_step
P16-1079_anno1	113-120	121-132	We make the coordination annotation publicly available ,	in hope that they will facilitate further research into coordination disambiguation .	We make the coordination annotation publicly available ,	in hope that they will facilitate further research into coordination disambiguation .	113-132	113-132	We make the coordination annotation publicly available , in hope that they will facilitate further research into coordination disambiguation .	We make the coordination annotation publicly available , in hope that they will facilitate further research into coordination disambiguation .	1<2	none	enablement	enablement
P16-1080_anno1	1-2,14-19	28-41	User traits <*> can be used to personalize applications	However , human perception of these traits is not perfectly aligned with reality .	User traits <*> can be used to personalize applications	However , human perception of these traits is not perfectly aligned with reality .	1-27	28-41	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	However , human perception of these traits is not perfectly aligned with reality .	1>2	none	contrast	contrast
P16-1080_anno1	1-2,14-19	3-7	User traits <*> can be used to personalize applications	disclosed through written text ,	User traits <*> can be used to personalize applications	disclosed through written text ,	1-27	1-27	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	1<2	none	elab-addition	elab-addition
P16-1080_anno1	1-2,14-19	8-13	User traits <*> can be used to personalize applications	such as age and gender ,	User traits <*> can be used to personalize applications	such as age and gender ,	1-27	1-27	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	1<2	none	elab-example	elab-example
P16-1080_anno1	14-19	20-27	can be used to personalize applications	such as recommender systems or conversational agents .	can be used to personalize applications	such as recommender systems or conversational agents .	1-27	1-27	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	User traits disclosed through written text , such as age and gender , can be used to personalize applications such as recommender systems or conversational agents .	1<2	none	elab-example	elab-example
P16-1080_anno1	28-41	42-51	However , human perception of these traits is not perfectly aligned with reality .	In this paper , we conduct a large-scale crowdsourcing experiment	However , human perception of these traits is not perfectly aligned with reality .	In this paper , we conduct a large-scale crowdsourcing experiment	28-41	42-59	However , human perception of these traits is not perfectly aligned with reality .	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	1>2	none	bg-compare	bg-compare
P16-1080_anno1	42-51	52-59	In this paper , we conduct a large-scale crowdsourcing experiment	on guessing age and gender from tweets .	In this paper , we conduct a large-scale crowdsourcing experiment	on guessing age and gender from tweets .	42-59	42-59	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	1<2	none	elab-addition	elab-addition
P16-1080_anno1	42-51	60-71	In this paper , we conduct a large-scale crowdsourcing experiment	We systematically analyze the quality and possible biases of these predictions .	In this paper , we conduct a large-scale crowdsourcing experiment	We systematically analyze the quality and possible biases of these predictions .	42-59	60-71	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	We systematically analyze the quality and possible biases of these predictions .	1<2	none	elab-addition	elab-addition
P16-1080_anno1	42-51	72-76	In this paper , we conduct a large-scale crowdsourcing experiment	We identify the textual cues	In this paper , we conduct a large-scale crowdsourcing experiment	We identify the textual cues	42-59	72-93	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice .	1<2	none	elab-addition	elab-addition
P16-1080_anno1	72-76	77-82	We identify the textual cues	which lead to miss-assessments of traits	We identify the textual cues	which lead to miss-assessments of traits	72-93	72-93	We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice .	We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice .	1<2	none	elab-addition	elab-addition
P16-1080_anno1	77-82	83-93	which lead to miss-assessments of traits	or make annotators more or less confident in their choice .	which lead to miss-assessments of traits	or make annotators more or less confident in their choice .	72-93	72-93	We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice .	We identify the textual cues which lead to miss-assessments of traits or make annotators more or less confident in their choice .	1<2	none	joint	joint
P16-1080_anno1	94-96	97-105	Our study demonstrates	that differences between real and perceived traits are noteworthy	Our study demonstrates	that differences between real and perceived traits are noteworthy	94-114	94-114	Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception .	Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception .	1>2	none	attribution	attribution
P16-1080_anno1	42-51	97-105	In this paper , we conduct a large-scale crowdsourcing experiment	that differences between real and perceived traits are noteworthy	In this paper , we conduct a large-scale crowdsourcing experiment	that differences between real and perceived traits are noteworthy	42-59	94-114	In this paper , we conduct a large-scale crowdsourcing experiment on guessing age and gender from tweets .	Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception .	1<2	none	evaluation	evaluation
P16-1080_anno1	97-105	106-114	that differences between real and perceived traits are noteworthy	and elucidates inaccurately used stereotypes in human perception .	that differences between real and perceived traits are noteworthy	and elucidates inaccurately used stereotypes in human perception .	94-114	94-114	Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception .	Our study demonstrates that differences between real and perceived traits are noteworthy and elucidates inaccurately used stereotypes in human perception .	1<2	none	joint	joint
P16-1081_anno1	1-7	26-30	Motivated by the findings in social science	we perform personalized sentiment classification	Motivated by the findings in social science	we perform personalized sentiment classification	1-37	1-37	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	1>2	none	bg-goal	bg-goal
P16-1081_anno1	8-15	16-25	that people 's opinions are diverse and variable	while together they are shaped by evolving social norms ,	that people's opinions are diverse and variable	while together they are shaped by evolving social norms ,	1-37	1-37	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	1>2	none	contrast	contrast
P16-1081_anno1	1-7	16-25	Motivated by the findings in social science	while together they are shaped by evolving social norms ,	Motivated by the findings in social science	while together they are shaped by evolving social norms ,	1-37	1-37	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	1<2	none	elab-addition	elab-addition
P16-1081_anno1	26-30	31-37	we perform personalized sentiment classification	via shared model adaptation over time .	we perform personalized sentiment classification	via shared model adaptation over time .	1-37	1-37	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	1<2	none	manner-means	manner-means
P16-1081_anno1	26-30	38-49	we perform personalized sentiment classification	In our proposed solution , a global sentiment model is constantly updated	we perform personalized sentiment classification	In our proposed solution , a global sentiment model is constantly updated	1-37	38-78	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	1<2	none	elab-addition	elab-addition
P16-1081_anno1	38-49	50-53	In our proposed solution , a global sentiment model is constantly updated	to capture the homogeneity	In our proposed solution , a global sentiment model is constantly updated	to capture the homogeneity	38-78	38-78	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	1<2	none	enablement	enablement
P16-1081_anno1	50-53	54-59	to capture the homogeneity	in which users express opinions ,	to capture the homogeneity	in which users express opinions ,	38-78	38-78	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	1<2	none	elab-addition	elab-addition
P16-1081_anno1	38-49	60-69	In our proposed solution , a global sentiment model is constantly updated	while personalized models are simultaneously adapted from the global model	In our proposed solution , a global sentiment model is constantly updated	while personalized models are simultaneously adapted from the global model	38-78	38-78	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	1<2	none	joint	joint
P16-1081_anno1	60-69	70-78	while personalized models are simultaneously adapted from the global model	to recognize the heterogeneity of opinions from individuals .	while personalized models are simultaneously adapted from the global model	to recognize the heterogeneity of opinions from individuals .	38-78	38-78	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	In our proposed solution , a global sentiment model is constantly updated to capture the homogeneity in which users express opinions , while personalized models are simultaneously adapted from the global model to recognize the heterogeneity of opinions from individuals .	1<2	none	enablement	enablement
P16-1081_anno1	26-30	79-86	we perform personalized sentiment classification	Global model sharing alleviates data sparsity issue ,	we perform personalized sentiment classification	Global model sharing alleviates data sparsity issue ,	1-37	79-96	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Global model sharing alleviates data sparsity issue , and individualized model adaptation enables efficient online model learning .	1<2	none	elab-addition	elab-addition
P16-1081_anno1	79-86	87-96	Global model sharing alleviates data sparsity issue ,	and individualized model adaptation enables efficient online model learning .	Global model sharing alleviates data sparsity issue ,	and individualized model adaptation enables efficient online model learning .	79-96	79-96	Global model sharing alleviates data sparsity issue , and individualized model adaptation enables efficient online model learning .	Global model sharing alleviates data sparsity issue , and individualized model adaptation enables efficient online model learning .	1<2	none	joint	joint
P16-1081_anno1	97-110	111-129	Extensive experimentations are performed on two large review collections from Amazon and Yelp ,	and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	Extensive experimentations are performed on two large review collections from Amazon and Yelp ,	and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	97-129	97-129	Extensive experimentations are performed on two large review collections from Amazon and Yelp , and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	Extensive experimentations are performed on two large review collections from Amazon and Yelp , and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	1>2	none	progression	progression
P16-1081_anno1	26-30	111-129	we perform personalized sentiment classification	and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	we perform personalized sentiment classification	and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	1-37	97-129	Motivated by the findings in social science that people 's opinions are diverse and variable while together they are shaped by evolving social norms , we perform personalized sentiment classification via shared model adaptation over time .	Extensive experimentations are performed on two large review collections from Amazon and Yelp , and encouraging performance gain is achieved against several state-of-the-art transfer learning and multi-task learning based sentiment classification solutions .	1<2	none	evaluation	evaluation
P16-1082_anno1	1-9	68-76	Our goal is to generate reading lists for students	Here we formulate an information-theoretic view of concept dependency	Our goal is to generate reading lists for students	Here we formulate an information-theoretic view of concept dependency	1-17	68-92	Our goal is to generate reading lists for students that help them optimally learn technical material .	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	1>2	none	bg-goal	bg-goal
P16-1082_anno1	1-9	10-17	Our goal is to generate reading lists for students	that help them optimally learn technical material .	Our goal is to generate reading lists for students	that help them optimally learn technical material .	1-17	1-17	Our goal is to generate reading lists for students that help them optimally learn technical material .	Our goal is to generate reading lists for students that help them optimally learn technical material .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	18-27	28-32	Existing retrieval algorithms return items directly relevant to a query	but do not return results	Existing retrieval algorithms return items directly relevant to a query	but do not return results	18-43	18-43	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	1>2	none	contrast	contrast
P16-1082_anno1	28-32	68-76	but do not return results	Here we formulate an information-theoretic view of concept dependency	but do not return results	Here we formulate an information-theoretic view of concept dependency	18-43	68-92	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	1>2	none	bg-compare	bg-compare
P16-1082_anno1	28-32	33-39	but do not return results	to help users read about the concepts	but do not return results	to help users read about the concepts	18-43	18-43	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	33-39	40-43	to help users read about the concepts	supporting their query .	to help users read about the concepts	supporting their query .	18-43	18-43	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	18-27	44-51,64-67	Existing retrieval algorithms return items directly relevant to a query	This is because the dependency structure of concepts <*> is never considered .	Existing retrieval algorithms return items directly relevant to a query	This is because the dependency structure of concepts <*> is never considered .	18-43	44-67	Existing retrieval algorithms return items directly relevant to a query but do not return results to help users read about the concepts supporting their query .	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	1<2	none	exp-reason	exp-reason
P16-1082_anno1	44-51,64-67	52-55	This is because the dependency structure of concepts <*> is never considered .	that must be understood	This is because the dependency structure of concepts <*> is never considered .	that must be understood	44-67	44-67	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	52-55	56-58	that must be understood	before reading material	that must be understood	before reading material	44-67	44-67	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	1<2	none	temporal	temporal
P16-1082_anno1	56-58	59-63	before reading material	pertaining to a given query	before reading material	pertaining to a given query	44-67	44-67	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	This is because the dependency structure of concepts that must be understood before reading material pertaining to a given query is never considered .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	68-76	77-79	Here we formulate an information-theoretic view of concept dependency	and present methods	Here we formulate an information-theoretic view of concept dependency	and present methods	68-92	68-92	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	1<2	none	joint	joint
P16-1082_anno1	77-79	80-92	and present methods	to construct a `` concept graph '' automatically from a text corpus .	and present methods	to construct a `` concept graph '' automatically from a text corpus .	68-92	68-92	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	1<2	none	enablement	enablement
P16-1082_anno1	93-102	112-120	We perform the first human evaluation of concept dependency edges	and the results verify the feasibility of automatic approaches	We perform the first human evaluation of concept dependency edges	and the results verify the feasibility of automatic approaches	93-128	93-128	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	1>2	none	progression	progression
P16-1082_anno1	93-102	103-111	We perform the first human evaluation of concept dependency edges	( to be published as open data ) ,	We perform the first human evaluation of concept dependency edges	( to be published as open data ) ,	93-128	93-128	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	68-76	112-120	Here we formulate an information-theoretic view of concept dependency	and the results verify the feasibility of automatic approaches	Here we formulate an information-theoretic view of concept dependency	and the results verify the feasibility of automatic approaches	68-92	93-128	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	1<2	none	evaluation	evaluation
P16-1082_anno1	112-120	121-128	and the results verify the feasibility of automatic approaches	for inferring concepts and their ependency relations .	and the results verify the feasibility of automatic approaches	for inferring concepts and their ependency relations .	93-128	93-128	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	We perform the first human evaluation of concept dependency edges ( to be published as open data ) , and the results verify the feasibility of automatic approaches for inferring concepts and their ependency relations .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	68-76	129-134	Here we formulate an information-theoretic view of concept dependency	This result can support search capabilities	Here we formulate an information-theoretic view of concept dependency	This result can support search capabilities	68-92	129-154	Here we formulate an information-theoretic view of concept dependency and present methods to construct a `` concept graph '' automatically from a text corpus .	This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query .	1<2	none	evaluation	evaluation
P16-1082_anno1	129-134	135-148	This result can support search capabilities	that may be tuned to help users learn a subject rather than retrieve documents	This result can support search capabilities	that may be tuned to help users learn a subject rather than retrieve documents	129-154	129-154	This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query .	This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query .	1<2	none	elab-addition	elab-addition
P16-1082_anno1	135-148	149-154	that may be tuned to help users learn a subject rather than retrieve documents	based on a single query .	that may be tuned to help users learn a subject rather than retrieve documents	based on a single query .	129-154	129-154	This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query .	This result can support search capabilities that may be tuned to help users learn a subject rather than retrieve documents based on a single query .	1<2	none	bg-general	bg-general
P16-1083_anno1	1-2	3-21	We prove	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	We prove	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	1-27	1-27	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	1>2	none	attribution	attribution
P16-1083_anno1	3-21	22-27	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	contradicting Hsu ( 2007 ) .	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	contradicting Hsu ( 2007 ) .	1-27	1-27	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	1<2	none	contrast	contrast
P16-1083_anno1	28-31	32-41	While prior work reported	that log-linear interpolation yields lower perplexity than linear interpolation ,	While prior work reported	that log-linear interpolation yields lower perplexity than linear interpolation ,	28-48	28-48	While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation , normalizing at query time was impractical .	While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation , normalizing at query time was impractical .	1>2	none	attribution	attribution
P16-1083_anno1	32-41	42-48	that log-linear interpolation yields lower perplexity than linear interpolation ,	normalizing at query time was impractical .	that log-linear interpolation yields lower perplexity than linear interpolation ,	normalizing at query time was impractical .	28-48	28-48	While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation , normalizing at query time was impractical .	While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation , normalizing at query time was impractical .	1>2	none	contrast	contrast
P16-1083_anno1	42-48	49-56	normalizing at query time was impractical .	We normalize the model offline in advance ,	normalizing at query time was impractical .	We normalize the model offline in advance ,	28-48	49-69	While prior work reported that log-linear interpolation yields lower perplexity than linear interpolation , normalizing at query time was impractical .	We normalize the model offline in advance , which is efficient due to a ecurrence relationship between the normalizing factors .	1>2	none	result	result
P16-1083_anno1	3-21	49-56	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	We normalize the model offline in advance ,	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	We normalize the model offline in advance ,	1-27	49-69	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	We normalize the model offline in advance , which is efficient due to a ecurrence relationship between the normalizing factors .	1<2	none	elab-addition	elab-addition
P16-1083_anno1	49-56	57-69	We normalize the model offline in advance ,	which is efficient due to a ecurrence relationship between the normalizing factors .	We normalize the model offline in advance ,	which is efficient due to a ecurrence relationship between the normalizing factors .	49-69	49-69	We normalize the model offline in advance , which is efficient due to a ecurrence relationship between the normalizing factors .	We normalize the model offline in advance , which is efficient due to a ecurrence relationship between the normalizing factors .	1<2	none	elab-addition	elab-addition
P16-1083_anno1	70-74	75-83	To tune interpolation weights ,	we apply Newton 's method to this convex problem	To tune interpolation weights ,	we apply Newton's method to this convex problem	70-97	70-97	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	1>2	none	enablement	enablement
P16-1083_anno1	3-21	75-83	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	we apply Newton 's method to this convex problem	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	we apply Newton's method to this convex problem	1-27	70-97	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	1<2	none	elab-addition	elab-addition
P16-1083_anno1	84-85	86-97	and show	that the derivatives can be computed efficiently in a batch process .	and show	that the derivatives can be computed efficiently in a batch process .	70-97	70-97	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	1>2	none	attribution	attribution
P16-1083_anno1	75-83	86-97	we apply Newton 's method to this convex problem	that the derivatives can be computed efficiently in a batch process .	we apply Newton's method to this convex problem	that the derivatives can be computed efficiently in a batch process .	70-97	70-97	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	1<2	none	progression	progression
P16-1083_anno1	86-97	98-107	that the derivatives can be computed efficiently in a batch process .	These findings are combined in new open-source interpolation tool ,	that the derivatives can be computed efficiently in a batch process .	These findings are combined in new open-source interpolation tool ,	70-97	98-113	To tune interpolation weights , we apply Newton 's method to this convex problem and show that the derivatives can be computed efficiently in a batch process .	These findings are combined in new open-source interpolation tool , which is distributed with KenLM .	1<2	none	elab-addition	elab-addition
P16-1083_anno1	98-107	108-113	These findings are combined in new open-source interpolation tool ,	which is distributed with KenLM .	These findings are combined in new open-source interpolation tool ,	which is distributed with KenLM .	98-113	98-113	These findings are combined in new open-source interpolation tool , which is distributed with KenLM .	These findings are combined in new open-source interpolation tool , which is distributed with KenLM .	1<2	none	elab-addition	elab-addition
P16-1083_anno1	3-21	114-127	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks ,	that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model ,	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks ,	1-27	114-134	We prove that log-linearly interpolated backoff language models can be efficiently and exactly collapsed into a single normalized backoff model , contradicting Hsu ( 2007 ) .	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks , compared to 75.91 for linear interpolation .	1<2	none	evaluation	evaluation
P16-1083_anno1	114-127	128-134	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks ,	compared to 75.91 for linear interpolation .	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks ,	compared to 75.91 for linear interpolation .	114-134	114-134	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks , compared to 75.91 for linear interpolation .	With 21 out-of-domain corpora , log-linear interpolation yields 72.58 perplexity on TED talks , compared to 75.91 for linear interpolation .	1<2	none	comparison	comparison
P16-1084_anno1	1-15	16-19,23-30	Recently a few systems for automatically solving math word problems have reported promising results .	However , the datasets <*> have limitations in both scale and diversity .	Recently a few systems for automatically solving math word problems have reported promising results .	However , the datasets <*> have limitations in both scale and diversity .	1-15	16-30	Recently a few systems for automatically solving math word problems have reported promising results .	However , the datasets used for evaluation have limitations in both scale and diversity .	1>2	none	contrast	contrast
P16-1084_anno1	16-19,23-30	31-39	However , the datasets <*> have limitations in both scale and diversity .	In this paper , we build a large-scale dataset	However , the datasets <*> have limitations in both scale and diversity .	In this paper , we build a large-scale dataset	16-30	31-58	However , the datasets used for evaluation have limitations in both scale and diversity .	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	1>2	none	bg-compare	bg-compare
P16-1084_anno1	16-19,23-30	20-22	However , the datasets <*> have limitations in both scale and diversity .	used for evaluation	However , the datasets <*> have limitations in both scale and diversity .	used for evaluation	16-30	16-30	However , the datasets used for evaluation have limitations in both scale and diversity .	However , the datasets used for evaluation have limitations in both scale and diversity .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	31-39	40-51	In this paper , we build a large-scale dataset	which is more than 9 times the size of previous ones ,	In this paper , we build a large-scale dataset	which is more than 9 times the size of previous ones ,	31-58	31-58	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	40-51	52-58	which is more than 9 times the size of previous ones ,	and contains many more problem types .	which is more than 9 times the size of previous ones ,	and contains many more problem types .	31-58	31-58	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	1<2	none	joint	joint
P16-1084_anno1	31-39	59-74	In this paper , we build a large-scale dataset	Problems in the dataset are semiautomatically obtained from community question-answering ( CQA ) web pages .	In this paper , we build a large-scale dataset	Problems in the dataset are semiautomatically obtained from community question-answering ( CQA ) web pages .	31-58	59-74	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	Problems in the dataset are semiautomatically obtained from community question-answering ( CQA ) web pages .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	31-39	75-80	In this paper , we build a large-scale dataset	A ranking SVM model is trained	In this paper , we build a large-scale dataset	A ranking SVM model is trained	31-58	75-101	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	75-80	81-89	A ranking SVM model is trained	to automatically extract problem answers from the answer text	A ranking SVM model is trained	to automatically extract problem answers from the answer text	75-101	75-101	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	1<2	none	enablement	enablement
P16-1084_anno1	81-89	90-94	to automatically extract problem answers from the answer text	provided by CQA users ,	to automatically extract problem answers from the answer text	provided by CQA users ,	75-101	75-101	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	75-80	95-101	A ranking SVM model is trained	which significantly reduces human annotation cost .	A ranking SVM model is trained	which significantly reduces human annotation cost .	75-101	75-101	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	A ranking SVM model is trained to automatically extract problem answers from the answer text provided by CQA users , which significantly reduces human annotation cost .	1<2	none	elab-addition	elab-addition
P16-1084_anno1	31-39	102,108-114	In this paper , we build a large-scale dataset	Experiments <*> lead to interesting and surprising results .	In this paper , we build a large-scale dataset	Experiments <*> lead to interesting and surprising results .	31-58	102-114	In this paper , we build a large-scale dataset which is more than 9 times the size of previous ones , and contains many more problem types .	Experiments conducted on the new dataset lead to interesting and surprising results .	1<2	none	evaluation	evaluation
P16-1084_anno1	102,108-114	103-107	Experiments <*> lead to interesting and surprising results .	conducted on the new dataset	Experiments <*> lead to interesting and surprising results .	conducted on the new dataset	102-114	102-114	Experiments conducted on the new dataset lead to interesting and surprising results .	Experiments conducted on the new dataset lead to interesting and surprising results .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	1-13	30-52	Recent years have seen a dramatic growth in the popularity of word embeddings	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	Recent years have seen a dramatic growth in the popularity of word embeddings	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	1-29	30-52	Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content .	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	1>2	none	result	result
P16-1085_anno1	1-13	14-18	Recent years have seen a dramatic growth in the popularity of word embeddings	mainly owing to their ability	Recent years have seen a dramatic growth in the popularity of word embeddings	mainly owing to their ability	1-29	1-29	Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content .	Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content .	1<2	none	exp-reason	exp-reason
P16-1085_anno1	14-18	19-29	mainly owing to their ability	to capture semantic information from massive amounts of textual content .	mainly owing to their ability	to capture semantic information from massive amounts of textual content .	1-29	1-29	Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content .	Recent years have seen a dramatic growth in the popularity of word embeddings mainly owing to their ability to capture semantic information from massive amounts of textual content .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	30-52	53-82	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	30-52	53-82	As a result , many tasks in Natural Language Processing have tried to take advantage of the potential of these distributional models .	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	1>2	none	bg-goal	bg-goal
P16-1085_anno1	53-82	83-86	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We propose different methods	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We propose different methods	53-82	83-113	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	83-86	87-101	We propose different methods	through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture ,	We propose different methods	through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture ,	83-113	83-113	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	83-86	102-106	We propose different methods	and perform a deep analysis	We propose different methods	and perform a deep analysis	83-113	83-113	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	1<2	none	joint	joint
P16-1085_anno1	102-106	107-113	and perform a deep analysis	of how different parameters affect performance .	and perform a deep analysis	of how different parameters affect performance .	83-113	83-113	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	We propose different methods through which word embeddings can be leveraged in a state-of-the-art supervised WSD system architecture , and perform a deep analysis of how different parameters affect performance .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	53-82	114-119,132-141	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	53-82	114-148	In this work , we study how word embeddings can be used in Word Sense Disambiguation , one of the oldest tasks in Natural Language Processing and Artificial Intelligence .	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	1<2	none	evaluation	evaluation
P16-1085_anno1	114-119,132-141	120-127	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	that makes use of word embeddings alone ,	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	that makes use of word embeddings alone ,	114-148	114-148	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	1<2	none	elab-addition	elab-addition
P16-1085_anno1	114-119,132-141	128-131	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	if designed properly ,	We show how a WSD system <*> can provide significant performance improvement over a state-ofthe-art WSD system	if designed properly ,	114-148	114-148	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	1<2	none	condition	condition
P16-1085_anno1	132-141	142-148	can provide significant performance improvement over a state-ofthe-art WSD system	that incorporates several standard WSD features .	can provide significant performance improvement over a state-ofthe-art WSD system	that incorporates several standard WSD features .	114-148	114-148	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	We show how a WSD system that makes use of word embeddings alone , if designed properly , can provide significant performance improvement over a state-ofthe-art WSD system that incorporates several standard WSD features .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	1-10	25-43	Several large cloze-style context-questionanswer datasets have been introduced recently :	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	Several large cloze-style context-questionanswer datasets have been introduced recently :	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	1-24	25-52	Several large cloze-style context-questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches .	1>2	none	result	result
P16-1086_anno1	1-10	11-24	Several large cloze-style context-questionanswer datasets have been introduced recently :	the CNN and Daily Mail news data and the Children 's Book Test .	Several large cloze-style context-questionanswer datasets have been introduced recently :	the CNN and Daily Mail news data and the Children's Book Test .	1-24	1-24	Several large cloze-style context-questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .	Several large cloze-style context-questionanswer datasets have been introduced recently : the CNN and Daily Mail news data and the Children 's Book Test .	1<2	none	elab-enumember	elab-enumember
P16-1086_anno1	25-43	53-59	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	We present a new , simple model	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	We present a new , simple model	25-52	53-92	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1>2	none	bg-goal	bg-goal
P16-1086_anno1	25-43	44-52	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	that currently seem to outperform all alternative approaches .	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques	that currently seem to outperform all alternative approaches .	25-52	25-52	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches .	Thanks to the size of these datasets , the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	53-59	60-62	We present a new , simple model	that uses attention	We present a new , simple model	that uses attention	53-92	53-92	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	60-62	63-70	that uses attention	to directly pick the answer from the context	that uses attention	to directly pick the answer from the context	53-92	53-92	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1<2	none	enablement	enablement
P16-1086_anno1	63-70	71-76	to directly pick the answer from the context	as opposed to computing the answer	to directly pick the answer from the context	as opposed to computing the answer	53-92	53-92	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1<2	none	contrast	contrast
P16-1086_anno1	71-76	77-85	as opposed to computing the answer	using a blended representation of words in the document	as opposed to computing the answer	using a blended representation of words in the document	53-92	53-92	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1<2	none	manner-means	manner-means
P16-1086_anno1	77-85	86-92	using a blended representation of words in the document	as is usual in similar models .	using a blended representation of words in the document	as is usual in similar models .	53-92	53-92	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	53-59	93-101	We present a new , simple model	This makes the model particularly suitable for questionanswering problems	We present a new , simple model	This makes the model particularly suitable for questionanswering problems	53-92	93-112	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	93-101	102-112	This makes the model particularly suitable for questionanswering problems	where the answer is a single word from the document .	This makes the model particularly suitable for questionanswering problems	where the answer is a single word from the document .	93-112	93-112	This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document .	This makes the model particularly suitable for questionanswering problems where the answer is a single word from the document .	1<2	none	elab-addition	elab-addition
P16-1086_anno1	53-59	113-127	We present a new , simple model	Ensemble of our models sets new state of the art on all evaluated datasets .	We present a new , simple model	Ensemble of our models sets new state of the art on all evaluated datasets .	53-92	113-127	We present a new , simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models .	Ensemble of our models sets new state of the art on all evaluated datasets .	1<2	none	evaluation	evaluation
P16-1087_anno1	1-20	21-23	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	that connect them	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	that connect them	1-34	1-34	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them - the first such attempt using a deep learning approach .	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them - the first such attempt using a deep learning approach .	1<2	none	elab-addition	elab-addition
P16-1087_anno1	1-20	24-34	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	- the first such attempt using a deep learning approach .	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	- the first such attempt using a deep learning approach .	1-34	1-34	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them - the first such attempt using a deep learning approach .	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them - the first such attempt using a deep learning approach .	1<2	none	elab-addition	elab-addition
P16-1087_anno1	35-39	40-64	Perhaps surprisingly , we find	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	Perhaps surprisingly , we find	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	35-72	35-72	Perhaps surprisingly , we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction , performing below even the standalone sequencetagging CRF .	Perhaps surprisingly , we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction , performing below even the standalone sequencetagging CRF .	1>2	none	attribution	attribution
P16-1087_anno1	40-64	73-89	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	35-72	73-140	Perhaps surprisingly , we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction , performing below even the standalone sequencetagging CRF .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1>2	none	contrast	contrast
P16-1087_anno1	40-64	65-72	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	performing below even the standalone sequencetagging CRF .	that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction ,	performing below even the standalone sequencetagging CRF .	35-72	35-72	Perhaps surprisingly , we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction , performing below even the standalone sequencetagging CRF .	Perhaps surprisingly , we find that standard LSTMs are not competitive with a state-of-the-art CRF+ILP joint inference approach ( Yang and Cardie , 2013 ) to opinion entities extraction , performing below even the standalone sequencetagging CRF .	1<2	none	exp-evidence	exp-evidence
P16-1087_anno1	1-20	73-89	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	1-34	73-140	We investigate the use of deep bidirectional LSTMs for joint extraction of opinion entities and the IS-FROM and ISABOUT relations that connect them - the first such attempt using a deep learning approach .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1<2	none	evaluation	evaluation
P16-1087_anno1	73-89	90-108	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ;	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ;	73-140	73-140	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1<2	none	joint	joint
P16-1087_anno1	90-108	109-120	and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ;	and to perform as well as the state-of-the-art for the IS-ABOUT relation	and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ;	and to perform as well as the state-of-the-art for the IS-ABOUT relation	73-140	73-140	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1<2	none	joint	joint
P16-1087_anno1	73-89	121-133	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	- all without access to opinion lexicons , parsers and other preprocessing components	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations	- all without access to opinion lexicons , parsers and other preprocessing components	73-140	73-140	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1<2	none	condition	condition
P16-1087_anno1	121-133	134-140	- all without access to opinion lexicons , parsers and other preprocessing components	required for the feature-rich CRF+ILP approach .	- all without access to opinion lexicons , parsers and other preprocessing components	required for the feature-rich CRF+ILP approach .	73-140	73-140	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	Incorporating sentence-level and a novel relation-level optimization , however , allows the LSTM to identify opinion relations and to perform within 1-3 % of the state-of-the-art joint model for opinion entities and the IS-FROM relation ; and to perform as well as the state-of-the-art for the IS-ABOUT relation - all without access to opinion lexicons , parsers and other preprocessing components required for the feature-rich CRF+ILP approach .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	1-6	7-12	This paper proposes a left-corner parser	which can identify nonlocal dependencies .	This paper proposes a left-corner parser	which can identify nonlocal dependencies .	1-12	1-12	This paper proposes a left-corner parser which can identify nonlocal dependencies .	This paper proposes a left-corner parser which can identify nonlocal dependencies .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	1-6	13-23	This paper proposes a left-corner parser	Our parser integrates nonlocal dependency identification into a transition-based system .	This paper proposes a left-corner parser	Our parser integrates nonlocal dependency identification into a transition-based system .	1-12	13-23	This paper proposes a left-corner parser which can identify nonlocal dependencies .	Our parser integrates nonlocal dependency identification into a transition-based system .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	1-6	24-28	This paper proposes a left-corner parser	We use a structured perceptron	This paper proposes a left-corner parser	We use a structured perceptron	1-12	24-41	This paper proposes a left-corner parser which can identify nonlocal dependencies .	We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	24-28	29-36	We use a structured perceptron	which enables our parser to utilize global features	We use a structured perceptron	which enables our parser to utilize global features	24-41	24-41	We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies .	We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	29-36	37-41	which enables our parser to utilize global features	captured by nonlocal dependencies .	which enables our parser to utilize global features	captured by nonlocal dependencies .	24-41	24-41	We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies .	We use a structured perceptron which enables our parser to utilize global features captured by nonlocal dependencies .	1<2	none	elab-addition	elab-addition
P16-1088_anno1	42-45	46-60	An experimental result demonstrates	that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	An experimental result demonstrates	that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	42-60	42-60	An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	1>2	none	attribution	attribution
P16-1088_anno1	1-6	46-60	This paper proposes a left-corner parser	that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	This paper proposes a left-corner parser	that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	1-12	42-60	This paper proposes a left-corner parser which can identify nonlocal dependencies .	An experimental result demonstrates that our parser achieves a good balance between constituent parsing and nonlocal dependency identification .	1<2	none	evaluation	evaluation
P16-1089_anno1	26-43	49-52,59-68	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way	However , word embeddings <*> are not optimized for the task of sentence representation ,	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way	However , word embeddings <*> are not optimized for the task of sentence representation ,	26-48	49-77	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings .	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	1>2	none	contrast	contrast
P16-1089_anno1	26-43	44-48	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way	of obtaining sentence embeddings .	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way	of obtaining sentence embeddings .	26-48	26-48	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings .	Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings .	1<2	none	elab-addition	elab-addition
P16-1089_anno1	49-52,59-68	78-82	However , word embeddings <*> are not optimized for the task of sentence representation ,	Siamese CBOW handles this problem	However , word embeddings <*> are not optimized for the task of sentence representation ,	Siamese CBOW handles this problem	49-77	78-94	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	1>2	none	bg-goal	bg-goal
P16-1089_anno1	49-52,59-68	53-58	However , word embeddings <*> are not optimized for the task of sentence representation ,	trained with the methods currently available	However , word embeddings <*> are not optimized for the task of sentence representation ,	trained with the methods currently available	49-77	49-77	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	1<2	none	elab-addition	elab-addition
P16-1089_anno1	49-52,59-68	69-77	However , word embeddings <*> are not optimized for the task of sentence representation ,	and , thus , likely to be suboptimal .	However , word embeddings <*> are not optimized for the task of sentence representation ,	and , thus , likely to be suboptimal .	49-77	49-77	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	However , word embeddings trained with the methods currently available are not optimized for the task of sentence representation , and , thus , likely to be suboptimal .	1<2	none	progression	progression
P16-1089_anno1	1-25	78-82	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	Siamese CBOW handles this problem	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	Siamese CBOW handles this problem	1-25	78-94	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	1<2	none	elab-addition	elab-addition
P16-1089_anno1	78-82	83-87	Siamese CBOW handles this problem	by training word embeddings directly	Siamese CBOW handles this problem	by training word embeddings directly	78-94	78-94	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	1<2	none	manner-means	manner-means
P16-1089_anno1	83-87	88-94	by training word embeddings directly	for the purpose of being averaged .	by training word embeddings directly	for the purpose of being averaged .	78-94	78-94	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged .	1<2	none	enablement	enablement
P16-1089_anno1	1-25	95-101	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	The underlying neural network learns word embeddings	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	The underlying neural network learns word embeddings	1-25	95-113	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	The underlying neural network learns word embeddings by predicting , from a sentence representation , its surrounding sentences .	1<2	none	elab-addition	elab-addition
P16-1089_anno1	95-101	102-113	The underlying neural network learns word embeddings	by predicting , from a sentence representation , its surrounding sentences .	The underlying neural network learns word embeddings	by predicting , from a sentence representation , its surrounding sentences .	95-113	95-113	The underlying neural network learns word embeddings by predicting , from a sentence representation , its surrounding sentences .	The underlying neural network learns word embeddings by predicting , from a sentence representation , its surrounding sentences .	1<2	none	manner-means	manner-means
P16-1089_anno1	1-25	114-122	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	We show the robustness of the Siamese CBOW model	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	We show the robustness of the Siamese CBOW model	1-25	114-136	We present the Siamese Continuous Bag of Words ( Siamese CBOW ) model , a neural network for efficient estimation of highquality sentence embeddings .	We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources .	1<2	none	evaluation	evaluation
P16-1089_anno1	114-122	123-128	We show the robustness of the Siamese CBOW model	by evaluating it on 20 datasets	We show the robustness of the Siamese CBOW model	by evaluating it on 20 datasets	114-136	114-136	We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources .	We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources .	1<2	none	manner-means	manner-means
P16-1089_anno1	123-128	129-136	by evaluating it on 20 datasets	stemming from a wide variety of sources .	by evaluating it on 20 datasets	stemming from a wide variety of sources .	114-136	114-136	We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources .	We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	1-5	31-37	Can we train a system	We answer the question in the affirmative	Can we train a system	We answer the question in the affirmative	1-30	31-44	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	We answer the question in the affirmative provided our model family is wellspecified .	1>2	none	bg-goal	bg-goal
P16-1090_anno1	1-5	6-19	Can we train a system	that , on any new input , either says `` do n't know ''	Can we train a system	that , on any new input , either says `` don't know ''	1-30	1-30	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	1<2	none	elab-addition	elab-addition
P16-1090_anno1	6-19	20-23	that , on any new input , either says `` do n't know ''	or makes a prediction	that , on any new input , either says `` don't know ''	or makes a prediction	1-30	1-30	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	1<2	none	joint	joint
P16-1090_anno1	20-23	24-30	or makes a prediction	that is guaranteed to be correct ?	or makes a prediction	that is guaranteed to be correct ?	1-30	1-30	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	Can we train a system that , on any new input , either says `` do n't know '' or makes a prediction that is guaranteed to be correct ?	1<2	none	elab-addition	elab-addition
P16-1090_anno1	31-37	38-44	We answer the question in the affirmative	provided our model family is wellspecified .	We answer the question in the affirmative	provided our model family is wellspecified .	31-44	31-44	We answer the question in the affirmative provided our model family is wellspecified .	We answer the question in the affirmative provided our model family is wellspecified .	1<2	none	condition	condition
P16-1090_anno1	31-37	45-51	We answer the question in the affirmative	Specifically , we introduce the unanimity principle	We answer the question in the affirmative	Specifically , we introduce the unanimity principle	31-44	45-67	We answer the question in the affirmative provided our model family is wellspecified .	Specifically , we introduce the unanimity principle : only predict when all models consistent with the training data predict the same output .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	45-51	52-67	Specifically , we introduce the unanimity principle	: only predict when all models consistent with the training data predict the same output .	Specifically , we introduce the unanimity principle	: only predict when all models consistent with the training data predict the same output .	45-67	45-67	Specifically , we introduce the unanimity principle : only predict when all models consistent with the training data predict the same output .	Specifically , we introduce the unanimity principle : only predict when all models consistent with the training data predict the same output .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	45-51	68-84	Specifically , we introduce the unanimity principle	We operationalize this principle for semantic parsing , the task of mapping utterances to logical forms .	Specifically , we introduce the unanimity principle	We operationalize this principle for semantic parsing , the task of mapping utterances to logical forms .	45-67	68-84	Specifically , we introduce the unanimity principle : only predict when all models consistent with the training data predict the same output .	We operationalize this principle for semantic parsing , the task of mapping utterances to logical forms .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	31-37	85-91	We answer the question in the affirmative	We develop a simple , efficient method	We answer the question in the affirmative	We develop a simple , efficient method	31-44	85-109	We answer the question in the affirmative provided our model family is wellspecified .	We develop a simple , efficient method that reasons over the infinite set of all consistent models by only checking two of the models .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	85-91	92-101	We develop a simple , efficient method	that reasons over the infinite set of all consistent models	We develop a simple , efficient method	that reasons over the infinite set of all consistent models	85-109	85-109	We develop a simple , efficient method that reasons over the infinite set of all consistent models by only checking two of the models .	We develop a simple , efficient method that reasons over the infinite set of all consistent models by only checking two of the models .	1<2	none	elab-addition	elab-addition
P16-1090_anno1	92-101	102-109	that reasons over the infinite set of all consistent models	by only checking two of the models .	that reasons over the infinite set of all consistent models	by only checking two of the models .	85-109	85-109	We develop a simple , efficient method that reasons over the infinite set of all consistent models by only checking two of the models .	We develop a simple , efficient method that reasons over the infinite set of all consistent models by only checking two of the models .	1<2	none	manner-means	manner-means
P16-1090_anno1	110-111	112-132	We prove	that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	We prove	that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	110-132	110-132	We prove that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	We prove that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	1>2	none	attribution	attribution
P16-1090_anno1	31-37	112-132	We answer the question in the affirmative	that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	We answer the question in the affirmative	that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	31-44	110-132	We answer the question in the affirmative provided our model family is wellspecified .	We prove that our method obtains 100 % precision even with a modest amount of training data from a possibly adversarial distribution .	1<2	none	evaluation	evaluation
P16-1090_anno1	31-37	133-147	We answer the question in the affirmative	Empirically , we demonstrate the effectiveness of our approach on the standard GeoQuery dataset .	We answer the question in the affirmative	Empirically , we demonstrate the effectiveness of our approach on the standard GeoQuery dataset .	31-44	133-147	We answer the question in the affirmative provided our model family is wellspecified .	Empirically , we demonstrate the effectiveness of our approach on the standard GeoQuery dataset .	1<2	none	evaluation	evaluation
P16-1091_anno1	1-8	23-35	Dialogue topic tracking is a sequential labelling problem	This paper presents various artificial neural network models for dialogue topic tracking ,	Dialogue topic tracking is a sequential labelling problem	This paper presents various artificial neural network models for dialogue topic tracking ,	1-22	23-65	Dialogue topic tracking is a sequential labelling problem of recognizing the topic state at each time step in given dialogue sequences .	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	1>2	none	bg-goal	bg-goal
P16-1091_anno1	1-8	9-22	Dialogue topic tracking is a sequential labelling problem	of recognizing the topic state at each time step in given dialogue sequences .	Dialogue topic tracking is a sequential labelling problem	of recognizing the topic state at each time step in given dialogue sequences .	1-22	1-22	Dialogue topic tracking is a sequential labelling problem of recognizing the topic state at each time step in given dialogue sequences .	Dialogue topic tracking is a sequential labelling problem of recognizing the topic state at each time step in given dialogue sequences .	1<2	none	elab-addition	elab-addition
P16-1091_anno1	23-35	36-39	This paper presents various artificial neural network models for dialogue topic tracking ,	including convolutional neural networks	This paper presents various artificial neural network models for dialogue topic tracking ,	including convolutional neural networks	23-65	23-65	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	1<2	none	elab-example	elab-example
P16-1091_anno1	36-39	40-48	including convolutional neural networks	to account for semantics at each individual utterance ,	including convolutional neural networks	to account for semantics at each individual utterance ,	23-65	23-65	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	1<2	none	enablement	enablement
P16-1091_anno1	36-39	49-52	including convolutional neural networks	and recurrent neural networks	including convolutional neural networks	and recurrent neural networks	23-65	23-65	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	1<2	none	joint	joint
P16-1091_anno1	49-52	53-65	and recurrent neural networks	to account for conversational contexts along multiple turns in the dialogue history .	and recurrent neural networks	to account for conversational contexts along multiple turns in the dialogue history .	23-65	23-65	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	1<2	none	enablement	enablement
P16-1091_anno1	66-69	70-83	The experimental results demonstrate	that our proposed models can significantly improve the tracking performances in human-human conversations .	The experimental results demonstrate	that our proposed models can significantly improve the tracking performances in human-human conversations .	66-83	66-83	The experimental results demonstrate that our proposed models can significantly improve the tracking performances in human-human conversations .	The experimental results demonstrate that our proposed models can significantly improve the tracking performances in human-human conversations .	1>2	none	attribution	attribution
P16-1091_anno1	23-35	70-83	This paper presents various artificial neural network models for dialogue topic tracking ,	that our proposed models can significantly improve the tracking performances in human-human conversations .	This paper presents various artificial neural network models for dialogue topic tracking ,	that our proposed models can significantly improve the tracking performances in human-human conversations .	23-65	66-83	This paper presents various artificial neural network models for dialogue topic tracking , including convolutional neural networks to account for semantics at each individual utterance , and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history .	The experimental results demonstrate that our proposed models can significantly improve the tracking performances in human-human conversations .	1<2	none	evaluation	evaluation
P16-1092_anno1	1-15	16-46	Lexico-semantic knowledge of our native language provides an initial foundation for second language learning .	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Lexico-semantic knowledge of our native language provides an initial foundation for second language learning .	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	1-15	16-46	Lexico-semantic knowledge of our native language provides an initial foundation for second language learning .	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	1>2	none	bg-goal	bg-goal
P16-1092_anno1	16-46	47-56	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Specifically , we focus on the problem of lexical choice	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Specifically , we focus on the problem of lexical choice	16-46	47-74	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	1<2	none	elab-addition	elab-addition
P16-1092_anno1	47-56	57-68	Specifically , we focus on the problem of lexical choice	and investigate it in the context of three typologically diverse languages :	Specifically , we focus on the problem of lexical choice	and investigate it in the context of three typologically diverse languages :	47-74	47-74	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	1<2	none	joint	joint
P16-1092_anno1	57-68	69-74	and investigate it in the context of three typologically diverse languages :	Russian , Spanish and English .	and investigate it in the context of three typologically diverse languages :	Russian , Spanish and English .	47-74	47-74	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	1<2	none	elab-enumember	elab-enumember
P16-1092_anno1	75-76	77-81,86-99	We show	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	We show	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	75-99	75-99	We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1 .	We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1 .	1>2	none	attribution	attribution
P16-1092_anno1	47-56	77-81,86-99	Specifically , we focus on the problem of lexical choice	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	Specifically , we focus on the problem of lexical choice	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	47-74	75-99	Specifically , we focus on the problem of lexical choice and investigate it in the context of three typologically diverse languages : Russian , Spanish and English .	We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1 .	1<2	none	elab-addition	elab-addition
P16-1092_anno1	77-81,86-99	82-85	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	learned from L1 data	that a statistical semantic model <*> improves automatic error detection in L2 for the speakers of the respective L1 .	learned from L1 data	75-99	75-99	We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1 .	We show that a statistical semantic model learned from L1 data improves automatic error detection in L2 for the speakers of the respective L1 .	1<2	none	elab-addition	elab-addition
P16-1092_anno1	16-46	100-107,113-121	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Finally , we investigate whether the semantic model <*> is portable to other , typologically related languages .	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Finally , we investigate whether the semantic model <*> is portable to other , typologically related languages .	16-46	100-121	In this paper , we investigate whether and to what extent the lexico-semantic models of the native language ( L1 ) are transferred to the second language ( L2 ) .	Finally , we investigate whether the semantic model learned from a particular L1 is portable to other , typologically related languages .	1<2	none	elab-addition	elab-addition
P16-1092_anno1	100-107,113-121	108-112	Finally , we investigate whether the semantic model <*> is portable to other , typologically related languages .	learned from a particular L1	Finally , we investigate whether the semantic model <*> is portable to other , typologically related languages .	learned from a particular L1	100-121	100-121	Finally , we investigate whether the semantic model learned from a particular L1 is portable to other , typologically related languages .	Finally , we investigate whether the semantic model learned from a particular L1 is portable to other , typologically related languages .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	1-14	47-50	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	We describe a system	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	We describe a system	1-14	47-67	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	1>2	none	bg-goal	bg-goal
P16-1093_anno1	1-14	15-23	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	An item displays a sentence with a blank ,	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	An item displays a sentence with a blank ,	1-14	15-34	Fill-in-the-blank items are commonly featured in computer-assisted language learning ( CALL ) systems .	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	15-23	24-30	An item displays a sentence with a blank ,	and often proposes a number of choices	An item displays a sentence with a blank ,	and often proposes a number of choices	15-34	15-34	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	1<2	none	joint	joint
P16-1093_anno1	24-30	31-34	and often proposes a number of choices	for filling it .	and often proposes a number of choices	for filling it .	15-34	15-34	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	24-30	35-46	and often proposes a number of choices	These choices should include one correct answer and several plausible distractors .	and often proposes a number of choices	These choices should include one correct answer and several plausible distractors .	15-34	35-46	An item displays a sentence with a blank , and often proposes a number of choices for filling it .	These choices should include one correct answer and several plausible distractors .	1<2	none	elab-enumember	elab-enumember
P16-1093_anno1	51-57	58-60	that , given an English corpus ,	automatically generates distractors	that , given an English corpus ,	automatically generates distractors	47-67	47-67	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	1>2	none	condition	condition
P16-1093_anno1	47-50	58-60	We describe a system	automatically generates distractors	We describe a system	automatically generates distractors	47-67	47-67	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	58-60	61-67	automatically generates distractors	to produce items for preposition usage .	automatically generates distractors	to produce items for preposition usage .	47-67	47-67	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	1<2	none	enablement	enablement
P16-1093_anno1	47-50	68-76	We describe a system	We report a comprehensive evaluation on this system ,	We describe a system	We report a comprehensive evaluation on this system ,	47-67	68-82	We describe a system that , given an English corpus , automatically generates distractors to produce items for preposition usage .	We report a comprehensive evaluation on this system , involving both experts and learners .	1<2	none	evaluation	evaluation
P16-1093_anno1	68-76	77-82	We report a comprehensive evaluation on this system ,	involving both experts and learners .	We report a comprehensive evaluation on this system ,	involving both experts and learners .	68-82	68-82	We report a comprehensive evaluation on this system , involving both experts and learners .	We report a comprehensive evaluation on this system , involving both experts and learners .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	68-76	83-96	We report a comprehensive evaluation on this system ,	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	We report a comprehensive evaluation on this system ,	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	68-82	83-108	We report a comprehensive evaluation on this system , involving both experts and learners .	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	1<2	none	elab-process_step	elab-process_step
P16-1093_anno1	83-96	97-99	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	comparing several methods	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	comparing several methods	83-108	83-108	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	1<2	none	manner-means	manner-means
P16-1093_anno1	97-99	100-108	comparing several methods	that exploit learner error and learner revision patterns .	comparing several methods	that exploit learner error and learner revision patterns .	83-108	83-108	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	109-110	111-122	We show	that the quality of machine-generated items approaches that of human-crafted ones .	We show	that the quality of machine-generated items approaches that of human-crafted ones .	109-122	109-122	We show that the quality of machine-generated items approaches that of human-crafted ones .	We show that the quality of machine-generated items approaches that of human-crafted ones .	1>2	none	attribution	attribution
P16-1093_anno1	83-96	111-122	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	that the quality of machine-generated items approaches that of human-crafted ones .	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors ,	that the quality of machine-generated items approaches that of human-crafted ones .	83-108	109-122	First , we analyze the difficulty levels of machine-generated carrier sentences and distractors , comparing several methods that exploit learner error and learner revision patterns .	We show that the quality of machine-generated items approaches that of human-crafted ones .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	68-76	123-128	We report a comprehensive evaluation on this system ,	Further , we investigate the extent	We report a comprehensive evaluation on this system ,	Further , we investigate the extent	68-82	123-145	We report a comprehensive evaluation on this system , involving both experts and learners .	Further , we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of distractors .	1<2	none	elab-process_step	elab-process_step
P16-1093_anno1	123-128	129-145	Further , we investigate the extent	to which mismatched L1 between the user and the learner corpora affects the quality of distractors .	Further , we investigate the extent	to which mismatched L1 between the user and the learner corpora affects the quality of distractors .	123-145	123-145	Further , we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of distractors .	Further , we investigate the extent to which mismatched L1 between the user and the learner corpora affects the quality of distractors .	1<2	none	elab-addition	elab-addition
P16-1093_anno1	68-76	146-168	We report a comprehensive evaluation on this system ,	Finally , we measure the system 's impact on the user 's language proficiency in both the short and the long term .	We report a comprehensive evaluation on this system ,	Finally , we measure the system's impact on the user's language proficiency in both the short and the long term .	68-82	146-168	We report a comprehensive evaluation on this system , involving both experts and learners .	Finally , we measure the system 's impact on the user 's language proficiency in both the short and the long term .	1<2	none	elab-process_step	elab-process_step
P16-1094_anno1	1-4	5-16	We present persona-based models	for handling the issue of speaker consistency in neural response generation .	We present persona-based models	for handling the issue of speaker consistency in neural response generation .	1-16	1-16	We present persona-based models for handling the issue of speaker consistency in neural response generation .	We present persona-based models for handling the issue of speaker consistency in neural response generation .	1<2	none	enablement	enablement
P16-1094_anno1	1-4	17-24	We present persona-based models	A speaker model encodes personas in distributed embeddings	We present persona-based models	A speaker model encodes personas in distributed embeddings	1-16	17-36	We present persona-based models for handling the issue of speaker consistency in neural response generation .	A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style .	1<2	none	elab-addition	elab-addition
P16-1094_anno1	17-24	25-28	A speaker model encodes personas in distributed embeddings	that capture individual characteristics	A speaker model encodes personas in distributed embeddings	that capture individual characteristics	17-36	17-36	A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style .	A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style .	1<2	none	elab-addition	elab-addition
P16-1094_anno1	25-28	29-36	that capture individual characteristics	such as background information and speaking style .	that capture individual characteristics	such as background information and speaking style .	17-36	17-36	A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style .	A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style .	1<2	none	elab-example	elab-example
P16-1094_anno1	1-4	37-48	We present persona-based models	A dyadic speakeraddressee model captures properties of interactions between two interlocutors .	We present persona-based models	A dyadic speakeraddressee model captures properties of interactions between two interlocutors .	1-16	37-48	We present persona-based models for handling the issue of speaker consistency in neural response generation .	A dyadic speakeraddressee model captures properties of interactions between two interlocutors .	1<2	none	elab-addition	elab-addition
P16-1094_anno1	1-4	49-71	We present persona-based models	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency	We present persona-based models	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency	1-16	49-77	We present persona-based models for handling the issue of speaker consistency in neural response generation .	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency as measured by human judges .	1<2	none	evaluation	evaluation
P16-1094_anno1	49-71	72-77	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency	as measured by human judges .	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency	as measured by human judges .	49-77	49-77	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency as measured by human judges .	Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models , with similar gains in speaker consistency as measured by human judges .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	1-12	22-32	Deep Random Walk ( DeepWalk ) can learn a latent space representation	However , for relational network classification , DeepWalk can be suboptimal	Deep Random Walk ( DeepWalk ) can learn a latent space representation	However , for relational network classification , DeepWalk can be suboptimal	1-21	22-46	Deep Random Walk ( DeepWalk ) can learn a latent space representation for describing the topological structure of a network .	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	1>2	none	contrast	contrast
P16-1095_anno1	1-12	13-21	Deep Random Walk ( DeepWalk ) can learn a latent space representation	for describing the topological structure of a network .	Deep Random Walk ( DeepWalk ) can learn a latent space representation	for describing the topological structure of a network .	1-21	1-21	Deep Random Walk ( DeepWalk ) can learn a latent space representation for describing the topological structure of a network .	Deep Random Walk ( DeepWalk ) can learn a latent space representation for describing the topological structure of a network .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	22-32	47-68	However , for relational network classification , DeepWalk can be suboptimal	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	However , for relational network classification , DeepWalk can be suboptimal	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	22-46	47-68	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	1>2	none	bg-compare	bg-compare
P16-1095_anno1	22-32	33-37	However , for relational network classification , DeepWalk can be suboptimal	as it lacks a mechanism	However , for relational network classification , DeepWalk can be suboptimal	as it lacks a mechanism	22-46	22-46	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	1<2	none	exp-reason	exp-reason
P16-1095_anno1	33-37	38-46	as it lacks a mechanism	to optimize the objective of the target task .	as it lacks a mechanism	to optimize the objective of the target task .	22-46	22-46	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	However , for relational network classification , DeepWalk can be suboptimal as it lacks a mechanism to optimize the objective of the target task .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	69-75	76-82	By solving a joint optimization problem ,	DDRW can learn the latent space representations	By solving a joint optimization problem ,	DDRW can learn the latent space representations	69-98	69-98	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	1>2	none	manner-means	manner-means
P16-1095_anno1	47-68	76-82	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW can learn the latent space representations	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW can learn the latent space representations	47-68	69-98	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	76-82	83-88	DDRW can learn the latent space representations	that well capture the topological structure	DDRW can learn the latent space representations	that well capture the topological structure	69-98	69-98	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	83-88	89-98	that well capture the topological structure	and meanwhile are discriminative for the network classification task .	that well capture the topological structure	and meanwhile are discriminative for the network classification task .	69-98	69-98	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	By solving a joint optimization problem , DDRW can learn the latent space representations that well capture the topological structure and meanwhile are discriminative for the network classification task .	1<2	none	joint	joint
P16-1095_anno1	99-107	108-118	Our experimental results on several real social networks demonstrate	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	Our experimental results on several real social networks demonstrate	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	99-128	99-128	Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classification tasks , while retaining the topological structure in the latent space .	Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classification tasks , while retaining the topological structure in the latent space .	1>2	none	attribution	attribution
P16-1095_anno1	47-68	108-118	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	47-68	99-128	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classification tasks , while retaining the topological structure in the latent space .	1<2	none	evaluation	evaluation
P16-1095_anno1	108-118	119-128	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	while retaining the topological structure in the latent space .	that DDRW significantly outperforms DeepWalk on multilabel network classification tasks ,	while retaining the topological structure in the latent space .	99-128	99-128	Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classification tasks , while retaining the topological structure in the latent space .	Our experimental results on several real social networks demonstrate that DDRW significantly outperforms DeepWalk on multilabel network classification tasks , while retaining the topological structure in the latent space .	1<2	none	joint	joint
P16-1095_anno1	47-68	129-131	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is stable	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is stable	47-68	129-144	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is stable and consistently outperforms the baseline methods by various percentages of labeled data .	1<2	none	evaluation	evaluation
P16-1095_anno1	129-131	132-144	DDRW is stable	and consistently outperforms the baseline methods by various percentages of labeled data .	DDRW is stable	and consistently outperforms the baseline methods by various percentages of labeled data .	129-144	129-144	DDRW is stable and consistently outperforms the baseline methods by various percentages of labeled data .	DDRW is stable and consistently outperforms the baseline methods by various percentages of labeled data .	1<2	none	joint	joint
P16-1095_anno1	47-68	145-150	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is also an online method	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is also an online method	47-68	145-159	In this paper , we present Discriminative Deep Random Walk ( DDRW ) , a novel method for relational network classification .	DDRW is also an online method that is scalable and can be naturally parallelized .	1<2	none	evaluation	evaluation
P16-1095_anno1	145-150	151-153	DDRW is also an online method	that is scalable	DDRW is also an online method	that is scalable	145-159	145-159	DDRW is also an online method that is scalable and can be naturally parallelized .	DDRW is also an online method that is scalable and can be naturally parallelized .	1<2	none	elab-addition	elab-addition
P16-1095_anno1	151-153	154-159	that is scalable	and can be naturally parallelized .	that is scalable	and can be naturally parallelized .	145-159	145-159	DDRW is also an online method that is scalable and can be naturally parallelized .	DDRW is also an online method that is scalable and can be naturally parallelized .	1<2	none	joint	joint
P16-1096_anno1	1-4,14-16	36-44,52-56	Automatically recognising medical concepts <*> enables several applications	However , the discrepancy between the type of language <*> poses a major challenge .	Automatically recognising medical concepts <*> enables several applications	However , the discrepancy between the type of language <*> poses a major challenge .	1-35	36-56	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge .	1>2	none	contrast	contrast
P16-1096_anno1	1-4,14-16	5-9	Automatically recognising medical concepts <*> enables several applications	mentioned in social media messages	Automatically recognising medical concepts <*> enables several applications	mentioned in social media messages	1-35	1-35	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	5-9	10-13	mentioned in social media messages	( e.g. tweets )	mentioned in social media messages	( e.g. tweets )	1-35	1-35	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	1<2	none	elab-example	elab-example
P16-1096_anno1	14-16	17-26	enables several applications	for enhancing health quality of people in a community ,	enables several applications	for enhancing health quality of people in a community ,	1-35	1-35	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	1<2	none	enablement	enablement
P16-1096_anno1	14-16	27-35	enables several applications	e.g. real-time monitoring of infectious diseases in population .	enables several applications	e.g. real-time monitoring of infectious diseases in population .	1-35	1-35	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	Automatically recognising medical concepts mentioned in social media messages ( e.g. tweets ) enables several applications for enhancing health quality of people in a community , e.g. real-time monitoring of infectious diseases in population .	1<2	none	elab-example	elab-example
P16-1096_anno1	36-44,52-56	77-91	However , the discrepancy between the type of language <*> poses a major challenge .	In this work , we handle the medical concept normalisation at the semantic level .	However , the discrepancy between the type of language <*> poses a major challenge .	In this work , we handle the medical concept normalisation at the semantic level .	36-56	77-91	However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge .	In this work , we handle the medical concept normalisation at the semantic level .	1>2	none	bg-goal	bg-goal
P16-1096_anno1	36-44,52-56	45-51	However , the discrepancy between the type of language <*> poses a major challenge .	used in social media and medical ontologies	However , the discrepancy between the type of language <*> poses a major challenge .	used in social media and medical ontologies	36-56	36-56	However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge .	However , the discrepancy between the type of language used in social media and medical ontologies poses a major challenge .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	57-62	77-91	Existing studies deal with this challenge	In this work , we handle the medical concept normalisation at the semantic level .	Existing studies deal with this challenge	In this work , we handle the medical concept normalisation at the semantic level .	57-76	77-91	Existing studies deal with this challenge by employing techniques , such as lexical term matching and statistical machine translation .	In this work , we handle the medical concept normalisation at the semantic level .	1>2	none	bg-compare	bg-compare
P16-1096_anno1	57-62	63-66	Existing studies deal with this challenge	by employing techniques ,	Existing studies deal with this challenge	by employing techniques ,	57-76	57-76	Existing studies deal with this challenge by employing techniques , such as lexical term matching and statistical machine translation .	Existing studies deal with this challenge by employing techniques , such as lexical term matching and statistical machine translation .	1<2	none	manner-means	manner-means
P16-1096_anno1	63-66	67-76	by employing techniques ,	such as lexical term matching and statistical machine translation .	by employing techniques ,	such as lexical term matching and statistical machine translation .	57-76	57-76	Existing studies deal with this challenge by employing techniques , such as lexical term matching and statistical machine translation .	Existing studies deal with this challenge by employing techniques , such as lexical term matching and statistical machine translation .	1<2	none	elab-example	elab-example
P16-1096_anno1	77-91	92-98	In this work , we handle the medical concept normalisation at the semantic level .	We investigate the use of neural networks	In this work , we handle the medical concept normalisation at the semantic level .	We investigate the use of neural networks	77-91	92-127	In this work , we handle the medical concept normalisation at the semantic level .	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	92-98	99-106	We investigate the use of neural networks	to learn the transition between layman 's language	We investigate the use of neural networks	to learn the transition between layman's language	92-127	92-127	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	1<2	none	enablement	enablement
P16-1096_anno1	99-106	107-111	to learn the transition between layman 's language	used in social media messages	to learn the transition between layman's language	used in social media messages	92-127	92-127	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	99-106	112-115	to learn the transition between layman 's language	and formal medical language	to learn the transition between layman's language	and formal medical language	92-127	92-127	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	1<2	none	joint	joint
P16-1096_anno1	112-115	116-127	and formal medical language	used in the descriptions of medical concepts in a standard ontology .	and formal medical language	used in the descriptions of medical concepts in a standard ontology .	92-127	92-127	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	We investigate the use of neural networks to learn the transition between layman 's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	128-131	154-165	We evaluate our approaches	that our proposed approaches significantly and consistently outperform existing effective baselines ,	We evaluate our approaches	that our proposed approaches significantly and consistently outperform existing effective baselines ,	128-149	150-182	We evaluate our approaches using three different datasets , where social media texts are extracted from Twitter messages and blog posts .	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	1>2	none	result	result
P16-1096_anno1	128-131	132-136	We evaluate our approaches	using three different datasets ,	We evaluate our approaches	using three different datasets ,	128-149	128-149	We evaluate our approaches using three different datasets , where social media texts are extracted from Twitter messages and blog posts .	We evaluate our approaches using three different datasets , where social media texts are extracted from Twitter messages and blog posts .	1<2	none	manner-means	manner-means
P16-1096_anno1	132-136	137-149	using three different datasets ,	where social media texts are extracted from Twitter messages and blog posts .	using three different datasets ,	where social media texts are extracted from Twitter messages and blog posts .	128-149	128-149	We evaluate our approaches using three different datasets , where social media texts are extracted from Twitter messages and blog posts .	We evaluate our approaches using three different datasets , where social media texts are extracted from Twitter messages and blog posts .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	150-153	154-165	Our experimental results show	that our proposed approaches significantly and consistently outperform existing effective baselines ,	Our experimental results show	that our proposed approaches significantly and consistently outperform existing effective baselines ,	150-182	150-182	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	1>2	none	attribution	attribution
P16-1096_anno1	77-91	154-165	In this work , we handle the medical concept normalisation at the semantic level .	that our proposed approaches significantly and consistently outperform existing effective baselines ,	In this work , we handle the medical concept normalisation at the semantic level .	that our proposed approaches significantly and consistently outperform existing effective baselines ,	77-91	150-182	In this work , we handle the medical concept normalisation at the semantic level .	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	1<2	none	evaluation	evaluation
P16-1096_anno1	154-165	166-176	that our proposed approaches significantly and consistently outperform existing effective baselines ,	which achieved state-of-the-art performance on several medical concept normalisation tasks ,	that our proposed approaches significantly and consistently outperform existing effective baselines ,	which achieved state-of-the-art performance on several medical concept normalisation tasks ,	150-182	150-182	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	1<2	none	elab-addition	elab-addition
P16-1096_anno1	166-176	177-182	which achieved state-of-the-art performance on several medical concept normalisation tasks ,	by up to 44 % .	which achieved state-of-the-art performance on several medical concept normalisation tasks ,	by up to 44 % .	150-182	150-182	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines , which achieved state-of-the-art performance on several medical concept normalisation tasks , by up to 44 % .	1<2	none	elab-addition	elab-addition
P16-1097_anno1	1-5	6-15	We introduce an agreement-based approach	to learning parallel lexicons and phrases from non-parallel corpora .	We introduce an agreement-based approach	to learning parallel lexicons and phrases from non-parallel corpora .	1-15	1-15	We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora .	We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora .	1<2	none	enablement	enablement
P16-1097_anno1	1-5	16-26	We introduce an agreement-based approach	The basic idea is to encourage two asymmetric latent-variable translation models	We introduce an agreement-based approach	The basic idea is to encourage two asymmetric latent-variable translation models	1-15	16-43	We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora .	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	1<2	none	elab-addition	elab-addition
P16-1097_anno1	16-26	27-33	The basic idea is to encourage two asymmetric latent-variable translation models	( i.e. , source-to-target and target-to-source )	The basic idea is to encourage two asymmetric latent-variable translation models	( i.e. , source-to-target and target-to-source )	16-43	16-43	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	1<2	none	elab-enumember	elab-enumember
P16-1097_anno1	16-26	34-43	The basic idea is to encourage two asymmetric latent-variable translation models	to agree on identifying latent phrase and word alignments .	The basic idea is to encourage two asymmetric latent-variable translation models	to agree on identifying latent phrase and word alignments .	16-43	16-43	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	1<2	none	enablement	enablement
P16-1097_anno1	34-43	44-54	to agree on identifying latent phrase and word alignments .	The agreement is defined at both word and phrase levels .	to agree on identifying latent phrase and word alignments .	The agreement is defined at both word and phrase levels .	16-43	44-54	The basic idea is to encourage two asymmetric latent-variable translation models ( i.e. , source-to-target and target-to-source ) to agree on identifying latent phrase and word alignments .	The agreement is defined at both word and phrase levels .	1<2	none	elab-addition	elab-addition
P16-1097_anno1	1-5	55-60	We introduce an agreement-based approach	We develop a Viterbi EM algorithm	We introduce an agreement-based approach	We develop a Viterbi EM algorithm	1-15	55-69	We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora .	We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently .	1<2	none	elab-addition	elab-addition
P16-1097_anno1	55-60	61-69	We develop a Viterbi EM algorithm	for jointly training the two unidirectional models efficiently .	We develop a Viterbi EM algorithm	for jointly training the two unidirectional models efficiently .	55-69	55-69	We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently .	We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently .	1<2	none	enablement	enablement
P16-1097_anno1	70-75	76-86	Experiments on the ChineseEnglish dataset show	that agreementbased learning significantly improves both alignment and translation performance .	Experiments on the ChineseEnglish dataset show	that agreementbased learning significantly improves both alignment and translation performance .	70-86	70-86	Experiments on the ChineseEnglish dataset show that agreementbased learning significantly improves both alignment and translation performance .	Experiments on the ChineseEnglish dataset show that agreementbased learning significantly improves both alignment and translation performance .	1>2	none	attribution	attribution
P16-1097_anno1	1-5	76-86	We introduce an agreement-based approach	that agreementbased learning significantly improves both alignment and translation performance .	We introduce an agreement-based approach	that agreementbased learning significantly improves both alignment and translation performance .	1-15	70-86	We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora .	Experiments on the ChineseEnglish dataset show that agreementbased learning significantly improves both alignment and translation performance .	1<2	none	evaluation	evaluation
P16-1098_anno1	1-6	19-33	Recently , there is rising interest	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Recently , there is rising interest	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	1-18	19-47	Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	1>2	none	bg-goal	bg-goal
P16-1098_anno1	1-6	7-18	Recently , there is rising interest	in modelling the interactions of text pair with deep neural networks .	Recently , there is rising interest	in modelling the interactions of text pair with deep neural networks .	1-18	1-18	Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .	Recently , there is rising interest in modelling the interactions of text pair with deep neural networks .	1<2	none	elab-addition	elab-addition
P16-1098_anno1	19-33	34-47	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	to model the strong interaction of text pair in a recursive matching way .	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	to model the strong interaction of text pair in a recursive matching way .	19-47	19-47	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	1<2	none	enablement	enablement
P16-1098_anno1	19-33	48-56	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Specifically , DF-LSTMs consist of two interdependent LSTMs ,	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Specifically , DF-LSTMs consist of two interdependent LSTMs ,	19-47	48-68	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	Specifically , DF-LSTMs consist of two interdependent LSTMs , each of which models a sequence under the influence of another .	1<2	none	elab-addition	elab-addition
P16-1098_anno1	48-56	57-68	Specifically , DF-LSTMs consist of two interdependent LSTMs ,	each of which models a sequence under the influence of another .	Specifically , DF-LSTMs consist of two interdependent LSTMs ,	each of which models a sequence under the influence of another .	48-68	48-68	Specifically , DF-LSTMs consist of two interdependent LSTMs , each of which models a sequence under the influence of another .	Specifically , DF-LSTMs consist of two interdependent LSTMs , each of which models a sequence under the influence of another .	1<2	none	elab-addition	elab-addition
P16-1098_anno1	19-33	69-73	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	We also use external memory	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	We also use external memory	19-47	69-88	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .	1<2	none	elab-addition	elab-addition
P16-1098_anno1	69-73	74-80	We also use external memory	to increase the capacity of LSTMs ,	We also use external memory	to increase the capacity of LSTMs ,	69-88	69-88	We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .	We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .	1<2	none	enablement	enablement
P16-1098_anno1	69-73	81-88	We also use external memory	thereby possibly capturing more complicated matching patterns .	We also use external memory	thereby possibly capturing more complicated matching patterns .	69-88	69-88	We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .	We also use external memory to increase the capacity of LSTMs , thereby possibly capturing more complicated matching patterns .	1<2	none	cause	cause
P16-1098_anno1	19-33	89-102	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Experiments on two very large datasets demonstrate the efficacy of our proposed architecture .	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Experiments on two very large datasets demonstrate the efficacy of our proposed architecture .	19-47	89-102	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	Experiments on two very large datasets demonstrate the efficacy of our proposed architecture .	1<2	none	evaluation	evaluation
P16-1098_anno1	19-33	103-114	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Furthermore , we present an elaborate qualitative analysis of our models ,	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs )	Furthermore , we present an elaborate qualitative analysis of our models ,	19-47	103-123	In this paper , we propose a model of deep fusion LSTMs ( DF-LSTMs ) to model the strong interaction of text pair in a recursive matching way .	Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .	1<2	none	evaluation	evaluation
P16-1098_anno1	103-114	115-118	Furthermore , we present an elaborate qualitative analysis of our models ,	giving an intuitive understanding	Furthermore , we present an elaborate qualitative analysis of our models ,	giving an intuitive understanding	103-123	103-123	Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .	Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .	1<2	none	elab-addition	elab-addition
P16-1098_anno1	115-118	119-123	giving an intuitive understanding	how our model worked .	giving an intuitive understanding	how our model worked .	103-123	103-123	Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .	Furthermore , we present an elaborate qualitative analysis of our models , giving an intuitive understanding how our model worked .	1<2	none	elab-addition	elab-addition
P16-1099_anno1	1-7	8-14	We construct a humans-in-the-loop supervised learning framework	that integrates crowdsourcing feedback and local knowledge	We construct a humans-in-the-loop supervised learning framework	that integrates crowdsourcing feedback and local knowledge	1-24	1-24	We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts .	We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts .	1<2	none	elab-addition	elab-addition
P16-1099_anno1	1-7	15-24	We construct a humans-in-the-loop supervised learning framework	to detect job-related tweets from individual and business accounts .	We construct a humans-in-the-loop supervised learning framework	to detect job-related tweets from individual and business accounts .	1-24	1-24	We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts .	We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts .	1<2	none	enablement	enablement
P16-1099_anno1	25-28	29-33	Using data-driven ethnography ,	we examine discourse about work	Using data-driven ethnography ,	we examine discourse about work	25-47	25-47	Using data-driven ethnography , we examine discourse about work by fusing languagebased analysis with temporal , geospational , and labor statistics information .	Using data-driven ethnography , we examine discourse about work by fusing languagebased analysis with temporal , geospational , and labor statistics information .	1>2	none	manner-means	manner-means
P16-1099_anno1	1-7	29-33	We construct a humans-in-the-loop supervised learning framework	we examine discourse about work	We construct a humans-in-the-loop supervised learning framework	we examine discourse about work	1-24	25-47	We construct a humans-in-the-loop supervised learning framework that integrates crowdsourcing feedback and local knowledge to detect job-related tweets from individual and business accounts .	Using data-driven ethnography , we examine discourse about work by fusing languagebased analysis with temporal , geospational , and labor statistics information .	1<2	none	evaluation	evaluation
P16-1099_anno1	29-33	34-47	we examine discourse about work	by fusing languagebased analysis with temporal , geospational , and labor statistics information .	we examine discourse about work	by fusing languagebased analysis with temporal , geospational , and labor statistics information .	25-47	25-47	Using data-driven ethnography , we examine discourse about work by fusing languagebased analysis with temporal , geospational , and labor statistics information .	Using data-driven ethnography , we examine discourse about work by fusing languagebased analysis with temporal , geospational , and labor statistics information .	1<2	none	manner-means	manner-means
P16-1100_anno1	1-22	29-35	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method	This paper presents a novel wordcharacter solution	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method	This paper presents a novel wordcharacter solution	1-28	29-41	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method to patch in unknown words .	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	1>2	none	bg-compare	bg-compare
P16-1100_anno1	1-22	23-28	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method	to patch in unknown words .	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method	to patch in unknown words .	1-28	1-28	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method to patch in unknown words .	Nearly all previous work on neural machine translation ( NMT ) has used quite restricted vocabularies , perhaps with a subsequent method to patch in unknown words .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	29-35	36-41	This paper presents a novel wordcharacter solution	to achieving open vocabulary NMT .	This paper presents a novel wordcharacter solution	to achieving open vocabulary NMT .	29-41	29-41	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	29-35	42-45	This paper presents a novel wordcharacter solution	We build hybrid systems	This paper presents a novel wordcharacter solution	We build hybrid systems	29-41	42-61	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	We build hybrid systems that translate mostly at the word level and consult the character components for rare words .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	42-45	46-52	We build hybrid systems	that translate mostly at the word level	We build hybrid systems	that translate mostly at the word level	42-61	42-61	We build hybrid systems that translate mostly at the word level and consult the character components for rare words .	We build hybrid systems that translate mostly at the word level and consult the character components for rare words .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	42-45	53-61	We build hybrid systems	and consult the character components for rare words .	We build hybrid systems	and consult the character components for rare words .	42-61	42-61	We build hybrid systems that translate mostly at the word level and consult the character components for rare words .	We build hybrid systems that translate mostly at the word level and consult the character components for rare words .	1<2	none	joint	joint
P16-1100_anno1	29-35	62-70	This paper presents a novel wordcharacter solution	Our character-level recurrent neural networks compute source word representations	This paper presents a novel wordcharacter solution	Our character-level recurrent neural networks compute source word representations	29-41	62-78	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	62-70	71-75	Our character-level recurrent neural networks compute source word representations	and recover unknown target words	Our character-level recurrent neural networks compute source word representations	and recover unknown target words	62-78	62-78	Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed .	Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed .	1<2	none	joint	joint
P16-1100_anno1	71-75	76-78	and recover unknown target words	when needed .	and recover unknown target words	when needed .	62-78	62-78	Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed .	Our character-level recurrent neural networks compute source word representations and recover unknown target words when needed .	1<2	none	temporal	temporal
P16-1100_anno1	29-35	79-94	This paper presents a novel wordcharacter solution	The twofold advantage of such a hybrid approach is that it is much faster and easier	This paper presents a novel wordcharacter solution	The twofold advantage of such a hybrid approach is that it is much faster and easier	29-41	79-118	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones ; at the same time , it never produces unknown words as in the case of word-based models .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	79-94	95-100	The twofold advantage of such a hybrid approach is that it is much faster and easier	to train than character-based ones ;	The twofold advantage of such a hybrid approach is that it is much faster and easier	to train than character-based ones ;	79-118	79-118	The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones ; at the same time , it never produces unknown words as in the case of word-based models .	The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones ; at the same time , it never produces unknown words as in the case of word-based models .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	79-94	101-118	The twofold advantage of such a hybrid approach is that it is much faster and easier	at the same time , it never produces unknown words as in the case of word-based models .	The twofold advantage of such a hybrid approach is that it is much faster and easier	at the same time , it never produces unknown words as in the case of word-based models .	79-118	79-118	The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones ; at the same time , it never produces unknown words as in the case of word-based models .	The twofold advantage of such a hybrid approach is that it is much faster and easier to train than character-based ones ; at the same time , it never produces unknown words as in the case of word-based models .	1<2	none	joint	joint
P16-1100_anno1	29-35	119-143	This paper presents a novel wordcharacter solution	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models	This paper presents a novel wordcharacter solution	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models	29-41	119-149	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models that already handle unknown words .	1<2	none	evaluation	evaluation
P16-1100_anno1	119-143	144-149	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models	that already handle unknown words .	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models	that already handle unknown words .	119-149	119-149	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models that already handle unknown words .	On the WMT '15 English to Czech translation task , this hybrid approach offers an addition boost of 2.1 to 11.4 BLEU points over models that already handle unknown words .	1<2	none	elab-addition	elab-addition
P16-1100_anno1	29-35	150-162	This paper presents a novel wordcharacter solution	Our best system achieves a new state-of-the-art result with 20.7 BLEU score .	This paper presents a novel wordcharacter solution	Our best system achieves a new state-of-the-art result with 20.7 BLEU score .	29-41	150-162	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	Our best system achieves a new state-of-the-art result with 20.7 BLEU score .	1<2	none	evaluation	evaluation
P16-1100_anno1	163-164	165-189	We demonstrate	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	We demonstrate	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	163-199	163-199	We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary , but also build correct representations for English source words .	We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary , but also build correct representations for English source words .	1>2	none	attribution	attribution
P16-1100_anno1	29-35	165-189	This paper presents a novel wordcharacter solution	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	This paper presents a novel wordcharacter solution	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	29-41	163-199	This paper presents a novel wordcharacter solution to achieving open vocabulary NMT .	We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary , but also build correct representations for English source words .	1<2	none	evaluation	evaluation
P16-1100_anno1	165-189	190-199	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	but also build correct representations for English source words .	that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary ,	but also build correct representations for English source words .	163-199	163-199	We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary , but also build correct representations for English source words .	We demonstrate that our character models can successfully learn to not only generate well-formed words for Czech , a highly-inflected language with a very complex vocabulary , but also build correct representations for English source words .	1<2	none	joint	joint
P86-1003	1-13,18-19	29-36	Tense , temporal adverbs , and temporal connectives provide information about when events <*> occur .	it must be parsed into a semantic representation	Tense , temporal adverbs , and temporal connectives provide information about when events <*> occur .	it must be parsed into a semantic representation	1-19	20-50	Tense , temporal adverbs , and temporal connectives provide information about when events described in English sentences occur .	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	1>2	none	bg-goal	bg-goal
P86-1003	1-13,18-19	14-17	Tense , temporal adverbs , and temporal connectives provide information about when events <*> occur .	described in English sentences	Tense , temporal adverbs , and temporal connectives provide information about when events <*> occur .	described in English sentences	1-19	1-19	Tense , temporal adverbs , and temporal connectives provide information about when events described in English sentences occur .	Tense , temporal adverbs , and temporal connectives provide information about when events described in English sentences occur .	1<2	none	elab-addition	elab-addition
P86-1003	20-28	29-36	To extract this temporal information from a sentence ,	it must be parsed into a semantic representation	To extract this temporal information from a sentence ,	it must be parsed into a semantic representation	20-50	20-50	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	1>2	none	enablement	enablement
P86-1003	29-36	37-50	it must be parsed into a semantic representation	which captures the meaning of tense , temporal adverbs , and temporal connectives .	it must be parsed into a semantic representation	which captures the meaning of tense , temporal adverbs , and temporal connectives .	20-50	20-50	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	1<2	none	elab-addition	elab-addition
P86-1003	29-36	51-71	it must be parsed into a semantic representation	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	it must be parsed into a semantic representation	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	20-50	51-71	To extract this temporal information from a sentence , it must be parsed into a semantic representation which captures the meaning of tense , temporal adverbs , and temporal connectives .	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	1<2	none	evaluation	evaluation
P86-1003	51-71	72-75	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	Five criteria were suggested	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	Five criteria were suggested	51-71	72-90	Representations were developed for the basic tenses , some temporal adverbs , as well as some of the temporal connectives .	Five criteria were suggested for judging these representations , and based on these criteria the representations were judged .	1<2	none	manner-means	manner-means
P86-1003	72-75	76-80	Five criteria were suggested	for judging these representations ,	Five criteria were suggested	for judging these representations ,	72-90	72-90	Five criteria were suggested for judging these representations , and based on these criteria the representations were judged .	Five criteria were suggested for judging these representations , and based on these criteria the representations were judged .	1<2	none	enablement	enablement
P86-1003	72-75	81-90	Five criteria were suggested	and based on these criteria the representations were judged .	Five criteria were suggested	and based on these criteria the representations were judged .	72-90	72-90	Five criteria were suggested for judging these representations , and based on these criteria the representations were judged .	Five criteria were suggested for judging these representations , and based on these criteria the representations were judged .	1<2	none	joint	joint
P86-1004	1-15	16-21	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	for processing natural language messages .	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	for processing natural language messages .	1-21	1-21	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	1<2	none	enablement	enablement
P86-1004	22-24	25-28	1 PUNDIT ,	written in Prolog ,	1 PUNDIT ,	written in Prolog ,	22-43	22-43	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	1<2	none	elab-addition	elab-addition
P86-1004	1-15	22-24,29-33	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	<*> 1 PUNDIT , <*> is a highly modular system	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	1 PUNDIT , <*> is a highly modular system	1-21	22-43	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	1<2	none	elab-aspect	elab-aspect
P86-1004	22-24,29-33	34-43	<*> 1 PUNDIT , <*> is a highly modular system	consisting of distinct syntactic , semantic and pragmatics components .	1 PUNDIT , <*> is a highly modular system	consisting of distinct syntactic , semantic and pragmatics components .	22-43	22-43	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	1<2	none	elab-addition	elab-addition
P86-1004	22-24,29-33	44-54	<*> 1 PUNDIT , <*> is a highly modular system	Each component draws on one or more sets of data ,	1 PUNDIT , <*> is a highly modular system	Each component draws on one or more sets of data ,	22-43	44-81	1 PUNDIT , written in Prolog , is a highly modular system consisting of distinct syntactic , semantic and pragmatics components .	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	1<2	none	elab-aspect	elab-aspect
P86-1004	44-54	55-69	Each component draws on one or more sets of data ,	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	Each component draws on one or more sets of data ,	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	44-81	44-81	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	1<2	none	elab-addition	elab-addition
P86-1004	55-69	70-76	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	mapping between syntactic and semantic constituents ,	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	mapping between syntactic and semantic constituents ,	44-81	44-81	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	1<2	none	elab-addition	elab-addition
P86-1004	55-69	77-81	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	and a domain model .	including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules	and a domain model .	44-81	44-81	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	Each component draws on one or more sets of data , including a lexicon , a broad-coverage grammar of EngLish , semantic verb decompositions , rules mapping between syntactic and semantic constituents , and a domain model .	1<2	none	joint	joint
P86-1004	1-15	82-94	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	This paper discusses the communication between the syntactic , semantic and pragmatic modules	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	This paper discusses the communication between the syntactic , semantic and pragmatic modules	1-21	82-104	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	This paper discusses the communication between the syntactic , semantic and pragmatic modules that is necessary for making implicit linguistic information explicit .	1<2	none	progression	progression
P86-1004	82-94	95-104	This paper discusses the communication between the syntactic , semantic and pragmatic modules	that is necessary for making implicit linguistic information explicit .	This paper discusses the communication between the syntactic , semantic and pragmatic modules	that is necessary for making implicit linguistic information explicit .	82-104	82-104	This paper discusses the communication between the syntactic , semantic and pragmatic modules that is necessary for making implicit linguistic information explicit .	This paper discusses the communication between the syntactic , semantic and pragmatic modules that is necessary for making implicit linguistic information explicit .	1<2	none	elab-addition	elab-addition
P86-1004	82-94	105-119	This paper discusses the communication between the syntactic , semantic and pragmatic modules	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities ,	This paper discusses the communication between the syntactic , semantic and pragmatic modules	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities ,	82-104	105-142	This paper discusses the communication between the syntactic , semantic and pragmatic modules that is necessary for making implicit linguistic information explicit .	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	1<2	none	elab-addition	elab-addition
P86-1004	105-119	120-128	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities ,	so that they can be labelled as such ,	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities ,	so that they can be labelled as such ,	105-142	105-142	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	1<2	none	cause	cause
P86-1004	120-128	129-134	so that they can be labelled as such ,	and referenee resolution can be directed	so that they can be labelled as such ,	and referenee resolution can be directed	105-142	105-142	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	1<2	none	joint	joint
P86-1004	129-134	135-142	and referenee resolution can be directed	to find specific referents for the entities .	and referenee resolution can be directed	to find specific referents for the entities .	105-142	105-142	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	The key is letting syntax and semantics recognize missing linguistic entities as implicit entities , so that they can be labelled as such , and referenee resolution can be directed to find specific referents for the entities .	1<2	none	enablement	enablement
P86-1004	143-147	148-153	In this way the task	of making implicit linguistic information explicit	In this way the task	of making implicit linguistic information explicit	143-164	143-164	In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution .	In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution .	1<2	none	elab-addition	elab-addition
P86-1004	1-15	143-147,154-159	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	<*> In this way the task <*> becomes a subset of the tasks	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	In this way the task <*> becomes a subset of the tasks	1-21	143-164	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution .	1<2	none	elab-aspect	elab-aspect
P86-1004	143-147,154-159	160-164	<*> In this way the task <*> becomes a subset of the tasks	performed by reference resolution .	In this way the task <*> becomes a subset of the tasks	performed by reference resolution .	143-164	143-164	In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution .	In this way the task of making implicit linguistic information explicit becomes a subset of the tasks performed by reference resolution .	1<2	none	elab-addition	elab-addition
P86-1004	1-15	165-178	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	The success of this approach is dependent on marking missing syntactic constituents as elided	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system	The success of this approach is dependent on marking missing syntactic constituents as elided	1-21	165-196	This paper describes the SDC PUNDIT , ( Prolog UNDerstands Integrated Text ) , system for processing natural language messages .	The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents .	1<2	none	evaluation	evaluation
P86-1004	165-178	179-184	The success of this approach is dependent on marking missing syntactic constituents as elided	and missing semantic roles as ESSENTIAL	The success of this approach is dependent on marking missing syntactic constituents as elided	and missing semantic roles as ESSENTIAL	165-196	165-196	The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents .	The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents .	1<2	none	joint	joint
P86-1004	165-178	185-196	The success of this approach is dependent on marking missing syntactic constituents as elided	so that reference resolution can know when to look for referents .	The success of this approach is dependent on marking missing syntactic constituents as elided	so that reference resolution can know when to look for referents .	165-196	165-196	The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents .	The success of this approach is dependent on marking missing syntactic constituents as elided and missing semantic roles as ESSENTIAL so that reference resolution can know when to look for referents .	1<2	none	cause	cause
P86-1005	1-3	4-27	We discuss ways	of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases	We discuss ways	of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases	1-32	1-32	We discuss ways of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases known to the system .	We discuss ways of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases known to the system .	1<2	none	elab-addition	elab-addition
P86-1005	4-27	28-32	of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases	known to the system .	of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases	known to the system .	1-32	1-32	We discuss ways of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases known to the system .	We discuss ways of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases known to the system .	1<2	none	elab-addition	elab-addition
P86-1005	33-55	76-85	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) ,	However , our techniques enable the design of customization modules	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) ,	However , our techniques enable the design of customization modules	33-75	76-104	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) , which answers English questions about tabular ( first normal-form ) data files and runs on a Symbolics Lisp Machine .	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	1>2	none	contrast	contrast
P86-1005	33-55	56-67	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) ,	which answers English questions about tabular ( first normal-form ) data files	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) ,	which answers English questions about tabular ( first normal-form ) data files	33-75	33-75	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) , which answers English questions about tabular ( first normal-form ) data files and runs on a Symbolics Lisp Machine .	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) , which answers English questions about tabular ( first normal-form ) data files and runs on a Symbolics Lisp Machine .	1<2	none	elab-addition	elab-addition
P86-1005	56-67	68-75	which answers English questions about tabular ( first normal-form ) data files	and runs on a Symbolics Lisp Machine .	which answers English questions about tabular ( first normal-form ) data files	and runs on a Symbolics Lisp Machine .	33-75	33-75	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) , which answers English questions about tabular ( first normal-form ) data files and runs on a Symbolics Lisp Machine .	An implementation of this work forms a critical portion of the knowledge acquisition component of our Transportable English-Language Interface ( TELl ) , which answers English questions about tabular ( first normal-form ) data files and runs on a Symbolics Lisp Machine .	1<2	none	joint	joint
P86-1005	1-3	76-85	We discuss ways	However , our techniques enable the design of customization modules	We discuss ways	However , our techniques enable the design of customization modules	1-32	76-104	We discuss ways of allowing the users of a natural language processor to define , examine , and modify the definitions of any domain-specific words or phrases known to the system .	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	1<2	none	evaluation	evaluation
P86-1005	76-85	86-99	However , our techniques enable the design of customization modules	that are largely independent of the syntactic and retrieval components of the specific system	However , our techniques enable the design of customization modules	that are largely independent of the syntactic and retrieval components of the specific system	76-104	76-104	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	1<2	none	elab-addition	elab-addition
P86-1005	86-99	100-104	that are largely independent of the syntactic and retrieval components of the specific system	they supply information to .	that are largely independent of the syntactic and retrieval components of the specific system	they supply information to .	76-104	76-104	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	1<2	none	elab-addition	elab-addition
P86-1005	76-85	105-118	However , our techniques enable the design of customization modules	In addition to its obvious practical value , this area of research is important	However , our techniques enable the design of customization modules	In addition to its obvious practical value , this area of research is important	76-104	105-144	However , our techniques enable the design of customization modules that are largely independent of the syntactic and retrieval components of the specific system they supply information to .	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	1<2	none	joint	joint
P86-1005	105-118	119-126	In addition to its obvious practical value , this area of research is important	because it requires careful attention to the formalisms	In addition to its obvious practical value , this area of research is important	because it requires careful attention to the formalisms	105-144	105-144	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	1<2	none	exp-reason	exp-reason
P86-1005	119-126	127-132	because it requires careful attention to the formalisms	used by a natural language system	because it requires careful attention to the formalisms	used by a natural language system	105-144	105-144	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	1<2	none	elab-addition	elab-addition
P86-1005	119-126	133-139	because it requires careful attention to the formalisms	and to the interactions among the modules	because it requires careful attention to the formalisms	and to the interactions among the modules	105-144	105-144	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	1<2	none	joint	joint
P86-1005	133-139	140-144	and to the interactions among the modules	based on those formalisms .	and to the interactions among the modules	based on those formalisms .	105-144	105-144	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	In addition to its obvious practical value , this area of research is important because it requires careful attention to the formalisms used by a natural language system and to the interactions among the modules based on those formalisms .	1<2	none	bg-general	bg-general
P86-1006	1-12	26-45	An important goal of computational linguistics has been to use linguistic theory	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	An important goal of computational linguistics has been to use linguistic theory	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	1-25	26-45	An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural language processing systems .	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	1>2	none	bg-goal	bg-goal
P86-1006	1-12	13-25	An important goal of computational linguistics has been to use linguistic theory	to guide the construction of computationally efficient real-world natural language processing systems .	An important goal of computational linguistics has been to use linguistic theory	to guide the construction of computationally efficient real-world natural language processing systems .	1-25	1-25	An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural language processing systems .	An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural language processing systems .	1<2	none	enablement	enablement
P86-1006	26-45	132-137	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	The paper pinpoints sources of complexity	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	The paper pinpoints sources of complexity	26-45	132-164	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	1>2	none	bg-goal	bg-goal
P86-1006	26-45	46-65	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	First , the precise formalisms of GPSG might be a direct and fransparent guide for parser design and implementation .	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	First , the precise formalisms of GPSG might be a direct and fransparent guide for parser design and implementation .	26-45	46-65	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	First , the precise formalisms of GPSG might be a direct and fransparent guide for parser design and implementation .	1<2	none	elab-enumember	elab-enumember
P86-1006	66-74	90-99	Second , since GPSG has weak context-free generative power	GPSG parsers would appear to run in polynomial time .	Second , since GPSG has weak context-free generative power	GPSG parsers would appear to run in polynomial time .	66-99	66-99	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	1>2	none	exp-reason	exp-reason
P86-1006	66-74	75-82	Second , since GPSG has weak context-free generative power	and context-free languages can be parsed in O(n)	Second , since GPSG has weak context-free generative power	and context-free languages can be parsed in O(n)	66-99	66-99	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	1<2	none	joint	joint
P86-1006	75-82	83-89	and context-free languages can be parsed in O(n)	by a wide range of algorithms ,	and context-free languages can be parsed in O(n)	by a wide range of algorithms ,	66-99	66-99	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	1<2	none	manner-means	manner-means
P86-1006	26-45	90-99	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	GPSG parsers would appear to run in polynomial time .	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	GPSG parsers would appear to run in polynomial time .	26-45	66-99	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	Second , since GPSG has weak context-free generative power and context-free languages can be parsed in O(n) by a wide range of algorithms , GPSG parsers would appear to run in polynomial time .	1<2	none	elab-enumember	elab-enumember
P86-1006	26-45	100-110	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	This widely-assumed GPSG " efficient parsability " result is misleading :	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	This widely-assumed GPSG " efficient parsability " result is misleading :	26-45	100-131	At first glance , generalized phrase structure grammar ( GPSG ) appears to be a blessing on two counts .	This widely-assumed GPSG " efficient parsability " result is misleading : here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	1<2	none	elab-addition	elab-addition
P86-1006	111-113	114-131	here we prove	that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	here we prove	that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	100-131	100-131	This widely-assumed GPSG " efficient parsability " result is misleading : here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	This widely-assumed GPSG " efficient parsability " result is misleading : here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	1>2	none	attribution	attribution
P86-1006	100-110	114-131	This widely-assumed GPSG " efficient parsability " result is misleading :	that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	This widely-assumed GPSG " efficient parsability " result is misleading :	that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	100-131	100-131	This widely-assumed GPSG " efficient parsability " result is misleading : here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	This widely-assumed GPSG " efficient parsability " result is misleading : here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard , and assuredly intractable .	1<2	none	elab-addition	elab-addition
P86-1006	132-137	138-147	The paper pinpoints sources of complexity	( e.g. metarules and the theory of syntactic features )	The paper pinpoints sources of complexity	( e.g. metarules and the theory of syntactic features )	132-164	132-164	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	1<2	none	elab-example	elab-example
P86-1006	132-137	148-152	The paper pinpoints sources of complexity	in the current GPSG theory	The paper pinpoints sources of complexity	in the current GPSG theory	132-164	132-164	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	1<2	none	elab-addition	elab-addition
P86-1006	132-137	153-164	The paper pinpoints sources of complexity	and concludes with some linguistically and computationally motivated restrictions on GPSG .	The paper pinpoints sources of complexity	and concludes with some linguistically and computationally motivated restrictions on GPSG .	132-164	132-164	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	The paper pinpoints sources of complexity ( e.g. metarules and the theory of syntactic features ) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG .	1<2	none	joint	joint
P86-1008	1-3	4-17	Taken abstractly ,	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	Taken abstractly ,	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	1-17	1-17	Taken abstractly , the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	Taken abstractly , the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	1>2	none	temporal	temporal
P86-1008	4-17	48-55	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	that natural-language problems may have a special structure	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	that natural-language problems may have a special structure	1-17	43-71	Taken abstractly , the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	1>2	none	contrast	contrast
P86-1008	4-17	18-42	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	For example , N + 1 small automata are sufficient to encode the Boolean satisfiability problem ( SAT ) for formulas in N variables .	the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	For example , N + 1 small automata are sufficient to encode the Boolean satisfiability problem ( SAT ) for formulas in N variables .	1-17	18-42	Taken abstractly , the two-level ( Kimmo ) morphological framework allows computationally difficult problems to arise .	For example , N + 1 small automata are sufficient to encode the Boolean satisfiability problem ( SAT ) for formulas in N variables .	1<2	none	elab-example	elab-example
P86-1008	43-47	48-55	However , the suspicion arises	that natural-language problems may have a special structure	However , the suspicion arises	that natural-language problems may have a special structure	43-71	43-71	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	1>2	none	attribution	attribution
P86-1008	48-55	102-112	that natural-language problems may have a special structure	it may be possible to solve the natural problems by methods	that natural-language problems may have a special structure	it may be possible to solve the natural problems by methods	43-71	97-119	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	1>2	none	bg-goal	bg-goal
P86-1008	48-55	56-61	that natural-language problems may have a special structure	-- not shared with SAT --	that natural-language problems may have a special structure	-- not shared with SAT --	43-71	43-71	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	1<2	none	elab-addition	elab-addition
P86-1008	48-55	62-71	that natural-language problems may have a special structure	that is not directly captured in the two-level model .	that natural-language problems may have a special structure	that is not directly captured in the two-level model .	43-71	43-71	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	1<2	none	elab-addition	elab-addition
P86-1008	48-55	72-85	that natural-language problems may have a special structure	In particular , the natural problems may generally have a modular and local nature	that natural-language problems may have a special structure	In particular , the natural problems may generally have a modular and local nature	43-71	72-96	However , the suspicion arises that natural-language problems may have a special structure -- not shared with SAT -- that is not directly captured in the two-level model .	In particular , the natural problems may generally have a modular and local nature that distinguishes them from more " global " SAT problems .	1<2	none	elab-aspect	elab-aspect
P86-1008	72-85	86-96	In particular , the natural problems may generally have a modular and local nature	that distinguishes them from more " global " SAT problems .	In particular , the natural problems may generally have a modular and local nature	that distinguishes them from more " global " SAT problems .	72-96	72-96	In particular , the natural problems may generally have a modular and local nature that distinguishes them from more " global " SAT problems .	In particular , the natural problems may generally have a modular and local nature that distinguishes them from more " global " SAT problems .	1<2	none	elab-addition	elab-addition
P86-1008	97-101	102-112	By exploiting this structure ,	it may be possible to solve the natural problems by methods	By exploiting this structure ,	it may be possible to solve the natural problems by methods	97-119	97-119	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	1>2	none	manner-means	manner-means
P86-1008	102-112	113-119	it may be possible to solve the natural problems by methods	that do not involve combinatorial search .	it may be possible to solve the natural problems by methods	that do not involve combinatorial search .	97-119	97-119	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	1<2	none	elab-addition	elab-addition
P86-1008	102-112	120-128	it may be possible to solve the natural problems by methods	We have explored this possibility in a preliminary way	it may be possible to solve the natural problems by methods	We have explored this possibility in a preliminary way	97-119	120-139	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	We have explored this possibility in a preliminary way by applying constraint propagation methods to Kimmo generation and recognition .	1<2	none	elab-aspect	elab-aspect
P86-1008	120-128	129-139	We have explored this possibility in a preliminary way	by applying constraint propagation methods to Kimmo generation and recognition .	We have explored this possibility in a preliminary way	by applying constraint propagation methods to Kimmo generation and recognition .	120-139	120-139	We have explored this possibility in a preliminary way by applying constraint propagation methods to Kimmo generation and recognition .	We have explored this possibility in a preliminary way by applying constraint propagation methods to Kimmo generation and recognition .	1<2	none	manner-means	manner-means
P86-1008	140-143	160-171	Constraint propagation can succeed	but it is insufficiently powerful to solve unnaturally hard SAT problems .	Constraint propagation can succeed	but it is insufficiently powerful to solve unnaturally hard SAT problems .	140-171	140-171	Constraint propagation can succeed when the solution falls into place step-by-step through a chain of limited and local inferences , but it is insufficiently powerful to solve unnaturally hard SAT problems .	Constraint propagation can succeed when the solution falls into place step-by-step through a chain of limited and local inferences , but it is insufficiently powerful to solve unnaturally hard SAT problems .	1>2	none	contrast	contrast
P86-1008	140-143	144-159	Constraint propagation can succeed	when the solution falls into place step-by-step through a chain of limited and local inferences ,	Constraint propagation can succeed	when the solution falls into place step-by-step through a chain of limited and local inferences ,	140-171	140-171	Constraint propagation can succeed when the solution falls into place step-by-step through a chain of limited and local inferences , but it is insufficiently powerful to solve unnaturally hard SAT problems .	Constraint propagation can succeed when the solution falls into place step-by-step through a chain of limited and local inferences , but it is insufficiently powerful to solve unnaturally hard SAT problems .	1<2	none	temporal	temporal
P86-1008	102-112	160-171	it may be possible to solve the natural problems by methods	but it is insufficiently powerful to solve unnaturally hard SAT problems .	it may be possible to solve the natural problems by methods	but it is insufficiently powerful to solve unnaturally hard SAT problems .	97-119	140-171	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	Constraint propagation can succeed when the solution falls into place step-by-step through a chain of limited and local inferences , but it is insufficiently powerful to solve unnaturally hard SAT problems .	1<2	none	elab-aspect	elab-aspect
P86-1008	172-174	175-190	Limited tests indicate	that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	Limited tests indicate	that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	172-190	172-190	Limited tests indicate that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	Limited tests indicate that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	1>2	none	attribution	attribution
P86-1008	102-112	175-190	it may be possible to solve the natural problems by methods	that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	it may be possible to solve the natural problems by methods	that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	97-119	172-190	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	Limited tests indicate that the constraint-propagation algorithm for Kimmo generation works for English , Turkish , and Warlpiri .	1<2	none	elab-aspect	elab-aspect
P86-1008	191-196	202-210	When applied to a Kimmo system	the algorithm succeeds on " easy " SAT problems	When applied to a Kimmo system	the algorithm succeeds on " easy " SAT problems	191-222	191-222	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	1>2	none	result	result
P86-1008	191-196	197-201	When applied to a Kimmo system	that encodes SAT problems ,	When applied to a Kimmo system	that encodes SAT problems ,	191-222	191-222	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	1<2	none	elab-addition	elab-addition
P86-1008	102-112	202-210	it may be possible to solve the natural problems by methods	the algorithm succeeds on " easy " SAT problems	it may be possible to solve the natural problems by methods	the algorithm succeeds on " easy " SAT problems	97-119	191-222	By exploiting this structure , it may be possible to solve the natural problems by methods that do not involve combinatorial search .	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	1<2	none	evaluation	evaluation
P86-1008	202-210	211-222	the algorithm succeeds on " easy " SAT problems	but fails ( as desired ) on " hard " problems .	the algorithm succeeds on " easy " SAT problems	but fails ( as desired ) on " hard " problems .	191-222	191-222	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	When applied to a Kimmo system that encodes SAT problems , the algorithm succeeds on " easy " SAT problems but fails ( as desired ) on " hard " problems .	1<2	none	contrast	contrast
P86-1009	1-27	28-36	Morphological analysis must take into account the spelling-change processes of a language as well as its possible configurations of stems , affixes , and inflectional markings .	The computational difficulty of the task can be clarified	Morphological analysis must take into account the spelling-change processes of a language as well as its possible configurations of stems , affixes , and inflectional markings .	The computational difficulty of the task can be clarified	1-27	28-44	Morphological analysis must take into account the spelling-change processes of a language as well as its possible configurations of stems , affixes , and inflectional markings .	The computational difficulty of the task can be clarified by investigating specific models of morphological processing .	1>2	none	bg-goal	bg-goal
P86-1009	28-36	37-44	The computational difficulty of the task can be clarified	by investigating specific models of morphological processing .	The computational difficulty of the task can be clarified	by investigating specific models of morphological processing .	28-44	28-44	The computational difficulty of the task can be clarified by investigating specific models of morphological processing .	The computational difficulty of the task can be clarified by investigating specific models of morphological processing .	1<2	none	manner-means	manner-means
P86-1009	45-66	71-78	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency ,	the model does not guarantee efficient processing .	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency ,	the model does not guarantee efficient processing .	45-78	45-78	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	1>2	none	contrast	contrast
P86-1009	67-70	71-78	but closer examination shows	the model does not guarantee efficient processing .	but closer examination shows	the model does not guarantee efficient processing .	45-78	45-78	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	1>2	none	attribution	attribution
P86-1009	28-36	71-78	The computational difficulty of the task can be clarified	the model does not guarantee efficient processing .	The computational difficulty of the task can be clarified	the model does not guarantee efficient processing .	28-44	45-78	The computational difficulty of the task can be clarified by investigating specific models of morphological processing .	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	1<2	none	evaluation	evaluation
P86-1009	79-84	85-102	Reductions of the satisfiability problem show	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	Reductions of the satisfiability problem show	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	79-102	79-102	Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	1>2	none	attribution	attribution
P86-1009	71-78	85-102	the model does not guarantee efficient processing .	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	the model does not guarantee efficient processing .	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	45-78	79-102	The use of finite-state machinery in the " twolevel " model by Kimmo Koskenniemi gives it the appearance of computational efficiency , but closer examination shows the model does not guarantee efficient processing .	Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	1<2	none	elab-aspect	elab-aspect
P86-1009	85-102	103-105	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	The difficulty increases	that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	The difficulty increases	79-102	103-115	Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult .	The difficulty increases if unrestricted deletions ( null characters ) are allowed .	1<2	none	elab-aspect	elab-aspect
P86-1009	103-105	106-115	The difficulty increases	if unrestricted deletions ( null characters ) are allowed .	The difficulty increases	if unrestricted deletions ( null characters ) are allowed .	103-115	103-115	The difficulty increases if unrestricted deletions ( null characters ) are allowed .	The difficulty increases if unrestricted deletions ( null characters ) are allowed .	1<2	none	condition	condition
P86-1010	1-13	14-21	Free-word order languages have long posed significant problems for standard parsing algorithms .	This paper reports on an implemented parser ,	Free-word order languages have long posed significant problems for standard parsing algorithms .	This paper reports on an implemented parser ,	1-13	14-52	Free-word order languages have long posed significant problems for standard parsing algorithms .	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	1>2	none	bg-goal	bg-goal
P86-1010	14-21	22-36	This paper reports on an implemented parser ,	based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) ,	This paper reports on an implemented parser ,	based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) ,	14-52	14-52	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	1<2	none	bg-general	bg-general
P86-1010	14-21	37-52	This paper reports on an implemented parser ,	for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	This paper reports on an implemented parser ,	for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	14-52	14-52	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	1<2	none	elab-addition	elab-addition
P86-1010	14-21	53-57	This paper reports on an implemented parser ,	The parser is explicitly designed	This paper reports on an implemented parser ,	The parser is explicitly designed	14-52	53-65	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	The parser is explicitly designed to transparently mirror the principles of GB .	1<2	none	elab-addition	elab-addition
P86-1010	53-57	58-65	The parser is explicitly designed	to transparently mirror the principles of GB .	The parser is explicitly designed	to transparently mirror the principles of GB .	53-65	53-65	The parser is explicitly designed to transparently mirror the principles of GB .	The parser is explicitly designed to transparently mirror the principles of GB .	1<2	none	enablement	enablement
P86-1010	14-21	66-92	This paper reports on an implemented parser ,	The operation of this parsing system is quite different in character from that of a rule-based parsing system , ~ e.g. , a context-free parsing method .	This paper reports on an implemented parser ,	The operation of this parsing system is quite different in character from that of a rule-based parsing system , ~ e.g. , a context-free parsing method .	14-52	66-92	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	The operation of this parsing system is quite different in character from that of a rule-based parsing system , ~ e.g. , a context-free parsing method .	1<2	none	elab-aspect	elab-aspect
P86-1010	14-21	93-117	This paper reports on an implemented parser ,	In this system , phrases are constructed via principles of selection , case-marking , caseassignment , and argument-linking , rather than by phrasal rules .	This paper reports on an implemented parser ,	In this system , phrases are constructed via principles of selection , case-marking , caseassignment , and argument-linking , rather than by phrasal rules .	14-52	93-117	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	In this system , phrases are constructed via principles of selection , case-marking , caseassignment , and argument-linking , rather than by phrasal rules .	1<2	none	elab-aspect	elab-aspect
P86-1010	14-21	118-135	This paper reports on an implemented parser ,	The output of the parser for a sample Warlpiri sentence of four words in length is given .	This paper reports on an implemented parser ,	The output of the parser for a sample Warlpiri sentence of four words in length is given .	14-52	118-135	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	The output of the parser for a sample Warlpiri sentence of four words in length is given .	1<2	none	elab-aspect	elab-aspect
P86-1010	136-150	157-160	The parser was executed on each of the 23 other permutations of the sentence ,	thereby demonstrating its ability	The parser was executed on each of the 23 other permutations of the sentence ,	thereby demonstrating its ability	136-171	136-171	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	1>2	none	exp-evidence	exp-evidence
P86-1010	136-150	151-156	The parser was executed on each of the 23 other permutations of the sentence ,	and it output equivalent parses ,	The parser was executed on each of the 23 other permutations of the sentence ,	and it output equivalent parses ,	136-171	136-171	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	1<2	none	joint	joint
P86-1010	14-21	157-160	This paper reports on an implemented parser ,	thereby demonstrating its ability	This paper reports on an implemented parser ,	thereby demonstrating its ability	14-52	136-171	This paper reports on an implemented parser , based on GovernmentBinding theory ( GB ) ( Chomsky , 1981 , 1982 ) , for a particular free-word order language , Warlpiri , an aboriginal language of central Australia .	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	1<2	none	evaluation	evaluation
P86-1010	157-160	161-167	thereby demonstrating its ability	to correctly handle the highly scrambled sentences	thereby demonstrating its ability	to correctly handle the highly scrambled sentences	136-171	136-171	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	1<2	none	enablement	enablement
P86-1010	161-167	168-171	to correctly handle the highly scrambled sentences	found in Warlpiri .	to correctly handle the highly scrambled sentences	found in Warlpiri .	136-171	136-171	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	The parser was executed on each of the 23 other permutations of the sentence , and it output equivalent parses , thereby demonstrating its ability to correctly handle the highly scrambled sentences found in Warlpiri .	1<2	none	elab-addition	elab-addition
P86-1011	1-10	18-28	We examine the relationship between the two grammatical formalisms :	We briefly investigate the weak equivalence of the two formalisms .	We examine the relationship between the two grammatical formalisms :	We briefly investigate the weak equivalence of the two formalisms .	1-17	18-28	We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .	We briefly investigate the weak equivalence of the two formalisms .	1>2	none	result	result
P86-1011	1-10	11-17	We examine the relationship between the two grammatical formalisms :	Tree Adjoining Grammars and Head Grammars .	We examine the relationship between the two grammatical formalisms :	Tree Adjoining Grammars and Head Grammars .	1-17	1-17	We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .	We examine the relationship between the two grammatical formalisms : Tree Adjoining Grammars and Head Grammars .	1<2	none	elab-enumember	elab-enumember
P86-1011	18-28	29-34	We briefly investigate the weak equivalence of the two formalisms .	We then turn to a discussion	We briefly investigate the weak equivalence of the two formalisms .	We then turn to a discussion	18-28	29-43	We briefly investigate the weak equivalence of the two formalisms .	We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .	1<2	none	progression	progression
P86-1011	29-34	35-43	We then turn to a discussion	comparing the linguistic expressiveness of the two formalisms .	We then turn to a discussion	comparing the linguistic expressiveness of the two formalisms .	29-43	29-43	We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .	We then turn to a discussion comparing the linguistic expressiveness of the two formalisms .	1<2	none	elab-addition	elab-addition
P86-1012	1-14	15-21	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	derived from those of Ades and Steedman	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	derived from those of Ades and Steedman	1-29	1-29	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	1<2	none	elab-addition	elab-addition
P86-1012	1-14	22-29	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	by varying the set of reduction rules .	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	by varying the set of reduction rules .	1-29	1-29	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	1<2	none	manner-means	manner-means
P86-1012	1-14	30-34,40-41	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	We characterize the reduction rules <*> as those	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	We characterize the reduction rules <*> as those	1-29	30-55	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction .	1<2	none	elab-aspect	elab-aspect
P86-1012	30-34,40-41	35-39	We characterize the reduction rules <*> as those	capable of generating context-sensitive languages	We characterize the reduction rules <*> as those	capable of generating context-sensitive languages	30-55	30-55	We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction .	We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction .	1<2	none	elab-addition	elab-addition
P86-1012	40-41	42-55	as those	having a partial combination rule and a combination rule in the reverse direction .	as those	having a partial combination rule and a combination rule in the reverse direction .	30-55	30-55	We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction .	We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction .	1<2	none	elab-addition	elab-addition
P86-1012	56-57	58-69	We show	that any categorial language is a permutation of some context-free language ,	We show	that any categorial language is a permutation of some context-free language ,	56-78	56-78	We show that any categorial language is a permutation of some context-free language , thus inheriting properties dependent on symbol counting only .	We show that any categorial language is a permutation of some context-free language , thus inheriting properties dependent on symbol counting only .	1>2	none	attribution	attribution
P86-1012	1-14	58-69	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	that any categorial language is a permutation of some context-free language ,	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	that any categorial language is a permutation of some context-free language ,	1-29	56-78	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	We show that any categorial language is a permutation of some context-free language , thus inheriting properties dependent on symbol counting only .	1<2	none	evaluation	evaluation
P86-1012	58-69	70-78	that any categorial language is a permutation of some context-free language ,	thus inheriting properties dependent on symbol counting only .	that any categorial language is a permutation of some context-free language ,	thus inheriting properties dependent on symbol counting only .	56-78	56-78	We show that any categorial language is a permutation of some context-free language , thus inheriting properties dependent on symbol counting only .	We show that any categorial language is a permutation of some context-free language , thus inheriting properties dependent on symbol counting only .	1<2	none	cause	cause
P86-1012	1-14	79-89	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	We compare some of their properties with other contemporary formalisms .	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars	We compare some of their properties with other contemporary formalisms .	1-29	79-89	We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steedman by varying the set of reduction rules .	We compare some of their properties with other contemporary formalisms .	1<2	none	elab-aspect	elab-aspect
P86-1013	1-13	17-22	Conjunctions have always been a source of problems for natural language parsers .	how these problems may be circumvented	Conjunctions have always been a source of problems for natural language parsers .	how these problems may be circumvented	1-13	14-31	Conjunctions have always been a source of problems for natural language parsers .	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	1>2	none	bg-goal	bg-goal
P86-1013	14-16	17-22	This paper shows	how these problems may be circumvented	This paper shows	how these problems may be circumvented	14-31	14-31	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	1>2	none	attribution	attribution
P86-1013	17-22	23-31	how these problems may be circumvented	using a rule based , walt-and-see parsing strategy .	how these problems may be circumvented	using a rule based , walt-and-see parsing strategy .	14-31	14-31	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	1<2	none	manner-means	manner-means
P86-1013	17-22	32-35	how these problems may be circumvented	A parser is presented	how these problems may be circumvented	A parser is presented	14-31	32-52	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	1<2	none	elab-aspect	elab-aspect
P86-1013	32-35	36-41	A parser is presented	which analyzes conjunction structures deterministically ,	A parser is presented	which analyzes conjunction structures deterministically ,	32-52	32-52	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	1<2	none	elab-addition	elab-addition
P86-1013	32-35	42-45,48-52	A parser is presented	and the specific rules <*> are described and illustrated .	A parser is presented	and the specific rules <*> are described and illustrated .	32-52	32-52	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	1<2	none	joint	joint
P86-1013	42-45,48-52	46-47	and the specific rules <*> are described and illustrated .	it uses	and the specific rules <*> are described and illustrated .	it uses	32-52	32-52	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	A parser is presented which analyzes conjunction structures deterministically , and the specific rules it uses are described and illustrated .	1<2	none	elab-addition	elab-addition
P86-1013	17-22	53-66	how these problems may be circumvented	This parser appears to be faster for conjunctions than other parsers in the literature	how these problems may be circumvented	This parser appears to be faster for conjunctions than other parsers in the literature	14-31	53-73	This paper shows how these problems may be circumvented using a rule based , walt-and-see parsing strategy .	This parser appears to be faster for conjunctions than other parsers in the literature and some comparative timings are given .	1<2	none	evaluation	evaluation
P86-1013	53-66	67-73	This parser appears to be faster for conjunctions than other parsers in the literature	and some comparative timings are given .	This parser appears to be faster for conjunctions than other parsers in the literature	and some comparative timings are given .	53-73	53-73	This parser appears to be faster for conjunctions than other parsers in the literature and some comparative timings are given .	This parser appears to be faster for conjunctions than other parsers in the literature and some comparative timings are given .	1<2	none	joint	joint
P86-1014	1-21	31-37	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken	However , this ignores those copying constructions	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken	However , this ignores those copying constructions	1-30	31-52	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken to mean that natural languages are slightly context-sensitive .	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	1>2	none	contrast	contrast
P86-1014	22-23	24-30	to mean	that natural languages are slightly context-sensitive .	to mean	that natural languages are slightly context-sensitive .	1-30	1-30	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken to mean that natural languages are slightly context-sensitive .	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken to mean that natural languages are slightly context-sensitive .	1>2	none	attribution	attribution
P86-1014	1-21	24-30	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken	that natural languages are slightly context-sensitive .	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken	that natural languages are slightly context-sensitive .	1-30	1-30	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken to mean that natural languages are slightly context-sensitive .	The documentation of ( unbounded-length ) copying and cross-serial constructions in a few languages in the recent literature is usually taken to mean that natural languages are slightly context-sensitive .	1<2	none	enablement	enablement
P86-1014	31-37	73-87	However , this ignores those copying constructions	that natural languages cannot be realistically represented by formal languages of the usual sort .	However , this ignores those copying constructions	that natural languages cannot be realistically represented by formal languages of the usual sort .	31-52	53-87	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	1>2	none	bg-goal	bg-goal
P86-1014	40-42	38-39,43-52	while productive ,	<*> which , <*> cannot be easily shown to apply to infinite sublanguages .	while productive ,	which , <*> cannot be easily shown to apply to infinite sublanguages .	31-52	31-52	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	1>2	none	temporal	temporal
P86-1014	31-37	38-39,43-52	However , this ignores those copying constructions	<*> which , <*> cannot be easily shown to apply to infinite sublanguages .	However , this ignores those copying constructions	which , <*> cannot be easily shown to apply to infinite sublanguages .	31-52	31-52	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	However , this ignores those copying constructions which , while productive , cannot be easily shown to apply to infinite sublanguages .	1<2	none	elab-addition	elab-addition
P86-1014	53-67	73-87	To allow such finite copying constructions to be taken into account in formal modeling ,	that natural languages cannot be realistically represented by formal languages of the usual sort .	To allow such finite copying constructions to be taken into account in formal modeling ,	that natural languages cannot be realistically represented by formal languages of the usual sort .	53-87	53-87	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	1>2	none	enablement	enablement
P86-1014	68-72	73-87	it is necessary to recognize	that natural languages cannot be realistically represented by formal languages of the usual sort .	it is necessary to recognize	that natural languages cannot be realistically represented by formal languages of the usual sort .	53-87	53-87	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	1>2	none	attribution	attribution
P86-1014	73-87	205-216	that natural languages cannot be realistically represented by formal languages of the usual sort .	A simple class of Context-free Queue Grammars is introduced and discussed .	that natural languages cannot be realistically represented by formal languages of the usual sort .	A simple class of Context-free Queue Grammars is introduced and discussed .	53-87	205-216	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	A simple class of Context-free Queue Grammars is introduced and discussed .	1>2	none	bg-goal	bg-goal
P86-1014	73-87	88-106	that natural languages cannot be realistically represented by formal languages of the usual sort .	Rather , they must be modeled as families of formal languages or as formal languages with indefinite vocabularies .	that natural languages cannot be realistically represented by formal languages of the usual sort .	Rather , they must be modeled as families of formal languages or as formal languages with indefinite vocabularies .	53-87	88-106	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	Rather , they must be modeled as families of formal languages or as formal languages with indefinite vocabularies .	1<2	none	joint	joint
P86-1014	107-111	112-125	Once this is done ,	we see copying as a truly pervasive and fundamental process in human language .	Once this is done ,	we see copying as a truly pervasive and fundamental process in human language .	107-125	107-125	Once this is done , we see copying as a truly pervasive and fundamental process in human language .	Once this is done , we see copying as a truly pervasive and fundamental process in human language .	1>2	none	condition	condition
P86-1014	73-87	112-125	that natural languages cannot be realistically represented by formal languages of the usual sort .	we see copying as a truly pervasive and fundamental process in human language .	that natural languages cannot be realistically represented by formal languages of the usual sort .	we see copying as a truly pervasive and fundamental process in human language .	53-87	107-125	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	Once this is done , we see copying as a truly pervasive and fundamental process in human language .	1<2	none	elab-aspect	elab-aspect
P86-1014	126-136	137-151	Furthermore , the absence of mirror-image constructions in human languages means	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	Furthermore , the absence of mirror-image constructions in human languages means	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	126-151	126-151	Furthermore , the absence of mirror-image constructions in human languages means that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	Furthermore , the absence of mirror-image constructions in human languages means that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	1>2	none	attribution	attribution
P86-1014	137-151	177-186	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	that human linguistic processes use queues rather than stacks ,	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	that human linguistic processes use queues rather than stacks ,	126-151	175-204	Furthermore , the absence of mirror-image constructions in human languages means that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	1>2	none	bg-general	bg-general
P86-1014	137-151	152-160	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	Instead , a class of grammars must be found	that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	Instead , a class of grammars must be found	126-151	152-174	Furthermore , the absence of mirror-image constructions in human languages means that it is not enough to extend Context-free Grammars in the direction of context-sensitivity .	Instead , a class of grammars must be found which handles ( context-sensitive ) copying but not ( context-free ) mirror images .	1<2	none	contrast	contrast
P86-1014	152-160	161-174	Instead , a class of grammars must be found	which handles ( context-sensitive ) copying but not ( context-free ) mirror images .	Instead , a class of grammars must be found	which handles ( context-sensitive ) copying but not ( context-free ) mirror images .	152-174	152-174	Instead , a class of grammars must be found which handles ( context-sensitive ) copying but not ( context-free ) mirror images .	Instead , a class of grammars must be found which handles ( context-sensitive ) copying but not ( context-free ) mirror images .	1<2	none	elab-addition	elab-addition
P86-1014	175-176	177-186	This suggests	that human linguistic processes use queues rather than stacks ,	This suggests	that human linguistic processes use queues rather than stacks ,	175-204	175-204	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	1>2	none	attribution	attribution
P86-1014	73-87	177-186	that natural languages cannot be realistically represented by formal languages of the usual sort .	that human linguistic processes use queues rather than stacks ,	that natural languages cannot be realistically represented by formal languages of the usual sort .	that human linguistic processes use queues rather than stacks ,	53-87	175-204	To allow such finite copying constructions to be taken into account in formal modeling , it is necessary to recognize that natural languages cannot be realistically represented by formal languages of the usual sort .	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	1<2	none	joint	joint
P86-1014	177-186	187-204	that human linguistic processes use queues rather than stacks ,	making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	that human linguistic processes use queues rather than stacks ,	making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	175-204	175-204	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	This suggests that human linguistic processes use queues rather than stacks , making imperative the development of a hierarchy of Queue Grammars as a counterweight to the Chomsky Grammars .	1<2	none	elab-addition	elab-addition
P86-1015	1-9	10-15	We outline a model of generation with revision ,	focusing on improving textual coherence .	We outline a model of generation with revision ,	focusing on improving textual coherence .	1-15	1-15	We outline a model of generation with revision , focusing on improving textual coherence .	We outline a model of generation with revision , focusing on improving textual coherence .	1<2	none	elab-addition	elab-addition
P86-1015	16-17	18-25	We argue	that high quality text is more easily produced	We argue	that high quality text is more easily produced	16-47	16-47	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	1>2	none	attribution	attribution
P86-1015	1-9	18-25	We outline a model of generation with revision ,	that high quality text is more easily produced	We outline a model of generation with revision ,	that high quality text is more easily produced	1-15	16-47	We outline a model of generation with revision , focusing on improving textual coherence .	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	1<2	none	bg-general	bg-general
P86-1015	18-25	26-31	that high quality text is more easily produced	by iteratively revising and regenerating ,	that high quality text is more easily produced	by iteratively revising and regenerating ,	16-47	16-47	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	1<2	none	manner-means	manner-means
P86-1015	18-25	32-35	that high quality text is more easily produced	as people do ,	that high quality text is more easily produced	as people do ,	16-47	16-47	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	1<2	none	elab-addition	elab-addition
P86-1015	26-31	36-47	by iteratively revising and regenerating ,	rather than by using an architecturally more complex single pass generator .	by iteratively revising and regenerating ,	rather than by using an architecturally more complex single pass generator .	16-47	16-47	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	We argue that high quality text is more easily produced by iteratively revising and regenerating , as people do , rather than by using an architecturally more complex single pass generator .	1<2	none	contrast	contrast
P86-1015	1-9	48-61	We outline a model of generation with revision ,	As a general area of study , the revision process presents interesting problems :	We outline a model of generation with revision ,	As a general area of study , the revision process presents interesting problems :	1-15	48-88	We outline a model of generation with revision , focusing on improving textual coherence .	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	1<2	none	bg-goal	bg-goal
P86-1015	48-61	62-70	As a general area of study , the revision process presents interesting problems :	Recognition of flaws in text requires a descriptive theory	As a general area of study , the revision process presents interesting problems :	Recognition of flaws in text requires a descriptive theory	48-88	48-88	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	1<2	none	elab-addition	elab-addition
P86-1015	62-70	71-76	Recognition of flaws in text requires a descriptive theory	of what constitutes well written prose	Recognition of flaws in text requires a descriptive theory	of what constitutes well written prose	48-88	48-88	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	1<2	none	elab-addition	elab-addition
P86-1015	62-70	77-79	Recognition of flaws in text requires a descriptive theory	and a parser	Recognition of flaws in text requires a descriptive theory	and a parser	48-88	48-88	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	1<2	none	joint	joint
P86-1015	77-79	80-88	and a parser	which can build a representation in those terms .	and a parser	which can build a representation in those terms .	48-88	48-88	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	As a general area of study , the revision process presents interesting problems : Recognition of flaws in text requires a descriptive theory of what constitutes well written prose and a parser which can build a representation in those terms .	1<2	none	elab-addition	elab-addition
P86-1015	89-98	99-112,118-125	Improving text requires associating flaws with strategies for improvement .	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	Improving text requires associating flaws with strategies for improvement .	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	89-98	99-125	Improving text requires associating flaws with strategies for improvement .	The strategies , in turn , need to know what adjustments to the decisions made during the initial generation will produce appropriate modifications to the text .	1>2	none	bg-goal	bg-goal
P86-1015	1-9	99-112,118-125	We outline a model of generation with revision ,	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	We outline a model of generation with revision ,	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	1-15	99-125	We outline a model of generation with revision , focusing on improving textual coherence .	The strategies , in turn , need to know what adjustments to the decisions made during the initial generation will produce appropriate modifications to the text .	1<2	none	elab-aspect	elab-aspect
P86-1015	99-112,118-125	113-117	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	made during the initial generation	The strategies , in turn , need to know what adjustments to the decisions <*> will produce appropriate modifications to the text .	made during the initial generation	99-125	99-125	The strategies , in turn , need to know what adjustments to the decisions made during the initial generation will produce appropriate modifications to the text .	The strategies , in turn , need to know what adjustments to the decisions made during the initial generation will produce appropriate modifications to the text .	1<2	none	elab-addition	elab-addition
P86-1015	1-9	126-152	We outline a model of generation with revision ,	We compare our treatment of revision with those of Mann and Moore ( 1981 ) , Gabriel ( 1984 ) , and Mann ( 1983 ) .	We outline a model of generation with revision ,	We compare our treatment of revision with those of Mann and Moore ( 1981 ) , Gabriel ( 1984 ) , and Mann ( 1983 ) .	1-15	126-152	We outline a model of generation with revision , focusing on improving textual coherence .	We compare our treatment of revision with those of Mann and Moore ( 1981 ) , Gabriel ( 1984 ) , and Mann ( 1983 ) .	1<2	none	evaluation	evaluation
P86-1016	1-11	12-21	As a user interacts with a database or expert system ,	she / he may reveal a misconception about the objects	As a user interacts with a database or expert system ,	she / he may reveal a misconception about the objects	1-26	1-26	As a user interacts with a database or expert system , she / he may reveal a misconception about the objects modeled by the system .	As a user interacts with a database or expert system , she / he may reveal a misconception about the objects modeled by the system .	1>2	none	temporal	temporal
P86-1016	12-21	27-32	she / he may reveal a misconception about the objects	This paper discusses the ROMPER system	she / he may reveal a misconception about the objects	This paper discusses the ROMPER system	1-26	27-46	As a user interacts with a database or expert system , she / he may reveal a misconception about the objects modeled by the system .	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	1>2	none	bg-general	bg-general
P86-1016	12-21	22-26	she / he may reveal a misconception about the objects	modeled by the system .	she / he may reveal a misconception about the objects	modeled by the system .	1-26	1-26	As a user interacts with a database or expert system , she / he may reveal a misconception about the objects modeled by the system .	As a user interacts with a database or expert system , she / he may reveal a misconception about the objects modeled by the system .	1<2	none	elab-addition	elab-addition
P86-1016	27-32	33-46	This paper discusses the ROMPER system	for responding to such misconceptions in a domain independent and context sensitive fashion .	This paper discusses the ROMPER system	for responding to such misconceptions in a domain independent and context sensitive fashion .	27-46	27-46	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	1<2	none	enablement	enablement
P86-1016	27-32	47-55	This paper discusses the ROMPER system	ROMPER reasons about possible sources of the misconception .	This paper discusses the ROMPER system	ROMPER reasons about possible sources of the misconception .	27-46	47-55	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	ROMPER reasons about possible sources of the misconception .	1<2	none	elab-addition	elab-addition
P86-1016	27-32	56-63	This paper discusses the ROMPER system	It operates on a model of the user	This paper discusses the ROMPER system	It operates on a model of the user	27-46	56-73	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	It operates on a model of the user and generates a cooperative response based on this reasoning .	1<2	none	elab-aspect	elab-aspect
P86-1016	56-63	64-68	It operates on a model of the user	and generates a cooperative response	It operates on a model of the user	and generates a cooperative response	56-73	56-73	It operates on a model of the user and generates a cooperative response based on this reasoning .	It operates on a model of the user and generates a cooperative response based on this reasoning .	1<2	none	joint	joint
P86-1016	64-68	69-73	and generates a cooperative response	based on this reasoning .	and generates a cooperative response	based on this reasoning .	56-73	56-73	It operates on a model of the user and generates a cooperative response based on this reasoning .	It operates on a model of the user and generates a cooperative response based on this reasoning .	1<2	none	elab-addition	elab-addition
P86-1016	27-32	74-79	This paper discusses the ROMPER system	The process is made context sensitive	This paper discusses the ROMPER system	The process is made context sensitive	27-46	74-104	This paper discusses the ROMPER system for responding to such misconceptions in a domain independent and context sensitive fashion .	The process is made context sensitive by augmenting the user model with a new notion of object perspective which highlights certain aspects of the user model due to previous discourse .	1<2	none	evaluation	evaluation
P86-1016	74-79	80-91	The process is made context sensitive	by augmenting the user model with a new notion of object perspective	The process is made context sensitive	by augmenting the user model with a new notion of object perspective	74-104	74-104	The process is made context sensitive by augmenting the user model with a new notion of object perspective which highlights certain aspects of the user model due to previous discourse .	The process is made context sensitive by augmenting the user model with a new notion of object perspective which highlights certain aspects of the user model due to previous discourse .	1<2	none	manner-means	manner-means
P86-1016	80-91	92-104	by augmenting the user model with a new notion of object perspective	which highlights certain aspects of the user model due to previous discourse .	by augmenting the user model with a new notion of object perspective	which highlights certain aspects of the user model due to previous discourse .	74-104	74-104	The process is made context sensitive by augmenting the user model with a new notion of object perspective which highlights certain aspects of the user model due to previous discourse .	The process is made context sensitive by augmenting the user model with a new notion of object perspective which highlights certain aspects of the user model due to previous discourse .	1<2	none	elab-addition	elab-addition
P86-1017	1-5	81-88	Here we address the problem	In this paper we demonstrate a semantic representation	Here we address the problem	In this paper we demonstrate a semantic representation	1-14	81-103	Here we address the problem of mapping phrase meanings into their conceptual representations .	In this paper we demonstrate a semantic representation which facilitates , for a wide variety of phrases , both learning and parsing .	1>2	none	bg-goal	bg-goal
P86-1017	1-5	6-14	Here we address the problem	of mapping phrase meanings into their conceptual representations .	Here we address the problem	of mapping phrase meanings into their conceptual representations .	1-14	1-14	Here we address the problem of mapping phrase meanings into their conceptual representations .	Here we address the problem of mapping phrase meanings into their conceptual representations .	1<2	none	elab-addition	elab-addition
P86-1017	1-5	15-22	Here we address the problem	Figurative phrases are pervasive in human communication ,	Here we address the problem	Figurative phrases are pervasive in human communication ,	1-14	15-30	Here we address the problem of mapping phrase meanings into their conceptual representations .	Figurative phrases are pervasive in human communication , yet they are difficult to explain theoretically .	1<2	none	elab-aspect	elab-aspect
P86-1017	15-22	23-30	Figurative phrases are pervasive in human communication ,	yet they are difficult to explain theoretically .	Figurative phrases are pervasive in human communication ,	yet they are difficult to explain theoretically .	15-30	15-30	Figurative phrases are pervasive in human communication , yet they are difficult to explain theoretically .	Figurative phrases are pervasive in human communication , yet they are difficult to explain theoretically .	1<2	none	contrast	contrast
P86-1017	31-35	36-41	In fact , the ability	to handle idiosyncratic behavior of phrases	In fact , the ability	to handle idiosyncratic behavior of phrases	31-52	31-52	In fact , the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation .	In fact , the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation .	1<2	none	enablement	enablement
P86-1017	1-5	31-35,42-52	Here we address the problem	<*> In fact , the ability <*> should be a criterion for any theory of lexical representation .	Here we address the problem	In fact , the ability <*> should be a criterion for any theory of lexical representation .	1-14	31-52	Here we address the problem of mapping phrase meanings into their conceptual representations .	In fact , the ability to handle idiosyncratic behavior of phrases should be a criterion for any theory of lexical representation .	1<2	none	elab-aspect	elab-aspect
P86-1017	53-65	66-80	Due to the huge number of such phrases in the English language ,	phrase representation must be amenable to parsing , generation , and also to learning .	Due to the huge number of such phrases in the English language ,	phrase representation must be amenable to parsing , generation , and also to learning .	53-80	53-80	Due to the huge number of such phrases in the English language , phrase representation must be amenable to parsing , generation , and also to learning .	Due to the huge number of such phrases in the English language , phrase representation must be amenable to parsing , generation , and also to learning .	1>2	none	exp-reason	exp-reason
P86-1017	1-5	66-80	Here we address the problem	phrase representation must be amenable to parsing , generation , and also to learning .	Here we address the problem	phrase representation must be amenable to parsing , generation , and also to learning .	1-14	53-80	Here we address the problem of mapping phrase meanings into their conceptual representations .	Due to the huge number of such phrases in the English language , phrase representation must be amenable to parsing , generation , and also to learning .	1<2	none	elab-aspect	elab-aspect
P86-1017	81-88	89-103	In this paper we demonstrate a semantic representation	which facilitates , for a wide variety of phrases , both learning and parsing .	In this paper we demonstrate a semantic representation	which facilitates , for a wide variety of phrases , both learning and parsing .	81-103	81-103	In this paper we demonstrate a semantic representation which facilitates , for a wide variety of phrases , both learning and parsing .	In this paper we demonstrate a semantic representation which facilitates , for a wide variety of phrases , both learning and parsing .	1<2	none	elab-addition	elab-addition
P86-1018	1-7	35-57	Natural language processing systems need large lexicons	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	Natural language processing systems need large lexicons	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster's seventh Collegiate Dictionary .	1-21	22-57	Natural language processing systems need large lexicons containing explicit information about lexical-semantlc relationships , selection restrictions , and verb categories .	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	1>2	none	bg-goal	bg-goal
P86-1018	1-7	8-21	Natural language processing systems need large lexicons	containing explicit information about lexical-semantlc relationships , selection restrictions , and verb categories .	Natural language processing systems need large lexicons	containing explicit information about lexical-semantlc relationships , selection restrictions , and verb categories .	1-21	1-21	Natural language processing systems need large lexicons containing explicit information about lexical-semantlc relationships , selection restrictions , and verb categories .	Natural language processing systems need large lexicons containing explicit information about lexical-semantlc relationships , selection restrictions , and verb categories .	1<2	none	elab-addition	elab-addition
P86-1018	22-24,32-34	35-57	Because the labor <*> is overwhelming ,	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	Because the labor <*> is overwhelming ,	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster's seventh Collegiate Dictionary .	22-57	22-57	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	1>2	none	exp-reason	exp-reason
P86-1018	22-24,32-34	25-31	Because the labor <*> is overwhelming ,	involved in constructing such lexicons by hand	Because the labor <*> is overwhelming ,	involved in constructing such lexicons by hand	22-57	22-57	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	1<2	none	elab-addition	elab-addition
P86-1018	35-57	58-65	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	This work is rich in implicit information ;	we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster's seventh Collegiate Dictionary .	This work is rich in implicit information ;	22-57	58-73	Because the labor involved in constructing such lexicons by hand is overwhelming , we have been trying to construct lexical entries automatically from information available in the machine-readable version of Webster 's seventh Collegiate Dictionary .	This work is rich in implicit information ; the problem is to make it explicit .	1>2	none	bg-general	bg-general
P86-1018	58-65	74-77	This work is rich in implicit information ;	This paper describes methods	This work is rich in implicit information ;	This paper describes methods	58-73	74-101	This work is rich in implicit information ; the problem is to make it explicit .	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	1>2	none	bg-goal	bg-goal
P86-1018	58-65	66-73	This work is rich in implicit information ;	the problem is to make it explicit .	This work is rich in implicit information ;	the problem is to make it explicit .	58-73	58-73	This work is rich in implicit information ; the problem is to make it explicit .	This work is rich in implicit information ; the problem is to make it explicit .	1<2	none	joint	joint
P86-1018	74-77	78-84	This paper describes methods	for finding taxonomy and set-membership relationships ,	This paper describes methods	for finding taxonomy and set-membership relationships ,	74-101	74-101	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	1<2	none	enablement	enablement
P86-1018	78-84	85-86	for finding taxonomy and set-membership relationships ,	recognizing nouns	for finding taxonomy and set-membership relationships ,	recognizing nouns	74-101	74-101	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	1<2	none	joint	joint
P86-1018	85-86	87-92	recognizing nouns	that ordinarily represent human beings ,	recognizing nouns	that ordinarily represent human beings ,	74-101	74-101	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	1<2	none	elab-addition	elab-addition
P86-1018	78-84	93-101	for finding taxonomy and set-membership relationships ,	and identifying active and stative verbs and adjectives .	for finding taxonomy and set-membership relationships ,	and identifying active and stative verbs and adjectives .	74-101	74-101	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	This paper describes methods for finding taxonomy and set-membership relationships , recognizing nouns that ordinarily represent human beings , and identifying active and stative verbs and adjectives .	1<2	none	joint	joint
P86-1020	1-6	34-42	Dictionary lookup is a computational activity	Several algorithms for parallel dictionary lookup are discussed ,	Dictionary lookup is a computational activity	Several algorithms for parallel dictionary lookup are discussed ,	1-33	34-66	Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	Several algorithms for parallel dictionary lookup are discussed , including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	1>2	none	bg-goal	bg-goal
P86-1020	1-6	7-11	Dictionary lookup is a computational activity	that can be greatly accelerated	Dictionary lookup is a computational activity	that can be greatly accelerated	1-33	1-33	Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	1<2	none	elab-addition	elab-addition
P86-1020	1-6	12-33	Dictionary lookup is a computational activity	when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	Dictionary lookup is a computational activity	when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	1-33	1-33	Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	Dictionary lookup is a computational activity that can be greatly accelerated when performed on large amounts of text by a parallel computer such as the Connection Machine TM Computer ( CM ) .	1<2	none	temporal	temporal
P86-1020	34-42	43-44	Several algorithms for parallel dictionary lookup are discussed ,	including one	Several algorithms for parallel dictionary lookup are discussed ,	including one	34-66	34-66	Several algorithms for parallel dictionary lookup are discussed , including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	Several algorithms for parallel dictionary lookup are discussed , including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	1<2	none	elab-addition	elab-addition
P86-1020	43-44	45-66	including one	that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	including one	that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	34-66	34-66	Several algorithms for parallel dictionary lookup are discussed , including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	Several algorithms for parallel dictionary lookup are discussed , including one that allows the CM to lookup words at a rate 450 times that of lookup on a Symbolics 3600 Lisp Machine .	1<2	none	elab-addition	elab-addition
P86-1021	1-10	11-14	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	based upon the hypothesis	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	based upon the hypothesis	1-30	1-30	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	1<2	none	bg-general	bg-general
P86-1021	11-14	15-30	based upon the hypothesis	that intonation conveys information about the intentional as well as the attentional structure of discourse .	based upon the hypothesis	that intonation conveys information about the intentional as well as the attentional structure of discourse .	1-30	1-30	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	1<2	none	elab-addition	elab-addition
P86-1021	31-35	36-54	In particular , we discuss	how variations in pitch range and choice of accent and tune can help to convey such information as :	In particular , we discuss	how variations in pitch range and choice of accent and tune can help to convey such information as :	31-95	31-95	In particular , we discuss how variations in pitch range and choice of accent and tune can help to convey such information as : discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	In particular , we discuss how variations in pitch range and choice of accent and tune can help to convey such information as : discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	1>2	none	attribution	attribution
P86-1021	1-10	36-54	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	how variations in pitch range and choice of accent and tune can help to convey such information as :	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	how variations in pitch range and choice of accent and tune can help to convey such information as :	1-30	31-95	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	In particular , we discuss how variations in pitch range and choice of accent and tune can help to convey such information as : discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	1<2	none	elab-aspect	elab-aspect
P86-1021	36-54	55-95	how variations in pitch range and choice of accent and tune can help to convey such information as :	discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	how variations in pitch range and choice of accent and tune can help to convey such information as :	discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	31-95	31-95	In particular , we discuss how variations in pitch range and choice of accent and tune can help to convey such information as : discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	In particular , we discuss how variations in pitch range and choice of accent and tune can help to convey such information as : discourse segmentation and topic structure , appropriate choice of referent , the distinction between " given " and " new " information , conceptual contrast or parallelism between mentioned items , and subordination relationships between propositions salient in the discourse .	1<2	none	elab-enumember	elab-enumember
P86-1021	1-10	96-106	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	Our goals for this research are practical as well as theoretical.	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	Our goals for this research are practical as well as theoretical.	1-30	96-106	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	Our goals for this research are practical as well as theoretical.	1<2	none	elab-aspect	elab-aspect
P86-1021	1-10	107-121	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	In particular , we are investigating the problem of intonational assignment in synthetic speech .	We propose a mapping between prosodic phenomena and semantico-pragmatic effects	In particular , we are investigating the problem of intonational assignment in synthetic speech .	1-30	107-121	We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse .	In particular , we are investigating the problem of intonational assignment in synthetic speech .	1<2	none	elab-aspect	elab-aspect
P86-1022	1-18	19-27	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody ,	the text-to-speech field has lacked a robust working system	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody ,	the text-to-speech field has lacked a robust working system	1-37	1-37	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .	1>2	none	temporal	temporal
P86-1022	19-27	38-42	the text-to-speech field has lacked a robust working system	We describe an implemented system	the text-to-speech field has lacked a robust working system	We describe an implemented system	1-37	38-59	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	1>2	none	bg-goal	bg-goal
P86-1022	19-27	28-37	the text-to-speech field has lacked a robust working system	to test the possible relations between syntax and prosody .	the text-to-speech field has lacked a robust working system	to test the possible relations between syntax and prosody .	1-37	1-37	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .	While various aspects of syntactic structure have been shown to bear on the determination of phraselevel prosody , the text-to-speech field has lacked a robust working system to test the possible relations between syntax and prosody .	1<2	none	enablement	enablement
P86-1022	38-42	43-48	We describe an implemented system	which uses the deterministic parser Fidditch	We describe an implemented system	which uses the deterministic parser Fidditch	38-59	38-59	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	1<2	none	elab-addition	elab-addition
P86-1022	43-48	49-59	which uses the deterministic parser Fidditch	to create the input for a set of prosody rules .	which uses the deterministic parser Fidditch	to create the input for a set of prosody rules .	38-59	38-59	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	1<2	none	enablement	enablement
P86-1022	38-42	60-66	We describe an implemented system	The prosody rules generate a prosody tree	We describe an implemented system	The prosody rules generate a prosody tree	38-59	60-78	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries .	1<2	none	elab-aspect	elab-aspect
P86-1022	60-66	67-78	The prosody rules generate a prosody tree	that specifies the location and relative strength of prosodic phrase boundaries .	The prosody rules generate a prosody tree	that specifies the location and relative strength of prosodic phrase boundaries .	60-78	60-78	The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries .	The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries .	1<2	none	elab-addition	elab-addition
P86-1022	60-66	79-90	The prosody rules generate a prosody tree	These specifications are converted to annotations for the Bell Labs text-to-speech system	The prosody rules generate a prosody tree	These specifications are converted to annotations for the Bell Labs text-to-speech system	60-78	79-102	The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries .	These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence .	1<2	none	elab-addition	elab-addition
P86-1022	79-90	91-102	These specifications are converted to annotations for the Bell Labs text-to-speech system	that dictate modulations in pitch and duration for the input sentence .	These specifications are converted to annotations for the Bell Labs text-to-speech system	that dictate modulations in pitch and duration for the input sentence .	79-102	79-102	These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence .	These specifications are converted to annotations for the Bell Labs text-to-speech system that dictate modulations in pitch and duration for the input sentence .	1<2	none	elab-addition	elab-addition
P86-1022	103-109	118-127	We discuss the results of an experiment	We are encouraged by an initial 5 percent error rate	We discuss the results of an experiment	We are encouraged by an initial 5 percent error rate	103-117	118-149	We discuss the results of an experiment to determine the performance of our system .	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	1>2	none	result	result
P86-1022	103-109	110-117	We discuss the results of an experiment	to determine the performance of our system .	We discuss the results of an experiment	to determine the performance of our system .	103-117	103-117	We discuss the results of an experiment to determine the performance of our system .	We discuss the results of an experiment to determine the performance of our system .	1<2	none	enablement	enablement
P86-1022	38-42	118-127	We describe an implemented system	We are encouraged by an initial 5 percent error rate	We describe an implemented system	We are encouraged by an initial 5 percent error rate	38-59	118-149	We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules .	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	1<2	none	evaluation	evaluation
P86-1022	118-127	128-141	We are encouraged by an initial 5 percent error rate	and we see the design of the parser and the modularity of the system	We are encouraged by an initial 5 percent error rate	and we see the design of the parser and the modularity of the system	118-149	118-149	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	1<2	none	joint	joint
P86-1022	128-141	142-143	and we see the design of the parser and the modularity of the system	allowing changes	and we see the design of the parser and the modularity of the system	allowing changes	118-149	118-149	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	1<2	none	elab-addition	elab-addition
P86-1022	142-143	144-149	allowing changes	that will upgrade this rate .	allowing changes	that will upgrade this rate .	118-149	118-149	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	We are encouraged by an initial 5 percent error rate and we see the design of the parser and the modularity of the system allowing changes that will upgrade this rate .	1<2	none	elab-addition	elab-addition
P86-1024	1-10	11-20	The following proposal is for a Japanese sentence analysis method	to be used in a Japanese book reading machine .	The following proposal is for a Japanese sentence analysis method	to be used in a Japanese book reading machine .	1-20	1-20	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	1<2	none	enablement	enablement
P86-1024	1-10	21-35	The following proposal is for a Japanese sentence analysis method	This method is designed to allow for several candidates in case of ambiguous characters .	The following proposal is for a Japanese sentence analysis method	This method is designed to allow for several candidates in case of ambiguous characters .	1-20	21-35	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	This method is designed to allow for several candidates in case of ambiguous characters .	1<2	none	elab-addition	elab-addition
P86-1024	1-10	36-39	The following proposal is for a Japanese sentence analysis method	Each sentence is analyzed	The following proposal is for a Japanese sentence analysis method	Each sentence is analyzed	1-20	36-53	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	Each sentence is analyzed to compose a data structure by defining the relationship between words and phrases .	1<2	none	elab-aspect	elab-aspect
P86-1024	36-39	40-44	Each sentence is analyzed	to compose a data structure	Each sentence is analyzed	to compose a data structure	36-53	36-53	Each sentence is analyzed to compose a data structure by defining the relationship between words and phrases .	Each sentence is analyzed to compose a data structure by defining the relationship between words and phrases .	1<2	none	enablement	enablement
P86-1024	36-39	45-53	Each sentence is analyzed	by defining the relationship between words and phrases .	Each sentence is analyzed	by defining the relationship between words and phrases .	36-53	36-53	Each sentence is analyzed to compose a data structure by defining the relationship between words and phrases .	Each sentence is analyzed to compose a data structure by defining the relationship between words and phrases .	1<2	none	manner-means	manner-means
P86-1024	1-10	54-55,61-69	The following proposal is for a Japanese sentence analysis method	This structure <*> involves all possible combinations of syntactically collect phrases .	The following proposal is for a Japanese sentence analysis method	This structure <*> involves all possible combinations of syntactically collect phrases .	1-20	54-69	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	This structure ( named network structure ) involves all possible combinations of syntactically collect phrases .	1<2	none	elab-addition	elab-addition
P86-1024	54-55,61-69	56-60	This structure <*> involves all possible combinations of syntactically collect phrases .	( named network structure )	This structure <*> involves all possible combinations of syntactically collect phrases .	( named network structure )	54-69	54-69	This structure ( named network structure ) involves all possible combinations of syntactically collect phrases .	This structure ( named network structure ) involves all possible combinations of syntactically collect phrases .	1<2	none	elab-addition	elab-addition
P86-1024	70-76	77-80	After network structure has been completed ,	heuristic rules are applied	After network structure has been completed ,	heuristic rules are applied	70-99	70-99	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	1>2	none	temporal	temporal
P86-1024	1-10	77-80	The following proposal is for a Japanese sentence analysis method	heuristic rules are applied	The following proposal is for a Japanese sentence analysis method	heuristic rules are applied	1-20	70-99	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	1<2	none	elab-aspect	elab-aspect
P86-1024	77-80	81-88	heuristic rules are applied	in order to determine the most probable way	heuristic rules are applied	in order to determine the most probable way	70-99	70-99	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	1<2	none	enablement	enablement
P86-1024	81-88	89-92	in order to determine the most probable way	to arrange the phrases	in order to determine the most probable way	to arrange the phrases	70-99	70-99	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	1<2	none	enablement	enablement
P86-1024	81-88	93-99	in order to determine the most probable way	and thus organize the best sentence .	in order to determine the most probable way	and thus organize the best sentence .	70-99	70-99	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	After network structure has been completed , heuristic rules are applied in order to determine the most probable way to arrange the phrases and thus organize the best sentence .	1<2	none	progression	progression
P86-1024	1-10	100-126	The following proposal is for a Japanese sentence analysis method	All information about each sentence ---- the pronunciation of each word with its accent and the structure of phrases ---- will be used during speech synthesis .	The following proposal is for a Japanese sentence analysis method	All information about each sentence ---- the pronunciation of each word with its accent and the structure of phrases ---- will be used during speech synthesis .	1-20	100-126	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	All information about each sentence ---- the pronunciation of each word with its accent and the structure of phrases ---- will be used during speech synthesis .	1<2	none	bg-general	bg-general
P86-1024	127-130	131-141	Experiment results reveal :	99.1 % of all characters were given their correct pronunciation .	Experiment results reveal :	99.1 % of all characters were given their correct pronunciation .	127-141	127-141	Experiment results reveal : 99.1 % of all characters were given their correct pronunciation .	Experiment results reveal : 99.1 % of all characters were given their correct pronunciation .	1>2	none	attribution	attribution
P86-1024	1-10	131-141	The following proposal is for a Japanese sentence analysis method	99.1 % of all characters were given their correct pronunciation .	The following proposal is for a Japanese sentence analysis method	99.1 % of all characters were given their correct pronunciation .	1-20	127-141	The following proposal is for a Japanese sentence analysis method to be used in a Japanese book reading machine .	Experiment results reveal : 99.1 % of all characters were given their correct pronunciation .	1<2	none	evaluation	evaluation
P86-1024	131-141	142-149	99.1 % of all characters were given their correct pronunciation .	Using several recognized character candidates is more efficient	99.1 % of all characters were given their correct pronunciation .	Using several recognized character candidates is more efficient	127-141	142-162	Experiment results reveal : 99.1 % of all characters were given their correct pronunciation .	Using several recognized character candidates is more efficient than only using first ranked characters as the input for sentence analysis .	1<2	none	progression	progression
P86-1024	142-149	150-162	Using several recognized character candidates is more efficient	than only using first ranked characters as the input for sentence analysis .	Using several recognized character candidates is more efficient	than only using first ranked characters as the input for sentence analysis .	142-162	142-162	Using several recognized character candidates is more efficient than only using first ranked characters as the input for sentence analysis .	Using several recognized character candidates is more efficient than only using first ranked characters as the input for sentence analysis .	1<2	none	comparison	comparison
P86-1024	142-149	163-173	Using several recognized character candidates is more efficient	Also this facility increases the efficiency of the book reading machine	Using several recognized character candidates is more efficient	Also this facility increases the efficiency of the book reading machine	142-162	163-187	Using several recognized character candidates is more efficient than only using first ranked characters as the input for sentence analysis .	Also this facility increases the efficiency of the book reading machine in that it enables the user to select other ways to organize sentences .	1<2	none	joint	joint
P86-1024	163-173	174-183	Also this facility increases the efficiency of the book reading machine	in that it enables the user to select other ways	Also this facility increases the efficiency of the book reading machine	in that it enables the user to select other ways	163-187	163-187	Also this facility increases the efficiency of the book reading machine in that it enables the user to select other ways to organize sentences .	Also this facility increases the efficiency of the book reading machine in that it enables the user to select other ways to organize sentences .	1<2	none	elab-addition	elab-addition
P86-1024	174-183	184-187	in that it enables the user to select other ways	to organize sentences .	in that it enables the user to select other ways	to organize sentences .	163-187	163-187	Also this facility increases the efficiency of the book reading machine in that it enables the user to select other ways to organize sentences .	Also this facility increases the efficiency of the book reading machine in that it enables the user to select other ways to organize sentences .	1<2	none	enablement	enablement
P86-1025	1-3,10-16	17-41	A computer program <*> implements our theory of Japanese intonation .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	A computer program <*> implements our theory of Japanese intonation .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	1-16	17-51	A computer program for synthesizing Japanese fundamental frequency contours implements our theory of Japanese intonation .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	1>2	none	bg-general	bg-general
P86-1025	1-3,10-16	4-9	A computer program <*> implements our theory of Japanese intonation .	for synthesizing Japanese fundamental frequency contours	A computer program <*> implements our theory of Japanese intonation .	for synthesizing Japanese fundamental frequency contours	1-16	1-16	A computer program for synthesizing Japanese fundamental frequency contours implements our theory of Japanese intonation .	A computer program for synthesizing Japanese fundamental frequency contours implements our theory of Japanese intonation .	1<2	none	enablement	enablement
P86-1025	17-41	42-51	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	precise enough to translate straightforwardly into a computational algorithm .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	precise enough to translate straightforwardly into a computational algorithm .	17-51	17-51	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	1<2	none	elab-addition	elab-addition
P86-1025	17-41	52-82	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	An important aspect of the description is that various features of the intonation pattern are designated to be phonological properties of different types of phrasal units in a hierarchical organization .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	An important aspect of the description is that various features of the intonation pattern are designated to be phonological properties of different types of phrasal units in a hierarchical organization .	17-51	52-82	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	An important aspect of the description is that various features of the intonation pattern are designated to be phonological properties of different types of phrasal units in a hierarchical organization .	1<2	none	elab-aspect	elab-aspect
P86-1025	17-41	83-92	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	This phrasal organization is known to play an important role	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	This phrasal organization is known to play an important role	17-51	83-96	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	This phrasal organization is known to play an important role in parsing speech .	1<2	none	elab-addition	elab-addition
P86-1025	83-92	93-96	This phrasal organization is known to play an important role	in parsing speech .	This phrasal organization is known to play an important role	in parsing speech .	83-96	83-96	This phrasal organization is known to play an important role in parsing speech .	This phrasal organization is known to play an important role in parsing speech .	1<2	none	elab-addition	elab-addition
P86-1025	17-41	97-109	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	Our research shows it also to be one reflex of intonational prominence ,	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	Our research shows it also to be one reflex of intonational prominence ,	17-51	97-118	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	Our research shows it also to be one reflex of intonational prominence , and hence of focus and other discourse structures .	1<2	none	evaluation	evaluation
P86-1025	97-109	110-118	Our research shows it also to be one reflex of intonational prominence ,	and hence of focus and other discourse structures .	Our research shows it also to be one reflex of intonational prominence ,	and hence of focus and other discourse structures .	97-118	97-118	Our research shows it also to be one reflex of intonational prominence , and hence of focus and other discourse structures .	Our research shows it also to be one reflex of intonational prominence , and hence of focus and other discourse structures .	1<2	none	joint	joint
P86-1025	17-41	119-135	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	The qualitative features of each phrasal level and their implementation in the synthesis program are described .	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing	The qualitative features of each phrasal level and their implementation in the synthesis program are described .	17-51	119-135	This theory provides a complete qualitative description of the known characteristics of Japanese intonation , as well as a quantitative model of tone-scaling and timing precise enough to translate straightforwardly into a computational algorithm .	The qualitative features of each phrasal level and their implementation in the synthesis program are described .	1<2	none	elab-aspect	elab-aspect
P86-1029	1-6	7-28	In this paper , I describe	how Donnellan 's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	In this paper , I describe	how Donnellan's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	1-28	1-28	In this paper , I describe how Donnellan 's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	In this paper , I describe how Donnellan 's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	1>2	none	attribution	attribution
P86-1029	29-38	39-59	After briefly discussing the significance of Donnellan 's distinction ,	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	After briefly discussing the significance of Donnellan's distinction ,	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	29-59	29-59	After briefly discussing the significance of Donnellan 's distinction , I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	After briefly discussing the significance of Donnellan 's distinction , I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	1>2	none	temporal	temporal
P86-1029	7-28	39-59	how Donnellan 's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	how Donnellan's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	1-28	29-59	In this paper , I describe how Donnellan 's distinction between referential and attributive uses of definite descriptions should be represented in a computational model of reference .	After briefly discussing the significance of Donnellan 's distinction , I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	1<2	none	elab-aspect	elab-aspect
P86-1029	39-59	60-68	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	I then present a cognitive model of referring ,	I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	I then present a cognitive model of referring ,	29-59	60-88	After briefly discussing the significance of Donnellan 's distinction , I reinterpret it as being three-tiered , relating to object representation , referring intentions , and choice of referring expression .	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	1<2	none	progression	progression
P86-1029	60-68	69-77	I then present a cognitive model of referring ,	the components of which correspond to this analysis ,	I then present a cognitive model of referring ,	the components of which correspond to this analysis ,	60-88	60-88	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	1<2	none	elab-addition	elab-addition
P86-1029	60-68	78-81	I then present a cognitive model of referring ,	and discuss the interaction	I then present a cognitive model of referring ,	and discuss the interaction	60-88	60-88	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	1<2	none	joint	joint
P86-1029	78-81	82-88	and discuss the interaction	that takes place among those components .	and discuss the interaction	that takes place among those components .	60-88	60-88	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	1<2	none	elab-addition	elab-addition
P86-1029	60-68	89-103	I then present a cognitive model of referring ,	Finally , the implementation of this model , now in progress , is described .	I then present a cognitive model of referring ,	Finally , the implementation of this model , now in progress , is described .	60-88	89-103	I then present a cognitive model of referring , the components of which correspond to this analysis , and discuss the interaction that takes place among those components .	Finally , the implementation of this model , now in progress , is described .	1<2	none	progression	progression
P86-1030	1	2-9	Ambiguities	related to intension and their consequent inference failures	Ambiguities	related to intension and their consequent inference failures	1-19	1-19	Ambiguities related to intension and their consequent inference failures are a diverse group , both syntactically and semantically .	Ambiguities related to intension and their consequent inference failures are a diverse group , both syntactically and semantically .	1<2	none	elab-addition	elab-addition
P86-1030	1,10-19	20-24,32-41	<*> Ambiguities <*> are a diverse group , both syntactically and semantically .	One particular kind of ambiguity <*> is whether it is the speaker or the third party	Ambiguities <*> are a diverse group , both syntactically and semantically .	One particular kind of ambiguity <*> is whether it is the speaker or the third party	1-19	20-55	Ambiguities related to intension and their consequent inference failures are a diverse group , both syntactically and semantically .	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	1>2	none	bg-general	bg-general
P86-1030	20-24,32-41	75-97	One particular kind of ambiguity <*> is whether it is the speaker or the third party	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	One particular kind of ambiguity <*> is whether it is the speaker or the third party	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	20-55	73-116	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	1>2	none	bg-general	bg-general
P86-1030	20-24,32-41	25-31	One particular kind of ambiguity <*> is whether it is the speaker or the third party	that has received little attention so far	One particular kind of ambiguity <*> is whether it is the speaker or the third party	that has received little attention so far	20-55	20-55	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	1<2	none	elab-addition	elab-addition
P86-1030	32-41	42-55	is whether it is the speaker or the third party	to whom a description in an opaque third-party attitude report should be attributed .	is whether it is the speaker or the third party	to whom a description in an opaque third-party attitude report should be attributed .	20-55	20-55	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	One particular kind of ambiguity that has received little attention so far is whether it is the speaker or the third party to whom a description in an opaque third-party attitude report should be attributed .	1<2	none	elab-addition	elab-addition
P86-1030	56-65	75-97	The different readings lead to different inferences in a system	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	The different readings lead to different inferences in a system	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	56-72	73-116	The different readings lead to different inferences in a system modeling the beliefs of external agents .	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	1>2	none	bg-general	bg-general
P86-1030	56-65	66-72	The different readings lead to different inferences in a system	modeling the beliefs of external agents .	The different readings lead to different inferences in a system	modeling the beliefs of external agents .	56-72	56-72	The different readings lead to different inferences in a system modeling the beliefs of external agents .	The different readings lead to different inferences in a system modeling the beliefs of external agents .	1<2	none	elab-addition	elab-addition
P86-1030	73-74	75-97	We propose	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	We propose	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	73-116	73-116	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	1>2	none	attribution	attribution
P86-1030	75-97	117-122	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	We describe such a representation ,	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	We describe such a representation ,	73-116	117-155	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	1>2	none	bg-general	bg-general
P86-1030	75-97	98-105	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	that is evaluated with respect to intensionality ,	that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor	that is evaluated with respect to intensionality ,	73-116	73-116	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	1<2	none	elab-addition	elab-addition
P86-1030	98-105	106-116	that is evaluated with respect to intensionality ,	the beliefs of agents , and a time of application .	that is evaluated with respect to intensionality ,	the beliefs of agents , and a time of application .	73-116	73-116	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	We propose that a unified approach to the representation of the alternative readings of intension-related ambiguities can be based on the notion of a descriptor that is evaluated with respect to intensionality , the beliefs of agents , and a time of application .	1<2	none	joint	joint
P86-1030	117-122	123-129	We describe such a representation ,	built on a standard modal logic ,	We describe such a representation ,	built on a standard modal logic ,	117-155	117-155	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	1<2	none	elab-addition	elab-addition
P86-1030	130-131	132-145	and show	how it may be used in conjunction with a knowledge base of background assumptions	and show	how it may be used in conjunction with a knowledge base of background assumptions	117-155	117-155	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	1>2	none	attribution	attribution
P86-1030	117-122	132-145	We describe such a representation ,	how it may be used in conjunction with a knowledge base of background assumptions	We describe such a representation ,	how it may be used in conjunction with a knowledge base of background assumptions	117-155	117-155	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	1<2	none	joint	joint
P86-1030	132-145	146-155	how it may be used in conjunction with a knowledge base of background assumptions	to license restricted substitution of equals in opaque contexts .	how it may be used in conjunction with a knowledge base of background assumptions	to license restricted substitution of equals in opaque contexts .	117-155	117-155	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	We describe such a representation , built on a standard modal logic , and show how it may be used in conjunction with a knowledge base of background assumptions to license restricted substitution of equals in opaque contexts .	1<2	none	enablement	enablement
P86-1031	1-14	15-24,33-39	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	1-14	15-39	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	This " property-sharing " constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property .	1<2	none	elab-addition	elab-addition
P86-1031	15-24,33-39	25-32	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	that retain the same Cb across adjacent utterances	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	that retain the same Cb across adjacent utterances	15-39	15-39	This " property-sharing " constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property .	This " property-sharing " constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property .	1<2	none	elab-addition	elab-addition
P86-1031	15-24,33-39	40-58	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	This " property-sharing " constraint requires that two pronominal expressions <*> share a certain common grammatical property .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	15-39	40-70	This " property-sharing " constraint requires that two pronominal expressions that retain the same Cb across adjacent utterances share a certain common grammatical property .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	1<2	none	elab-aspect	elab-aspect
P86-1031	40-58	59-65	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	where different pronominal forms are primarily used	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	where different pronominal forms are primarily used	40-70	40-70	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	1<2	none	elab-addition	elab-addition
P86-1031	59-65	66-70	where different pronominal forms are primarily used	to realize the Cb .	where different pronominal forms are primarily used	to realize the Cb .	40-70	40-70	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	1<2	none	enablement	enablement
P86-1031	40-58	71-88	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	It is the zero pronominal in Japanese , and the ( unstressed ) overt pronoun in English .	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses ,	It is the zero pronominal in Japanese , and the ( unstressed ) overt pronoun in English .	40-70	71-88	This property is expressed along the dimension of the grammatical function SUBJECT for both Japanese and English discourses , where different pronominal forms are primarily used to realize the Cb .	It is the zero pronominal in Japanese , and the ( unstressed ) overt pronoun in English .	1<2	none	elab-addition	elab-addition
P86-1031	1-14	89-96	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	The resulting constraint complements the original Centering ,	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	The resulting constraint complements the original Centering ,	1-14	89-112	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	1<2	none	elab-aspect	elab-aspect
P86-1031	89-96	97-101	The resulting constraint complements the original Centering ,	accounting for its apparent violations	The resulting constraint complements the original Centering ,	accounting for its apparent violations	89-112	89-112	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	1<2	none	elab-addition	elab-addition
P86-1031	97-101	102-112	accounting for its apparent violations	and providing a solution to the interpretation of multi-pronominal utterances .	accounting for its apparent violations	and providing a solution to the interpretation of multi-pronominal utterances .	89-112	89-112	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	1<2	none	joint	joint
P86-1031	89-96	113-121	The resulting constraint complements the original Centering ,	It also provides an alternative account of anaphora interpretation	The resulting constraint complements the original Centering ,	It also provides an alternative account of anaphora interpretation	89-112	113-130	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism .	1<2	none	elab-aspect	elab-aspect
P86-1031	113-121	122-130	It also provides an alternative account of anaphora interpretation	that appears to be due to structural parallelism .	It also provides an alternative account of anaphora interpretation	that appears to be due to structural parallelism .	113-130	113-130	It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism .	It also provides an alternative account of anaphora interpretation that appears to be due to structural parallelism .	1<2	none	elab-addition	elab-addition
P86-1031	89-96	131-141	The resulting constraint complements the original Centering ,	This reconciliation of centering/focusing and parallelism is a major advantage .	The resulting constraint complements the original Centering ,	This reconciliation of centering/focusing and parallelism is a major advantage .	89-112	131-141	The resulting constraint complements the original Centering , accounting for its apparent violations and providing a solution to the interpretation of multi-pronominal utterances .	This reconciliation of centering/focusing and parallelism is a major advantage .	1<2	none	elab-aspect	elab-aspect
P86-1031	1-14	142-147,154-156	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	I will then add another dimension <*> to the constraint	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	I will then add another dimension <*> to the constraint	1-14	142-167	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	1<2	none	elab-aspect	elab-aspect
P86-1031	142-147,154-156	148-153	I will then add another dimension <*> to the constraint	called the " speaker identification "	I will then add another dimension <*> to the constraint	called the " speaker identification "	142-167	142-167	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	1<2	none	elab-addition	elab-addition
P86-1031	142-147,154-156	157-167	I will then add another dimension <*> to the constraint	to handle a group of special eases in Japanese discourse .	I will then add another dimension <*> to the constraint	to handle a group of special eases in Japanese discourse .	142-167	142-167	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	1<2	none	enablement	enablement
P86-1031	142-147,154-156	168-180	I will then add another dimension <*> to the constraint	It indicates a close association between centering and the speaker 's viewpoint ,	I will then add another dimension <*> to the constraint	It indicates a close association between centering and the speaker's viewpoint ,	142-167	168-196	I will then add another dimension called the " speaker identification " to the constraint to handle a group of special eases in Japanese discourse .	It indicates a close association between centering and the speaker 's viewpoint , and sheds light on what underlies the effect of perception reports on pronoun resolution in general.	1<2	none	elab-aspect	elab-aspect
P86-1031	168-180	181-196	It indicates a close association between centering and the speaker 's viewpoint ,	and sheds light on what underlies the effect of perception reports on pronoun resolution in general.	It indicates a close association between centering and the speaker's viewpoint ,	and sheds light on what underlies the effect of perception reports on pronoun resolution in general.	168-196	168-196	It indicates a close association between centering and the speaker 's viewpoint , and sheds light on what underlies the effect of perception reports on pronoun resolution in general.	It indicates a close association between centering and the speaker 's viewpoint , and sheds light on what underlies the effect of perception reports on pronoun resolution in general.	1<2	none	joint	joint
P86-1031	200-209	197-199,210-218	by drawing on facts in two very different languages ,	<*> These results , <*> demonstrate the cross-linguistic applicability of the centering framework .	by drawing on facts in two very different languages ,	These results , <*> demonstrate the cross-linguistic applicability of the centering framework .	197-218	197-218	These results , by drawing on facts in two very different languages , demonstrate the cross-linguistic applicability of the centering framework .	These results , by drawing on facts in two very different languages , demonstrate the cross-linguistic applicability of the centering framework .	1>2	none	manner-means	manner-means
P86-1031	1-14	197-199,210-218	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	<*> These results , <*> demonstrate the cross-linguistic applicability of the centering framework .	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	These results , <*> demonstrate the cross-linguistic applicability of the centering framework .	1-14	197-218	A constraint is proposed in the Centering approach to pronoun resolution in discourse .	These results , by drawing on facts in two very different languages , demonstrate the cross-linguistic applicability of the centering framework .	1<2	none	evaluation	evaluation
P86-1032	1-12	13-15,35-43	Existing models of plan inference ( PI ) in conversation have assumed	<*> that the agent <*> have identical beliefs about actions in the domain .	Existing models of plan inference ( PI ) in conversation have assumed	that the agent <*> have identical beliefs about actions in the domain .	1-43	1-43	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	1>2	none	attribution	attribution
P86-1032	13-15	16-24	that the agent	whose plan is being inferred ( the actor )	that the agent	whose plan is being inferred ( the actor )	1-43	1-43	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	1<2	none	elab-addition	elab-addition
P86-1032	13-15	25-27	that the agent	and the agent	that the agent	and the agent	1-43	1-43	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	1<2	none	joint	joint
P86-1032	25-27	28-34	and the agent	drawing the inference ( the observer )	and the agent	drawing the inference ( the observer )	1-43	1-43	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	1<2	none	elab-addition	elab-addition
P86-1032	13-15,35-43	46-61	<*> that the agent <*> have identical beliefs about actions in the domain .	that this assumption often results in failure of both the PI process and the communicative process	that the agent <*> have identical beliefs about actions in the domain .	that this assumption often results in failure of both the PI process and the communicative process	1-43	44-68	Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain .	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	1>2	none	bg-general	bg-general
P86-1032	44-45	46-61	I argue	that this assumption often results in failure of both the PI process and the communicative process	I argue	that this assumption often results in failure of both the PI process and the communicative process	44-68	44-68	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	1>2	none	attribution	attribution
P86-1032	46-61	88-93	that this assumption often results in failure of both the PI process and the communicative process	I describe a model of P1	that this assumption often results in failure of both the PI process and the communicative process	I describe a model of P1	44-68	88-98	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	I describe a model of P1 that abandons this assumption .	1>2	none	bg-goal	bg-goal
P86-1032	46-61	62-68	that this assumption often results in failure of both the PI process and the communicative process	that PI is meant to support .	that this assumption often results in failure of both the PI process and the communicative process	that PI is meant to support .	44-68	44-68	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	1<2	none	elab-addition	elab-addition
P86-1032	46-61	69-81	that this assumption often results in failure of both the PI process and the communicative process	In particular , it precludes the principled generation of appropriate responses to queries	that this assumption often results in failure of both the PI process and the communicative process	In particular , it precludes the principled generation of appropriate responses to queries	44-68	69-87	I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support .	In particular , it precludes the principled generation of appropriate responses to queries that arise from invalid plans .	1<2	none	joint	joint
P86-1032	69-81	82-87	In particular , it precludes the principled generation of appropriate responses to queries	that arise from invalid plans .	In particular , it precludes the principled generation of appropriate responses to queries	that arise from invalid plans .	69-87	69-87	In particular , it precludes the principled generation of appropriate responses to queries that arise from invalid plans .	In particular , it precludes the principled generation of appropriate responses to queries that arise from invalid plans .	1<2	none	elab-addition	elab-addition
P86-1032	88-93	94-98	I describe a model of P1	that abandons this assumption .	I describe a model of P1	that abandons this assumption .	88-98	88-98	I describe a model of P1 that abandons this assumption .	I describe a model of P1 that abandons this assumption .	1<2	none	elab-addition	elab-addition
P86-1032	88-93	99-109	I describe a model of P1	It rests on an analysis of plans as mental phenomena .	I describe a model of P1	It rests on an analysis of plans as mental phenomena .	88-98	99-109	I describe a model of P1 that abandons this assumption .	It rests on an analysis of plans as mental phenomena .	1<2	none	elab-addition	elab-addition
P86-1032	110	111-115	Judgements	that a plan is invalid	Judgements	that a plan is invalid	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	elab-addition	elab-addition
P86-1032	88-93	110,116-123	I describe a model of P1	<*> Judgements <*> are associated with particular discrepancies between the beliefs	I describe a model of P1	Judgements <*> are associated with particular discrepancies between the beliefs	88-98	110-150	I describe a model of P1 that abandons this assumption .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	elab-aspect	elab-aspect
P86-1032	110,116-123	124-130	<*> Judgements <*> are associated with particular discrepancies between the beliefs	that the observer ascribes to the actor	Judgements <*> are associated with particular discrepancies between the beliefs	that the observer ascribes to the actor	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	elab-addition	elab-addition
P86-1032	131-134	135-141	when the former believes	that the latter has some plan ,	when the former believes	that the latter has some plan ,	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1>2	none	attribution	attribution
P86-1032	110,116-123	135-141	<*> Judgements <*> are associated with particular discrepancies between the beliefs	that the latter has some plan ,	Judgements <*> are associated with particular discrepancies between the beliefs	that the latter has some plan ,	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	temporal	temporal
P86-1032	110,116-123	142-144	<*> Judgements <*> are associated with particular discrepancies between the beliefs	and the beliefs	Judgements <*> are associated with particular discrepancies between the beliefs	and the beliefs	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	joint	joint
P86-1032	142-144	145-150	and the beliefs	that the observer herself holds .	and the beliefs	that the observer herself holds .	110-150	110-150	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan , and the beliefs that the observer herself holds .	1<2	none	elab-addition	elab-addition
P86-1032	151-152	153-173	I show	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	I show	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	151-186	151-186	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	1>2	none	attribution	attribution
P86-1032	88-93	153-173	I describe a model of P1	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	I describe a model of P1	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	88-98	151-186	I describe a model of P1 that abandons this assumption .	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	1<2	none	elab-aspect	elab-aspect
P86-1032	153-173	174-180	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	judged to be present in the plan	that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief	judged to be present in the plan	151-186	151-186	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	1<2	none	elab-addition	elab-addition
P86-1032	174-180	181-186	judged to be present in the plan	inferred to underlie that query .	judged to be present in the plan	inferred to underlie that query .	151-186	151-186	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query .	1<2	none	elab-addition	elab-addition
P86-1032	187-189	190-191	The PI model	described here	The PI model	described here	187-211	187-211	The PI model described here has been implemented in SPIRIT , a small demonstration system that answers questions about the domain of computer mail .	The PI model described here has been implemented in SPIRIT , a small demonstration system that answers questions about the domain of computer mail .	1<2	none	elab-addition	elab-addition
P86-1032	88-93	187-189,192-201	I describe a model of P1	<*> The PI model <*> has been implemented in SPIRIT , a small demonstration system	I describe a model of P1	The PI model <*> has been implemented in SPIRIT , a small demonstration system	88-98	187-211	I describe a model of P1 that abandons this assumption .	The PI model described here has been implemented in SPIRIT , a small demonstration system that answers questions about the domain of computer mail .	1<2	none	evaluation	evaluation
P86-1032	187-189,192-201	202-211	<*> The PI model <*> has been implemented in SPIRIT , a small demonstration system	that answers questions about the domain of computer mail .	The PI model <*> has been implemented in SPIRIT , a small demonstration system	that answers questions about the domain of computer mail .	187-211	187-211	The PI model described here has been implemented in SPIRIT , a small demonstration system that answers questions about the domain of computer mail .	The PI model described here has been implemented in SPIRIT , a small demonstration system that answers questions about the domain of computer mail .	1<2	none	elab-addition	elab-addition
P86-1033	1-8	9-20	To fully understand a sequence of utterances ,	one must be able to infer implicit relationships between the utterances .	To fully understand a sequence of utterances ,	one must be able to infer implicit relationships between the utterances .	1-20	1-20	To fully understand a sequence of utterances , one must be able to infer implicit relationships between the utterances .	To fully understand a sequence of utterances , one must be able to infer implicit relationships between the utterances .	1>2	none	enablement	enablement
P86-1033	9-20	55-71	one must be able to infer implicit relationships between the utterances .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	one must be able to infer implicit relationships between the utterances .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	1-20	55-71	To fully understand a sequence of utterances , one must be able to infer implicit relationships between the utterances .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	1>2	none	bg-goal	bg-goal
P86-1033	21-37	38-54	Although the identification of sets of utterance relationships forms the basis for many theories of discourse ,	the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	Although the identification of sets of utterance relationships forms the basis for many theories of discourse ,	the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	21-54	21-54	Although the identification of sets of utterance relationships forms the basis for many theories of discourse , the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	Although the identification of sets of utterance relationships forms the basis for many theories of discourse , the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	1>2	none	contrast	contrast
P86-1033	38-54	55-71	the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	21-54	55-71	Although the identification of sets of utterance relationships forms the basis for many theories of discourse , the formalization and recognition of such relationships has proven to be an extremely difficult computational task .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	1>2	none	bg-goal	bg-goal
P86-1033	55-71	72-78	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	Relationships are formulated as discourse plans ,	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	Relationships are formulated as discourse plans ,	55-71	72-96	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	Relationships are formulated as discourse plans , which allows their representation in terms of planning operators and their computation via a plan recognition process .	1<2	none	elab-aspect	elab-aspect
P86-1033	72-78	79-96	Relationships are formulated as discourse plans ,	which allows their representation in terms of planning operators and their computation via a plan recognition process .	Relationships are formulated as discourse plans ,	which allows their representation in terms of planning operators and their computation via a plan recognition process .	72-96	72-96	Relationships are formulated as discourse plans , which allows their representation in terms of planning operators and their computation via a plan recognition process .	Relationships are formulated as discourse plans , which allows their representation in terms of planning operators and their computation via a plan recognition process .	1<2	none	elab-addition	elab-addition
P86-1033	97-101	102-121	By incorporating complex inferential processes	relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	By incorporating complex inferential processes	relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	97-121	97-121	By incorporating complex inferential processes relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	By incorporating complex inferential processes relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	1>2	none	manner-means	manner-means
P86-1033	55-71	102-121	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	55-71	97-121	This paper presents a plan-based approach to the representation and recognition of implicit relationships between utterances .	By incorporating complex inferential processes relating utterances into a plan-based framework , a formalization and computability not available in the earlier works is provided .	1<2	none	elab-aspect	elab-aspect
P86-1034	1-9	10-19	Novice users engaged in task-oriented dialogues with an adviser	to learn how to use an unfamiliar statistical package .	Novice users engaged in task-oriented dialogues with an adviser	to learn how to use an unfamiliar statistical package .	1-19	1-19	Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package .	Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package .	1<2	none	enablement	enablement
P86-1034	1-9	20-26	Novice users engaged in task-oriented dialogues with an adviser	The users ' , task was analyzed	Novice users engaged in task-oriented dialogues with an adviser	The users ' , task was analyzed	1-19	20-33	Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package .	The users ' , task was analyzed and a task structure was derived .	1<2	none	elab-aspect	elab-aspect
P86-1034	20-26	27-33	The users ' , task was analyzed	and a task structure was derived .	The users ' , task was analyzed	and a task structure was derived .	20-33	20-33	The users ' , task was analyzed and a task structure was derived .	The users ' , task was analyzed and a task structure was derived .	1<2	none	joint	joint
P86-1034	1-9	34-38	Novice users engaged in task-oriented dialogues with an adviser	The task structure was used	Novice users engaged in task-oriented dialogues with an adviser	The task structure was used	1-19	34-53	Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package .	The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task .	1<2	none	elab-aspect	elab-aspect
P86-1034	34-38	39-44	The task structure was used	to segment the dialogue into subdialogues	The task structure was used	to segment the dialogue into subdialogues	34-53	34-53	The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task .	The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task .	1<2	none	enablement	enablement
P86-1034	39-44	45-53	to segment the dialogue into subdialogues	associated with the subtasks of the overall task .	to segment the dialogue into subdialogues	associated with the subtasks of the overall task .	34-53	34-53	The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task .	The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task .	1<2	none	elab-addition	elab-addition
P86-1034	1-9	54-65,73-79	Novice users engaged in task-oriented dialogues with an adviser	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	Novice users engaged in task-oriented dialogues with an adviser	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	1-19	54-79	Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package .	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	1<2	none	elab-aspect	elab-aspect
P86-1034	54-65,73-79	66-72	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	partly corresponding to the task structure ,	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	partly corresponding to the task structure ,	54-79	54-79	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	1<2	none	elab-addition	elab-addition
P86-1034	54-65,73-79	80-104	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	54-79	80-104	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	1<2	none	elab-process_step	elab-process_step
P86-1034	80-104	105-118	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	80-104	105-133	First , the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	1<2	none	elab-aspect	elab-aspect
P86-1034	105-118	119-122	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	as can be expected	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	as can be expected	105-133	105-133	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	1<2	none	elab-addition	elab-addition
P86-1034	105-118	123-133	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	since one of their functions is to indicate topic shifts .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	since one of their functions is to indicate topic shifts .	105-133	105-133	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	1<2	none	exp-reason	exp-reason
P86-1034	105-118	134-160	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later ,	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	105-133	134-173	Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later , as can be expected since one of their functions is to indicate topic shifts .	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	1<2	none	joint	joint
P86-1034	134-160	161-164	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	as can be expected	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	as can be expected	134-173	134-173	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	1<2	none	elab-addition	elab-addition
P86-1034	134-160	165-168	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	since they are used	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues ,	since they are used	134-173	134-173	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	1<2	none	exp-reason	exp-reason
P86-1034	165-168	169-173	since they are used	to indicate topic continuity .	since they are used	to indicate topic continuity .	134-173	134-173	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	On the other hand , pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues , as can be expected since they are used to indicate topic continuity .	1<2	none	enablement	enablement
P86-1034	54-65,73-79	174-199	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	Second , the distributions of the antecedents of pronominal noun phrases and of non-pronominal noun phrases showed a pattern consistent with the derived dialogue structure .	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	Second , the distributions of the antecedents of pronominal noun phrases and of non-pronominal noun phrases showed a pattern consistent with the derived dialogue structure .	54-79	174-199	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	Second , the distributions of the antecedents of pronominal noun phrases and of non-pronominal noun phrases showed a pattern consistent with the derived dialogue structure .	1<2	none	elab-process_step	elab-process_step
P86-1034	54-65,73-79	200-218	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	FinMly , distinctive clue words and phrases were found reliably at the boundaries of subdialogues with different functions .	The representation of the dialogue structure into a hierarchy of subdialogues , <*> was validated by three converging analyses .	FinMly , distinctive clue words and phrases were found reliably at the boundaries of subdialogues with different functions .	54-79	200-218	The representation of the dialogue structure into a hierarchy of subdialogues , partly corresponding to the task structure , was validated by three converging analyses .	FinMly , distinctive clue words and phrases were found reliably at the boundaries of subdialogues with different functions .	1<2	none	elab-process_step	elab-process_step
P86-1036	1-5	6-10	A new method is presented	for simplifying the logical expressions	A new method is presented	for simplifying the logical expressions	1-21	1-21	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	1<2	none	enablement	enablement
P86-1036	6-10	11-21	for simplifying the logical expressions	used to represent utterance meaning in a natural language system .	for simplifying the logical expressions	used to represent utterance meaning in a natural language system .	1-21	1-21	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	1<2	none	elab-addition	elab-addition
P86-1036	1-5	22-39	A new method is presented	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system	A new method is presented	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system	1-21	22-48	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system to reduce the constituent structure of logical expressions .	1<2	none	elab-addition	elab-addition
P86-1036	22-39	40-48	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system	to reduce the constituent structure of logical expressions .	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system	to reduce the constituent structure of logical expressions .	22-48	22-48	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system to reduce the constituent structure of logical expressions .	This simplification method utilizes the encoded knowledge and the limited inference-making capability of a taxonomic knowledge representation system to reduce the constituent structure of logical expressions .	1<2	none	enablement	enablement
P86-1036	1-5	49-55	A new method is presented	The specific application is to the problem	A new method is presented	The specific application is to the problem	1-21	49-73	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	The specific application is to the problem of mapping expressions of the meaning representation language to a database language capable of retrieving actual responses .	1<2	none	bg-goal	bg-goal
P86-1036	49-55	56-67	The specific application is to the problem	of mapping expressions of the meaning representation language to a database language	The specific application is to the problem	of mapping expressions of the meaning representation language to a database language	49-73	49-73	The specific application is to the problem of mapping expressions of the meaning representation language to a database language capable of retrieving actual responses .	The specific application is to the problem of mapping expressions of the meaning representation language to a database language capable of retrieving actual responses .	1<2	none	elab-addition	elab-addition
P86-1036	56-67	68-73	of mapping expressions of the meaning representation language to a database language	capable of retrieving actual responses .	of mapping expressions of the meaning representation language to a database language	capable of retrieving actual responses .	49-73	49-73	The specific application is to the problem of mapping expressions of the meaning representation language to a database language capable of retrieving actual responses .	The specific application is to the problem of mapping expressions of the meaning representation language to a database language capable of retrieving actual responses .	1<2	none	elab-addition	elab-addition
P86-1036	1-5	74-85	A new method is presented	Particular account is taken of the model-theoretic aspects of this problem .	A new method is presented	Particular account is taken of the model-theoretic aspects of this problem .	1-21	74-85	A new method is presented for simplifying the logical expressions used to represent utterance meaning in a natural language system .	Particular account is taken of the model-theoretic aspects of this problem .	1<2	none	elab-aspect	elab-aspect
P86-1037	1-22	23-42	Consideration of the question of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	Consideration of the question of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	1-22	23-64	Consideration of the question of meaning in the framework of linguistics often requires an allusion to sets and other higher-order notions .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	1>2	none	bg-general	bg-general
P86-1037	23-42	65-80	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	In this paper we shall consider the use of a higher-order logic for this task .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	In this paper we shall consider the use of a higher-order logic for this task .	23-64	65-80	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	In this paper we shall consider the use of a higher-order logic for this task .	1>2	none	bg-compare	bg-compare
P86-1037	23-42	43-49	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	that are either based on first-order logic	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems	that are either based on first-order logic	23-64	23-64	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	1<2	none	elab-addition	elab-addition
P86-1037	43-49	50-53	that are either based on first-order logic	or that use mechanisms	that are either based on first-order logic	or that use mechanisms	23-64	23-64	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	1<2	none	joint	joint
P86-1037	50-53	54-64	or that use mechanisms	whose formal justifications are to be provided after the fact .	or that use mechanisms	whose formal justifications are to be provided after the fact .	23-64	23-64	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	The traditional approach to representing and reasoning about meaning in a computational setting has been to use knowledge representation systems that are either based on first-order logic or that use mechanisms whose formal justifications are to be provided after the fact .	1<2	none	elab-addition	elab-addition
P86-1037	65-80	81-93	In this paper we shall consider the use of a higher-order logic for this task .	We first present a version of definite clauses ( positive Horn clauses )	In this paper we shall consider the use of a higher-order logic for this task .	We first present a version of definite clauses ( positive Horn clauses )	65-80	81-100	In this paper we shall consider the use of a higher-order logic for this task .	We first present a version of definite clauses ( positive Horn clauses ) that is based on this logic .	1<2	none	elab-aspect	elab-aspect
P86-1037	81-93	94-100	We first present a version of definite clauses ( positive Horn clauses )	that is based on this logic .	We first present a version of definite clauses ( positive Horn clauses )	that is based on this logic .	81-100	81-100	We first present a version of definite clauses ( positive Horn clauses ) that is based on this logic .	We first present a version of definite clauses ( positive Horn clauses ) that is based on this logic .	1<2	none	elab-addition	elab-addition
P86-1037	81-93	101-109	We first present a version of definite clauses ( positive Horn clauses )	Predicate and function variables may occur in such clauses	We first present a version of definite clauses ( positive Horn clauses )	Predicate and function variables may occur in such clauses	81-100	101-120	We first present a version of definite clauses ( positive Horn clauses ) that is based on this logic .	Predicate and function variables may occur in such clauses and the terms in the language are the typed λ-terms .	1<2	none	elab-aspect	elab-aspect
P86-1037	101-109	110-120	Predicate and function variables may occur in such clauses	and the terms in the language are the typed λ-terms .	Predicate and function variables may occur in such clauses	and the terms in the language are the typed λ-terms .	101-120	101-120	Predicate and function variables may occur in such clauses and the terms in the language are the typed λ-terms .	Predicate and function variables may occur in such clauses and the terms in the language are the typed λ-terms .	1<2	none	joint	joint
P86-1037	81-93	121-126	We first present a version of definite clauses ( positive Horn clauses )	Such term structures have a richness	We first present a version of definite clauses ( positive Horn clauses )	Such term structures have a richness	81-100	121-134	We first present a version of definite clauses ( positive Horn clauses ) that is based on this logic .	Such term structures have a richness that may be exploited in representing meanings .	1<2	none	elab-aspect	elab-aspect
P86-1037	121-126	127-134	Such term structures have a richness	that may be exploited in representing meanings .	Such term structures have a richness	that may be exploited in representing meanings .	121-134	121-134	Such term structures have a richness that may be exploited in representing meanings .	Such term structures have a richness that may be exploited in representing meanings .	1<2	none	elab-addition	elab-addition
P86-1037	101-109	135-143	Predicate and function variables may occur in such clauses	We also describe a higher-order logic programming language ,	Predicate and function variables may occur in such clauses	We also describe a higher-order logic programming language ,	101-120	135-161	Predicate and function variables may occur in such clauses and the terms in the language are the typed λ-terms .	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	1<2	none	joint	joint
P86-1037	135-143	144-146	We also describe a higher-order logic programming language ,	called λProlog ,	We also describe a higher-order logic programming language ,	called λProlog ,	135-161	135-161	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	1<2	none	elab-addition	elab-addition
P86-1037	135-143	147-153	We also describe a higher-order logic programming language ,	which represents programs as higher-order definite clauses	We also describe a higher-order logic programming language ,	which represents programs as higher-order definite clauses	135-161	135-161	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	1<2	none	elab-addition	elab-addition
P86-1037	135-143	154-156	We also describe a higher-order logic programming language ,	and interprets them	We also describe a higher-order logic programming language ,	and interprets them	135-161	135-161	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	1<2	none	joint	joint
P86-1037	154-156	157-161	and interprets them	using a depth-first interpreter .	and interprets them	using a depth-first interpreter .	135-161	135-161	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	1<2	none	manner-means	manner-means
P86-1037	135-143	162-176	We also describe a higher-order logic programming language ,	A virtue of this language is that it is possible to write programs in it	We also describe a higher-order logic programming language ,	A virtue of this language is that it is possible to write programs in it	135-161	162-187	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	A virtue of this language is that it is possible to write programs in it that integrate syntactic and semantic analyses into one computational paradigm .	1<2	none	elab-aspect	elab-aspect
P86-1037	162-176	177-187	A virtue of this language is that it is possible to write programs in it	that integrate syntactic and semantic analyses into one computational paradigm .	A virtue of this language is that it is possible to write programs in it	that integrate syntactic and semantic analyses into one computational paradigm .	162-187	162-187	A virtue of this language is that it is possible to write programs in it that integrate syntactic and semantic analyses into one computational paradigm .	A virtue of this language is that it is possible to write programs in it that integrate syntactic and semantic analyses into one computational paradigm .	1<2	none	elab-addition	elab-addition
P86-1037	135-143	188-197	We also describe a higher-order logic programming language ,	This is to be contrasted with the more common practice	We also describe a higher-order logic programming language ,	This is to be contrasted with the more common practice	135-161	188-221	We also describe a higher-order logic programming language , called λProlog , which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter .	This is to be contrasted with the more common practice of using two entirely different computation paradigms , such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	1<2	none	elab-aspect	elab-aspect
P86-1037	188-197	198-205	This is to be contrasted with the more common practice	of using two entirely different computation paradigms ,	This is to be contrasted with the more common practice	of using two entirely different computation paradigms ,	188-221	188-221	This is to be contrasted with the more common practice of using two entirely different computation paradigms , such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	This is to be contrasted with the more common practice of using two entirely different computation paradigms , such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	1<2	none	elab-addition	elab-addition
P86-1037	198-205	206-221	of using two entirely different computation paradigms ,	such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	of using two entirely different computation paradigms ,	such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	188-221	188-221	This is to be contrasted with the more common practice of using two entirely different computation paradigms , such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	This is to be contrasted with the more common practice of using two entirely different computation paradigms , such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing .	1<2	none	elab-example	elab-example
P86-1037	222-229	239-256	We illustrate such an integration in this language	that its use makes the task of providing formal justifications for the computations specified much more direct .	We illustrate such an integration in this language	that its use makes the task of providing formal justifications for the computations specified much more direct .	222-256	222-256	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	1>2	none	joint	joint
P86-1037	222-229	230-235	We illustrate such an integration in this language	by considering a simple example ,	We illustrate such an integration in this language	by considering a simple example ,	222-256	222-256	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	1<2	none	manner-means	manner-means
P86-1037	236-238	239-256	and we claim	that its use makes the task of providing formal justifications for the computations specified much more direct .	and we claim	that its use makes the task of providing formal justifications for the computations specified much more direct .	222-256	222-256	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	1>2	none	attribution	attribution
P86-1037	65-80	239-256	In this paper we shall consider the use of a higher-order logic for this task .	that its use makes the task of providing formal justifications for the computations specified much more direct .	In this paper we shall consider the use of a higher-order logic for this task .	that its use makes the task of providing formal justifications for the computations specified much more direct .	65-80	222-256	In this paper we shall consider the use of a higher-order logic for this task .	We illustrate such an integration in this language by considering a simple example , and we claim that its use makes the task of providing formal justifications for the computations specified much more direct .	1<2	none	evaluation	evaluation
P86-1038	1-5	48-52	Unification-based grammar formalisms use structures	We have developed a model	Unification-based grammar formalisms use structures	We have developed a model	1-14	48-76	Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	1>2	none	bg-general	bg-general
P86-1038	1-5	6-9	Unification-based grammar formalisms use structures	containing sets of features	Unification-based grammar formalisms use structures	containing sets of features	1-14	1-14	Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	1<2	none	elab-addition	elab-addition
P86-1038	1-5	10-14	Unification-based grammar formalisms use structures	to describe linguistic objects .	Unification-based grammar formalisms use structures	to describe linguistic objects .	1-14	1-14	Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects .	1<2	none	enablement	enablement
P86-1038	15-30	31-36	Although computational algorithms for unification of feature structures have been worked out in experimental research ,	these algcwithms become quite complicated ,	Although computational algorithms for unification of feature structures have been worked out in experimental research ,	these algcwithms become quite complicated ,	15-47	15-47	Although computational algorithms for unification of feature structures have been worked out in experimental research , these algcwithms become quite complicated , and a more precise description of feature structures is desirable .	Although computational algorithms for unification of feature structures have been worked out in experimental research , these algcwithms become quite complicated , and a more precise description of feature structures is desirable .	1>2	none	contrast	contrast
P86-1038	31-36	48-52	these algcwithms become quite complicated ,	We have developed a model	these algcwithms become quite complicated ,	We have developed a model	15-47	48-76	Although computational algorithms for unification of feature structures have been worked out in experimental research , these algcwithms become quite complicated , and a more precise description of feature structures is desirable .	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	1>2	none	bg-compare	bg-compare
P86-1038	31-36	37-47	these algcwithms become quite complicated ,	and a more precise description of feature structures is desirable .	these algcwithms become quite complicated ,	and a more precise description of feature structures is desirable .	15-47	15-47	Although computational algorithms for unification of feature structures have been worked out in experimental research , these algcwithms become quite complicated , and a more precise description of feature structures is desirable .	Although computational algorithms for unification of feature structures have been worked out in experimental research , these algcwithms become quite complicated , and a more precise description of feature structures is desirable .	1<2	none	joint	joint
P86-1038	48-52	53-65	We have developed a model	in which descriptions of feature structures can be regarded as logical formulas ,	We have developed a model	in which descriptions of feature structures can be regarded as logical formulas ,	48-76	48-76	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	1<2	none	elab-addition	elab-addition
P86-1038	53-65	66-72	in which descriptions of feature structures can be regarded as logical formulas ,	and interpreted by sets of directed graphs	in which descriptions of feature structures can be regarded as logical formulas ,	and interpreted by sets of directed graphs	48-76	48-76	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	1<2	none	joint	joint
P86-1038	66-72	73-76	and interpreted by sets of directed graphs	which satisfy them .	and interpreted by sets of directed graphs	which satisfy them .	48-76	48-76	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	1<2	none	elab-addition	elab-addition
P86-1038	66-72	77-94	and interpreted by sets of directed graphs	These graphs are , in fact , transition graphs for a special type of deterministic finite automaton .	and interpreted by sets of directed graphs	These graphs are , in fact , transition graphs for a special type of deterministic finite automaton .	48-76	77-94	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	These graphs are , in fact , transition graphs for a special type of deterministic finite automaton .	1<2	none	elab-addition	elab-addition
P86-1038	48-52	95-109	We have developed a model	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- ,	We have developed a model	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- ,	48-76	95-127	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	1<2	none	elab-aspect	elab-aspect
P86-1038	95-109	110-115	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- ,	by providing an interpretation for values	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- ,	by providing an interpretation for values	95-127	95-127	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	1<2	none	manner-means	manner-means
P86-1038	110-115	116-123	by providing an interpretation for values	which are specified by disjunctions and path values	by providing an interpretation for values	which are specified by disjunctions and path values	95-127	95-127	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	1<2	none	elab-addition	elab-addition
P86-1038	116-123	124-127	which are specified by disjunctions and path values	embedded within disjunctions .	which are specified by disjunctions and path values	embedded within disjunctions .	95-127	95-127	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	This semantics for feature structures extends the ideas of Pereira and Shieber -LSB-11 -RSB- , by providing an interpretation for values which are specified by disjunctions and path values embedded within disjunctions .	1<2	none	elab-addition	elab-addition
P86-1038	48-52	128-136	We have developed a model	Our interpretati6n differs from that of Pereira and Shieber	We have developed a model	Our interpretati6n differs from that of Pereira and Shieber	48-76	128-148	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	Our interpretati6n differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .	1<2	none	elab-aspect	elab-aspect
P86-1038	128-136	137-148	Our interpretati6n differs from that of Pereira and Shieber	by using a logical model in place of a denotational semantics .	Our interpretati6n differs from that of Pereira and Shieber	by using a logical model in place of a denotational semantics .	128-148	128-148	Our interpretati6n differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .	Our interpretati6n differs from that of Pereira and Shieber by using a logical model in place of a denotational semantics .	1<2	none	manner-means	manner-means
P86-1038	48-52	149-157	We have developed a model	This logical model yields a calculus of equivalences ,	We have developed a model	This logical model yields a calculus of equivalences ,	48-76	149-165	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	This logical model yields a calculus of equivalences , which can be used to simplify formulas .	1<2	none	elab-aspect	elab-aspect
P86-1038	149-157	158-161	This logical model yields a calculus of equivalences ,	which can be used	This logical model yields a calculus of equivalences ,	which can be used	149-165	149-165	This logical model yields a calculus of equivalences , which can be used to simplify formulas .	This logical model yields a calculus of equivalences , which can be used to simplify formulas .	1<2	none	elab-addition	elab-addition
P86-1038	158-161	162-165	which can be used	to simplify formulas .	which can be used	to simplify formulas .	149-165	149-165	This logical model yields a calculus of equivalences , which can be used to simplify formulas .	This logical model yields a calculus of equivalences , which can be used to simplify formulas .	1<2	none	enablement	enablement
P86-1038	166-169	175-181	Unification is attractive ,	but it is often computationally inefficient .	Unification is attractive ,	but it is often computationally inefficient .	166-181	166-181	Unification is attractive , because of its generality , but it is often computationally inefficient .	Unification is attractive , because of its generality , but it is often computationally inefficient .	1>2	none	contrast	contrast
P86-1038	166-169	170-174	Unification is attractive ,	because of its generality ,	Unification is attractive ,	because of its generality ,	166-181	166-181	Unification is attractive , because of its generality , but it is often computationally inefficient .	Unification is attractive , because of its generality , but it is often computationally inefficient .	1<2	none	exp-reason	exp-reason
P86-1038	48-52	175-181	We have developed a model	but it is often computationally inefficient .	We have developed a model	but it is often computationally inefficient .	48-76	166-181	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	Unification is attractive , because of its generality , but it is often computationally inefficient .	1<2	none	elab-aspect	elab-aspect
P86-1038	48-52	182-195	We have developed a model	Our mode -RSB- allows a careful examination of the computational complexity of unification .	We have developed a model	Our mode -RSB- allows a careful examination of the computational complexity of unification .	48-76	182-195	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	Our mode -RSB- allows a careful examination of the computational complexity of unification .	1<2	none	elab-aspect	elab-aspect
P86-1038	196-198	199-210	We have shown	that the consistency problem for formulas with disjunctive values is NP-complete .	We have shown	that the consistency problem for formulas with disjunctive values is NP-complete .	196-210	196-210	We have shown that the consistency problem for formulas with disjunctive values is NP-complete .	We have shown that the consistency problem for formulas with disjunctive values is NP-complete .	1>2	none	attribution	attribution
P86-1038	199-210	219-227	that the consistency problem for formulas with disjunctive values is NP-complete .	how disjunctive values can be specified in a way	that the consistency problem for formulas with disjunctive values is NP-complete .	how disjunctive values can be specified in a way	196-210	211-235	We have shown that the consistency problem for formulas with disjunctive values is NP-complete .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1>2	none	bg-goal	bg-goal
P86-1038	211-216	219-227	To deal with this complexity ,	how disjunctive values can be specified in a way	To deal with this complexity ,	how disjunctive values can be specified in a way	211-235	211-235	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1>2	none	enablement	enablement
P86-1038	217-218	219-227	we describe	how disjunctive values can be specified in a way	we describe	how disjunctive values can be specified in a way	211-235	211-235	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1>2	none	attribution	attribution
P86-1038	48-52	219-227	We have developed a model	how disjunctive values can be specified in a way	We have developed a model	how disjunctive values can be specified in a way	48-76	211-235	We have developed a model in which descriptions of feature structures can be regarded as logical formulas , and interpreted by sets of directed graphs which satisfy them .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1<2	none	elab-aspect	elab-aspect
P86-1038	219-227	228-230	how disjunctive values can be specified in a way	which delays expansion	how disjunctive values can be specified in a way	which delays expansion	211-235	211-235	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1<2	none	elab-addition	elab-addition
P86-1038	228-230	231-235	which delays expansion	to disjunctive normal form .	which delays expansion	to disjunctive normal form .	211-235	211-235	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	To deal with this complexity , we describe how disjunctive values can be specified in a way which delays expansion to disjunctive normal form .	1<2	none	enablement	enablement
P96-1001	1-6	7-15,22-25	In this paper , we show	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	In this paper , we show	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	1-43	1-43	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	1>2	none	attribution	attribution
P96-1001	7-15,22-25	16-21	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	developed for automated theorem proving -	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	developed for automated theorem proving -	1-43	1-43	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	1<2	none	elab-addition	elab-addition
P96-1001	7-15,22-25	26-43	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	1-43	1-43	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	1<2	none	enablement	enablement
P96-1001	7-15,22-25	44-56	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	In particular , it provides the general theory for the Primary Occurrence Restriction	that Higher-Order Coloured Unification - a form of unification <*> provides a general theory	In particular , it provides the general theory for the Primary Occurrence Restriction	1-43	44-69	In this paper , we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic , non semantic information .	In particular , it provides the general theory for the Primary Occurrence Restriction which ( Dalrymple et al. , 1991 ) 's analysis called for .	1<2	none	elab-aspect	elab-aspect
P96-1001	44-56	57-69	In particular , it provides the general theory for the Primary Occurrence Restriction	which ( Dalrymple et al. , 1991 ) 's analysis called for .	In particular , it provides the general theory for the Primary Occurrence Restriction	which ( Dalrymple et al. , 1991 )'s analysis called for .	44-69	44-69	In particular , it provides the general theory for the Primary Occurrence Restriction which ( Dalrymple et al. , 1991 ) 's analysis called for .	In particular , it provides the general theory for the Primary Occurrence Restriction which ( Dalrymple et al. , 1991 ) 's analysis called for .	1<2	none	elab-addition	elab-addition
P96-1002	1-26	67-78	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism	this model-theoretic approach can offer simpler and significantly clearer expression of theories	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism	this model-theoretic approach can offer simpler and significantly clearer expression of theories	1-44	45-99	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism --to express syntactic theories purely in terms of the properties of the class of structures they license .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1>2	none	bg-goal	bg-goal
P96-1002	1-26	27-41	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism	--to express syntactic theories purely in terms of the properties of the class of structures	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism	--to express syntactic theories purely in terms of the properties of the class of structures	1-44	1-44	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism --to express syntactic theories purely in terms of the properties of the class of structures they license .	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism --to express syntactic theories purely in terms of the properties of the class of structures they license .	1<2	none	elab-addition	elab-addition
P96-1002	27-41	42-44	--to express syntactic theories purely in terms of the properties of the class of structures	they license .	--to express syntactic theories purely in terms of the properties of the class of structures	they license .	1-44	1-44	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism --to express syntactic theories purely in terms of the properties of the class of structures they license .	A natural next step in the evolution of constraint-based grammar formalisms from rewriting formalisms is to abstract fully away from the details of the grammar mechanism --to express syntactic theories purely in terms of the properties of the class of structures they license .	1<2	none	elab-addition	elab-addition
P96-1002	45-56	67-78	By focusing on the structural properties of languages rather than on mechanisms	this model-theoretic approach can offer simpler and significantly clearer expression of theories	By focusing on the structural properties of languages rather than on mechanisms	this model-theoretic approach can offer simpler and significantly clearer expression of theories	45-99	45-99	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1>2	none	manner-means	manner-means
P96-1002	45-56	57-61	By focusing on the structural properties of languages rather than on mechanisms	for generating or checking structures	By focusing on the structural properties of languages rather than on mechanisms	for generating or checking structures	45-99	45-99	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1<2	none	enablement	enablement
P96-1002	57-61	62-66	for generating or checking structures	that exhibit those properties ,	for generating or checking structures	that exhibit those properties ,	45-99	45-99	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1<2	none	elab-addition	elab-addition
P96-1002	67-78	100-116	this model-theoretic approach can offer simpler and significantly clearer expression of theories	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	this model-theoretic approach can offer simpler and significantly clearer expression of theories	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	45-99	100-156	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	1>2	none	bg-general	bg-general
P96-1002	67-78	79-86	this model-theoretic approach can offer simpler and significantly clearer expression of theories	and can potentially provide a uniform formalization ,	this model-theoretic approach can offer simpler and significantly clearer expression of theories	and can potentially provide a uniform formalization ,	45-99	45-99	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1<2	none	joint	joint
P96-1002	79-86	87-99	and can potentially provide a uniform formalization ,	allowing disparate theories to be compared on the basis of those properties .	and can potentially provide a uniform formalization ,	allowing disparate theories to be compared on the basis of those properties .	45-99	45-99	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	By focusing on the structural properties of languages rather than on mechanisms for generating or checking structures that exhibit those properties , this model-theoretic approach can offer simpler and significantly clearer expression of theories and can potentially provide a uniform formalization , allowing disparate theories to be compared on the basis of those properties .	1<2	none	elab-addition	elab-addition
P96-1002	100-116	117-126,144-151	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	that has the distinctive virtue of being superficially expressive - <*> languages are definable in L2K , p iff	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	that has the distinctive virtue of being superficially expressive - <*> languages are definable in L2K , p iff	100-156	100-156	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	1<2	none	elab-addition	elab-addition
P96-1002	117-126,144-151	127-136	that has the distinctive virtue of being superficially expressive - <*> languages are definable in L2K , p iff	supporting direct statement of most linguistically significant syntactic properties -	that has the distinctive virtue of being superficially expressive - <*> languages are definable in L2K , p iff	supporting direct statement of most linguistically significant syntactic properties -	100-156	100-156	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	1<2	none	elab-addition	elab-addition
P96-1002	127-136	137-143	supporting direct statement of most linguistically significant syntactic properties -	but having well-defined strong generative capacity -	supporting direct statement of most linguistically significant syntactic properties -	but having well-defined strong generative capacity -	100-156	100-156	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	1<2	none	contrast	contrast
P96-1002	100-116	152-156	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	they are strongly context-free .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	they are strongly context-free .	100-156	100-156	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	1<2	none	elab-addition	elab-addition
P96-1002	100-116	157-167	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	We draw examples from the realms of GPSG and GB .	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax	We draw examples from the realms of GPSG and GB .	100-156	157-167	We discuss L2 , p , a monadic second-order logical framework for such an approach to syntax that has the distinctive virtue of being superficially expressive - supporting direct statement of most linguistically significant syntactic properties - but having well-defined strong generative capacity - languages are definable in L2K , p iff they are strongly context-free .	We draw examples from the realms of GPSG and GB .	1<2	none	elab-aspect	elab-aspect
P96-1003	1-10	26-43	Information retrieval is an important application area of natural-language processing	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	Information retrieval is an important application area of natural-language processing	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	1-25	26-51	Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text .	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.	1>2	none	bg-goal	bg-goal
P96-1003	1-10	11-16	Information retrieval is an important application area of natural-language processing	where one encounters the genuine challenge	Information retrieval is an important application area of natural-language processing	where one encounters the genuine challenge	1-25	1-25	Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text .	Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text .	1<2	none	elab-addition	elab-addition
P96-1003	11-16	17-25	where one encounters the genuine challenge	of processing large quantities of unrestricted natural-language text .	where one encounters the genuine challenge	of processing large quantities of unrestricted natural-language text .	1-25	1-25	Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text .	Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text .	1<2	none	elab-addition	elab-addition
P96-1003	26-43	44-51	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	to create better indexing phrases for information retrieval.	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	to create better indexing phrases for information retrieval.	26-51	26-51	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.	1<2	none	enablement	enablement
P96-1003	26-43	52-74	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases	26-51	52-82	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics .	1<2	none	elab-aspect	elab-aspect
P96-1003	52-74	75-82	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases	using both corpus statistics and linguistic heuristics .	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases	using both corpus statistics and linguistic heuristics .	52-82	52-82	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics .	In particular , we describe a hybrid approach to the extraction of meaningful ( continuous or discontinuous ) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics .	1<2	none	manner-means	manner-means
P96-1003	83-86	87-104	Results of experiments show	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	Results of experiments show	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	83-104	83-104	Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	1>2	none	attribution	attribution
P96-1003	26-43	87-104	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	26-51	83-104	This paper reports on the application of a few simple , yet robust and efficient nounphrase analysis techniques to create better indexing phrases for information retrieval.	Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	1<2	none	evaluation	evaluation
P96-1003	87-104	105-120	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction .	that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction .	83-104	105-120	Results of experiments show that indexing based on such extracted subcompounds improves both recall and precision in an information retrieval system .	The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction .	1<2	none	elab-aspect	elab-aspect
P96-1004	1-10	27-32	Most natural language processing tasks require lexical semantic information .	This paper describes an acquisition method	Most natural language processing tasks require lexical semantic information .	This paper describes an acquisition method	1-10	27-46	Most natural language processing tasks require lexical semantic information .	This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information .	1>2	none	bg-goal	bg-goal
P96-1004	1-10	11-26	Most natural language processing tasks require lexical semantic information .	Automated acquisition of this information would thus increase the robustness and portability of NLP systems .	Most natural language processing tasks require lexical semantic information .	Automated acquisition of this information would thus increase the robustness and portability of NLP systems .	1-10	11-26	Most natural language processing tasks require lexical semantic information .	Automated acquisition of this information would thus increase the robustness and portability of NLP systems .	1<2	none	elab-aspect	elab-aspect
P96-1004	27-32	33-46	This paper describes an acquisition method	which makes use of fixed correspondences between derivational affixes and lexical semantic information .	This paper describes an acquisition method	which makes use of fixed correspondences between derivational affixes and lexical semantic information .	27-46	27-46	This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information .	This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information .	1<2	none	elab-addition	elab-addition
P96-1004	27-32	47-56,66-74	This paper describes an acquisition method	One advantage of this method , and of other methods <*> is that the necessary input is currently available .	This paper describes an acquisition method	One advantage of this method , and of other methods <*> is that the necessary input is currently available .	27-46	47-74	This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information .	One advantage of this method , and of other methods that rely only on surface characteristics of language , is that the necessary input is currently available .	1<2	none	elab-aspect	elab-aspect
P96-1004	47-56,66-74	57-65	One advantage of this method , and of other methods <*> is that the necessary input is currently available .	that rely only on surface characteristics of language ,	One advantage of this method , and of other methods <*> is that the necessary input is currently available .	that rely only on surface characteristics of language ,	47-74	47-74	One advantage of this method , and of other methods that rely only on surface characteristics of language , is that the necessary input is currently available .	One advantage of this method , and of other methods that rely only on surface characteristics of language , is that the necessary input is currently available .	1<2	none	elab-addition	elab-addition
P96-1005	1-24	25-33	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	The analysis is based on a set of LRs	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	The analysis is based on a set of LRs	1-24	25-48	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora .	1<2	none	elab-addition	elab-addition
P96-1005	25-33	34-48	The analysis is based on a set of LRs	implemented and tested on the basis of Spanish and English business- and finance-related corpora .	The analysis is based on a set of LRs	implemented and tested on the basis of Spanish and English business- and finance-related corpora .	25-48	25-48	The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora .	The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora .	1<2	none	elab-addition	elab-addition
P96-1005	49-52	61-66	We show that ,	they do not come costfree .	We show that ,	they do not come costfree .	49-66	49-66	We show that , though the use of LRs is justified , they do not come costfree .	We show that , though the use of LRs is justified , they do not come costfree .	1>2	none	attribution	attribution
P96-1005	53-60	61-66	though the use of LRs is justified ,	they do not come costfree .	though the use of LRs is justified ,	they do not come costfree .	49-66	49-66	We show that , though the use of LRs is justified , they do not come costfree .	We show that , though the use of LRs is justified , they do not come costfree .	1>2	none	contrast	contrast
P96-1005	1-24	61-66	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	they do not come costfree .	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	they do not come costfree .	1-24	49-66	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	We show that , though the use of LRs is justified , they do not come costfree .	1<2	none	elab-aspect	elab-aspect
P96-1005	67-78	82-87	Semi-automatic output checking is required , even with blocking and preemtion procedures	Nevertheless , largescope LRs are justified	Semi-automatic output checking is required , even with blocking and preemtion procedures	Nevertheless , largescope LRs are justified	67-81	82-99	Semi-automatic output checking is required , even with blocking and preemtion procedures built in .	Nevertheless , largescope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	1>2	none	contrast	contrast
P96-1005	67-78	79-81	Semi-automatic output checking is required , even with blocking and preemtion procedures	built in .	Semi-automatic output checking is required , even with blocking and preemtion procedures	built in .	67-81	67-81	Semi-automatic output checking is required , even with blocking and preemtion procedures built in .	Semi-automatic output checking is required , even with blocking and preemtion procedures built in .	1<2	none	elab-addition	elab-addition
P96-1005	1-24	82-87	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	Nevertheless , largescope LRs are justified	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	Nevertheless , largescope LRs are justified	1-24	82-99	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	Nevertheless , largescope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	1<2	none	elab-aspect	elab-aspect
P96-1005	82-87	88-99	Nevertheless , largescope LRs are justified	because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	Nevertheless , largescope LRs are justified	because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	82-99	82-99	Nevertheless , largescope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	Nevertheless , largescope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition .	1<2	none	exp-reason	exp-reason
P96-1005	100-102	103-116	We also argue	that the place of LRs in the computational process is a complex issue .	We also argue	that the place of LRs in the computational process is a complex issue .	100-116	100-116	We also argue that the place of LRs in the computational process is a complex issue .	We also argue that the place of LRs in the computational process is a complex issue .	1>2	none	attribution	attribution
P96-1005	1-24	103-116	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	that the place of LRs in the computational process is a complex issue .	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	that the place of LRs in the computational process is a complex issue .	1-24	100-116	This paper deals with the discovery , representation , and use of lexical rules ( LRs ) during large-scale semi-automatic computational lexicon acquisition .	We also argue that the place of LRs in the computational process is a complex issue .	1<2	none	elab-aspect	elab-aspect
P96-1006	1-16	17-22	In this paper , we present a new approach for word sense disambiguation ( WSD )	using an exemplar-based learning algorithm .	In this paper , we present a new approach for word sense disambiguation ( WSD )	using an exemplar-based learning algorithm .	1-22	1-22	In this paper , we present a new approach for word sense disambiguation ( WSD ) using an exemplar-based learning algorithm .	In this paper , we present a new approach for word sense disambiguation ( WSD ) using an exemplar-based learning algorithm .	1<2	none	manner-means	manner-means
P96-1006	1-16	23-31	In this paper , we present a new approach for word sense disambiguation ( WSD )	This approach integrates a diverse set of knowledge sources	In this paper , we present a new approach for word sense disambiguation ( WSD )	This approach integrates a diverse set of knowledge sources	1-22	23-62	In this paper , we present a new approach for word sense disambiguation ( WSD ) using an exemplar-based learning algorithm .	This approach integrates a diverse set of knowledge sources to disambiguate word sense , including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	1<2	none	elab-aspect	elab-aspect
P96-1006	23-31	32-36	This approach integrates a diverse set of knowledge sources	to disambiguate word sense ,	This approach integrates a diverse set of knowledge sources	to disambiguate word sense ,	23-62	23-62	This approach integrates a diverse set of knowledge sources to disambiguate word sense , including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	This approach integrates a diverse set of knowledge sources to disambiguate word sense , including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	1<2	none	enablement	enablement
P96-1006	23-31	37-62	This approach integrates a diverse set of knowledge sources	including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	This approach integrates a diverse set of knowledge sources	including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	23-62	23-62	This approach integrates a diverse set of knowledge sources to disambiguate word sense , including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	This approach integrates a diverse set of knowledge sources to disambiguate word sense , including part of speech of neighboring words , morphological form , the unordered set of surrounding words , local collocations , and verb-object syntactic relation .	1<2	none	elab-enumember	elab-enumember
P96-1006	1-16	63-68	In this paper , we present a new approach for word sense disambiguation ( WSD )	We tested our WSD program ,	In this paper , we present a new approach for word sense disambiguation ( WSD )	We tested our WSD program ,	1-22	63-95	In this paper , we present a new approach for word sense disambiguation ( WSD ) using an exemplar-based learning algorithm .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	elab-aspect	elab-aspect
P96-1006	63-68	69-71	We tested our WSD program ,	named LEXAS ,	We tested our WSD program ,	named LEXAS ,	63-95	63-95	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	elab-addition	elab-addition
P96-1006	63-68	72-77	We tested our WSD program ,	on both a common data set	We tested our WSD program ,	on both a common data set	63-95	63-95	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	elab-addition	elab-addition
P96-1006	72-77	78-82	on both a common data set	used in previous work ,	on both a common data set	used in previous work ,	63-95	63-95	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	elab-addition	elab-addition
P96-1006	72-77	83-90	on both a common data set	as well as on a large sense-tagged corpus	on both a common data set	as well as on a large sense-tagged corpus	63-95	63-95	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	joint	joint
P96-1006	83-90	91-95	as well as on a large sense-tagged corpus	that we separately constructed .	as well as on a large sense-tagged corpus	that we separately constructed .	63-95	63-95	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	We tested our WSD program , named LEXAS , on both a common data set used in previous work , as well as on a large sense-tagged corpus that we separately constructed .	1<2	none	elab-addition	elab-addition
P96-1006	1-16	96-106	In this paper , we present a new approach for word sense disambiguation ( WSD )	LEXAS achieves a higher accuracy on the common data set ,	In this paper , we present a new approach for word sense disambiguation ( WSD )	LEXAS achieves a higher accuracy on the common data set ,	1-22	96-131	In this paper , we present a new approach for word sense disambiguation ( WSD ) using an exemplar-based learning algorithm .	LEXAS achieves a higher accuracy on the common data set , and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET .	1<2	none	evaluation	evaluation
P96-1006	96-106	107-123	LEXAS achieves a higher accuracy on the common data set ,	and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus	LEXAS achieves a higher accuracy on the common data set ,	and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus	96-131	96-131	LEXAS achieves a higher accuracy on the common data set , and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET .	LEXAS achieves a higher accuracy on the common data set , and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET .	1<2	none	joint	joint
P96-1006	107-123	124-131	and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus	tagged with the refined senses of WoRDNET .	and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus	tagged with the refined senses of WoRDNET .	96-131	96-131	LEXAS achieves a higher accuracy on the common data set , and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET .	LEXAS achieves a higher accuracy on the common data set , and performs better than the most frequent heuristic on the highly ambiguous words in the large corpus tagged with the refined senses of WoRDNET .	1<2	none	elab-addition	elab-addition
P96-1007	1-7	8-13	We present an overview of recent work	in which eye movements are monitored	We present an overview of recent work	in which eye movements are monitored	1-28	1-28	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	1<2	none	elab-addition	elab-addition
P96-1007	1-7	14-18	We present an overview of recent work	as people follow spoken instructions	We present an overview of recent work	as people follow spoken instructions	1-28	1-28	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	1<2	none	temporal	temporal
P96-1007	14-18	19-28	as people follow spoken instructions	to move objects or pictures in a visual workspace .	as people follow spoken instructions	to move objects or pictures in a visual workspace .	1-28	1-28	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	1<2	none	enablement	enablement
P96-1007	1-7	29-35	We present an overview of recent work	Subjects naturally make saccadic eye-movements to objects	We present an overview of recent work	Subjects naturally make saccadic eye-movements to objects	1-28	29-46	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	Subjects naturally make saccadic eye-movements to objects that are closely time-locked to relevant information in the instruction .	1<2	none	bg-general	bg-general
P96-1007	29-35	36-46	Subjects naturally make saccadic eye-movements to objects	that are closely time-locked to relevant information in the instruction .	Subjects naturally make saccadic eye-movements to objects	that are closely time-locked to relevant information in the instruction .	29-46	29-46	Subjects naturally make saccadic eye-movements to objects that are closely time-locked to relevant information in the instruction .	Subjects naturally make saccadic eye-movements to objects that are closely time-locked to relevant information in the instruction .	1<2	none	elab-addition	elab-addition
P96-1007	29-35	47-57	Subjects naturally make saccadic eye-movements to objects	Thus the eye-movements provide a window into the rapid mental processes	Subjects naturally make saccadic eye-movements to objects	Thus the eye-movements provide a window into the rapid mental processes	29-46	47-63	Subjects naturally make saccadic eye-movements to objects that are closely time-locked to relevant information in the instruction .	Thus the eye-movements provide a window into the rapid mental processes that underlie spoken language comprehension .	1<2	none	cause	cause
P96-1007	47-57	58-63	Thus the eye-movements provide a window into the rapid mental processes	that underlie spoken language comprehension .	Thus the eye-movements provide a window into the rapid mental processes	that underlie spoken language comprehension .	47-63	47-63	Thus the eye-movements provide a window into the rapid mental processes that underlie spoken language comprehension .	Thus the eye-movements provide a window into the rapid mental processes that underlie spoken language comprehension .	1<2	none	elab-addition	elab-addition
P96-1007	1-7	64-81	We present an overview of recent work	We review studies of reference resolution , word recognition , and pragmatic effects on syntactic ambiguity resolution .	We present an overview of recent work	We review studies of reference resolution , word recognition , and pragmatic effects on syntactic ambiguity resolution .	1-28	64-81	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	We review studies of reference resolution , word recognition , and pragmatic effects on syntactic ambiguity resolution .	1<2	none	elab-aspect	elab-aspect
P96-1007	82-84	85-104	Our studies show	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	Our studies show	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	82-104	82-104	Our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	Our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	1>2	none	attribution	attribution
P96-1007	1-7	85-104	We present an overview of recent work	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	We present an overview of recent work	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	1-28	82-104	We present an overview of recent work in which eye movements are monitored as people follow spoken instructions to move objects or pictures in a visual workspace .	Our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	1<2	none	evaluation	evaluation
P96-1007	85-104	105-120	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	Moreover , referentially relevant non-linguistic information immediately affects how the linguistic input is initially structured .	that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	Moreover , referentially relevant non-linguistic information immediately affects how the linguistic input is initially structured .	82-104	105-120	Our studies show that people seek to establish reference with respect to their behavioral goals during the earliest moments of linguistic processing .	Moreover , referentially relevant non-linguistic information immediately affects how the linguistic input is initially structured .	1<2	none	elab-addition	elab-addition
P96-1008	1-7	8-16	We present a natural language interface system	which is based entirely on trained statistical models .	We present a natural language interface system	which is based entirely on trained statistical models .	1-16	1-16	We present a natural language interface system which is based entirely on trained statistical models .	We present a natural language interface system which is based entirely on trained statistical models .	1<2	none	elab-addition	elab-addition
P96-1008	1-7	17-25	We present a natural language interface system	The system consists of three stages of processing :	We present a natural language interface system	The system consists of three stages of processing :	1-16	17-33	We present a natural language interface system which is based entirely on trained statistical models .	The system consists of three stages of processing : parsing , semantic interpretation , and discourse .	1<2	none	elab-addition	elab-addition
P96-1008	17-25	26-33	The system consists of three stages of processing :	parsing , semantic interpretation , and discourse .	The system consists of three stages of processing :	parsing , semantic interpretation , and discourse .	17-33	17-33	The system consists of three stages of processing : parsing , semantic interpretation , and discourse .	The system consists of three stages of processing : parsing , semantic interpretation , and discourse .	1<2	none	elab-enumember	elab-enumember
P96-1008	17-25	34-44	The system consists of three stages of processing :	Each of these stages is modeled as a statistical process .	The system consists of three stages of processing :	Each of these stages is modeled as a statistical process .	17-33	34-44	The system consists of three stages of processing : parsing , semantic interpretation , and discourse .	Each of these stages is modeled as a statistical process .	1<2	none	elab-addition	elab-addition
P96-1008	1-7	45-50	We present a natural language interface system	The models are fully integrated ,	We present a natural language interface system	The models are fully integrated ,	1-16	45-64	We present a natural language interface system which is based entirely on trained statistical models .	The models are fully integrated , resulting in an end-to-end system that maps input utterances into meaning representation frames .	1<2	none	evaluation	evaluation
P96-1008	45-50	51-55	The models are fully integrated ,	resulting in an end-to-end system	The models are fully integrated ,	resulting in an end-to-end system	45-64	45-64	The models are fully integrated , resulting in an end-to-end system that maps input utterances into meaning representation frames .	The models are fully integrated , resulting in an end-to-end system that maps input utterances into meaning representation frames .	1<2	none	elab-addition	elab-addition
P96-1008	51-55	56-64	resulting in an end-to-end system	that maps input utterances into meaning representation frames .	resulting in an end-to-end system	that maps input utterances into meaning representation frames .	45-64	45-64	The models are fully integrated , resulting in an end-to-end system that maps input utterances into meaning representation frames .	The models are fully integrated , resulting in an end-to-end system that maps input utterances into meaning representation frames .	1<2	none	elab-addition	elab-addition
P96-1009	1-5	6-13	This paper describes a system	that leads us to believe in the feasibility	This paper describes a system	that leads us to believe in the feasibility	1-23	1-23	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	1<2	none	elab-addition	elab-addition
P96-1009	6-13	14-23	that leads us to believe in the feasibility	of constructing natural spoken dialogue systems in task-oriented domains .	that leads us to believe in the feasibility	of constructing natural spoken dialogue systems in task-oriented domains .	1-23	1-23	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	1<2	none	elab-addition	elab-addition
P96-1009	1-5	24-40	This paper describes a system	It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors .	This paper describes a system	It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors .	1-23	24-40	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors .	1<2	none	elab-addition	elab-addition
P96-1009	1-5	41-65	This paper describes a system	Robustness is achieved by a combination of statistical error post-correction , syntactically- and semantically-driven robust parsing , and extensive use of the dialogue context .	This paper describes a system	Robustness is achieved by a combination of statistical error post-correction , syntactically- and semantically-driven robust parsing , and extensive use of the dialogue context .	1-23	41-65	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	Robustness is achieved by a combination of statistical error post-correction , syntactically- and semantically-driven robust parsing , and extensive use of the dialogue context .	1<2	none	elab-aspect	elab-aspect
P96-1009	66-72	84-99	We present an evaluation of the system	that most native speakers of English can use the system successfully with virtually no training .	We present an evaluation of the system	that most native speakers of English can use the system successfully with virtually no training .	66-99	66-99	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	1>2	none	result	result
P96-1009	66-72	73-81	We present an evaluation of the system	using time-to-completion and the quality of the final solution	We present an evaluation of the system	using time-to-completion and the quality of the final solution	66-99	66-99	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	1<2	none	manner-means	manner-means
P96-1009	82-83	84-99	that suggests	that most native speakers of English can use the system successfully with virtually no training .	that suggests	that most native speakers of English can use the system successfully with virtually no training .	66-99	66-99	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	1>2	none	attribution	attribution
P96-1009	1-5	84-99	This paper describes a system	that most native speakers of English can use the system successfully with virtually no training .	This paper describes a system	that most native speakers of English can use the system successfully with virtually no training .	1-23	66-99	This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task-oriented domains .	We present an evaluation of the system using time-to-completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training .	1<2	none	evaluation	evaluation
P96-1010	1-5	6-9	This paper addresses the problem	of correcting spelling errors	This paper addresses the problem	of correcting spelling errors	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	elab-addition	elab-addition
P96-1010	6-9	10-17	of correcting spelling errors	that result in valid , though unintended words	of correcting spelling errors	that result in valid , though unintended words	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	elab-addition	elab-addition
P96-1010	10-17	18-29	that result in valid , though unintended words	( such as peace and piece , or quiet and quite )	that result in valid , though unintended words	( such as peace and piece , or quiet and quite )	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	elab-example	elab-example
P96-1010	1-5	30-33	This paper addresses the problem	and also the problem	This paper addresses the problem	and also the problem	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	joint	joint
P96-1010	30-33	34-39	and also the problem	of correcting particular word usage errors	and also the problem	of correcting particular word usage errors	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	elab-addition	elab-addition
P96-1010	34-39	40-52	of correcting particular word usage errors	( such as amount and number , or among and between ) .	of correcting particular word usage errors	( such as amount and number , or among and between ) .	1-52	1-52	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	1<2	none	elab-example	elab-example
P96-1010	1-5	53-57	This paper addresses the problem	Such corrections require contextual information	This paper addresses the problem	Such corrections require contextual information	1-52	53-70	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell .	1<2	none	elab-addition	elab-addition
P96-1010	53-57	58-65	Such corrections require contextual information	and are not handled by conventional spelling programs	Such corrections require contextual information	and are not handled by conventional spelling programs	53-70	53-70	Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell .	Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell .	1<2	none	progression	progression
P96-1010	58-65	66-70	and are not handled by conventional spelling programs	such as Unix spell .	and are not handled by conventional spelling programs	such as Unix spell .	53-70	53-70	Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell .	Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell .	1<2	none	elab-example	elab-example
P96-1010	1-5	71-76	This paper addresses the problem	First , we introduce a method	This paper addresses the problem	First , we introduce a method	1-52	71-87	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	1<2	none	elab-aspect	elab-aspect
P96-1010	71-76	77-78	First , we introduce a method	called Trigrams	First , we introduce a method	called Trigrams	71-87	71-87	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	1<2	none	elab-addition	elab-addition
P96-1010	71-76	79-82	First , we introduce a method	that uses part-of-speech trigrams	First , we introduce a method	that uses part-of-speech trigrams	71-87	71-87	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	1<2	none	elab-addition	elab-addition
P96-1010	79-82	83-87	that uses part-of-speech trigrams	to encode the context .	that uses part-of-speech trigrams	to encode the context .	71-87	71-87	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	1<2	none	enablement	enablement
P96-1010	71-76	88-95	First , we introduce a method	This method uses a small number of parameters	First , we introduce a method	This method uses a small number of parameters	71-87	88-104	First , we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context .	This method uses a small number of parameters compared to previous methods based on word trigrams .	1<2	none	elab-addition	elab-addition
P96-1010	88-95	96-99	This method uses a small number of parameters	compared to previous methods	This method uses a small number of parameters	compared to previous methods	88-104	88-104	This method uses a small number of parameters compared to previous methods based on word trigrams .	This method uses a small number of parameters compared to previous methods based on word trigrams .	1<2	none	comparison	comparison
P96-1010	96-99	100-104	compared to previous methods	based on word trigrams .	compared to previous methods	based on word trigrams .	88-104	88-104	This method uses a small number of parameters compared to previous methods based on word trigrams .	This method uses a small number of parameters compared to previous methods based on word trigrams .	1<2	none	bg-general	bg-general
P96-1010	105-114	123-130	However , it is effectively unable to distinguish among words	For this case , an alternative feature-based method	However , it is effectively unable to distinguish among words	For this case , an alternative feature-based method	105-122	123-152	However , it is effectively unable to distinguish among words that have the same part of speech .	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	1>2	none	bg-goal	bg-goal
P96-1010	105-114	115-122	However , it is effectively unable to distinguish among words	that have the same part of speech .	However , it is effectively unable to distinguish among words	that have the same part of speech .	105-122	105-122	However , it is effectively unable to distinguish among words that have the same part of speech .	However , it is effectively unable to distinguish among words that have the same part of speech .	1<2	none	elab-addition	elab-addition
P96-1010	123-130	136-142	For this case , an alternative feature-based method	but Bayes is less effective than Trigrams	For this case , an alternative feature-based method	but Bayes is less effective than Trigrams	123-152	123-152	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	1>2	none	comparison	comparison
P96-1010	123-130	131-135	For this case , an alternative feature-based method	called Bayes performs better ;	For this case , an alternative feature-based method	called Bayes performs better ;	123-152	123-152	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	1<2	none	elab-addition	elab-addition
P96-1010	1-5	136-142	This paper addresses the problem	but Bayes is less effective than Trigrams	This paper addresses the problem	but Bayes is less effective than Trigrams	1-52	123-152	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	1<2	none	elab-aspect	elab-aspect
P96-1010	136-142	143-152	but Bayes is less effective than Trigrams	when the distinction among words depends on syntactic constraints .	but Bayes is less effective than Trigrams	when the distinction among words depends on syntactic constraints .	123-152	123-152	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	For this case , an alternative feature-based method called Bayes performs better ; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints .	1<2	none	temporal	temporal
P96-1010	153-155	156-157	A hybrid method	called Tribayes	A hybrid method	called Tribayes	153-170	153-170	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	1<2	none	elab-addition	elab-addition
P96-1010	153-155,158-160	161-170	<*> A hybrid method <*> is then introduced	that combines the best of the previous two methods .	A hybrid method <*> is then introduced	that combines the best of the previous two methods .	153-170	153-170	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	1>2	none	attribution	attribution
P96-1010	1-5	161-170	This paper addresses the problem	that combines the best of the previous two methods .	This paper addresses the problem	that combines the best of the previous two methods .	1-52	153-170	This paper addresses the problem of correcting spelling errors that result in valid , though unintended words ( such as peace and piece , or quiet and quite ) and also the problem of correcting particular word usage errors ( such as amount and number , or among and between ) .	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	1<2	none	elab-aspect	elab-aspect
P96-1010	161-170	171-183	that combines the best of the previous two methods .	The improvement in performance of Tribayes over its components is verified experimentally .	that combines the best of the previous two methods .	The improvement in performance of Tribayes over its components is verified experimentally .	153-170	171-183	A hybrid method called Tribayes is then introduced that combines the best of the previous two methods .	The improvement in performance of Tribayes over its components is verified experimentally .	1<2	none	evaluation	evaluation
P96-1010	171-183	184-195	The improvement in performance of Tribayes over its components is verified experimentally .	Tribayes is also compared with the grammar checker in Microsoft Word ,	The improvement in performance of Tribayes over its components is verified experimentally .	Tribayes is also compared with the grammar checker in Microsoft Word ,	171-183	184-204	The improvement in performance of Tribayes over its components is verified experimentally .	Tribayes is also compared with the grammar checker in Microsoft Word , and is found to have substantially higher performance .	1<2	none	joint	joint
P96-1010	184-195	196-204	Tribayes is also compared with the grammar checker in Microsoft Word ,	and is found to have substantially higher performance .	Tribayes is also compared with the grammar checker in Microsoft Word ,	and is found to have substantially higher performance .	184-204	184-204	Tribayes is also compared with the grammar checker in Microsoft Word , and is found to have substantially higher performance .	Tribayes is also compared with the grammar checker in Microsoft Word , and is found to have substantially higher performance .	1<2	none	joint	joint
P96-1011	1-3	11-20	Under categorial grammars	a simple n-word sentence can have exponentially many parses .	Under categorial grammars	a simple n-word sentence can have exponentially many parses .	1-20	1-20	Under categorial grammars that have powerful rules like composition , a simple n-word sentence can have exponentially many parses .	Under categorial grammars that have powerful rules like composition , a simple n-word sentence can have exponentially many parses .	1>2	none	bg-general	bg-general
P96-1011	1-3	4-10	Under categorial grammars	that have powerful rules like composition ,	Under categorial grammars	that have powerful rules like composition ,	1-20	1-20	Under categorial grammars that have powerful rules like composition , a simple n-word sentence can have exponentially many parses .	Under categorial grammars that have powerful rules like composition , a simple n-word sentence can have exponentially many parses .	1<2	none	elab-addition	elab-addition
P96-1011	11-20	37-51	a simple n-word sentence can have exponentially many parses .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	a simple n-word sentence can have exponentially many parses .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	1-20	37-67	Under categorial grammars that have powerful rules like composition , a simple n-word sentence can have exponentially many parses .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal-form parsing technique .	1>2	none	bg-general	bg-general
P96-1011	21-25	37-51	Generating all parses is inefficient	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	Generating all parses is inefficient	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	21-36	37-67	Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal-form parsing technique .	1>2	none	bg-goal	bg-goal
P96-1011	21-25	26-36	Generating all parses is inefficient	and obscures whatever true semantic ambiguities are in the input .	Generating all parses is inefficient	and obscures whatever true semantic ambiguities are in the input .	21-36	21-36	Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .	Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .	1<2	none	joint	joint
P96-1011	37-51	52-67	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	by means of an efficient , correct , and easy to implement normal-form parsing technique .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	by means of an efficient , correct , and easy to implement normal-form parsing technique .	37-67	37-67	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal-form parsing technique .	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal-form parsing technique .	1<2	none	manner-means	manner-means
P96-1011	37-51	68-85	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ;	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar ,	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ;	37-67	68-105	This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar , by means of an efficient , correct , and easy to implement normal-form parsing technique .	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .	1<2	none	evaluation	evaluation
P96-1011	91-95	86-90,96-105	( as carefully defined )	<*> that is , spurious ambiguity <*> is shown to be both safely and completely eliminated .	( as carefully defined )	that is , spurious ambiguity <*> is shown to be both safely and completely eliminated .	68-105	68-105	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .	1>2	none	elab-addition	elab-addition
P96-1011	68-85	86-90,96-105	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ;	<*> that is , spurious ambiguity <*> is shown to be both safely and completely eliminated .	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ;	that is , spurious ambiguity <*> is shown to be both safely and completely eliminated .	68-105	68-105	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .	The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .	1<2	none	elab-addition	elab-addition
P96-1012	1-23	24-37	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	1-37	1-37	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	1<2	none	elab-addition	elab-addition
P96-1012	1-23	38-57	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar	1-37	38-75	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in L which lead to x .	1<2	none	elab-aspect	elab-aspect
P96-1012	38-57	58-70	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar	whose sentences are all ( and exclusively ) valid derivation sequences in L	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar	whose sentences are all ( and exclusively ) valid derivation sequences in L	38-75	38-75	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in L which lead to x .	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in L which lead to x .	1<2	none	elab-addition	elab-addition
P96-1012	58-70	71-75	whose sentences are all ( and exclusively ) valid derivation sequences in L	which lead to x .	whose sentences are all ( and exclusively ) valid derivation sequences in L	which lead to x .	38-75	38-75	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in L which lead to x .	For a LIG L and an input string x of length n , we build a non ambiguous context-free grammar whose sentences are all ( and exclusively ) valid derivation sequences in L which lead to x .	1<2	none	elab-addition	elab-addition
P96-1012	76-77	78-91	We show	that this grammar can be built in ( O ( n^6 ) ) time	We show	that this grammar can be built in ( O ( n^6 ) ) time	76-110	76-110	We show that this grammar can be built in ( O ( n^6 ) ) time and that individual parses can be extracted in linear time with the size of the extracted parse tree .	We show that this grammar can be built in ( O ( n^6 ) ) time and that individual parses can be extracted in linear time with the size of the extracted parse tree .	1>2	none	attribution	attribution
P96-1012	1-23	78-91	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	that this grammar can be built in ( O ( n^6 ) ) time	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	that this grammar can be built in ( O ( n^6 ) ) time	1-37	76-110	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	We show that this grammar can be built in ( O ( n^6 ) ) time and that individual parses can be extracted in linear time with the size of the extracted parse tree .	1<2	none	elab-aspect	elab-aspect
P96-1012	78-91	92-110	that this grammar can be built in ( O ( n^6 ) ) time	and that individual parses can be extracted in linear time with the size of the extracted parse tree .	that this grammar can be built in ( O ( n^6 ) ) time	and that individual parses can be extracted in linear time with the size of the extracted parse tree .	76-110	76-110	We show that this grammar can be built in ( O ( n^6 ) ) time and that individual parses can be extracted in linear time with the size of the extracted parse tree .	We show that this grammar can be built in ( O ( n^6 ) ) time and that individual parses can be extracted in linear time with the size of the extracted parse tree .	1<2	none	joint	joint
P96-1012	111-125	126-132	Though this O ( n^6 ) upper bound does not improve over previous results ,	the average case behaves much better .	Though this O ( n^6 ) upper bound does not improve over previous results ,	the average case behaves much better .	111-132	111-132	Though this O ( n^6 ) upper bound does not improve over previous results , the average case behaves much better .	Though this O ( n^6 ) upper bound does not improve over previous results , the average case behaves much better .	1>2	none	contrast	contrast
P96-1012	1-23	126-132	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	the average case behaves much better .	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one	the average case behaves much better .	1-37	111-132	In this paper we present a new parsing algorithm for linear indexed grammars ( LIGs ) in the same spirit as the one described in ( Vijay-Shanker and Weir , 1993 ) for tree adjoining grammars .	Though this O ( n^6 ) upper bound does not improve over previous results , the average case behaves much better .	1<2	none	evaluation	evaluation
P96-1012	126-132	133-146	the average case behaves much better .	Moreover , practical parsing times can be decreased by some statically performed computations .	the average case behaves much better .	Moreover , practical parsing times can be decreased by some statically performed computations .	111-132	133-146	Though this O ( n^6 ) upper bound does not improve over previous results , the average case behaves much better .	Moreover , practical parsing times can be decreased by some statically performed computations .	1<2	none	joint	joint
P96-1013	1-16	17-20	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	that we call semidirectional.	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	that we call semidirectional.	1-20	1-20	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.	1<2	none	elab-addition	elab-addition
P96-1013	1-16	21-32	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule	1-20	21-53	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	1<2	none	elab-aspect	elab-aspect
P96-1013	21-32	33-35	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule	allowing the formula	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule	allowing the formula	21-53	21-53	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	1<2	none	elab-addition	elab-addition
P96-1013	33-35	36-37	allowing the formula	abstracted over	allowing the formula	abstracted over	21-53	21-53	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	1<2	none	elab-addition	elab-addition
P96-1013	33-35	38-48	allowing the formula	to appear anywhere in the premise sequent 's left-hand side ,	allowing the formula	to appear anywhere in the premise sequent's left-hand side ,	21-53	21-53	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	1<2	none	enablement	enablement
P96-1013	38-48	49-53	to appear anywhere in the premise sequent 's left-hand side ,	thus permitting non-peripheral extraction .	to appear anywhere in the premise sequent's left-hand side ,	thus permitting non-peripheral extraction .	21-53	21-53	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	In semidirectional Lambek calculus SD-LSB- there is an additional nondirectional abstraction rule allowing the formula abstracted over to appear anywhere in the premise sequent 's left-hand side , thus permitting non-peripheral extraction .	1<2	none	cause	cause
P96-1013	1-16	54-67	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	SD-LSB- grammars are able to generate each context-free language and more than that .	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	SD-LSB- grammars are able to generate each context-free language and more than that .	1-20	54-67	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.	SD-LSB- grammars are able to generate each context-free language and more than that .	1<2	none	elab-aspect	elab-aspect
P96-1013	68-69	70-87	We show	that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	We show	that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	68-87	68-87	We show that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	We show that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	1>2	none	attribution	attribution
P96-1013	1-16	70-87	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar	that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	1-20	68-87	We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.	We show that the parsing problem for semidireetional Lambek Grammar is NP-complete by a reduction of the 3-Partition problem .	1<2	none	evaluation	evaluation
P96-1014	1-5	6-19	This paper describes an algorithm	for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	This paper describes an algorithm	for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	1-19	1-19	This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	1<2	none	enablement	enablement
P96-1014	1-5	20-37	This paper describes an algorithm	This algorithm extends Tesar 's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 )	This paper describes an algorithm	This algorithm extends Tesar's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 )	1-19	20-48	This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	This algorithm extends Tesar 's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 ) to computing optimal structural descriptions from regular to context-free structures .	1<2	none	elab-addition	elab-addition
P96-1014	20-37	38-48	This algorithm extends Tesar 's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 )	to computing optimal structural descriptions from regular to context-free structures .	This algorithm extends Tesar's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 )	to computing optimal structural descriptions from regular to context-free structures .	20-48	20-48	This algorithm extends Tesar 's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 ) to computing optimal structural descriptions from regular to context-free structures .	This algorithm extends Tesar 's dynamic programming approach ( Tesar , 1994 ) ( Tesar , 1995 ) to computing optimal structural descriptions from regular to context-free structures .	1<2	none	enablement	enablement
P96-1014	1-5	49-58	This paper describes an algorithm	The generalization to context free structures creates several complications ,	This paper describes an algorithm	The generalization to context free structures creates several complications ,	1-19	49-71	This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	The generalization to context free structures creates several complications , all of which are overcome without compromising the core dynamic programming approach .	1<2	none	elab-aspect	elab-aspect
P96-1014	49-58	59-63	The generalization to context free structures creates several complications ,	all of which are overcome	The generalization to context free structures creates several complications ,	all of which are overcome	49-71	49-71	The generalization to context free structures creates several complications , all of which are overcome without compromising the core dynamic programming approach .	The generalization to context free structures creates several complications , all of which are overcome without compromising the core dynamic programming approach .	1<2	none	elab-addition	elab-addition
P96-1014	59-63	64-71	all of which are overcome	without compromising the core dynamic programming approach .	all of which are overcome	without compromising the core dynamic programming approach .	49-71	49-71	The generalization to context free structures creates several complications , all of which are overcome without compromising the core dynamic programming approach .	The generalization to context free structures creates several complications , all of which are overcome without compromising the core dynamic programming approach .	1<2	none	elab-addition	elab-addition
P96-1014	1-5	72-86	This paper describes an algorithm	The resulting algorithm has a time complexity cubic in the length of the input ,	This paper describes an algorithm	The resulting algorithm has a time complexity cubic in the length of the input ,	1-19	72-99	This paper describes an algorithm for computing optimal structural descriptions for Optimality Theory grammars with context-free position structures .	The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .	1<2	none	evaluation	evaluation
P96-1014	72-86	87-94	The resulting algorithm has a time complexity cubic in the length of the input ,	and is applicable to grammars with universal constraints	The resulting algorithm has a time complexity cubic in the length of the input ,	and is applicable to grammars with universal constraints	72-99	72-99	The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .	The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .	1<2	none	joint	joint
P96-1014	87-94	95-99	and is applicable to grammars with universal constraints	that exhibit context-free locality .	and is applicable to grammars with universal constraints	that exhibit context-free locality .	72-99	72-99	The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .	The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .	1<2	none	elab-addition	elab-addition
P96-1015	15-26	34-46	In contrast to the simple replace expression , UPPER -> LOWER ,	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	In contrast to the simple replace expression , UPPER -> LOWER ,	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	15-56	15-56	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	1>2	none	contrast	contrast
P96-1015	15-26	27-33	In contrast to the simple replace expression , UPPER -> LOWER ,	defined in Karttunen ( 1995 ) ,	In contrast to the simple replace expression , UPPER -> LOWER ,	defined in Karttunen ( 1995 ) ,	15-56	15-56	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	1<2	none	elab-addition	elab-addition
P96-1015	1-14	34-46	This paper introduces to the finite-state calculus a family of directed replace operators .	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	This paper introduces to the finite-state calculus a family of directed replace operators .	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	1-14	15-56	This paper introduces to the finite-state calculus a family of directed replace operators .	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	1<2	none	elab-aspect	elab-aspect
P96-1015	34-46	47-56	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	if the lower language consists of a single string .	the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer	if the lower language consists of a single string .	15-56	15-56	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	In contrast to the simple replace expression , UPPER -> LOWER , defined in Karttunen ( 1995 ) , the new directed version , UPPER ©-> LOWER , yields an unambiguous transducer if the lower language consists of a single string .	1<2	none	condition	condition
P96-1015	1-14	57-66	This paper introduces to the finite-state calculus a family of directed replace operators .	It transduces the input string from left to right ,	This paper introduces to the finite-state calculus a family of directed replace operators .	It transduces the input string from left to right ,	1-14	57-76	This paper introduces to the finite-state calculus a family of directed replace operators .	It transduces the input string from left to right , making only the longest possible replacement at each point .	1<2	none	elab-addition	elab-addition
P96-1015	57-66	67-76	It transduces the input string from left to right ,	making only the longest possible replacement at each point .	It transduces the input string from left to right ,	making only the longest possible replacement at each point .	57-76	57-76	It transduces the input string from left to right , making only the longest possible replacement at each point .	It transduces the input string from left to right , making only the longest possible replacement at each point .	1<2	none	elab-addition	elab-addition
P96-1015	1-14	77-93	This paper introduces to the finite-state calculus a family of directed replace operators .	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	This paper introduces to the finite-state calculus a family of directed replace operators .	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	1-14	77-104	This paper introduces to the finite-state calculus a family of directed replace operators .	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	1<2	none	elab-aspect	elab-aspect
P96-1015	77-93	94-98	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	that inserts text around strings	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	that inserts text around strings	77-104	77-104	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	1<2	none	elab-addition	elab-addition
P96-1015	94-98	99-104	that inserts text around strings	that are instances of UPPER .	that inserts text around strings	that are instances of UPPER .	77-104	77-104	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	1<2	none	elab-addition	elab-addition
P96-1015	77-93	105-114	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	The symbol ... denotes the matching part of the input	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	The symbol ... denotes the matching part of the input	77-104	105-119	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	The symbol ... denotes the matching part of the input which itself remains unchanged .	1<2	none	elab-aspect	elab-aspect
P96-1015	105-114	115-119	The symbol ... denotes the matching part of the input	which itself remains unchanged .	The symbol ... denotes the matching part of the input	which itself remains unchanged .	105-119	105-119	The symbol ... denotes the matching part of the input which itself remains unchanged .	The symbol ... denotes the matching part of the input which itself remains unchanged .	1<2	none	elab-addition	elab-addition
P96-1015	77-93	120-125	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	PREFIX and SUFFIX are regular expressions	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	PREFIX and SUFFIX are regular expressions	77-104	120-129	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	PREFIX and SUFFIX are regular expressions describing the insertions .	1<2	none	elab-aspect	elab-aspect
P96-1015	120-125	126-129	PREFIX and SUFFIX are regular expressions	describing the insertions .	PREFIX and SUFFIX are regular expressions	describing the insertions .	120-129	120-129	PREFIX and SUFFIX are regular expressions describing the insertions .	PREFIX and SUFFIX are regular expressions describing the insertions .	1<2	none	elab-addition	elab-addition
P96-1015	77-93	130-142	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used	77-104	130-162	A new type of replacement expression , UPPER @ -> PREFIX ... SUFFIX , yields a transducer that inserts text around strings that are instances of UPPER .	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used to compose a deterministic parser for a `` local grammar '' in the sense of Gross ( 1989 ) .	1<2	none	elab-aspect	elab-aspect
P96-1015	130-142	143-162	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used	to compose a deterministic parser for a `` local grammar '' in the sense of Gross ( 1989 ) .	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used	to compose a deterministic parser for a `` local grammar '' in the sense of Gross ( 1989 ) .	130-162	130-162	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used to compose a deterministic parser for a `` local grammar '' in the sense of Gross ( 1989 ) .	Expressions of the type UPPER @ -> PREFIX ... SUFFIX may be used to compose a deterministic parser for a `` local grammar '' in the sense of Gross ( 1989 ) .	1<2	none	enablement	enablement
P96-1015	1-14	163-176	This paper introduces to the finite-state calculus a family of directed replace operators .	Other useful applications of directed replacement include tokenization and filtering of text streams .	This paper introduces to the finite-state calculus a family of directed replace operators .	Other useful applications of directed replacement include tokenization and filtering of text streams .	1-14	163-176	This paper introduces to the finite-state calculus a family of directed replace operators .	Other useful applications of directed replacement include tokenization and filtering of text streams .	1<2	none	elab-aspect	elab-aspect
P96-1016	1-24	25-31	In synchronous rewriting , the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings .	We present a new synchronous rewriting system	In synchronous rewriting , the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings .	We present a new synchronous rewriting system	1-24	25-48	In synchronous rewriting , the productions of two rewriting systems are paired and applied synchronously in the derivation of a pair of strings .	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	1>2	none	bg-general	bg-general
P96-1016	32-33	34-39	and argue	that it can handle certain phenomena	and argue	that it can handle certain phenomena	25-48	25-48	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	1>2	none	attribution	attribution
P96-1016	25-31	34-39	We present a new synchronous rewriting system	that it can handle certain phenomena	We present a new synchronous rewriting system	that it can handle certain phenomena	25-48	25-48	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	1<2	none	joint	joint
P96-1016	34-39	40-48	that it can handle certain phenomena	that are not covered by existing synchronous systems .	that it can handle certain phenomena	that are not covered by existing synchronous systems .	25-48	25-48	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	1<2	none	elab-addition	elab-addition
P96-1016	25-31	49-59	We present a new synchronous rewriting system	We also prove some interesting formal/computational properties of our system .	We present a new synchronous rewriting system	We also prove some interesting formal/computational properties of our system .	25-48	49-59	We present a new synchronous rewriting system and argue that it can handle certain phenomena that are not covered by existing synchronous systems .	We also prove some interesting formal/computational properties of our system .	1<2	none	joint	joint
P96-1017	1-6	7-17	We propose a treatment of coordination	based on the concepts of functor , argument and subcategorization .	We propose a treatment of coordination	based on the concepts of functor , argument and subcategorization .	1-17	1-17	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	1<2	none	bg-general	bg-general
P96-1017	1-6	18-22	We propose a treatment of coordination	Its formalization comprises two parts	We propose a treatment of coordination	Its formalization comprises two parts	1-17	18-27	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	Its formalization comprises two parts which are conceptually independent .	1<2	none	elab-aspect	elab-aspect
P96-1017	18-22	23-27	Its formalization comprises two parts	which are conceptually independent .	Its formalization comprises two parts	which are conceptually independent .	18-27	18-27	Its formalization comprises two parts which are conceptually independent .	Its formalization comprises two parts which are conceptually independent .	1<2	none	elab-addition	elab-addition
P96-1017	1-6	28-38	We propose a treatment of coordination	On one hand , we have extended the feature structure unification	We propose a treatment of coordination	On one hand , we have extended the feature structure unification	1-17	28-59	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	On one hand , we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	1<2	none	elab-aspect	elab-aspect
P96-1017	28-38	39-43	On one hand , we have extended the feature structure unification	to disjunctive and set values	On one hand , we have extended the feature structure unification	to disjunctive and set values	28-59	28-59	On one hand , we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	On one hand , we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	1<2	none	enablement	enablement
P96-1017	39-43	44-59	to disjunctive and set values	in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	to disjunctive and set values	in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	28-59	28-59	On one hand , we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	On one hand , we have extended the feature structure unification to disjunctive and set values in order to check the compatibility and the satisfiability of subcategorization requirements by structured complements .	1<2	none	enablement	enablement
P96-1017	1-6	60-83	We propose a treatment of coordination	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure ,	We propose a treatment of coordination	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure ,	1-17	60-104	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure , so that coordinate structures stem simply from the subcategorization specifications of et and the general schemata of a head saturation .	1<2	none	elab-aspect	elab-aspect
P96-1017	60-83	84-104	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure ,	so that coordinate structures stem simply from the subcategorization specifications of et and the general schemata of a head saturation .	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure ,	so that coordinate structures stem simply from the subcategorization specifications of et and the general schemata of a head saturation .	60-104	60-104	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure , so that coordinate structures stem simply from the subcategorization specifications of et and the general schemata of a head saturation .	On the other hand , we have considered the conjunction " et " ( and ) as the head of the coordinate structure , so that coordinate structures stem simply from the subcategorization specifications of et and the general schemata of a head saturation .	1<2	none	enablement	enablement
P96-1017	1-6	105-111	We propose a treatment of coordination	Both parts have been encoded within HPSG	We propose a treatment of coordination	Both parts have been encoded within HPSG	1-17	105-128	We propose a treatment of coordination based on the concepts of functor , argument and subcategorization .	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	1<2	none	elab-aspect	elab-aspect
P96-1017	105-111	112-115	Both parts have been encoded within HPSG	using the same resource	Both parts have been encoded within HPSG	using the same resource	105-128	105-128	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	1<2	none	manner-means	manner-means
P96-1017	112-115	116-122	using the same resource	that is the subcategorization and its principle	using the same resource	that is the subcategorization and its principle	105-128	105-128	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	1<2	none	elab-addition	elab-addition
P96-1017	116-122	123-128	that is the subcategorization and its principle	which we have just extended .	that is the subcategorization and its principle	which we have just extended .	105-128	105-128	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	Both parts have been encoded within HPSG using the same resource that is the subcategorization and its principle which we have just extended .	1<2	none	elab-addition	elab-addition
P96-1018	1-15	16-35	This paper describes an accurate and robust text alignment system for structurally different languages .	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences	This paper describes an accurate and robust text alignment system for structurally different languages .	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences	1-15	16-41	This paper describes an accurate and robust text alignment system for structurally different languages .	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences that can be statistically acquired .	1<2	none	bg-general	bg-general
P96-1018	16-35	36-41	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences	that can be statistically acquired .	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences	that can be statistically acquired .	16-41	16-41	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences that can be statistically acquired .	Among structurally different languages such as Japanese and English , there is a limitation on the amount of word correspondences that can be statistically acquired .	1<2	none	elab-addition	elab-addition
P96-1018	1-15	42-52	This paper describes an accurate and robust text alignment system for structurally different languages .	The proposed method makes use of two kinds of word correspondences	This paper describes an accurate and robust text alignment system for structurally different languages .	The proposed method makes use of two kinds of word correspondences	1-15	42-57	This paper describes an accurate and robust text alignment system for structurally different languages .	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	1<2	none	elab-addition	elab-addition
P96-1018	42-52	53-57	The proposed method makes use of two kinds of word correspondences	in aligning bilingual texts .	The proposed method makes use of two kinds of word correspondences	in aligning bilingual texts .	42-57	42-57	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	1<2	none	elab-addition	elab-addition
P96-1018	42-52	58-66	The proposed method makes use of two kinds of word correspondences	One is a bilingual dictionary of general use .	The proposed method makes use of two kinds of word correspondences	One is a bilingual dictionary of general use .	42-57	58-66	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	One is a bilingual dictionary of general use .	1<2	none	elab-enumember	elab-enumember
P96-1018	42-52	67-72	The proposed method makes use of two kinds of word correspondences	The other is the word correspondences	The proposed method makes use of two kinds of word correspondences	The other is the word correspondences	42-57	67-81	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	The other is the word correspondences that are statistically acquired in the alignment process .	1<2	none	elab-enumember	elab-enumember
P96-1018	67-72	73-81	The other is the word correspondences	that are statistically acquired in the alignment process .	The other is the word correspondences	that are statistically acquired in the alignment process .	67-81	67-81	The other is the word correspondences that are statistically acquired in the alignment process .	The other is the word correspondences that are statistically acquired in the alignment process .	1<2	none	elab-addition	elab-addition
P96-1018	1-15	82-90	This paper describes an accurate and robust text alignment system for structurally different languages .	Our method gradually determines sentence pairs ( anchors )	This paper describes an accurate and robust text alignment system for structurally different languages .	Our method gradually determines sentence pairs ( anchors )	1-15	82-99	This paper describes an accurate and robust text alignment system for structurally different languages .	Our method gradually determines sentence pairs ( anchors ) that correspond to each other by relaxing parameters .	1<2	none	elab-aspect	elab-aspect
P96-1018	82-90	91-95	Our method gradually determines sentence pairs ( anchors )	that correspond to each other	Our method gradually determines sentence pairs ( anchors )	that correspond to each other	82-99	82-99	Our method gradually determines sentence pairs ( anchors ) that correspond to each other by relaxing parameters .	Our method gradually determines sentence pairs ( anchors ) that correspond to each other by relaxing parameters .	1<2	none	elab-addition	elab-addition
P96-1018	82-90	96-99	Our method gradually determines sentence pairs ( anchors )	by relaxing parameters .	Our method gradually determines sentence pairs ( anchors )	by relaxing parameters .	82-99	82-99	Our method gradually determines sentence pairs ( anchors ) that correspond to each other by relaxing parameters .	Our method gradually determines sentence pairs ( anchors ) that correspond to each other by relaxing parameters .	1<2	none	manner-means	manner-means
P96-1018	103-110	100-102,111-118	by combining two kinds of word correspondences ,	<*> The method , <*> achieves adequate word correspondences for complete alignment .	by combining two kinds of word correspondences ,	The method , <*> achieves adequate word correspondences for complete alignment .	100-118	100-118	The method , by combining two kinds of word correspondences , achieves adequate word correspondences for complete alignment .	The method , by combining two kinds of word correspondences , achieves adequate word correspondences for complete alignment .	1>2	none	manner-means	manner-means
P96-1018	100-102,111-118	119-141	<*> The method , <*> achieves adequate word correspondences for complete alignment .	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	The method , <*> achieves adequate word correspondences for complete alignment .	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	100-118	119-141	The method , by combining two kinds of word correspondences , achieves adequate word correspondences for complete alignment .	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	1>2	none	result	result
P96-1018	42-52	119-141	The proposed method makes use of two kinds of word correspondences	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	The proposed method makes use of two kinds of word correspondences	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	42-57	119-141	The proposed method makes use of two kinds of word correspondences in aligning bilingual texts .	As a result , texts of various length and of various genres in structurally different languages can be aligned with high precision .	1<2	none	elab-aspect	elab-aspect
P96-1018	142-144	145-156	Experimental results show	our system outperforms conventional methods for various kinds of Japanese-English texts .	Experimental results show	our system outperforms conventional methods for various kinds of Japanese-English texts .	142-156	142-156	Experimental results show our system outperforms conventional methods for various kinds of Japanese-English texts .	Experimental results show our system outperforms conventional methods for various kinds of Japanese-English texts .	1>2	none	attribution	attribution
P96-1018	1-15	145-156	This paper describes an accurate and robust text alignment system for structurally different languages .	our system outperforms conventional methods for various kinds of Japanese-English texts .	This paper describes an accurate and robust text alignment system for structurally different languages .	our system outperforms conventional methods for various kinds of Japanese-English texts .	1-15	142-156	This paper describes an accurate and robust text alignment system for structurally different languages .	Experimental results show our system outperforms conventional methods for various kinds of Japanese-English texts .	1<2	none	evaluation	evaluation
P96-1019	1-5	6-15	We present an iterative procedure	to build a Chinese language model ( LM ) .	We present an iterative procedure	to build a Chinese language model ( LM ) .	1-15	1-15	We present an iterative procedure to build a Chinese language model ( LM ) .	We present an iterative procedure to build a Chinese language model ( LM ) .	1<2	none	enablement	enablement
P96-1019	16-21	30-42	We segment Chinese text into words	However , the construction of a Chinese LM itself requires word boundaries .	We segment Chinese text into words	However , the construction of a Chinese LM itself requires word boundaries .	16-29	30-42	We segment Chinese text into words based on a word-based Chinese language model .	However , the construction of a Chinese LM itself requires word boundaries .	1>2	none	contrast	contrast
P96-1019	16-21	22-29	We segment Chinese text into words	based on a word-based Chinese language model .	We segment Chinese text into words	based on a word-based Chinese language model .	16-29	16-29	We segment Chinese text into words based on a word-based Chinese language model .	We segment Chinese text into words based on a word-based Chinese language model .	1<2	none	bg-general	bg-general
P96-1019	1-5	30-42	We present an iterative procedure	However , the construction of a Chinese LM itself requires word boundaries .	We present an iterative procedure	However , the construction of a Chinese LM itself requires word boundaries .	1-15	30-42	We present an iterative procedure to build a Chinese language model ( LM ) .	However , the construction of a Chinese LM itself requires word boundaries .	1<2	none	bg-goal	bg-goal
P96-1019	43-50	51-55	To get out of the chicken-and-egg problem ,	we propose an iterative procedure	To get out of the chicken-and-egg problem ,	we propose an iterative procedure	43-69	43-69	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	1>2	none	enablement	enablement
P96-1019	1-5	51-55	We present an iterative procedure	we propose an iterative procedure	We present an iterative procedure	we propose an iterative procedure	1-15	43-69	We present an iterative procedure to build a Chinese language model ( LM ) .	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	1<2	none	elab-addition	elab-addition
P96-1019	51-55	56-60	we propose an iterative procedure	that alternates two operations :	we propose an iterative procedure	that alternates two operations :	43-69	43-69	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	1<2	none	elab-addition	elab-addition
P96-1019	56-60	61-64	that alternates two operations :	segmenting text into words	that alternates two operations :	segmenting text into words	43-69	43-69	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	1<2	none	elab-enumember	elab-enumember
P96-1019	61-64	65-69	segmenting text into words	and building an LM .	segmenting text into words	and building an LM .	43-69	43-69	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	To get out of the chicken-and-egg problem , we propose an iterative procedure that alternates two operations : segmenting text into words and building an LM .	1<2	none	joint	joint
P96-1019	70-78	83-87	Starting with an initial segmented corpus and an LM	we use a Viterbi-liek algorithm	Starting with an initial segmented corpus and an LM	we use a Viterbi-liek algorithm	70-94	70-94	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	1>2	none	elab-addition	elab-addition
P96-1019	70-78	79-82	Starting with an initial segmented corpus and an LM	based upon it ,	Starting with an initial segmented corpus and an LM	based upon it ,	70-94	70-94	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	1<2	none	bg-general	bg-general
P96-1019	1-5	83-87	We present an iterative procedure	we use a Viterbi-liek algorithm	We present an iterative procedure	we use a Viterbi-liek algorithm	1-15	70-94	We present an iterative procedure to build a Chinese language model ( LM ) .	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	1<2	none	elab-aspect	elab-aspect
P96-1019	83-87	88-94	we use a Viterbi-liek algorithm	to segment another set of data .	we use a Viterbi-liek algorithm	to segment another set of data .	70-94	70-94	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	Starting with an initial segmented corpus and an LM based upon it , we use a Viterbi-liek algorithm to segment another set of data .	1<2	none	enablement	enablement
P96-1019	1-5	95-100	We present an iterative procedure	Then , we build an LM	We present an iterative procedure	Then , we build an LM	1-15	95-117	We present an iterative procedure to build a Chinese language model ( LM ) .	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	1<2	none	elab-aspect	elab-aspect
P96-1019	95-100	101-105	Then , we build an LM	based on the second set	Then , we build an LM	based on the second set	95-117	95-117	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	1<2	none	bg-general	bg-general
P96-1019	95-100	106-110	Then , we build an LM	and use the resulting LM	Then , we build an LM	and use the resulting LM	95-117	95-117	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	1<2	none	joint	joint
P96-1019	106-110	111-117	and use the resulting LM	to segment again the first corpus .	and use the resulting LM	to segment again the first corpus .	95-117	95-117	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	Then , we build an LM based on the second set and use the resulting LM to segment again the first corpus .	1<2	none	enablement	enablement
P96-1019	1-5	118-127	We present an iterative procedure	The alternating procedure provides a self-organized way for the segmenter	We present an iterative procedure	The alternating procedure provides a self-organized way for the segmenter	1-15	118-137	We present an iterative procedure to build a Chinese language model ( LM ) .	The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors .	1<2	none	elab-aspect	elab-aspect
P96-1019	118-127	128-132	The alternating procedure provides a self-organized way for the segmenter	to detect automatically unseen words	The alternating procedure provides a self-organized way for the segmenter	to detect automatically unseen words	118-137	118-137	The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors .	The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors .	1<2	none	enablement	enablement
P96-1019	128-132	133-137	to detect automatically unseen words	and correct segmentation errors .	to detect automatically unseen words	and correct segmentation errors .	118-137	118-137	The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors .	The alternating procedure provides a self-organized way for the segmenter to detect automatically unseen words and correct segmentation errors .	1<2	none	joint	joint
P96-1019	138-141	142-154	Our preliminary experiment shows	that the alternating procedure not only improves the accuracy of our segmentation ,	Our preliminary experiment shows	that the alternating procedure not only improves the accuracy of our segmentation ,	138-161	138-161	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	1>2	none	attribution	attribution
P96-1019	1-5	142-154	We present an iterative procedure	that the alternating procedure not only improves the accuracy of our segmentation ,	We present an iterative procedure	that the alternating procedure not only improves the accuracy of our segmentation ,	1-15	138-161	We present an iterative procedure to build a Chinese language model ( LM ) .	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	1<2	none	evaluation	evaluation
P96-1019	142-154	155-161	that the alternating procedure not only improves the accuracy of our segmentation ,	but discovers unseen words surprisingly well .	that the alternating procedure not only improves the accuracy of our segmentation ,	but discovers unseen words surprisingly well .	138-161	138-161	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	1<2	none	joint	joint
P96-1019	142-154	162-176	that the alternating procedure not only improves the accuracy of our segmentation ,	The resulting word-based LM has a perplexity of 188 for a general Chinese corpus .	that the alternating procedure not only improves the accuracy of our segmentation ,	The resulting word-based LM has a perplexity of 188 for a general Chinese corpus .	138-161	162-176	Our preliminary experiment shows that the alternating procedure not only improves the accuracy of our segmentation , but discovers unseen words surprisingly well .	The resulting word-based LM has a perplexity of 188 for a general Chinese corpus .	1<2	none	exp-evidence	exp-evidence
P96-1020	1-14	15-23	This paper proposes the use of " patternbased " context-free grammars as a basis	for building machine translation ( MT ) systems ,	This paper proposes the use of " patternbased " context-free grammars as a basis	for building machine translation ( MT ) systems ,	1-42	1-42	This paper proposes the use of " patternbased " context-free grammars as a basis for building machine translation ( MT ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .	This paper proposes the use of " patternbased " context-free grammars as a basis for building machine translation ( MT ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .	1<2	none	enablement	enablement
P96-1020	15-23	24-42	for building machine translation ( MT ) systems ,	which are now being adopted as personal tools by a broad range of users in the cyberspace society .	for building machine translation ( MT ) systems ,	which are now being adopted as personal tools by a broad range of users in the cyberspace society .	1-42	1-42	This paper proposes the use of " patternbased " context-free grammars as a basis for building machine translation ( MT ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .	This paper proposes the use of " patternbased " context-free grammars as a basis for building machine translation ( MT ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .	1<2	none	elab-addition	elab-addition
P96-1020	1-14	43-50	This paper proposes the use of " patternbased " context-free grammars as a basis	We discuss major requirements for such tools ,	This paper proposes the use of " patternbased " context-free grammars as a basis	We discuss major requirements for such tools ,	1-42	43-86	This paper proposes the use of " patternbased " context-free grammars as a basis for building machine translation ( MT ) systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	1<2	none	elab-aspect	elab-aspect
P96-1020	43-50	51-77	We discuss major requirements for such tools ,	including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) ,	We discuss major requirements for such tools ,	including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) ,	43-86	43-86	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	1<2	none	elab-enumember	elab-enumember
P96-1020	78-79	80-86	and describe	how our approach meets these requirements .	and describe	how our approach meets these requirements .	43-86	43-86	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	1>2	none	attribution	attribution
P96-1020	43-50	80-86	We discuss major requirements for such tools ,	how our approach meets these requirements .	We discuss major requirements for such tools ,	how our approach meets these requirements .	43-86	43-86	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the translation algorithm , and scalability ( incremental improvement in translation quality through user interaction ) , and describe how our approach meets these requirements .	1<2	none	joint	joint
P96-1021	1-10	11-31	We introduce a polynomial-time algorithm for statistical machine translation .	This algorithm can be used in place of the expensive , slow best-first search strategies in current statistical translation architectures .	We introduce a polynomial-time algorithm for statistical machine translation .	This algorithm can be used in place of the expensive , slow best-first search strategies in current statistical translation architectures .	1-10	11-31	We introduce a polynomial-time algorithm for statistical machine translation .	This algorithm can be used in place of the expensive , slow best-first search strategies in current statistical translation architectures .	1<2	none	elab-addition	elab-addition
P96-1021	1-10	32-43	We introduce a polynomial-time algorithm for statistical machine translation .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	We introduce a polynomial-time algorithm for statistical machine translation .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	1-10	32-61	We introduce a polynomial-time algorithm for statistical machine translation .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	1<2	none	elab-aspect	elab-aspect
P96-1021	32-43	44-46	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	we recently introduced	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	we recently introduced	32-61	32-61	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	1<2	none	elab-addition	elab-addition
P96-1021	32-43	47-54	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	to replace earlier word alignment channel models ,	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model	to replace earlier word alignment channel models ,	32-61	32-61	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	1<2	none	enablement	enablement
P96-1021	47-54	55-61	to replace earlier word alignment channel models ,	while retaining a bigram language model .	to replace earlier word alignment channel models ,	while retaining a bigram language model .	32-61	32-61	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	The approach employs the stochastic bracketing transduction grammar ( SBTG ) model we recently introduced to replace earlier word alignment channel models , while retaining a bigram language model .	1<2	none	temporal	temporal
P96-1021	1-10	62-78	We introduce a polynomial-time algorithm for statistical machine translation .	The new algorithm in our experience yields major speed improvement with no significant loss of accuracy .	We introduce a polynomial-time algorithm for statistical machine translation .	The new algorithm in our experience yields major speed improvement with no significant loss of accuracy .	1-10	62-78	We introduce a polynomial-time algorithm for statistical machine translation .	The new algorithm in our experience yields major speed improvement with no significant loss of accuracy .	1<2	none	evaluation	evaluation
P96-1022	1-8	9-17	This paper presents a generalised two level implementation	which can handle linear and non-linear morphological operations .	This paper presents a generalised two level implementation	which can handle linear and non-linear morphological operations .	1-17	1-17	This paper presents a generalised two level implementation which can handle linear and non-linear morphological operations .	This paper presents a generalised two level implementation which can handle linear and non-linear morphological operations .	1<2	none	elab-addition	elab-addition
P96-1022	1-8	18-29	This paper presents a generalised two level implementation	An algorithm for the interpretation of multi-tape two-level rules is described .	This paper presents a generalised two level implementation	An algorithm for the interpretation of multi-tape two-level rules is described .	1-17	18-29	This paper presents a generalised two level implementation which can handle linear and non-linear morphological operations .	An algorithm for the interpretation of multi-tape two-level rules is described .	1<2	none	elab-aspect	elab-aspect
P96-1022	1-8	30-36,43-49	This paper presents a generalised two level implementation	In addition , a number of issues <*> are discussed with examples from Syriac .	This paper presents a generalised two level implementation	In addition , a number of issues <*> are discussed with examples from Syriac .	1-17	30-49	This paper presents a generalised two level implementation which can handle linear and non-linear morphological operations .	In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .	1<2	none	elab-aspect	elab-aspect
P96-1022	30-36,43-49	37-38	In addition , a number of issues <*> are discussed with examples from Syriac .	which arise	In addition , a number of issues <*> are discussed with examples from Syriac .	which arise	30-49	30-49	In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .	In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .	1<2	none	elab-addition	elab-addition
P96-1022	37-38	39-42	which arise	when developing non-linear grammars	which arise	when developing non-linear grammars	30-49	30-49	In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .	In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .	1<2	none	temporal	temporal
P96-1023	1-5	6-15	We present a language model	consisting of a collection of costed bidirectional finite state automata	We present a language model	consisting of a collection of costed bidirectional finite state automata	1-23	1-23	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	1<2	none	elab-addition	elab-addition
P96-1023	6-15	16-23	consisting of a collection of costed bidirectional finite state automata	associated with the head words of phrases .	consisting of a collection of costed bidirectional finite state automata	associated with the head words of phrases .	1-23	1-23	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	1<2	none	elab-addition	elab-addition
P96-1023	1-5	24-44	We present a language model	The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations .	We present a language model	The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations .	1-23	24-44	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations .	1<2	none	elab-addition	elab-addition
P96-1023	1-5	45-54	We present a language model	We also present a model and algorithm for machine translation	We present a language model	We also present a model and algorithm for machine translation	1-23	45-71	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	We also present a model and algorithm for machine translation involving optimal " tiling " of a dependency tree with entries of a costed bilingual lexicon .	1<2	none	joint	joint
P96-1023	45-54	55-71	We also present a model and algorithm for machine translation	involving optimal " tiling " of a dependency tree with entries of a costed bilingual lexicon .	We also present a model and algorithm for machine translation	involving optimal " tiling " of a dependency tree with entries of a costed bilingual lexicon .	45-71	45-71	We also present a model and algorithm for machine translation involving optimal " tiling " of a dependency tree with entries of a costed bilingual lexicon .	We also present a model and algorithm for machine translation involving optimal " tiling " of a dependency tree with entries of a costed bilingual lexicon .	1<2	none	elab-addition	elab-addition
P96-1023	72-75	86-103	Experimental results are reported	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	Experimental results are reported	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	72-85	86-103	Experimental results are reported comparing methods for assigning cost functions to these models .	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	1>2	none	result	result
P96-1023	72-75	76-77	Experimental results are reported	comparing methods	Experimental results are reported	comparing methods	72-85	72-85	Experimental results are reported comparing methods for assigning cost functions to these models .	Experimental results are reported comparing methods for assigning cost functions to these models .	1<2	none	elab-addition	elab-addition
P96-1023	76-77	78-81	comparing methods	for assigning cost functions	comparing methods	for assigning cost functions	72-85	72-85	Experimental results are reported comparing methods for assigning cost functions to these models .	Experimental results are reported comparing methods for assigning cost functions to these models .	1<2	none	enablement	enablement
P96-1023	76-77	82-85	comparing methods	to these models .	comparing methods	to these models .	72-85	72-85	Experimental results are reported comparing methods for assigning cost functions to these models .	Experimental results are reported comparing methods for assigning cost functions to these models .	1<2	none	comparison	comparison
P96-1023	1-5	86-103	We present a language model	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	We present a language model	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	1-23	86-103	We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases .	We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation .	1<2	none	summary	summary
P96-1024	1-4	26-31,37-46	Many different metrics exist	<*> However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	Many different metrics exist	However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	1-25	26-53	Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	1>2	none	contrast	contrast
P96-1024	1-4	5-9	Many different metrics exist	for evaluating parsing results ,	Many different metrics exist	for evaluating parsing results ,	1-25	1-25	Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	1<2	none	enablement	enablement
P96-1024	1-4	10-25	Many different metrics exist	including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	Many different metrics exist	including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	1-25	1-25	Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .	1<2	none	elab-enumember	elab-enumember
P96-1024	26-31	32-36	However , most parsing algorithms ,	including the Viterbi algorithm ,	However , most parsing algorithms ,	including the Viterbi algorithm ,	26-53	26-53	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	1<2	none	elab-enumember	elab-enumember
P96-1024	26-31,37-46	71-76	<*> However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	We present two new algorithms :	However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	We present two new algorithms :	26-53	71-106	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1>2	none	bg-goal	bg-goal
P96-1024	26-31,37-46	47-53	<*> However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	of getting the correct labelled tree .	However , most parsing algorithms , <*> attempt to optimize the same metric , namely the probability	of getting the correct labelled tree .	26-53	26-53	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree .	1<2	none	elab-addition	elab-addition
P96-1024	54-64	65-70	By choosing a parsing algorithm appropriate for the evaluation metric ,	better performance can be achieved .	By choosing a parsing algorithm appropriate for the evaluation metric ,	better performance can be achieved .	54-70	54-70	By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved .	By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved .	1>2	none	manner-means	manner-means
P96-1024	65-70	71-76	better performance can be achieved .	We present two new algorithms :	better performance can be achieved .	We present two new algorithms :	54-70	71-106	By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1>2	none	exp-reason	exp-reason
P96-1024	71-76	77-83	We present two new algorithms :	the " Labelled Recall Algorithm , "	We present two new algorithms :	the " Labelled Recall Algorithm , "	71-106	71-106	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1<2	none	elab-addition	elab-addition
P96-1024	77-83	84-91	the " Labelled Recall Algorithm , "	which maximizes the expected Labelled Recall Rate ,	the " Labelled Recall Algorithm , "	which maximizes the expected Labelled Recall Rate ,	71-106	71-106	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1<2	none	elab-addition	elab-addition
P96-1024	77-83	92-99	the " Labelled Recall Algorithm , "	and the " Bracketed Recall Algorithm , "	the " Labelled Recall Algorithm , "	and the " Bracketed Recall Algorithm , "	71-106	71-106	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1<2	none	joint	joint
P96-1024	92-99	100-106	and the " Bracketed Recall Algorithm , "	which maximizes the Bracketed Recall Rate .	and the " Bracketed Recall Algorithm , "	which maximizes the Bracketed Recall Rate .	71-106	71-106	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	1<2	none	elab-addition	elab-addition
P96-1024	71-76	107-111	We present two new algorithms :	Experimental results are given ,	We present two new algorithms :	Experimental results are given ,	71-106	107-135	We present two new algorithms : the " Labelled Recall Algorithm , " which maximizes the expected Labelled Recall Rate , and the " Bracketed Recall Algorithm , " which maximizes the Bracketed Recall Rate .	Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize .	1<2	none	evaluation	evaluation
P96-1024	107-111	112-131	Experimental results are given ,	showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones	Experimental results are given ,	showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones	107-135	107-135	Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize .	Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize .	1<2	none	elab-addition	elab-addition
P96-1024	112-131	132-135	showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones	that they optimize .	showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones	that they optimize .	107-135	107-135	Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize .	Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize .	1<2	none	elab-addition	elab-addition
P96-1025	1-7	8-21	This paper describes a new statistical parser	which is based on probabilities of dependencies between head-words in the parse tree .	This paper describes a new statistical parser	which is based on probabilities of dependencies between head-words in the parse tree .	1-21	1-21	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	1<2	none	elab-addition	elab-addition
P96-1025	1-7	22-28	This paper describes a new statistical parser	Standard bigram probability estimation techniques are extended	This paper describes a new statistical parser	Standard bigram probability estimation techniques are extended	1-21	22-38	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words .	1<2	none	elab-aspect	elab-aspect
P96-1025	22-28	29-38	Standard bigram probability estimation techniques are extended	to calculate probabilities of dependencies between pairs of words .	Standard bigram probability estimation techniques are extended	to calculate probabilities of dependencies between pairs of words .	22-38	22-38	Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words .	Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words .	1<2	none	enablement	enablement
P96-1025	39	40-44	Tests	using Wall Street Journal data	Tests	using Wall Street Journal data	39-79	39-79	Tests using Wall Street Journal data show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) , which has the best published results for a statistical parser on this task .	Tests using Wall Street Journal data show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) , which has the best published results for a statistical parser on this task .	1<2	none	elab-addition	elab-addition
P96-1025	1-7	45-65	This paper describes a new statistical parser	<*> show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) ,	This paper describes a new statistical parser	show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) ,	1-21	39-79	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	Tests using Wall Street Journal data show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) , which has the best published results for a statistical parser on this task .	1<2	none	evaluation	evaluation
P96-1025	45-65	66-79	<*> show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) ,	which has the best published results for a statistical parser on this task .	show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) ,	which has the best published results for a statistical parser on this task .	39-79	39-79	Tests using Wall Street Journal data show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) , which has the best published results for a statistical parser on this task .	Tests using Wall Street Journal data show that the method performs at least as well as SPATTER ( Magerman 95 ; Jelinek et al. 94 ) , which has the best published results for a statistical parser on this task .	1<2	none	elab-addition	elab-addition
P96-1025	1-7	80-96	This paper describes a new statistical parser	The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes .	This paper describes a new statistical parser	The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes .	1-21	80-96	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	The simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes .	1<2	none	elab-aspect	elab-aspect
P96-1025	97-102	103-118	With a beam search strategy parsing	speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	With a beam search strategy parsing	speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	97-118	97-118	With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	1>2	none	elab-addition	elab-addition
P96-1025	1-7	103-118	This paper describes a new statistical parser	speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	This paper describes a new statistical parser	speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	1-21	97-118	This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree .	With a beam search strategy parsing speed can be improved to over 200 sentences a minute with negligible loss in accuracy .	1<2	none	elab-aspect	elab-aspect
P96-1027	1-10	11-23	Charts constitute a natural uniform architecture for parsing and generation	provided string position is replaced by a notion more appropriate to logical forms	Charts constitute a natural uniform architecture for parsing and generation	provided string position is replaced by a notion more appropriate to logical forms	1-37	1-37	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	1<2	none	elab-addition	elab-addition
P96-1027	1-10	24-32	Charts constitute a natural uniform architecture for parsing and generation	and that measures are taken to curtail generation paths	Charts constitute a natural uniform architecture for parsing and generation	and that measures are taken to curtail generation paths	1-37	1-37	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	1<2	none	joint	joint
P96-1027	24-32	33-37	and that measures are taken to curtail generation paths	containing semantically incomplete phrases .	and that measures are taken to curtail generation paths	containing semantically incomplete phrases .	1-37	1-37	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	Charts constitute a natural uniform architecture for parsing and generation provided string position is replaced by a notion more appropriate to logical forms and that measures are taken to curtail generation paths containing semantically incomplete phrases .	1<2	none	elab-addition	elab-addition
P96-1028	1-19	20-24	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	used by the system STREAK	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	used by the system STREAK	1-31	1-31	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries .	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries .	1<2	none	elab-addition	elab-addition
P96-1028	1-19	25-31	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	to incrementally generate newswire sports summaries .	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	to incrementally generate newswire sports summaries .	1-31	1-31	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries .	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries .	1<2	none	enablement	enablement
P96-1028	32-46	70-91	The evaluation consists of searching a test corpus of stock market reports for sentence pairs	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	The evaluation consists of searching a test corpus of stock market reports for sentence pairs	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	32-66	67-91	The evaluation consists of searching a test corpus of stock market reports for sentence pairs whose ( semantic and syntactic ) structures respectively match the triggering condition and application result of each revision rule .	The results show that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	1>2	none	result	result
P96-1028	32-46	47-66	The evaluation consists of searching a test corpus of stock market reports for sentence pairs	whose ( semantic and syntactic ) structures respectively match the triggering condition and application result of each revision rule .	The evaluation consists of searching a test corpus of stock market reports for sentence pairs	whose ( semantic and syntactic ) structures respectively match the triggering condition and application result of each revision rule .	32-66	32-66	The evaluation consists of searching a test corpus of stock market reports for sentence pairs whose ( semantic and syntactic ) structures respectively match the triggering condition and application result of each revision rule .	The evaluation consists of searching a test corpus of stock market reports for sentence pairs whose ( semantic and syntactic ) structures respectively match the triggering condition and application result of each revision rule .	1<2	none	elab-addition	elab-addition
P96-1028	67-69	70-91	The results show	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	The results show	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	67-91	67-91	The results show that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	The results show that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	1>2	none	attribution	attribution
P96-1028	1-19	70-91	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy	that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	1-31	67-91	This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries .	The results show that at least 59 % of all rule classes are fully portable , with at least another 7 % partially portable .	1<2	none	evaluation	evaluation
P96-1030	1-2	3-27	We show	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	We show	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	1-32	1-32	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	1>2	none	attribution	attribution
P96-1030	3-27	28-32	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	based on explanation-based learning .	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	based on explanation-based learning .	1-32	1-32	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	1<2	none	bg-general	bg-general
P96-1030	3-27	33-44	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	These methods together give an order of magnitude increase in speed ,	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	These methods together give an order of magnitude increase in speed ,	1-32	33-63	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	1<2	none	elab-aspect	elab-aspect
P96-1030	45-48	49-52	and the coverage loss	entailed by grammar specialization	and the coverage loss	entailed by grammar specialization	33-63	33-63	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	1<2	none	elab-addition	elab-addition
P96-1030	33-44	45-48,53-57	These methods together give an order of magnitude increase in speed ,	<*> and the coverage loss <*> is reduced to approximately half	These methods together give an order of magnitude increase in speed ,	and the coverage loss <*> is reduced to approximately half	33-63	33-63	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	1<2	none	joint	joint
P96-1030	45-48,53-57	58-63	<*> and the coverage loss <*> is reduced to approximately half	that reported in previous work .	and the coverage loss <*> is reduced to approximately half	that reported in previous work .	33-63	33-63	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	These methods together give an order of magnitude increase in speed , and the coverage loss entailed by grammar specialization is reduced to approximately half that reported in previous work .	1<2	none	elab-addition	elab-addition
P96-1030	64	65-66	Experiments	described here	Experiments	described here	64-94	64-94	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application .	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application .	1<2	none	elab-addition	elab-addition
P96-1030	3-27	64,67-78	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	<*> Experiments <*> suggest that the loss of coverage has been reduced to the point	how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization	Experiments <*> suggest that the loss of coverage has been reduced to the point	1-32	64-94	We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning .	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application .	1<2	none	evaluation	evaluation
P96-1030	64,67-78	79-94	<*> Experiments <*> suggest that the loss of coverage has been reduced to the point	where it no longer causes significant performance degradation in the context of a real application .	Experiments <*> suggest that the loss of coverage has been reduced to the point	where it no longer causes significant performance degradation in the context of a real application .	64-94	64-94	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application .	Experiments described here suggest that the loss of coverage has been reduced to the point where it no longer causes significant performance degradation in the context of a real application .	1<2	none	elab-addition	elab-addition
P96-1031	1-15	28-40	Context-dependent rewrite rules are used in many areas of natural language and speech processing .	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	Context-dependent rewrite rules are used in many areas of natural language and speech processing .	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	1-15	16-40	Context-dependent rewrite rules are used in many areas of natural language and speech processing .	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	1>2	none	bg-general	bg-general
P96-1031	16-21	28-40	Work in computational phonology has demonstrated	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	Work in computational phonology has demonstrated	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	16-40	16-40	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	1>2	none	attribution	attribution
P96-1031	22-27	28-40	that , given certain conditions ,	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	that , given certain conditions ,	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	16-40	16-40	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	1>2	none	condition	condition
P96-1031	28-40	41-45	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	We describe a new algorithm	such rewrite rules can be represented as finite-state transducers ( FSTs ) .	We describe a new algorithm	16-40	41-52	Work in computational phonology has demonstrated that , given certain conditions , such rewrite rules can be represented as finite-state transducers ( FSTs ) .	We describe a new algorithm for compiling rewrite rules into FSTs .	1>2	none	bg-goal	bg-goal
P96-1031	41-45	46-52	We describe a new algorithm	for compiling rewrite rules into FSTs .	We describe a new algorithm	for compiling rewrite rules into FSTs .	41-52	41-52	We describe a new algorithm for compiling rewrite rules into FSTs .	We describe a new algorithm for compiling rewrite rules into FSTs .	1<2	none	enablement	enablement
P96-1031	41-45	53-66	We describe a new algorithm	We show the algorithm to be simpler and more efficient than existing algorithms .	We describe a new algorithm	We show the algorithm to be simpler and more efficient than existing algorithms .	41-52	53-66	We describe a new algorithm for compiling rewrite rules into FSTs .	We show the algorithm to be simpler and more efficient than existing algorithms .	1<2	none	evaluation	evaluation
P96-1031	41-45	67-75	We describe a new algorithm	Further , many of our applications demand the ability	We describe a new algorithm	Further , many of our applications demand the ability	41-52	67-91	We describe a new algorithm for compiling rewrite rules into FSTs .	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	1<2	none	elab-aspect	elab-aspect
P96-1031	67-75	76-83	Further , many of our applications demand the ability	to compile weighted rules into weighted FSTs ,	Further , many of our applications demand the ability	to compile weighted rules into weighted FSTs ,	67-91	67-91	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	1<2	none	enablement	enablement
P96-1031	76-83	84-85	to compile weighted rules into weighted FSTs ,	transducers generalized	to compile weighted rules into weighted FSTs ,	transducers generalized	67-91	67-91	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	1<2	none	elab-addition	elab-addition
P96-1031	84-85	86-91	transducers generalized	by providing transitions with weights .	transducers generalized	by providing transitions with weights .	67-91	67-91	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	Further , many of our applications demand the ability to compile weighted rules into weighted FSTs , transducers generalized by providing transitions with weights .	1<2	none	manner-means	manner-means
P96-1031	41-45	92-96	We describe a new algorithm	We have extended the algorithm	We describe a new algorithm	We have extended the algorithm	41-52	92-101	We describe a new algorithm for compiling rewrite rules into FSTs .	We have extended the algorithm to allow for this .	1<2	none	elab-aspect	elab-aspect
P96-1031	92-96	97-101	We have extended the algorithm	to allow for this .	We have extended the algorithm	to allow for this .	92-101	92-101	We have extended the algorithm to allow for this .	We have extended the algorithm to allow for this .	1<2	none	enablement	enablement
P96-1032	1-10	11-21	We give a new treatment of tabular LR parsing ,	which is an alternative to Tomita 's generalized LR algorithm .	We give a new treatment of tabular LR parsing ,	which is an alternative to Tomita's generalized LR algorithm .	1-21	1-21	We give a new treatment of tabular LR parsing , which is an alternative to Tomita 's generalized LR algorithm .	We give a new treatment of tabular LR parsing , which is an alternative to Tomita 's generalized LR algorithm .	1<2	none	elab-addition	elab-addition
P96-1032	1-10	22-26	We give a new treatment of tabular LR parsing ,	The advantage is twofold .	We give a new treatment of tabular LR parsing ,	The advantage is twofold .	1-21	22-26	We give a new treatment of tabular LR parsing , which is an alternative to Tomita 's generalized LR algorithm .	The advantage is twofold .	1<2	none	elab-aspect	elab-aspect
P96-1032	22-26	27-34	The advantage is twofold .	Firstly , our treatment is conceptually more attractive	The advantage is twofold .	Firstly , our treatment is conceptually more attractive	22-26	27-54	The advantage is twofold .	Firstly , our treatment is conceptually more attractive because it uses simpler concepts , such as grammar transformations and standard tabulation techniques also know as chart parsing .	1<2	none	elab-enumember	elab-enumember
P96-1032	27-34	35-48	Firstly , our treatment is conceptually more attractive	because it uses simpler concepts , such as grammar transformations and standard tabulation techniques	Firstly , our treatment is conceptually more attractive	because it uses simpler concepts , such as grammar transformations and standard tabulation techniques	27-54	27-54	Firstly , our treatment is conceptually more attractive because it uses simpler concepts , such as grammar transformations and standard tabulation techniques also know as chart parsing .	Firstly , our treatment is conceptually more attractive because it uses simpler concepts , such as grammar transformations and standard tabulation techniques also know as chart parsing .	1<2	none	exp-reason	exp-reason
P96-1032	35-48	49-54	because it uses simpler concepts , such as grammar transformations and standard tabulation techniques	also know as chart parsing .	because it uses simpler concepts , such as grammar transformations and standard tabulation techniques	also know as chart parsing .	27-54	27-54	Firstly , our treatment is conceptually more attractive because it uses simpler concepts , such as grammar transformations and standard tabulation techniques also know as chart parsing .	Firstly , our treatment is conceptually more attractive because it uses simpler concepts , such as grammar transformations and standard tabulation techniques also know as chart parsing .	1<2	none	joint	joint
P96-1032	22-26	55-74	The advantage is twofold .	Secondly , the static and dynamic complexity of parsing , both in space and time , is significantly reduced .	The advantage is twofold .	Secondly , the static and dynamic complexity of parsing , both in space and time , is significantly reduced .	22-26	55-74	The advantage is twofold .	Secondly , the static and dynamic complexity of parsing , both in space and time , is significantly reduced .	1<2	none	elab-enumember	elab-enumember
P96-1033	1-5,8-15	47-49,58-59	Off-line compilation of logic grammars <*> allows an incorporation of filtering into the logic	Two filter optimizations <*> are discussed	Off-line compilation of logic grammars <*> allows an incorporation of filtering into the logic	Two filter optimizations <*> are discussed	1-19	47-67	Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	1>2	none	bg-general	bg-general
P96-1033	1-5,8-15	6-7	Off-line compilation of logic grammars <*> allows an incorporation of filtering into the logic	using Magic	Off-line compilation of logic grammars <*> allows an incorporation of filtering into the logic	using Magic	1-19	1-19	Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .	Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .	1<2	none	elab-addition	elab-addition
P96-1033	8-15	16-19	allows an incorporation of filtering into the logic	underlying the grammar .	allows an incorporation of filtering into the logic	underlying the grammar .	1-19	1-19	Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .	Off-line compilation of logic grammars using Magic allows an incorporation of filtering into the logic underlying the grammar .	1<2	none	elab-addition	elab-addition
P96-1033	20-26,31-46	47-49,58-59	The explicit definite clause characterization of filtering <*> allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	Two filter optimizations <*> are discussed	The explicit definite clause characterization of filtering <*> allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	Two filter optimizations <*> are discussed	20-46	47-67	The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	1>2	none	bg-general	bg-general
P96-1033	20-26,31-46	27-30	The explicit definite clause characterization of filtering <*> allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	resulting from Magic compilation	The explicit definite clause characterization of filtering <*> allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	resulting from Magic compilation	20-46	20-46	The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	The explicit definite clause characterization of filtering resulting from Magic compilation allows processor independent and logically clean optimizations of dynamic bottom-up processing with respect to goal-directedness .	1<2	none	elab-addition	elab-addition
P96-1033	47-49,58-59	50-57	Two filter optimizations <*> are discussed	based on the program transformation technique of Unfolding	Two filter optimizations <*> are discussed	based on the program transformation technique of Unfolding	47-67	47-67	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	1<2	none	bg-general	bg-general
P96-1033	58-59	60-67	are discussed	which are of practical and theoretical interest .	are discussed	which are of practical and theoretical interest .	47-67	47-67	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	Two filter optimizations based on the program transformation technique of Unfolding are discussed which are of practical and theoretical interest .	1<2	none	elab-addition	elab-addition
P96-1034	1-20	45-51	In transformation-based parsing , a finite sequence of tree rewriting rules are checked for application to an input structure .	We exploit this sparseness in rule applications	In transformation-based parsing , a finite sequence of tree rewriting rules are checked for application to an input structure .	We exploit this sparseness in rule applications	1-20	45-68	In transformation-based parsing , a finite sequence of tree rewriting rules are checked for application to an input structure .	We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	1>2	none	bg-general	bg-general
P96-1034	21-36	37-44	Since in practice only a small percentage of rules are applied to any particular structure ,	the naive parsing algorithm is rather inefficient .	Since in practice only a small percentage of rules are applied to any particular structure ,	the naive parsing algorithm is rather inefficient .	21-44	21-44	Since in practice only a small percentage of rules are applied to any particular structure , the naive parsing algorithm is rather inefficient .	Since in practice only a small percentage of rules are applied to any particular structure , the naive parsing algorithm is rather inefficient .	1>2	none	temporal	temporal
P96-1034	37-44	45-51	the naive parsing algorithm is rather inefficient .	We exploit this sparseness in rule applications	the naive parsing algorithm is rather inefficient .	We exploit this sparseness in rule applications	21-44	45-68	Since in practice only a small percentage of rules are applied to any particular structure , the naive parsing algorithm is rather inefficient .	We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	1>2	none	bg-goal	bg-goal
P96-1034	45-51	52-68	We exploit this sparseness in rule applications	to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	We exploit this sparseness in rule applications	to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	45-68	45-68	We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	We exploit this sparseness in rule applications to derive an algorithm two to three orders of magnitude faster than the standard parsing algorithm .	1<2	none	enablement	enablement
P96-1035	1-8	9-16	We propose an Mgorithm to resolve anaphors ,	tackling mainly the problem of intrasentential antecedents .	We propose an Mgorithm to resolve anaphors ,	tackling mainly the problem of intrasentential antecedents .	1-16	1-16	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	1<2	none	elab-addition	elab-addition
P96-1035	1-8	17-23	We propose an Mgorithm to resolve anaphors ,	We base our methodology on the fact	We propose an Mgorithm to resolve anaphors ,	We base our methodology on the fact	1-16	17-34	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	We base our methodology on the fact that such antecedents are likely to occur in embedded sentences .	1<2	none	elab-aspect	elab-aspect
P96-1035	17-23	24-34	We base our methodology on the fact	that such antecedents are likely to occur in embedded sentences .	We base our methodology on the fact	that such antecedents are likely to occur in embedded sentences .	17-34	17-34	We base our methodology on the fact that such antecedents are likely to occur in embedded sentences .	We base our methodology on the fact that such antecedents are likely to occur in embedded sentences .	1<2	none	elab-addition	elab-addition
P96-1035	1-8	35-50	We propose an Mgorithm to resolve anaphors ,	Sidner 's focusing mechanism is used as the basic algorithm in a more complete approach .	We propose an Mgorithm to resolve anaphors ,	Sidner's focusing mechanism is used as the basic algorithm in a more complete approach .	1-16	35-50	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	Sidner 's focusing mechanism is used as the basic algorithm in a more complete approach .	1<2	none	elab-aspect	elab-aspect
P96-1035	1-8	51-67	We propose an Mgorithm to resolve anaphors ,	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly	We propose an Mgorithm to resolve anaphors ,	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly	1-16	51-71	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly to process pronouns .	1<2	none	elab-aspect	elab-aspect
P96-1035	51-67	68-71	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly	to process pronouns .	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly	to process pronouns .	51-71	51-71	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly to process pronouns .	The proposed algorithm has been tested and implemented as a part of a conceptual analyser , mainly to process pronouns .	1<2	none	elab-addition	elab-addition
P96-1035	1-8	72-78	We propose an Mgorithm to resolve anaphors ,	Details of an evaluation are given .	We propose an Mgorithm to resolve anaphors ,	Details of an evaluation are given .	1-16	72-78	We propose an Mgorithm to resolve anaphors , tackling mainly the problem of intrasentential antecedents .	Details of an evaluation are given .	1<2	none	elab-aspect	elab-aspect
P96-1036	1-13	14-21	Based on empirical evidence from a free word order language ( German )	we propose a fundamental revision of the principles	Based on empirical evidence from a free word order language ( German )	we propose a fundamental revision of the principles	1-36	1-36	Based on empirical evidence from a free word order language ( German ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model .	Based on empirical evidence from a free word order language ( German ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model .	1>2	none	bg-general	bg-general
P96-1036	14-21	22-36	we propose a fundamental revision of the principles	guiding the ordering of discourse entities in the forward-looking centers within the centering model .	we propose a fundamental revision of the principles	guiding the ordering of discourse entities in the forward-looking centers within the centering model .	1-36	1-36	Based on empirical evidence from a free word order language ( German ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model .	Based on empirical evidence from a free word order language ( German ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model .	1<2	none	elab-addition	elab-addition
P96-1036	37-38	39-67	We claim	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	We claim	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	37-67	37-67	We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	1>2	none	attribution	attribution
P96-1036	14-21	39-67	we propose a fundamental revision of the principles	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	we propose a fundamental revision of the principles	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	1-36	37-67	Based on empirical evidence from a free word order language ( German ) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model .	We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	1<2	none	summary	summary
P96-1036	39-67	68-80	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	This claim is backed up by an empirical evaluation of functional centering .	that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	This claim is backed up by an empirical evaluation of functional centering .	37-67	68-80	We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances , i.e. , the distinction between context-bound and unbound discourse elements .	This claim is backed up by an empirical evaluation of functional centering .	1<2	none	elab-addition	elab-addition
P96-1037	14-15	16-27	We show	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	We show	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	14-42	14-42	We show how to incorporate initiative changing in a task-oriented human-computer dialogue system , and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	We show how to incorporate initiative changing in a task-oriented human-computer dialogue system , and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	1>2	none	attribution	attribution
P96-1037	1-13	16-27	In this paper , we examine mechanisms for automatic dialogue initiative setting .	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	In this paper , we examine mechanisms for automatic dialogue initiative setting .	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	1-13	14-42	In this paper , we examine mechanisms for automatic dialogue initiative setting .	We show how to incorporate initiative changing in a task-oriented human-computer dialogue system , and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	1<2	none	elab-addition	elab-addition
P96-1037	16-27	28-42	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	how to incorporate initiative changing in a task-oriented human-computer dialogue system ,	and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	14-42	14-42	We show how to incorporate initiative changing in a task-oriented human-computer dialogue system , and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	We show how to incorporate initiative changing in a task-oriented human-computer dialogue system , and we evaluate the effects of initiative both analytically and via computer-computer dialogue simulation .	1<2	none	joint	joint
P96-1039	1-16	17-34	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation	1-16	17-54	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient , coherent speech that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	1<2	none	elab-aspect	elab-aspect
P96-1039	17-34	35-40	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation	to produce efficient , coherent speech	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation	to produce efficient , coherent speech	17-54	17-54	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient , coherent speech that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient , coherent speech that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	1<2	none	enablement	enablement
P96-1039	35-40	41-54	to produce efficient , coherent speech	that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	to produce efficient , coherent speech	that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	17-54	17-54	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient , coherent speech that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	A twotiered information structure representation is used in the high-level content planning and sentence planning stages of generation to produce efficient , coherent speech that makes certain discourse relationships , such as explicit contrasts , appropriately salient .	1<2	none	elab-addition	elab-addition
P96-1039	1-16	55-63	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	The system is able to produce appropriate intonational patterns	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	The system is able to produce appropriate intonational patterns	1-16	55-80	This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation .	The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions .	1<2	none	evaluation	evaluation
P96-1039	55-63	64-70	The system is able to produce appropriate intonational patterns	that cannot be generated by other systems	The system is able to produce appropriate intonational patterns	that cannot be generated by other systems	55-80	55-80	The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions .	The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions .	1<2	none	elab-addition	elab-addition
P96-1039	64-70	71-80	that cannot be generated by other systems	which rely solely on word class and given/new distinctions .	that cannot be generated by other systems	which rely solely on word class and given/new distinctions .	55-80	55-80	The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions .	The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions .	1<2	none	elab-addition	elab-addition
P96-1041	1-17	18-19	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	including those	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	including those	1-41	1-41	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	1<2	none	elab-addition	elab-addition
P96-1041	18-19	20-41	including those	described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	including those	described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	1-41	1-41	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	1<2	none	elab-addition	elab-addition
P96-1041	42-47	48-82	We investigate for the first time	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	We investigate for the first time	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	42-92	42-92	We investigate for the first time how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods , which we measure through the cross-entropy of test data .	We investigate for the first time how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods , which we measure through the cross-entropy of test data .	1>2	none	attribution	attribution
P96-1041	1-17	48-82	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	1-41	42-92	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	We investigate for the first time how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods , which we measure through the cross-entropy of test data .	1<2	none	elab-aspect	elab-aspect
P96-1041	48-82	83-92	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	which we measure through the cross-entropy of test data .	how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods ,	which we measure through the cross-entropy of test data .	42-92	42-92	We investigate for the first time how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods , which we measure through the cross-entropy of test data .	We investigate for the first time how factors such as training data size , corpus ( e.g. , Brown versus Wall Street Journal ) , and n-gram order ( bigram versus trigram ) affect the relative performance of these methods , which we measure through the cross-entropy of test data .	1<2	none	elab-addition	elab-addition
P96-1041	1-17	93-117	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique ,	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling ,	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique ,	1-41	93-124	We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods .	1<2	none	evaluation	evaluation
P96-1041	93-117	118-124	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique ,	both of which outperform existing methods .	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique ,	both of which outperform existing methods .	93-124	93-124	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods .	In addition , we introduce two novel smoothing techniques , one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods .	1<2	none	elab-addition	elab-addition
P96-1042	1-11	20-23	Corpus-based methods for natural language processing often use supervised training ,	This paper investigates methods	Corpus-based methods for natural language processing often use supervised training ,	This paper investigates methods	1-19	20-31	Corpus-based methods for natural language processing often use supervised training , requiring expensive manual annotation of training corpora .	This paper investigates methods for reducing annotation cost by sample selection .	1>2	none	bg-goal	bg-goal
P96-1042	1-11	12-19	Corpus-based methods for natural language processing often use supervised training ,	requiring expensive manual annotation of training corpora .	Corpus-based methods for natural language processing often use supervised training ,	requiring expensive manual annotation of training corpora .	1-19	1-19	Corpus-based methods for natural language processing often use supervised training , requiring expensive manual annotation of training corpora .	Corpus-based methods for natural language processing often use supervised training , requiring expensive manual annotation of training corpora .	1<2	none	elab-addition	elab-addition
P96-1042	20-23	24-27	This paper investigates methods	for reducing annotation cost	This paper investigates methods	for reducing annotation cost	20-31	20-31	This paper investigates methods for reducing annotation cost by sample selection .	This paper investigates methods for reducing annotation cost by sample selection .	1<2	none	enablement	enablement
P96-1042	24-27	28-31	for reducing annotation cost	by sample selection .	for reducing annotation cost	by sample selection .	20-31	20-31	This paper investigates methods for reducing annotation cost by sample selection .	This paper investigates methods for reducing annotation cost by sample selection .	1<2	none	manner-means	manner-means
P96-1042	32-37	38-44	In this approach , during training	the learning program examines many unlabeled examples	In this approach , during training	the learning program examines many unlabeled examples	32-61	32-61	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	1>2	none	temporal	temporal
P96-1042	20-23	38-44	This paper investigates methods	the learning program examines many unlabeled examples	This paper investigates methods	the learning program examines many unlabeled examples	20-31	32-61	This paper investigates methods for reducing annotation cost by sample selection .	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	1<2	none	elab-aspect	elab-aspect
P96-1042	38-44	45-53	the learning program examines many unlabeled examples	and selects for labeling ( annotation ) only those	the learning program examines many unlabeled examples	and selects for labeling ( annotation ) only those	32-61	32-61	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	1<2	none	joint	joint
P96-1042	45-53	54-61	and selects for labeling ( annotation ) only those	that are most informative at each stage .	and selects for labeling ( annotation ) only those	that are most informative at each stage .	32-61	32-61	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	1<2	none	elab-addition	elab-addition
P96-1042	38-44	62-66	the learning program examines many unlabeled examples	This avoids redundantly annotating examples	the learning program examines many unlabeled examples	This avoids redundantly annotating examples	32-61	62-72	In this approach , during training the learning program examines many unlabeled examples and selects for labeling ( annotation ) only those that are most informative at each stage .	This avoids redundantly annotating examples that contribute little new information .	1<2	none	elab-addition	elab-addition
P96-1042	62-66	67-72	This avoids redundantly annotating examples	that contribute little new information .	This avoids redundantly annotating examples	that contribute little new information .	62-72	62-72	This avoids redundantly annotating examples that contribute little new information .	This avoids redundantly annotating examples that contribute little new information .	1<2	none	elab-addition	elab-addition
P96-1042	20-23	73-86	This paper investigates methods	This paper extends our previous work on committee-based sample selection for probabilistic classifiers .	This paper investigates methods	This paper extends our previous work on committee-based sample selection for probabilistic classifiers .	20-31	73-86	This paper investigates methods for reducing annotation cost by sample selection .	This paper extends our previous work on committee-based sample selection for probabilistic classifiers .	1<2	none	elab-aspect	elab-aspect
P96-1042	20-23	87-97	This paper investigates methods	We describe a family of methods for committee-based sample selection ,	This paper investigates methods	We describe a family of methods for committee-based sample selection ,	20-31	87-110	This paper investigates methods for reducing annotation cost by sample selection .	We describe a family of methods for committee-based sample selection , and report experimental results for the task of stochastic part-of speech tagging .	1<2	none	elab-aspect	elab-aspect
P96-1042	87-97	98-110	We describe a family of methods for committee-based sample selection ,	and report experimental results for the task of stochastic part-of speech tagging .	We describe a family of methods for committee-based sample selection ,	and report experimental results for the task of stochastic part-of speech tagging .	87-110	87-110	We describe a family of methods for committee-based sample selection , and report experimental results for the task of stochastic part-of speech tagging .	We describe a family of methods for committee-based sample selection , and report experimental results for the task of stochastic part-of speech tagging .	1<2	none	joint	joint
P96-1042	111-112	113-123	We find	that all variants achieve a significant reduction in annotation cost ,	We find	that all variants achieve a significant reduction in annotation cost ,	111-129	111-129	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	1>2	none	attribution	attribution
P96-1042	20-23	113-123	This paper investigates methods	that all variants achieve a significant reduction in annotation cost ,	This paper investigates methods	that all variants achieve a significant reduction in annotation cost ,	20-31	111-129	This paper investigates methods for reducing annotation cost by sample selection .	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	1<2	none	evaluation	evaluation
P96-1042	113-123	124-129	that all variants achieve a significant reduction in annotation cost ,	though their computational efficiency differs .	that all variants achieve a significant reduction in annotation cost ,	though their computational efficiency differs .	111-129	111-129	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	1<2	none	manner-means	manner-means
P96-1042	113-123	130-136,144-147	that all variants achieve a significant reduction in annotation cost ,	In particular , the simplest method , <*> gives excellent results .	that all variants achieve a significant reduction in annotation cost ,	In particular , the simplest method , <*> gives excellent results .	111-129	130-147	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	In particular , the simplest method , which has no parameters to tune , gives excellent results .	1<2	none	elab-addition	elab-addition
P96-1042	130-136,144-147	137-143	In particular , the simplest method , <*> gives excellent results .	which has no parameters to tune ,	In particular , the simplest method , <*> gives excellent results .	which has no parameters to tune ,	130-147	130-147	In particular , the simplest method , which has no parameters to tune , gives excellent results .	In particular , the simplest method , which has no parameters to tune , gives excellent results .	1<2	none	elab-addition	elab-addition
P96-1042	148-150	151-163	We also show	that sample selection yields a significant reduction in the size of the model	We also show	that sample selection yields a significant reduction in the size of the model	148-168	148-168	We also show that sample selection yields a significant reduction in the size of the model used by the tagger .	We also show that sample selection yields a significant reduction in the size of the model used by the tagger .	1>2	none	attribution	attribution
P96-1042	113-123	151-163	that all variants achieve a significant reduction in annotation cost ,	that sample selection yields a significant reduction in the size of the model	that all variants achieve a significant reduction in annotation cost ,	that sample selection yields a significant reduction in the size of the model	111-129	148-168	We find that all variants achieve a significant reduction in annotation cost , though their computational efficiency differs .	We also show that sample selection yields a significant reduction in the size of the model used by the tagger .	1<2	none	joint	joint
P96-1042	151-163	164-168	that sample selection yields a significant reduction in the size of the model	used by the tagger .	that sample selection yields a significant reduction in the size of the model	used by the tagger .	148-168	148-168	We also show that sample selection yields a significant reduction in the size of the model used by the tagger .	We also show that sample selection yields a significant reduction in the size of the model used by the tagger .	1<2	none	elab-addition	elab-addition
P96-1043	1-13	14-27	Words unknown to the lexicon present a substantial problem to part-of-speech tagging .	In this paper we present a technique for fully unsupervised statistical acquisition of rules	Words unknown to the lexicon present a substantial problem to part-of-speech tagging .	In this paper we present a technique for fully unsupervised statistical acquisition of rules	1-13	14-35	Words unknown to the lexicon present a substantial problem to part-of-speech tagging .	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words .	1>2	none	bg-goal	bg-goal
P96-1043	14-27	28-35	In this paper we present a technique for fully unsupervised statistical acquisition of rules	which guess possible partsof-speech for unknown words .	In this paper we present a technique for fully unsupervised statistical acquisition of rules	which guess possible partsof-speech for unknown words .	14-35	14-35	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words .	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words .	1<2	none	elab-addition	elab-addition
P96-1043	14-27	36-51	In this paper we present a technique for fully unsupervised statistical acquisition of rules	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus :	In this paper we present a technique for fully unsupervised statistical acquisition of rules	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus :	14-35	36-62	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words .	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .	1<2	none	elab-aspect	elab-aspect
P96-1043	36-51	52-62	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus :	prefix morphological rules , suffix morphological rules and ending-guessing rules .	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus :	prefix morphological rules , suffix morphological rules and ending-guessing rules .	36-62	36-62	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .	Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus : prefix morphological rules , suffix morphological rules and ending-guessing rules .	1<2	none	elab-enumember	elab-enumember
P96-1043	63-71	72-88	The learning was performed on the Brown Corpus data	and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	The learning was performed on the Brown Corpus data	and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	63-88	63-88	The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	1>2	none	progression	progression
P96-1043	14-27	72-88	In this paper we present a technique for fully unsupervised statistical acquisition of rules	and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	In this paper we present a technique for fully unsupervised statistical acquisition of rules	and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	14-35	63-88	In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words .	The learning was performed on the Brown Corpus data and rule-sets , with a highly competitive performance , were produced and compared with the state-of-the-art .	1<2	none	evaluation	evaluation
P96-1044	1-5	6-15	This paper discusses the problem	of learning language from unprocessed text and speech signals ,	This paper discusses the problem	of learning language from unprocessed text and speech signals ,	1-24	1-24	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	1<2	none	elab-addition	elab-addition
P96-1044	6-15	16-24	of learning language from unprocessed text and speech signals ,	concentrating on the problem of learning a lexicon .	of learning language from unprocessed text and speech signals ,	concentrating on the problem of learning a lexicon .	1-24	1-24	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	1<2	none	elab-addition	elab-addition
P96-1044	1-5	25-34	This paper discusses the problem	In particular , it argues for a representation of language	This paper discusses the problem	In particular , it argues for a representation of language	1-24	25-50	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	In particular , it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters .	1<2	none	elab-aspect	elab-aspect
P96-1044	25-34	35-42	In particular , it argues for a representation of language	in which linguistic parameters like words are built	In particular , it argues for a representation of language	in which linguistic parameters like words are built	25-50	25-50	In particular , it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters .	In particular , it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters .	1<2	none	elab-addition	elab-addition
P96-1044	35-42	43-50	in which linguistic parameters like words are built	by perturbing a composition of existing parameters .	in which linguistic parameters like words are built	by perturbing a composition of existing parameters .	25-50	25-50	In particular , it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters .	In particular , it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters .	1<2	none	manner-means	manner-means
P96-1044	1-5	51-87	This paper discusses the problem	The power of the representation is demonstrated by several examples in text segmentation and compression , acquisition of a lexicon from raw speech , and the acquisition of mappings between text and artificial representations of meaning .	This paper discusses the problem	The power of the representation is demonstrated by several examples in text segmentation and compression , acquisition of a lexicon from raw speech , and the acquisition of mappings between text and artificial representations of meaning .	1-24	51-87	This paper discusses the problem of learning language from unprocessed text and speech signals , concentrating on the problem of learning a lexicon .	The power of the representation is demonstrated by several examples in text segmentation and compression , acquisition of a lexicon from raw speech , and the acquisition of mappings between text and artificial representations of meaning .	1<2	none	evaluation	evaluation
P96-1045	1-10	11-19	Lexicalized Tree Adjoining Grammars have proved useful for NLP .	However , numerous redundancy problems face LTAGs developers ,	Lexicalized Tree Adjoining Grammars have proved useful for NLP .	However , numerous redundancy problems face LTAGs developers ,	1-10	11-29	Lexicalized Tree Adjoining Grammars have proved useful for NLP .	However , numerous redundancy problems face LTAGs developers , as highlighted by Vijay-Shanker and Schabes ( 92 ) .	1>2	none	contrast	contrast
P96-1045	11-19	30-34	However , numerous redundancy problems face LTAGs developers ,	We present and a tool	However , numerous redundancy problems face LTAGs developers ,	We present and a tool	11-29	30-44	However , numerous redundancy problems face LTAGs developers , as highlighted by Vijay-Shanker and Schabes ( 92 ) .	We present and a tool that automatically generates the tree families of an LTAG .	1>2	none	bg-goal	bg-goal
P96-1045	11-19	20-29	However , numerous redundancy problems face LTAGs developers ,	as highlighted by Vijay-Shanker and Schabes ( 92 ) .	However , numerous redundancy problems face LTAGs developers ,	as highlighted by Vijay-Shanker and Schabes ( 92 ) .	11-29	11-29	However , numerous redundancy problems face LTAGs developers , as highlighted by Vijay-Shanker and Schabes ( 92 ) .	However , numerous redundancy problems face LTAGs developers , as highlighted by Vijay-Shanker and Schabes ( 92 ) .	1<2	none	elab-addition	elab-addition
P96-1045	30-34	35-44	We present and a tool	that automatically generates the tree families of an LTAG .	We present and a tool	that automatically generates the tree families of an LTAG .	30-44	30-44	We present and a tool that automatically generates the tree families of an LTAG .	We present and a tool that automatically generates the tree families of an LTAG .	1<2	none	elab-addition	elab-addition
P96-1045	30-34	45-54	We present and a tool	It starts from a compact hierarchical organization of syntactic descriptions	We present and a tool	It starts from a compact hierarchical organization of syntactic descriptions	30-44	45-69	We present and a tool that automatically generates the tree families of an LTAG .	It starts from a compact hierarchical organization of syntactic descriptions that is linguistically motivated and carries out all the relevant combinations of linguistic phenomena .	1<2	none	elab-addition	elab-addition
P96-1045	45-54	55-58	It starts from a compact hierarchical organization of syntactic descriptions	that is linguistically motivated	It starts from a compact hierarchical organization of syntactic descriptions	that is linguistically motivated	45-69	45-69	It starts from a compact hierarchical organization of syntactic descriptions that is linguistically motivated and carries out all the relevant combinations of linguistic phenomena .	It starts from a compact hierarchical organization of syntactic descriptions that is linguistically motivated and carries out all the relevant combinations of linguistic phenomena .	1<2	none	elab-addition	elab-addition
P96-1045	45-54	59-69	It starts from a compact hierarchical organization of syntactic descriptions	and carries out all the relevant combinations of linguistic phenomena .	It starts from a compact hierarchical organization of syntactic descriptions	and carries out all the relevant combinations of linguistic phenomena .	45-69	45-69	It starts from a compact hierarchical organization of syntactic descriptions that is linguistically motivated and carries out all the relevant combinations of linguistic phenomena .	It starts from a compact hierarchical organization of syntactic descriptions that is linguistically motivated and carries out all the relevant combinations of linguistic phenomena .	1<2	none	joint	joint
P96-1046	1-10	11-21	This paper describes a prototype disambiguation module , KANKEI ,	which was tested on two corpora of the TRAINS project .	This paper describes a prototype disambiguation module , KANKEI ,	which was tested on two corpora of the TRAINS project .	1-21	1-21	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	1<2	none	elab-addition	elab-addition
P96-1046	1-10	22-52	This paper describes a prototype disambiguation module , KANKEI ,	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ;	This paper describes a prototype disambiguation module , KANKEI ,	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ;	1-21	22-87	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ; in the first , the correct attachment is to the VP 88.7 % of the time , while in the second , the correct attachment is to the NP 73.5 % of the time .	1<2	none	elab-aspect	elab-aspect
P96-1046	22-52	53-69	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ;	in the first , the correct attachment is to the VP 88.7 % of the time ,	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ;	in the first , the correct attachment is to the VP 88.7 % of the time ,	22-87	22-87	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ; in the first , the correct attachment is to the VP 88.7 % of the time , while in the second , the correct attachment is to the NP 73.5 % of the time .	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ; in the first , the correct attachment is to the VP 88.7 % of the time , while in the second , the correct attachment is to the NP 73.5 % of the time .	1<2	none	elab-addition	elab-addition
P96-1046	53-69	70-87	in the first , the correct attachment is to the VP 88.7 % of the time ,	while in the second , the correct attachment is to the NP 73.5 % of the time .	in the first , the correct attachment is to the VP 88.7 % of the time ,	while in the second , the correct attachment is to the NP 73.5 % of the time .	22-87	22-87	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ; in the first , the correct attachment is to the VP 88.7 % of the time , while in the second , the correct attachment is to the NP 73.5 % of the time .	In ambiguous verb phrases of form V ... NP PP or V ... NP adverb ( s ) , the two corpora have very different PP and adverb attachment patterns ; in the first , the correct attachment is to the VP 88.7 % of the time , while in the second , the correct attachment is to the NP 73.5 % of the time .	1<2	none	comparison	comparison
P96-1046	1-10	88-100	This paper describes a prototype disambiguation module , KANKEI ,	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities ,	This paper describes a prototype disambiguation module , KANKEI ,	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities ,	1-21	88-134	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	1<2	none	elab-addition	elab-addition
P96-1046	88-100	101-111	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities ,	and assigns parse trees ( with these ambiguities ) a score	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities ,	and assigns parse trees ( with these ambiguities ) a score	88-134	88-134	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	1<2	none	joint	joint
P96-1046	101-111	112-119	and assigns parse trees ( with these ambiguities ) a score	based on a linear combination of the frequencies	and assigns parse trees ( with these ambiguities ) a score	based on a linear combination of the frequencies	88-134	88-134	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	1<2	none	bg-general	bg-general
P96-1046	101-111	120-134	and assigns parse trees ( with these ambiguities ) a score	with which these patterns appear with NP and VP attachments in the TRAINS corpora .	and assigns parse trees ( with these ambiguities ) a score	with which these patterns appear with NP and VP attachments in the TRAINS corpora .	88-134	88-134	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	KANKEI uses various n-gram patterns of the phrase heads around these ambiguities , and assigns parse trees ( with these ambiguities ) a score based on a linear combination of the frequencies with which these patterns appear with NP and VP attachments in the TRAINS corpora .	1<2	none	elab-addition	elab-addition
P96-1046	1-10	135-158	This paper describes a prototype disambiguation module , KANKEI ,	Unlike previous statistical disambiguation systems , this technique thus combines evidence from bigrams , trigrams , and the 4-gram around an ambiguous attachment .	This paper describes a prototype disambiguation module , KANKEI ,	Unlike previous statistical disambiguation systems , this technique thus combines evidence from bigrams , trigrams , and the 4-gram around an ambiguous attachment .	1-21	135-158	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	Unlike previous statistical disambiguation systems , this technique thus combines evidence from bigrams , trigrams , and the 4-gram around an ambiguous attachment .	1<2	none	elab-addition	elab-addition
P96-1046	159-169	170-187	In the current experiments , equal weights are used for simplicity	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	In the current experiments , equal weights are used for simplicity	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	159-187	159-187	In the current experiments , equal weights are used for simplicity but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	In the current experiments , equal weights are used for simplicity but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	1>2	none	contrast	contrast
P96-1046	1-10	170-187	This paper describes a prototype disambiguation module , KANKEI ,	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	This paper describes a prototype disambiguation module , KANKEI ,	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	1-21	159-187	This paper describes a prototype disambiguation module , KANKEI , which was tested on two corpora of the TRAINS project .	In the current experiments , equal weights are used for simplicity but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	1<2	none	evaluation	evaluation
P96-1046	188-200	201-217	Despite the large statistical differences in attachment preferences in the two corpora ,	training on the first corpus and testing on the second gives an accuracy of 90.9 % .	Despite the large statistical differences in attachment preferences in the two corpora ,	training on the first corpus and testing on the second gives an accuracy of 90.9 % .	188-217	188-217	Despite the large statistical differences in attachment preferences in the two corpora , training on the first corpus and testing on the second gives an accuracy of 90.9 % .	Despite the large statistical differences in attachment preferences in the two corpora , training on the first corpus and testing on the second gives an accuracy of 90.9 % .	1>2	none	contrast	contrast
P96-1046	170-187	201-217	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	training on the first corpus and testing on the second gives an accuracy of 90.9 % .	but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	training on the first corpus and testing on the second gives an accuracy of 90.9 % .	159-187	188-217	In the current experiments , equal weights are used for simplicity but results are still good on the TRAINS corpora ( 92.2 % and 92.4 % accuracy ) .	Despite the large statistical differences in attachment preferences in the two corpora , training on the first corpus and testing on the second gives an accuracy of 90.9 % .	1<2	none	joint	joint
P96-1047	1-13	14-27	This paper stems from an ongoing research project on verb phrase ellipsis .	The project 's goals are to implement a verb phrase ellipsis resolution algorithm ,	This paper stems from an ongoing research project on verb phrase ellipsis .	The project's goals are to implement a verb phrase ellipsis resolution algorithm ,	1-13	14-44	This paper stems from an ongoing research project on verb phrase ellipsis .	The project 's goals are to implement a verb phrase ellipsis resolution algorithm , automatically test the algorithm on corpus data , then automatically evaluate the algorithm against human-generated answers .	1<2	none	bg-goal	bg-goal
P96-1047	14-27	28-35	The project 's goals are to implement a verb phrase ellipsis resolution algorithm ,	automatically test the algorithm on corpus data ,	The project's goals are to implement a verb phrase ellipsis resolution algorithm ,	automatically test the algorithm on corpus data ,	14-44	14-44	The project 's goals are to implement a verb phrase ellipsis resolution algorithm , automatically test the algorithm on corpus data , then automatically evaluate the algorithm against human-generated answers .	The project 's goals are to implement a verb phrase ellipsis resolution algorithm , automatically test the algorithm on corpus data , then automatically evaluate the algorithm against human-generated answers .	1<2	none	progression	progression
P96-1047	28-35	36-44	automatically test the algorithm on corpus data ,	then automatically evaluate the algorithm against human-generated answers .	automatically test the algorithm on corpus data ,	then automatically evaluate the algorithm against human-generated answers .	14-44	14-44	The project 's goals are to implement a verb phrase ellipsis resolution algorithm , automatically test the algorithm on corpus data , then automatically evaluate the algorithm against human-generated answers .	The project 's goals are to implement a verb phrase ellipsis resolution algorithm , automatically test the algorithm on corpus data , then automatically evaluate the algorithm against human-generated answers .	1<2	none	progression	progression
P96-1047	1-13	45-54	This paper stems from an ongoing research project on verb phrase ellipsis .	The paper will establish the current status of the algorithm	This paper stems from an ongoing research project on verb phrase ellipsis .	The paper will establish the current status of the algorithm	1-13	45-65	This paper stems from an ongoing research project on verb phrase ellipsis .	The paper will establish the current status of the algorithm based on this automatic evaluation , categorizing current problem situations .	1<2	none	elab-addition	elab-addition
P96-1047	45-54	55-60	The paper will establish the current status of the algorithm	based on this automatic evaluation ,	The paper will establish the current status of the algorithm	based on this automatic evaluation ,	45-65	45-65	The paper will establish the current status of the algorithm based on this automatic evaluation , categorizing current problem situations .	The paper will establish the current status of the algorithm based on this automatic evaluation , categorizing current problem situations .	1<2	none	bg-general	bg-general
P96-1047	45-54	61-65	The paper will establish the current status of the algorithm	categorizing current problem situations .	The paper will establish the current status of the algorithm	categorizing current problem situations .	45-65	45-65	The paper will establish the current status of the algorithm based on this automatic evaluation , categorizing current problem situations .	The paper will establish the current status of the algorithm based on this automatic evaluation , categorizing current problem situations .	1<2	none	elab-addition	elab-addition
P96-1047	66-67	68-79	An algorithm	to handle one of these problems , the ease of subdeletion ,	An algorithm	to handle one of these problems , the ease of subdeletion ,	66-85	66-85	An algorithm to handle one of these problems , the ease of subdeletion , will be described and evaluated .	An algorithm to handle one of these problems , the ease of subdeletion , will be described and evaluated .	1<2	none	enablement	enablement
P96-1047	66-67,80-85	86-93	<*> An algorithm <*> will be described and evaluated .	The algorithm attempts to detect and solve subdeletion	An algorithm <*> will be described and evaluated .	The algorithm attempts to detect and solve subdeletion	66-85	86-108	An algorithm to handle one of these problems , the ease of subdeletion , will be described and evaluated .	The algorithm attempts to detect and solve subdeletion by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	1>2	none	result	result
P96-1047	1-13	86-93	This paper stems from an ongoing research project on verb phrase ellipsis .	The algorithm attempts to detect and solve subdeletion	This paper stems from an ongoing research project on verb phrase ellipsis .	The algorithm attempts to detect and solve subdeletion	1-13	86-108	This paper stems from an ongoing research project on verb phrase ellipsis .	The algorithm attempts to detect and solve subdeletion by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	1<2	none	evaluation	evaluation
P96-1047	86-93	94-108	The algorithm attempts to detect and solve subdeletion	by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	The algorithm attempts to detect and solve subdeletion	by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	86-108	86-108	The algorithm attempts to detect and solve subdeletion by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	The algorithm attempts to detect and solve subdeletion by locating adjuncts of similar types in a verb phrase ellipsis and corresponding antecedent .	1<2	none	manner-means	manner-means
P96-1048	1-10	11-15	In this paper , we propose a textual clue approach	to help metaphor detection ,	In this paper , we propose a textual clue approach	to help metaphor detection ,	1-26	1-26	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	1<2	none	enablement	enablement
P96-1048	11-15	16-26	to help metaphor detection ,	in order to improve the semantic processing of this figure .	to help metaphor detection ,	in order to improve the semantic processing of this figure .	1-26	1-26	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	1<2	none	enablement	enablement
P96-1048	1-10	27-38	In this paper , we propose a textual clue approach	The previous works in the domain studied the semantic regularities only ,	In this paper , we propose a textual clue approach	The previous works in the domain studied the semantic regularities only ,	1-26	27-45	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	The previous works in the domain studied the semantic regularities only , overlooking an obvious set of regularities .	1<2	none	bg-compare	bg-compare
P96-1048	27-38	39-45	The previous works in the domain studied the semantic regularities only ,	overlooking an obvious set of regularities .	The previous works in the domain studied the semantic regularities only ,	overlooking an obvious set of regularities .	27-45	27-45	The previous works in the domain studied the semantic regularities only , overlooking an obvious set of regularities .	The previous works in the domain studied the semantic regularities only , overlooking an obvious set of regularities .	1<2	none	elab-addition	elab-addition
P96-1048	1-10	46-54	In this paper , we propose a textual clue approach	A corpus-based analysis shows the existence of surface regularities	In this paper , we propose a textual clue approach	A corpus-based analysis shows the existence of surface regularities	1-26	46-58	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	A corpus-based analysis shows the existence of surface regularities related to metaphors .	1<2	none	elab-aspect	elab-aspect
P96-1048	46-54	55-58	A corpus-based analysis shows the existence of surface regularities	related to metaphors .	A corpus-based analysis shows the existence of surface regularities	related to metaphors .	46-58	46-58	A corpus-based analysis shows the existence of surface regularities related to metaphors .	A corpus-based analysis shows the existence of surface regularities related to metaphors .	1<2	none	elab-addition	elab-addition
P96-1048	46-54	59-70	A corpus-based analysis shows the existence of surface regularities	These clues can be characterized by syntactic structures and lexical markers .	A corpus-based analysis shows the existence of surface regularities	These clues can be characterized by syntactic structures and lexical markers .	46-58	59-70	A corpus-based analysis shows the existence of surface regularities related to metaphors .	These clues can be characterized by syntactic structures and lexical markers .	1<2	none	elab-addition	elab-addition
P96-1048	1-10	71-76	In this paper , we propose a textual clue approach	We present an object oriented model	In this paper , we propose a textual clue approach	We present an object oriented model	1-26	71-85	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	We present an object oriented model for representing the textual clues that were found .	1<2	none	elab-aspect	elab-aspect
P96-1048	71-76	77-81	We present an object oriented model	for representing the textual clues	We present an object oriented model	for representing the textual clues	71-85	71-85	We present an object oriented model for representing the textual clues that were found .	We present an object oriented model for representing the textual clues that were found .	1<2	none	enablement	enablement
P96-1048	77-81	82-85	for representing the textual clues	that were found .	for representing the textual clues	that were found .	71-85	71-85	We present an object oriented model for representing the textual clues that were found .	We present an object oriented model for representing the textual clues that were found .	1<2	none	elab-addition	elab-addition
P96-1048	71-76	86-105	We present an object oriented model	This representation is designed to help the choice of a semantic processing , in terms of possible non-literal meanings .	We present an object oriented model	This representation is designed to help the choice of a semantic processing , in terms of possible non-literal meanings .	71-85	86-105	We present an object oriented model for representing the textual clues that were found .	This representation is designed to help the choice of a semantic processing , in terms of possible non-literal meanings .	1<2	none	elab-addition	elab-addition
P96-1048	106-107	108-110	A prototype	implementing this model	A prototype	implementing this model	106-123	106-123	A prototype implementing this model is currently under development , within an incremental approach allowing step-by-step evaluations .	A prototype implementing this model is currently under development , within an incremental approach allowing step-by-step evaluations .	1<2	none	elab-addition	elab-addition
P96-1048	1-10	106-107,111-119	In this paper , we propose a textual clue approach	<*> A prototype <*> is currently under development , within an incremental approach	In this paper , we propose a textual clue approach	A prototype <*> is currently under development , within an incremental approach	1-26	106-123	In this paper , we propose a textual clue approach to help metaphor detection , in order to improve the semantic processing of this figure .	A prototype implementing this model is currently under development , within an incremental approach allowing step-by-step evaluations .	1<2	none	evaluation	evaluation
P96-1048	106-107,111-119	120-123	<*> A prototype <*> is currently under development , within an incremental approach	allowing step-by-step evaluations .	A prototype <*> is currently under development , within an incremental approach	allowing step-by-step evaluations .	106-123	106-123	A prototype implementing this model is currently under development , within an incremental approach allowing step-by-step evaluations .	A prototype implementing this model is currently under development , within an incremental approach allowing step-by-step evaluations .	1<2	none	elab-addition	elab-addition
P96-1049	1-21	27-40	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena ,	In this work-in-progress , we build on the work of Ellison ( 1994 )	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena ,	In this work-in-progress , we build on the work of Ellison ( 1994 )	1-26	27-59	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena , including infixation and reduplication .	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	1>2	none	bg-general	bg-general
P96-1049	1-21	22-26	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena ,	including infixation and reduplication .	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena ,	including infixation and reduplication .	1-26	1-26	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena , including infixation and reduplication .	Optimality Theory , a constraint-based phonology and morphology paradigm , has allowed linguists to make elegant analyses of many phenomena , including infixation and reduplication .	1<2	none	elab-addition	elab-addition
P96-1049	27-40	41-44	In this work-in-progress , we build on the work of Ellison ( 1994 )	to investigate the possibility	In this work-in-progress , we build on the work of Ellison ( 1994 )	to investigate the possibility	27-59	27-59	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	1<2	none	enablement	enablement
P96-1049	41-44	45-51	to investigate the possibility	of using OT as a parsing tool	to investigate the possibility	of using OT as a parsing tool	27-59	27-59	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	1<2	none	elab-addition	elab-addition
P96-1049	45-51	52-59	of using OT as a parsing tool	that derives underlying forms from surface forms .	of using OT as a parsing tool	that derives underlying forms from surface forms .	27-59	27-59	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	In this work-in-progress , we build on the work of Ellison ( 1994 ) to investigate the possibility of using OT as a parsing tool that derives underlying forms from surface forms .	1<2	none	elab-addition	elab-addition
P96-1050	1-10	64-72	The development of natural language processing ( NLP ) systems	This work develops an approach to multilingual name recognition	The development of natural language processing ( NLP ) systems	This work develops an approach to multilingual name recognition	1-35	64-91	The development of natural language processing ( NLP ) systems that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	1>2	none	bg-goal	bg-goal
P96-1050	1-10	11-35	The development of natural language processing ( NLP ) systems	that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	The development of natural language processing ( NLP ) systems	that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	1-35	1-35	The development of natural language processing ( NLP ) systems that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	The development of natural language processing ( NLP ) systems that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	1<2	none	elab-addition	elab-addition
P96-1050	36-43	44-50	While various name recognizers have been developed ,	they suffer from being too limited ;	While various name recognizers have been developed ,	they suffer from being too limited ;	36-63	36-63	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	1>2	none	temporal	temporal
P96-1050	1-10	44-50	The development of natural language processing ( NLP ) systems	they suffer from being too limited ;	The development of natural language processing ( NLP ) systems	they suffer from being too limited ;	1-35	36-63	The development of natural language processing ( NLP ) systems that perform machine translation ( MT ) and information retrieval ( IR ) has highlighted the need for the automatic recognition of proper names .	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	1<2	none	elab-addition	elab-addition
P96-1050	44-50	51-57	they suffer from being too limited ;	some only recognize one name class ,	they suffer from being too limited ;	some only recognize one name class ,	36-63	36-63	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	1<2	none	elab-enumember	elab-enumember
P96-1050	44-50	58-63	they suffer from being too limited ;	and all are language specific .	they suffer from being too limited ;	and all are language specific .	36-63	36-63	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	While various name recognizers have been developed , they suffer from being too limited ; some only recognize one name class , and all are language specific .	1<2	none	elab-enumember	elab-enumember
P96-1050	64-72	73-80	This work develops an approach to multilingual name recognition	that uses machine learning and a portable framework	This work develops an approach to multilingual name recognition	that uses machine learning and a portable framework	64-91	64-91	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	1<2	none	elab-addition	elab-addition
P96-1050	73-80	81-85	that uses machine learning and a portable framework	to simplify the porting task	that uses machine learning and a portable framework	to simplify the porting task	64-91	64-91	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	1<2	none	enablement	enablement
P96-1050	64-72	86-91	This work develops an approach to multilingual name recognition	by maximizing reuse and automation .	This work develops an approach to multilingual name recognition	by maximizing reuse and automation .	64-91	64-91	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	This work develops an approach to multilingual name recognition that uses machine learning and a portable framework to simplify the porting task by maximizing reuse and automation .	1<2	none	manner-means	manner-means
P96-1051	1-16	17-21	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	The method relies on information	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	The method relies on information	1-16	17-26	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	The method relies on information provided by WordNet 1.5 .	1<2	none	elab-addition	elab-addition
P96-1051	17-21	22-26	The method relies on information	provided by WordNet 1.5 .	The method relies on information	provided by WordNet 1.5 .	17-26	17-26	The method relies on information provided by WordNet 1.5 .	The method relies on information provided by WordNet 1.5 .	1<2	none	elab-addition	elab-addition
P96-1051	1-16	27-38	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	We first classify prepositional attachments according to semantic equivalence of phrase heads	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	We first classify prepositional attachments according to semantic equivalence of phrase heads	1-16	27-51	This paper presents a method for word sense disambiguation and coherence understanding of prepositional relations .	We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures .	1<2	none	elab-aspect	elab-aspect
P96-1051	27-38	39-43	We first classify prepositional attachments according to semantic equivalence of phrase heads	and then apply inferential heuristics	We first classify prepositional attachments according to semantic equivalence of phrase heads	and then apply inferential heuristics	27-51	27-51	We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures .	We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures .	1<2	none	joint	joint
P96-1051	39-43	44-51	and then apply inferential heuristics	for understanding the validity of prepositional structures .	and then apply inferential heuristics	for understanding the validity of prepositional structures .	27-51	27-51	We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures .	We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures .	1<2	none	enablement	enablement
P96-1052	1-13	31-37	Little work has been done in NLP on the subject of punctuation ,	This paper described early work in progress	Little work has been done in NLP on the subject of punctuation ,	This paper described early work in progress	1-30	31-45	Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based .	This paper described early work in progress to try to construct such a theory .	1>2	none	bg-general	bg-general
P96-1052	1-13	14-22	Little work has been done in NLP on the subject of punctuation ,	owing mainly to a lack of a good theory	Little work has been done in NLP on the subject of punctuation ,	owing mainly to a lack of a good theory	1-30	1-30	Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based .	Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based .	1<2	none	elab-addition	elab-addition
P96-1052	14-22	23-30	owing mainly to a lack of a good theory	on which computational treatments could be based .	owing mainly to a lack of a good theory	on which computational treatments could be based .	1-30	1-30	Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based .	Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based .	1<2	none	elab-addition	elab-addition
P96-1052	31-37	38-45	This paper described early work in progress	to try to construct such a theory .	This paper described early work in progress	to try to construct such a theory .	31-45	31-45	This paper described early work in progress to try to construct such a theory .	This paper described early work in progress to try to construct such a theory .	1<2	none	enablement	enablement
P96-1052	31-37	46-47,56-58	This paper described early work in progress	Two approaches <*> are discussed ,	This paper described early work in progress	Two approaches <*> are discussed ,	31-45	46-85	This paper described early work in progress to try to construct such a theory .	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	1<2	none	elab-aspect	elab-aspect
P96-1052	46-47,56-58	48-55	Two approaches <*> are discussed ,	to finding the syntactic function of punctuation marks	Two approaches <*> are discussed ,	to finding the syntactic function of punctuation marks	46-85	46-85	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	1<2	none	enablement	enablement
P96-1052	46-47,56-58	59-62	Two approaches <*> are discussed ,	and procedures are described	Two approaches <*> are discussed ,	and procedures are described	46-85	46-85	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	1<2	none	joint	joint
P96-1052	59-62	63-85	and procedures are described	by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	and procedures are described	by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	46-85	46-85	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work .	1<2	none	manner-means	manner-means
P96-1052	31-37	86-100	This paper described early work in progress	Suggestions are made for the use of these results , and for future work .	This paper described early work in progress	Suggestions are made for the use of these results , and for future work .	31-45	86-100	This paper described early work in progress to try to construct such a theory .	Suggestions are made for the use of these results , and for future work .	1<2	none	elab-aspect	elab-aspect
P96-1053	1-8	9-18	I examine how terminological languages can be used	to manage linguistic data during NL research and development .	I examine how terminological languages can be used	to manage linguistic data during NL research and development .	1-18	1-18	I examine how terminological languages can be used to manage linguistic data during NL research and development .	I examine how terminological languages can be used to manage linguistic data during NL research and development .	1<2	none	enablement	enablement
P96-1053	1-8	19-27	I examine how terminological languages can be used	In particular , I consider the lexical semantics task	I examine how terminological languages can be used	In particular , I consider the lexical semantics task	1-18	19-68	I examine how terminological languages can be used to manage linguistic data during NL research and development .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	joint	joint
P96-1053	19-27	28-32	In particular , I consider the lexical semantics task	of characterizing semantic verb classes	In particular , I consider the lexical semantics task	of characterizing semantic verb classes	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	elab-addition	elab-addition
P96-1053	33-34	35-40	and show	how the language can be extended	and show	how the language can be extended	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1>2	none	attribution	attribution
P96-1053	19-27	35-40	In particular , I consider the lexical semantics task	how the language can be extended	In particular , I consider the lexical semantics task	how the language can be extended	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	progression	progression
P96-1053	35-40	41-48	how the language can be extended	to flag inconsistencies in verb class definitions ,	how the language can be extended	to flag inconsistencies in verb class definitions ,	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	enablement	enablement
P96-1053	41-48	49-56	to flag inconsistencies in verb class definitions ,	identify the need for new verb classes ,	to flag inconsistencies in verb class definitions ,	identify the need for new verb classes ,	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	joint	joint
P96-1053	49-56	57-68	identify the need for new verb classes ,	and identify appropriate linguistic hypotheses for a new verb 's behavior .	identify the need for new verb classes ,	and identify appropriate linguistic hypotheses for a new verb's behavior .	19-68	19-68	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	In particular , I consider the lexical semantics task of characterizing semantic verb classes and show how the language can be extended to flag inconsistencies in verb class definitions , identify the need for new verb classes , and identify appropriate linguistic hypotheses for a new verb 's behavior .	1<2	none	joint	joint
P96-1054	1-6	7-20	This paper describes an on-going study	which applies the concept of transitivity to news discourse for text processing tasks .	This paper describes an on-going study	which applies the concept of transitivity to news discourse for text processing tasks .	1-20	1-20	This paper describes an on-going study which applies the concept of transitivity to news discourse for text processing tasks .	This paper describes an on-going study which applies the concept of transitivity to news discourse for text processing tasks .	1<2	none	elab-addition	elab-addition
P96-1054	1-6	21-27	This paper describes an on-going study	The complex notion of transitivity is defined	This paper describes an on-going study	The complex notion of transitivity is defined	1-20	21-38	This paper describes an on-going study which applies the concept of transitivity to news discourse for text processing tasks .	The complex notion of transitivity is defined and the relationship between transitivity and information foregrounding is explained .	1<2	none	elab-aspect	elab-aspect
P96-1054	21-27	28-38	The complex notion of transitivity is defined	and the relationship between transitivity and information foregrounding is explained .	The complex notion of transitivity is defined	and the relationship between transitivity and information foregrounding is explained .	21-38	21-38	The complex notion of transitivity is defined and the relationship between transitivity and information foregrounding is explained .	The complex notion of transitivity is defined and the relationship between transitivity and information foregrounding is explained .	1<2	none	joint	joint
P96-1054	1-6	39-50	This paper describes an on-going study	A sample corpus of news articles has been coded for transitivity .	This paper describes an on-going study	A sample corpus of news articles has been coded for transitivity .	1-20	39-50	This paper describes an on-going study which applies the concept of transitivity to news discourse for text processing tasks .	A sample corpus of news articles has been coded for transitivity .	1<2	none	evaluation	evaluation
P96-1054	39-50	51-61	A sample corpus of news articles has been coded for transitivity .	The corpus is being used in two text processing experiments .	A sample corpus of news articles has been coded for transitivity .	The corpus is being used in two text processing experiments .	39-50	51-61	A sample corpus of news articles has been coded for transitivity .	The corpus is being used in two text processing experiments .	1<2	none	elab-addition	elab-addition
P96-1055	1-5	6-11	We use a statistical method	to select the most probable structure	We use a statistical method	to select the most probable structure	1-18	1-18	We use a statistical method to select the most probable structure or parse for a given sentence .	We use a statistical method to select the most probable structure or parse for a given sentence .	1<2	none	enablement	enablement
P96-1055	6-11	12-18	to select the most probable structure	or parse for a given sentence .	to select the most probable structure	or parse for a given sentence .	1-18	1-18	We use a statistical method to select the most probable structure or parse for a given sentence .	We use a statistical method to select the most probable structure or parse for a given sentence .	1<2	none	joint	joint
P96-1055	1-5	19-34	We use a statistical method	It takes as input the dependency structures generated for the sentence by a dependency grammar ,	We use a statistical method	It takes as input the dependency structures generated for the sentence by a dependency grammar ,	1-18	19-70	We use a statistical method to select the most probable structure or parse for a given sentence .	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	1<2	none	elab-aspect	elab-aspect
P96-1055	19-34	35-45	It takes as input the dependency structures generated for the sentence by a dependency grammar ,	finds all triple of modifier , particle and modificant relations ,	It takes as input the dependency structures generated for the sentence by a dependency grammar ,	finds all triple of modifier , particle and modificant relations ,	19-70	19-70	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	1<2	none	progression	progression
P96-1055	35-45	46-51	finds all triple of modifier , particle and modificant relations ,	calculates mutual information of each relation	finds all triple of modifier , particle and modificant relations ,	calculates mutual information of each relation	19-70	19-70	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	1<2	none	progression	progression
P96-1055	46-51	52-55	calculates mutual information of each relation	and chooses the structure	calculates mutual information of each relation	and chooses the structure	19-70	19-70	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	1<2	none	progression	progression
P96-1055	52-55	56-70	and chooses the structure	for which the product of the mutual information of its relations is the highest .	and chooses the structure	for which the product of the mutual information of its relations is the highest .	19-70	19-70	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	It takes as input the dependency structures generated for the sentence by a dependency grammar , finds all triple of modifier , particle and modificant relations , calculates mutual information of each relation and chooses the structure for which the product of the mutual information of its relations is the highest .	1<2	none	elab-addition	elab-addition
P96-1056	22-30	39-52	The algorithm presented handles modifications to the input grammar	In this paper , a lazy generation of LRtype parsers for TALs is defined	The algorithm presented handles modifications to the input grammar	In this paper , a lazy generation of LRtype parsers for TALs is defined	22-38	39-63	The algorithm presented handles modifications to the input grammar by updating the parser generated so far .	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	1>2	none	bg-compare	bg-compare
P96-1056	22-30	31-34	The algorithm presented handles modifications to the input grammar	by updating the parser	The algorithm presented handles modifications to the input grammar	by updating the parser	22-38	22-38	The algorithm presented handles modifications to the input grammar by updating the parser generated so far .	The algorithm presented handles modifications to the input grammar by updating the parser generated so far .	1<2	none	manner-means	manner-means
P96-1056	31-34	35-38	by updating the parser	generated so far .	by updating the parser	generated so far .	22-38	22-38	The algorithm presented handles modifications to the input grammar by updating the parser generated so far .	The algorithm presented handles modifications to the input grammar by updating the parser generated so far .	1<2	none	elab-addition	elab-addition
P96-1056	1-21	39-52	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	In this paper , a lazy generation of LRtype parsers for TALs is defined	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	In this paper , a lazy generation of LRtype parsers for TALs is defined	1-21	39-63	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	1<2	none	elab-addition	elab-addition
P96-1056	39-52	53-60	In this paper , a lazy generation of LRtype parsers for TALs is defined	in which parse tables are created by need	In this paper , a lazy generation of LRtype parsers for TALs is defined	in which parse tables are created by need	39-63	39-63	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	1<2	none	elab-addition	elab-addition
P96-1056	53-60	61-63	in which parse tables are created by need	while parsing .	in which parse tables are created by need	while parsing .	39-63	39-63	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	In this paper , a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing .	1<2	none	temporal	temporal
P96-1056	1-21	64-72	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	We then describe an incremental parser generator for TALs	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	We then describe an incremental parser generator for TALs	1-21	64-88	This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages ( TALs ) .	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	1<2	none	elab-aspect	elab-aspect
P96-1056	64-72	73-80	We then describe an incremental parser generator for TALs	which responds to modification of the input grammar	We then describe an incremental parser generator for TALs	which responds to modification of the input grammar	64-88	64-88	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	1<2	none	elab-addition	elab-addition
P96-1056	73-80	81-84	which responds to modification of the input grammar	by updating parse tables	which responds to modification of the input grammar	by updating parse tables	64-88	64-88	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	1<2	none	manner-means	manner-means
P96-1056	81-84	85-88	by updating parse tables	built so far .	by updating parse tables	built so far .	64-88	64-88	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far .	1<2	none	elab-addition	elab-addition
P96-1057	12-13	14-19	and specify	how to handle complex sentences .	and specify	how to handle complex sentences .	1-19	1-19	We extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences .	We extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences .	1>2	none	attribution	attribution
P96-1057	1-11	14-19	We extend the centering model for the resolution of intia-sentential anaphora	how to handle complex sentences .	We extend the centering model for the resolution of intia-sentential anaphora	how to handle complex sentences .	1-19	1-19	We extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences .	We extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences .	1<2	none	joint	joint
P96-1057	23	20-22,24-38	indicates	<*> An empirical evaluation <*> that the functional information structure guides the search for an antecedent within the sentence .	indicates	An empirical evaluation <*> that the functional information structure guides the search for an antecedent within the sentence .	20-38	20-38	An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence .	An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence .	1>2	none	attribution	attribution
P96-1057	1-11	20-22,24-38	We extend the centering model for the resolution of intia-sentential anaphora	<*> An empirical evaluation <*> that the functional information structure guides the search for an antecedent within the sentence .	We extend the centering model for the resolution of intia-sentential anaphora	An empirical evaluation <*> that the functional information structure guides the search for an antecedent within the sentence .	1-19	20-38	We extend the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences .	An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence .	1<2	none	evaluation	evaluation
P96-1058	1-19	20-31	A left-corner parsing algorithm with topdown filtering has been reported to show very efficient performance for unificationbased systems .	However , due to the nontermination of parsing with left-recursive grammars ,	A left-corner parsing algorithm with topdown filtering has been reported to show very efficient performance for unificationbased systems .	However , due to the nontermination of parsing with left-recursive grammars ,	1-19	20-37	A left-corner parsing algorithm with topdown filtering has been reported to show very efficient performance for unificationbased systems .	However , due to the nontermination of parsing with left-recursive grammars , top-down constraints must be weakened .	1>2	none	contrast	contrast
P96-1058	20-31	32-37	However , due to the nontermination of parsing with left-recursive grammars ,	top-down constraints must be weakened .	However , due to the nontermination of parsing with left-recursive grammars ,	top-down constraints must be weakened .	20-37	20-37	However , due to the nontermination of parsing with left-recursive grammars , top-down constraints must be weakened .	However , due to the nontermination of parsing with left-recursive grammars , top-down constraints must be weakened .	1>2	none	exp-reason	exp-reason
P96-1058	32-37	38-44,49-51	top-down constraints must be weakened .	In this paper , a general method <*> is proposed .	top-down constraints must be weakened .	In this paper , a general method <*> is proposed .	20-37	38-51	However , due to the nontermination of parsing with left-recursive grammars , top-down constraints must be weakened .	In this paper , a general method of maximizing top-down constraints is proposed .	1>2	none	bg-goal	bg-goal
P96-1058	38-44,49-51	45-48	In this paper , a general method <*> is proposed .	of maximizing top-down constraints	In this paper , a general method <*> is proposed .	of maximizing top-down constraints	38-51	38-51	In this paper , a general method of maximizing top-down constraints is proposed .	In this paper , a general method of maximizing top-down constraints is proposed .	1<2	none	elab-addition	elab-addition
P96-1058	38-44,49-51	52-56	In this paper , a general method <*> is proposed .	The method provides a procedure	In this paper , a general method <*> is proposed .	The method provides a procedure	38-51	52-83	In this paper , a general method of maximizing top-down constraints is proposed .	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	1<2	none	elab-addition	elab-addition
P96-1058	52-56	57-66	The method provides a procedure	to dynamically compute restrictor , a minimum set of features	The method provides a procedure	to dynamically compute restrictor , a minimum set of features	52-83	52-83	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	1<2	none	enablement	enablement
P96-1058	57-66	67-76	to dynamically compute restrictor , a minimum set of features	involved in an infinite loop for every propagation path ;	to dynamically compute restrictor , a minimum set of features	involved in an infinite loop for every propagation path ;	52-83	52-83	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	1<2	none	elab-addition	elab-addition
P96-1058	52-56	77-83	The method provides a procedure	thus top-down constraints are maximally propagated .	The method provides a procedure	thus top-down constraints are maximally propagated .	52-83	52-83	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	The method provides a procedure to dynamically compute restrictor , a minimum set of features involved in an infinite loop for every propagation path ; thus top-down constraints are maximally propagated .	1<2	none	cause	cause
